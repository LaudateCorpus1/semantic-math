\chapter*{NOTATION AND TERMINOLOGY}

\section*{Some Algebraic Objects}\label{algebraic_objects}

Let $S$ be a nonempty set.  Consider the following axioms:

 \begin{enumerate}
  \item $+\colon S \times S \sto S$. \qquad ($\,+\,$ is a \emph{binary operation,} called
 \index{binary operation}%
 \index{operation!binary}%
\emph{addition}, on~$S$)
  \item $(x + y) + z = x + (y + z)$ for all $x$, $y$, $z \in S$. \qquad
 \index{associativity}%
(\emph{associativity} of addition)
  \item There exists $\vc 0_S \in S$ such that $x + \vc 0_S = \vc 0_S + x = x$ for
 \index{additive!identity}%
 \index{identity!additive}%
all $x \in S$. \qquad (existence of an \emph{additive identity})
  \item For every $x \in S$ there exists $-x \in S$ such that $x + (-x) = (-x) + x
 \index{additive!inverse}%
 \index{inverse!additive}%
= \vc 0_S$. \qquad (existence of \emph{additive inverses})
  \item $x + y = y + x$ for all $x$, $y \in S$.  \qquad (\emph{commutativity}
 \index{commutativity}%
of addition)
  \item $\cdot\colon S \times S \sto S\colon (x,y) \mapsto x\cdot y$. \qquad (the map
$(x,y) \mapsto x\cdot y$ is a \emph{binary operation}, called \emph{multiplication},
on~$S$. \emph{Convention:} We will usually write $xy$ instead of $x\cdot y$.)
 \index{conventions!for multiplication write $xy$ for $x \cdot y$}%
  \item $(xy)z = x(yz)$ for all $x$, $y$, $z \in S$. \qquad (\emph{associativity} of
multiplication)
  \item\label{distributive_laws} $(x + y)z = xz + yz$ and $x(y + z) = xy + xz$ for all
$x$, $y$, $z \in S$.
 \index{distributivity}%
\qquad (\emph{multiplication distributes over addition})
  \item There exists $\vc 1_S$ in $S$ such that $x\,\vc 1_S = \vc 1_S\,x = x$ for
 \index{multiplicative!identity}%
 \index{identity!multiplicative}%
 \index{unit}%
all $x \in S$. \qquad (existence of a \emph{multiplicative identity} or {unit})
  \item $\vc 1_S \ne \vc 0_S$.
  \item For every $x \in S$ such that $x \ne \vc 0_S$ there exists $x^{-1} \in S$
 \index{multiplicative!inverse}%
 \index{inverse!multiplicative}%
such that $xx^{-1} = x^{-1}x = \vc 1_S$. \qquad (existence of \emph{multiplicative
inverses})
  \item $xy = yx$ for all $x$, $y \in S$.  \qquad (\emph{commutativity} of multiplication)
 \end{enumerate}

\subsection*{Definitions}
 \begin{itemize}
  \item $(S,+)$ is a \df{semigroup} if it satisfies axioms (1)--(2).
 \index{semigroup}%
  \item $(S,+)$ is a \df{monoid} if it satisfies axioms (1)--(3).
 \index{monoid}%
  \item $(S,+)$ is a \df{group} if it satisfies axioms (1)--(4).
 \index{group}%
  \item $(S,+)$ is an \df{Abelian group} if it satisfies axioms (1)--(5).
 \index{group!Abelian}%
 \index{Abelian group}%
  \item $(S,+,m)$ is a \df{ring} if it satisfies axioms (1)--(8).
 \index{ring}%
  \item $(S,+,m)$ is a \df{commutative ring} if it satisfies axioms (1)--(8) and~(12).
 \index{ring!commutative}%
 \index{commutative!ring}%
  \item $(S,+,m)$ is a \df{unital ring} (or \df{ring with identity}, or \df{unitary ring})
if it satisfies axioms (1)--(9).
 \index{unital!ring}%
 \index{ring!with identity}%
  \item $(S,+,m)$ is a \df{division ring} (or \df{skew field}) if it satisfies
axioms (1)--(11).
 \index{ring!division}%
 \index{division!ring}%
  \item $(S,+,m)$ is a \df{field} if it satisfies axioms (1)--(12).
 \index{field}%
 \end{itemize}

\subsection*{Remarks}
 \begin{itemize}
  \item A binary operation is often written additively, $(x,y) \mapsto x + y$, if it is
commutative and multiplicatively, $(x,y) \mapsto xy$, if it is not.  This is by no means
always the case: in a commutative ring (the real numbers or the complex numbers, for
example), both addition and multiplication are commutative.
  \item When no confusion is likely to result we often write $\vc 0$ for $\vc 0_S$ and
$\vc 1$ for~$\vc 1_S$.
  \item Many authors require a \emph{ring} to satisfy axioms (1)--(9).
  \item It is easy to see that axiom (10) holds in any unital ring except the
\emph{trivial ring} $S~=~\{\vc 0\}$.
 \index{conventions!all unital rings are nontrivial}%
\emph{Convention:} Unless the contrary is stated we will assume that every unital ring is
nontrivial.
 \end{itemize}









 \section*{Notation for Sets of Numbers}\label{C0009}
Here is a list of fairly standard notations for some sets of numbers which occur
frequently in these notes:
 \index{real!line!special subsets of}%
 \index{numbers!special sets of}%
 \index{C@$\C$ (set of complex numbers)}%
 \index{R@$\R$ (set of real numbers)}%
 \index{R@$\R^n$ (set of $n$-tuples of real numbers)}%
 \index{Q@$\Q$ (set of rational numbers)}%
 \index{Z@$\Z$ (set of integers)}%
 \index{N@$\N$ (set of natural numbers)}%
 \index{N@$\N_n$ (first $n$ natural numbers)}%
% \index{D@$\Di$ (open unit disk)}%
 \index{S@$\Sp^1$ (unit circle)}%
\begin{align*}
 &\C \text{ is the set of complex numbers} \\
 &\R \text{ is the set of real numbers} \\
 &\R^n \text{ is the set of all $n$-tuples $(r_1,r_2,\dots,r_n)$ of real numbers} \\
 &\R^+ = \{x \in \R\colon x \ge 0\}, \text{ the positive real numbers} \\
 &\Q \text{ is the set of rational numbers} \\
 &\Q^+ = \{x \in \Q\colon x \ge 0\}, \text{ the positive rational numbers} \\
 &\Z \text{ is the set of integers} \\
 &\Z^+ = \{0,1,2,\dots\}, \text{ the positive integers} \\
 &\N = \{1,2,3,\dots\}\text{, the set of natural numbers} \\
 &\N_n = \{1,2,3,\dots,n\}\text{ the first $n$ natural numbers} \\
 &[a,b] = \{x \in \R\colon a \le x \le b\} \\
 &[a,b) = \{x \in \R\colon a \le x < b\} \\
 &(a,b] = \{x \in \R\colon a < x \le b\} \\
 &(a,b) = \{x \in \R\colon a < x < b\} \\
 &[a,\infty) = \{x \in \R\colon a \le x\} \\
 &(a,\infty) = \{x \in \R\colon a < x\} \\
 &(-\infty,b] = \{x \in \R\colon x \le b\} \\
 &(-\infty,b) = \{x \in \R\colon  x < b\} \\
\index{open!unit disc}%
% &\Di = \{(x,y) \in \R^2\colon x^2 + y^2 < 1\},\text{ the open unit disc} \\
 &\Sp^1 = \{(x,y) \in \R^2\colon x^2 + y^2 = 1\},\text{ the unit circle} \\
\end{align*}



\vfill\eject



\section*{Greek Letters}
 \index{Greek letters}%
\[ \table{}
   Upper case  !  Lower case                   !  English name (approximate pronunciation)  \rr
   A           !  $\alpha$                     !  Alpha (AL-fuh)                            \r
   B           !  $\beta$                      !  Beta (BAY-tuh)                            \r
   $\Gamma$    !  $\gamma$                     !  Gamma (GAM-uh)                            \r
   $\Delta$    !  $\delta$                     !  Delta (DEL-tuh)                           \r
   $E$         !  $\epsilon$ or $\varepsilon$  !  Epsilon (EPP-suh-lon)                     \r
   Z           !  $\zeta$                      !  Zeta (ZAY-tuh)                            \r
   H           !  $\eta$                       !  Eta  (AY-tuh)                             \r
   $\Theta$    !  $\theta$                     !  Theta (THAY-tuh)                          \r
   I           !  $\iota$                       !  Iota  (eye-OH-tuh)                        \r
   K           !  $\kappa$                     !  Kappa  (KAP-uh)                           \r
   $\Lambda$   !  $\lambda$                    !  Lambda  (LAM-duh)                         \r
   M           !  $\mu$                        !  Mu  (MYOO)                                \r
   N           !  $\nu$                        !  Nu  (NOO)                                 \r
   $\Xi$       !  $\xi$                        !  Xi  (KSEE)                                \r
   O           !  o                            !  Omicron  (OHM-ih-kron)                    \r
   $\Pi$       !  $\pi$                        !  Pi  (PIE)                                 \r
   P           !  $\rho$                       !  Rho  (ROH)                                \r
   $\Sigma$    !  $\sigma$                     !  Sigma  (SIG-muh)                          \r
   T           !  $\tau$                       !  Tau  (TAU)                                \r
   Y           !  $\upsilon$                   !  Upsilon  (OOP-suh-lon)                    \r
   $\Phi$      !  $\phi$                       !  Phi  (FEE or FAHY)                        \r
   X           !  $\chi$                       !  Chi  (KHAY)                               \r
   $\Psi$      !  $\psi$                       !  Psi  (PSEE or PSAHY)                      \r
   $\Omega$    !  $\omega$                     !  Omega  (oh-MAY-guh)                    \hfil
  \caption{} \]


\vfill\eject


\section*{Fraktur Fonts}
In these notes Fraktur fonts are used (most often for families of sets and families of linear maps).  Below are the Roman equivalents for each letter.
When writing longhand or presenting material on a blackboard it is usually best to substitute script English letters.
 \index{Fraktur fonts}

\[ \table{}
                Fraktur  !  Fraktur           !   Roman           \r
              Upper case !  Lower case        !  Lower Case       \rr
         $\mathfrak A$   !  $\mathfrak a$     !     a             \r
         $\mathfrak B$   !  $\mathfrak b$     !     b             \r
         $\mathfrak C$   !  $\mathfrak c$     !     c             \r
         $\mathfrak D$   !  $\mathfrak d$     !     d             \r
         $\mathfrak E$   !  $\mathfrak e$     !     e             \r
         $\mathfrak F$   !  $\mathfrak f$     !     f             \r
         $\mathfrak G$   !  $\mathfrak g$     !     g             \r
         $\mathfrak H$   !  $\mathfrak h$     !     h             \r
         $\mathfrak I$   !  $\mathfrak i$     !     i             \r
         $\mathfrak J$   !  $\mathfrak j$     !     j             \r
         $\mathfrak K$   !  $\mathfrak k$     !     k             \r
         $\mathfrak L$   !  $\mathfrak l$     !     l             \r
         $\mathfrak M$   !  $\mathfrak m$     !     m             \r
         $\mathfrak N$   !  $\mathfrak n$     !     n             \r
         $\mathfrak O$   !  $\mathfrak o$     !     o             \r
         $\mathfrak P$   !  $\mathfrak p$     !     p             \r
         $\mathfrak Q$   !  $\mathfrak q$     !     q             \r
         $\mathfrak R$   !  $\mathfrak r$     !     r             \r
         $\mathfrak S$   !  $\mathfrak s$     !     s             \r
         $\mathfrak T$   !  $\mathfrak t$     !     t             \r
         $\mathfrak U$   !  $\mathfrak u$     !     u             \r
         $\mathfrak V$   !  $\mathfrak v$     !     v             \r
         $\mathfrak W$   !  $\mathfrak w$     !     w             \r
         $\mathfrak X$   !  $\mathfrak x$     !     x             \r
         $\mathfrak Y$   !  $\mathfrak y$     !     y             \r
         $\mathfrak Z$   !  $\mathfrak z$     !     z             \r
 \caption{}  \]



\endinput
\chapter{CLIFFORD ALGEBRAS}

In this last chapter we will barely scratch the surface of the fascinating subject of Clifford algebras. For a more substantial
introduction there are now, fortunately, many sources available.  Among my favorites are  \cite{AblamowiczS:2004}, \cite{Denker:2006},
\cite{DoranL:2003}, \cite{Garling:2011}, \cite{HestenesS:1984}, \cite{Lounesto:2001}, \cite{LundholmS:2009},and \cite{Snygg:2012}.


\section{Quadratic Forms}

\begin{defn} A bilinear form $B$ on a real vector space $V$ is
 \index{symmetric!bilinear form}%
 \index{bilinear!form!symmetric}%
\df{symmetric} if $B(u,v) = B(v,u)$ for all $u$, $v \in V$.
\end{defn}

\begin{defn} Let $V$ be a real vector space. A function $Q\colon V \sto \R$ is a
 \index{quadratic form}%
 \index{form!quadratic}%
\df{quadratic form} if
 \begin{enumerate}
  \item[(i)] $Q(v) = Q(-v)$ for all $v$, and
  \item[(ii)] the map $B\colon V \times V \sto \R \colon (u,v) \mapsto Q(u+v) - Q(u) - Q(v)$ is a bilinear form.
 \end{enumerate}
In this case $B$ is the bilinear form
 \index{bilinear!form!associated with a quadratic form}%
 \index{quadratic form!associated with a bilinear form}%
\df{associated with} the quadratic form~$Q$. It is obviously symmetric. \emph{Note:} In many texts $B(u,v)$ is defined to be $\frac12[Q(u+v) - Q(u) - Q(v)]$.
\end{defn}

\begin{prop} Let $Q$ be a quadratic form on a real vector space. Then $Q(0) = 0$.
\end{prop}

\begin{exam} Let $B$ be a symmetric bilinear form on a real vector space~$V$. If we define $Q\colon V \sto \R$ by $Q(v) = B(v,v)$, then
$Q$ is a quadratic form on~$V$.
\end{exam}

\begin{prop}\label{clalg003} If $Q$ is a quadratic form on a real vector space $V$, then
   \[ Q(u + v + w) - Q(u + v) - Q(u + w) - Q(v + w) + Q(u) + Q(v) + Q(w) = 0 \]
for all $u$, $v$, $w \in V$.
\end{prop}

\begin{cor} If $Q$ is a quadratic form on a real vector space $V$, then $Q(2v) = 4Q(v)$ for every $v \in V$.
\end{cor}

\begin{proof}[\emph{Hint for proof}]  Look at $Q(v + v - v)$.
\ns \end{proof}

\begin{prop} If $Q$ is a quadratic form on a real vector space $V$, then $Q(\alpha v) = \alpha^2 Q(v)$ for all $\alpha \in \R$ and $v \in V$.
\end{prop}






















\section{Definition of Clifford Algebra}

\begin{defn}  Let $V$ be a real vector space.  A pair $(U,\iota)$, where $U$ is a real unital algebra and $\iota\colon V \sto U$ is a linear map, is
 \index{universal}%
\df{universal over~$V$} if for every real unital algebra $A$ and every linear map $f \colon V \sto A$ there exists a unique unital algebra homomorphism
$\wt f\colon U \sto A$ such that $\wt f \circ \iota = f$.
\end{defn}

\begin{prop} Let $V$ be a real vector space. If $U$ and $U'$ are unital algebras universal over~$V$, then they are isomorphic.
\end{prop}

\begin{exam} For every real vector space $V$ there is a unital algebra which is universal over~$V$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] See \ref{grassmannalg021def}.  \ns
\end{proof}

\begin{defn}\label{clalg008def} Let $V$ be a real vector space with a quadratic form~$Q$ and $A$ be a real unital algebra. A map
$f \colon V \sto A$ is a
 \index{Clifford!map}%
\df{Clifford map} if
 \begin{enumerate}
  \item[(i)] $f$ is linear, and
  \item[(ii)] $\bigl(f(v)\bigr)^2 = Q(v)\vc 1_A$ for every $v \in V$.
 \end{enumerate}
\end{defn}

\begin{prop} Condition (ii) in definition~\ref{clalg008def} is equivalent to
 \begin{enumerate}
   \item[(ii\,${}'$)] $f(u)f(v) + f(v)f(u) = B(u,v)\vc 1_A$ for all $u$, $v \in V$,
 \end{enumerate}
where $B$ is the bilinear form associated with~$Q$.
\end{prop}

\begin{defn}  Let $V$ be a real vector space with a quadratic form~$Q$. The
 \index{Clifford!algebra}%
 \index{algebra!Clifford}%
\df{Clifford algebra over~$V$} is a
 \index{clvq@$\cl(V,Q)$ (Clifford algebra)}%
real unital algebra $\cl(V,Q)$, together with a Clifford map $j\colon V \sto \cl(V,Q)$, which satisfies the following \emph{universal condition:} for every
real unital algebra $A$ and every Clifford map $f \colon V \sto A$, there exists a unique unital algebra homomorphism $\wh f\colon \cl(V,Q) \sto A$ such
that $\wh f \circ j = f$.
\end{defn}

\begin{prop} Let $V$ be a real finite dimensional vector space with a quadratic form~$Q$. If the Clifford algebra $\cl(V,Q)$ exists, then it is unique up to
isomorphism.
\end{prop}

\begin{exam} For every real finite dimensional vector space $V$ with a quadratic form~$Q$ the Clifford algebra $\cl(V,Q)$ exists.
\end{exam}

\begin{proof}[\emph{Hint for proof}]  Try $\ten(V)/J$ where $J$ is the ideal in $\ten(V)$ generated by elements of the form
$v \otimes v - Q(v)\sbsb{\vc 1}{\ten(V)}$ where $v \in V$.   \ns
\end{proof}



























\section{Orthogonality with Respect to Bilinear Forms}

\begin{defn} Let $B$ be a symmetric bilinear form on a real vector space~$V$. Vectors $v$ and $w$ in $V$ are
 \index{orthogonal!with respect to a bilinear form}%
\df{orthogonal}, in which case we write $v \perp w$, if $B(v,w) = 0$. The
 \index{kernel!of a bilinear form}%
\df{kernel} of $B$ is the set of all $k \in V$ such that $k \perp v$ for every $v \in V$.  The bilinear form is
 \index{nondegenerate bilinear form}%
 \index{bilinear!form!nondegenerate}%
\df{nondegenerate} if its kernel is~$\{\vc 0\}$.
\end{defn}

\begin{exer} One often sees the claim that ``the classification of Clifford algebras amounts to classifying vector spaces with quadratic forms''.  Explain
precisely what is meant by this assertion.
\end{exer}

\begin{prop} Let $B$ be a symmetric bilinear form on a real finite dimensional vector space~$V$. Suppose that $V$ is an orthogonal direct sum
$V_1 \oplus \dots \oplus V_n$ of subspaces. Then $B$ is nondegenerate if and only if the restriction of $B$ to $V_k$ is nondegenerate for each~$k$.
In fact, if we denote by $B_j$ the restriction of $B$ to $V_j$, then $\ker B = \ker B_1 \oplus \dots \oplus \ker B_n$.
\end{prop}

\begin{prop} Let $Q$ be a quadratic form on a real finite dimensional vector space $V$ and let $\{e_1, \dots, e_n\}$ be a basis for~$V$ which is
orthogonal with respect to the bilinear form $B$ associated with~$Q$. If $Q(e_k)$ is nonzero for $1 \le k \le p$ and $Q(e_k) = 0$ for $p < k \le n$,
then the kernel of $B$ is the span of $\{e_{p+1}, \dots, e_n\}$.
\end{prop}

\begin{prop} Let $Q$ be a quadratic form on a real finite dimensional vector space $V$.  If $\dim V = n$, then $\dim \cl(V,Q) = 2^n$.
\end{prop}

\begin{exer} Let $V$ be a real finite dimensional vector space and let $Q$ be the quadratic form which is identically zero on~$V$.  Identify $\cl(V,Q)$.
\end{exer}
















\section{Examples of Clifford Algebras}

\begin{defn} Let $V$ be a finite dimensional real vector space and $B$ be a symmetric bilinear form on~$V$.  An ordered basis $E = (e^1, \dots, e^n)$ for $V$ is
 \index{borthonormal@$B$-orthonormal}%
 \index{orthonormal, $B$-}%
\df{$B$-orthonormal} if
 \begin{enumerate}
    \item[(a)] $B(e^i,e^j) = 0$ whenever $i \ne j$ and
    \item[(b)] for each $i \in \N_n$ the number $B(e^i,e^i)$ is $-1$ or $+1$ or~$0$.
 \end{enumerate}
\end{defn}

\begin{thm} If $V$ is a finite dimensional real vector space and $B$ is a symmetric bilinear form on~$V$, then $V$ has a $B$-orthonormal basis.
\end{thm}

\begin{proof} See \cite{Brown:1988}, chapter 1, theorem 7.6. \ns
\end{proof}

\begin{conv} Let $V$ be a finite dimensional real vector space, let $B$ be a symmetric bilinear form on~$V$, and let $Q$ be the quadratic form associated with~$B$.
 \index{conventions!on ordering $B$-orthonormal bases}%
Let us agree that whenever $E = (e^1, \dots, e^n)$ is an ordered $B$-orthonormal basis for $V$, we order the basis elements in such a way that for some positive
integers $p$ and $q$
   \[ Q(e^i) = \left\{%
      \begin{array}{ll}
          1, & \hbox{if $1 \le i \le p$;} \\
         -1, & \hbox{if $p+1 \le i \le p+q$;} \\
          0, & \hbox{ if $p+q+1 \le i \le n$.} \\
      \end{array}%
  \right. \]
\end{conv}

\begin{thm}\label{clalg031thm} Let $V$ be a finite dimensional real vector space, let $B$ be a symmetric bilinear form on~$V$, and let $Q$ be the quadratic form
associated with~$B$. Then there exist $p$, $q \in \Z^+$ such that if $E = (e^1, \dots, e^n)$ is a $B$-orthonormal basis for $V$ and $v = \sum v_k e^k$, then
   \[ Q(v) = \sum_{k=1}^p {v_k}^2 - \sum_{k=p+1}^{p+q} {v_k}^2\,. \]
\end{thm}

\begin{proof} See \cite{Brown:1988}, chapter 1, theorem 7.11.  \ns
\end{proof}

\begin{notn} If $(V,Q)$ is a finite dimensional real vector space $V$ with a nondegenerate quadratic form $Q$, we often denote the Clifford algebra $\cl(V,Q)$ by
$\cl(p,q)$ where $p$ and $q$ are as in theorem~\ref{clalg031thm}.
\end{notn}

\begin{prop} Let $f\colon (V,Q) \sto (W,R)$ be a linear map between finite dimensional real vector spaces with quadratic forms. If $R(f(v)) = Q(v)$ for every
$v \in V$ we say that $f$ is an
 \index{isometry}%
\df{isometry}.
 \begin{enumerate}
   \item[(a)] If $f$ is such a linear isometry, then there exists a unique unital algebra homomorphism $\cl(f)\colon \cl(V,Q) \sto \cl(W,R)$ such that
$\cl(f)(v) = f(v)$ for every $v~\in~V$.
   \item[(b)] The pair of mappings $\cl{}$ described above is a covariant functor from the category of vector spaces with quadratic forms and linear
isometries to the category of Clifford algebras and unital algebra homomorphisms.
   \item[(c)] If a linear isometry $f$ is an isomorphism, then $\cl(f)$ is an algebra isomorphism.
 \end{enumerate}
\end{prop}

\begin{prop} Let $V$ be a real finite dimensional vector space, $Q$ be a quadratic form on~$V$, and $A = \cl(V,Q)$ be the associated Clifford algebra.
 \begin{enumerate}
  \item[(a)] The map $f\colon V \sto V\colon v \mapsto -v$ is a linear isometry.
  \item[(b)] If $\omega = \cl(f)$, then $\omega^2 = \id{}$.
  \item[(c)] Let $A_0 = \{a \in A\colon \omega(a) = a\}$ and $A_1 = \{a \in A\colon \omega(a) = -a\}$.  Then $A = A_0 \oplus A_1$.
  \item[(d)] If $i$, $j \in \{0,1\}$, then $A_iA_j \subseteq A_{i+j}$ (where $i + j$ indicates addition modulo~$2$).  This says that a Clifford algebra
is a \df{$\Z_2$-graded} (or \df{$\Z/2\Z$\,-graded}) algebra.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof of (c)}]  If $a \in A$, let $a_0 = \frac12(a + \omega(a))$.   \ns
\end{proof}

\begin{exam} Let $V = \R$ and $Q(v) = v^2$ for every $v \in V$. Then the Clifford algebra $\cl(1,0)$ associated with $(\R,Q)$ is isomorphic to $\R \oplus \R$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Let $f\colon \R \sto \cl(1,0)$ be the Clifford map associated with~$Q$ and let $e := f(1)$.  Try the function 
$\phi\colon \cl(1,0) \sto \R\oplus\R$ under which $\vc 1 \mapsto (1,1)$ and $e \mapsto (-1,1)$.  \ns
\end{proof}

\begin{exam} Let $V = \R$ and $Q(v) = -v^2$ for every $v \in V$.  
 \begin{enumerate}
  \item[(a)] The algebra $\cl(0,1)$ is isomorphic to $\C$.
  \item[(b)] The algebra $\cl(0,1)$ can be represented as a subalgebra of~$M_2(\R)$.
 \end{enumerate}
\end{exam}

\begin{proof}[\emph{Hint for proof}] For part (a) keep in mind that as a real algebra $\C$ is $2$-dimensional.  For part (b) consider the matrices
$\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ and $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$   \ns
\end{proof}

\begin{exam} Let $V = \R^2$ and $Q(v) = {v_1}^2 + {v_2}^2$ for every $v \in V$.  Then
$\cl(2,0) \cong M_2(\R)$.  (See~\ref{def_Cliff_alg} and~\ref{notn_cl20}.)
\end{exam}

\begin{exam} Let $V = \R^3$ and $Q(v) = {v_1}^2 + {v_2}^2 + {v_3}^2$ for every $v \in V$.  Then $\cl(3,0)$ is isomorphic to both a subalgebra of
$\mathbf M_4$ and the algebra $\C(2)$ generated by the \emph{Pauli spin matrices}.  (See~\ref{repr_cl30_1} and~\ref{repr_cl30_2}.)  
\end{exam}

In 1833, when William Rowan Hamilton was still in his twenties, he presented a paper to the \emph{Royal Irish Academy} in which he explained how to
represent complex numbers as pairs of real numbers.  He was fascinated by the connections this produced between complex arithmetic and the geometry 
of the plane.  For more than a decade he searched, sometimes quite obsessively, for a $3$-dimensional algebraic analog which would facilitate studies 
in the geometry of $3$-space. (In modern language, he was seeking a $3$-dimensional normed division algebra.  As it turns out, there aren't any!)  On 
October 16, 1843, while walking with his wife along the Royal Canal in Dublin on his way to chair a meeting of the \emph{Royal Irish Academy}, he suddenly 
understood that to manage triples of real numbers algebraically, he would need to add, in some sense, a fourth dimension.  The objects he could work with 
were of the form $a + \vc i b + \vc j c + \vc k d$ where $a$, $b$, $c$, and $d$ are real numbers.  In this flash of inspiration he saw the necessary rules 
for multiplication of these objects.  Although, as far as we know, he had no previous history as a graffiti artist, he stopped with his wife to carve into 
the stones of Brougham Bridge, which they were passing, the key to the multiplication of these new objects, quaternions:
    \[ {\vc i}^2 = {\vc j}^2 = {\vc k}^2 = \vc i \vc j \vc k = -1\,. \] 

\begin{exer} Derive from the preceding equations the multiplication table for quaternions.  
\end{exer}

\begin{exam} Let $V = \R^2$ and $Q(v) = -v_1^2-v_2^2$ for every $v \in V$.
 \begin{enumerate}
  \item[(a)] The algebra $\cl(0,2)$ is isomorphic to the algebra $\h$ of quaternions.
  \item[(b)] The algebra $\cl(0,2)$ can be represented as a subalgebra of~$M_4(\R)$.
 \end{enumerate}
\end{exam}

\begin{proof}[\emph{Hint for proof}] For part (b) consider the matrices $\begin{bmatrix}
                                                                           0 & -1 &  0 &  0 \\
                                                                           1 &  0 &  0 &  0 \\
                                                                           0 &  0 &  0 & -1 \\
                                                                           0 &  0 &  1 &  0 
                                                                         \end{bmatrix}$ and $\begin{bmatrix}
                                                                                               0 &  0 & -1 &  0 \\
                                                                                               0 &  0 &  0 &  1 \\
                                                                                               1 &  0 &  0 &  0 \\
                                                                                               0 & -1 &  0 &  0
                                                                                             \end{bmatrix}$\,.   \ns 
\end{proof}

\begin{exam} The Clifford algebra $\cl(1,1)$ is isomorphic to $\mathbf M_2(\R)$.  
\end{exam}

\begin{proof}[\emph{Hint for proof}] Consider the matrices $\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$ and $\begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}$. \ns 
\end{proof}

\begin{exer}\label{clalg026} Take a look at the web page~\cite{Lounesto:1997} written by Pertti Lounesto.
\end{exer}

\begin{exer} The Clifford algebra $\cl(3,1)$ (\emph{Minkowski space-time algebra}) is isomorphic to $M_4(\R)$.
\end{exer}

\begin{proof}[\emph{Hint for proof}] Exercise~\ref{clalg026}.  \ns
\end{proof}







\endinput




\chapter{A BRIEF REVIEW OF DIFFERENTIAL CALCULUS}

We now pause for a \emph{very} brief review of differential calculus.  The central concept here is differentiability.  A function $f$ between normed linear
spaces is said to be \emph{differentiable} at a point $p$ if (when the point $(p,f(p))$ is translated to the origin) the function is tangent to some continuous
linear map.  In this chapter (much of which is just chapter 13 of my online text~\cite{Erdman:2007}) we make this idea precise and record a few important facts about
differentiability.  A more detailed nd leisurely treatment can be found in my \emph{ProblemText in Advanced Calculus}~\cite{Erdman:2005}, chapters 25--29.

There are two sorts of textbooks on differential calculus: concept oriented and computation oriented.  It is my belief that students who understand the concepts
behind differentiation can do the calculations, while students who study calculations only often get stuck.  Among the most masterful presentations of concept
oriented differential calculus are \cite{Dieudonne:1970} (volume~I, chapter~8) and \cite{LoomisS:1990} (chapter~3).  As of this writing the latter book is available
without charge at the website of one of the authors:
  \begin{center}
  \begin{verbatim}
        http://www.math.harvard.edu/~shlomo/docs/Advanced_Calculus.pdf
  \end{verbatim}
  \end{center}

The material in this chapter will benefit primarily those whose only encounter with multivariate calculus has been through partial derivatives and a \emph{chain rule}
that looks something like
 \begin{equation}\label{C025101i}
    \frac{\partial w}{\partial u}
     = \frac{\partial w}{\partial x}\frac{\partial x}{\partial u}
     + \frac{\partial w}{\partial y}\frac{\partial y}{\partial u}
     + \frac{\partial w}{\partial z}\frac{\partial z}{\partial u}\\
 \end{equation}
The approach here is intended to be more geometric, emphasizing the role of \emph{tangency}.






\section{Tangency}
\begin{notn}  Let $V$ and $W$ be normed linear spaces and $a \in V$.  (If you are unfamiliar with, or uncomfortable working in, normed linear spaces, just pretend that
all the spaces involved are $n$-dimensional Euclidean spaces.  The only thing you may lose by so doing is the pleasant feeling of assurance that differential calculus
is no harder in infinite dimensional spaces than on the real line.)  We denote
 \index{functionsfromV@$\fml F_a(V,W)$!functions in a neighborhood of~$a$}%
by~$\fml F_a(V,W)$ the family of all functions defined on a neighborhood of $a$ taking values in~$W$.  That is, $f$ belongs to $\fml F_a(V,W)$ if there exists an
open set $U$ such that $a \in U \subseteq \dom f \subseteq V$ and if the image of $f$ is contained in~$W$. We shorten $\fml F_a(V,W)$ to $\fml F_a$ when no confusion will result.
Notice that for each $a \in V$, the set $\fml F_a$ is closed under addition and scalar multiplication.  (As usual, we define the sum of two functions $f$ and $g$ in
$\fml F_a$ to be the function $f + g$ whose value at $x$ is $f(x) + g(x)$ whenever $x$ belongs to $\dom f \,\cap\, \dom g$.)  Despite the closure of $\fml F_a$ under
these operations, $\fml F_a$ is \emph{not} a vector space. (Why not?)
\end{notn}

\begin{defn}  Let $V$ and $W$ be normed linear spaces.  A function $f$ in $\fml F_0(V,W)$ belongs to
 \index{oh@$\lobo O(V,W)$ (``big-oh'' functions)}%
$\lobo O(V,W)$ if there exist numbers $c > 0$ and $\delta > 0$ such that
    \[ \norm{f(x)} \le c\,\norm x \]
whenever $\norm x < \delta$.

\vskip 5 pt

A function $f$ in $\fml F_0(V,W)$ belongs to
 \index{oh@$\lobo o(V,W)$ (``little-oh'' functions)}%
$\lobo o(V,W)$ if for every $c > 0$ there exists $\delta > 0$ such that
   \[ \norm {f(x)} \le c\,\norm x \]
whenever $\norm x < \delta$.  Notice that $f$ belongs to $\lobo o(V,W)$ if and only if $f(0) = 0$ and
   \[ \lim_{h \sto 0}\frac{\norm{f(h)}}{\norm h} = 0\,. \]
When no confusion seems likely we will shorten $\lobo O(V,W)$ to $\lobo O$ and $\lobo o(V,W)$ to $\lobo o$.
\end{defn}

\begin{exer} Here is a list summarizing the important facts about the families $\lobo O$ and~$\lobo o$.  State precisely what each of these says and give a proof.
(Here $\ofml B$ is the set of continuous linear maps between normed linear spaces $V$ and $W$, and $\fml C_0$ is the set of all functions in $\fml F_0(V,W)$ which
are continuous at~$0$.)
 \begin{align*}
   &(1)\qquad \lobo B \cup \lobo o \subseteq \lobo O \subseteq
                   \fml C_0 \, .\\
   &(2)\qquad \lobo B \cap \lobo o = {0}\,. \\
   &(3)\qquad \lobo O + \lobo O \subseteq \lobo O\,; \qquad
                    \alpha\,\lobo O \subseteq \lobo O\,. \\
   &(4)\qquad \lobo o + \lobo o \subseteq \lobo o\,; \qquad
                    \alpha\,\lobo o \subseteq \lobo o\,. \\
   &(5)\qquad \lobo o \circ \lobo O \subseteq \lobo o\,. \\
   &(6)\qquad \lobo O \circ \lobo o \subseteq \lobo o\,. \\
   &(7)\qquad \lobo o(V,\R)\cdot W\subseteq \lobo o(V,W)\,.\\
   &(8)\qquad \lobo O(V,\R)\cdot \lobo O(V,W) \subseteq
                     \lobo o(V,W)\,.
 \end{align*}
\end{exer}

\begin{defn}  Let $V$ and $W$ be normed linear spaces.  Two functions $f$ and $g$ in $\fml F_0(V,W)$ are
 \index{tangent (at zero)}%
\df{tangent (at zero)}, in which case we write
 \index{<sim@$\simeq$ (tangency at~$0$)}%
$f \simeq g$, if $f - g \in \lobo o(V,W)$.
\end{defn}

\begin{prop}\label{C025124}  The relation of tangency enjoys the following properties.
 \begin{enumerate}
  \item[(a)] ``Tangency at zero'' is an equivalence relation on $\fml F_0$.
  \item[(b)] Let $S$, $T \in \ofml B$ and $f \in \fml F_0$. If $S \simeq f$ and $T \simeq f$, then $S = T$.
  \item[(c)] If $f \simeq g$ and $j \simeq k$, then $f + j \simeq g + k$, and furthermore, $\alpha f \simeq \alpha g$ for all $\alpha \in~\R$.
  \item[(d)] Let $\phi, \psi \in \fml F_0(V,\R)$ and $w \in W$.  If $\phi \simeq \psi$, then $\phi w \simeq \psi w$.
  \item[(e)] Let $f$, $g \in \fml F_0(V,W)$ and $T \in \ofml B(W,X)$.  If $f \simeq g$, then $T \circ f \simeq T \circ g$.
  \item[(f)] Let $h \in \lobo O(V,W)$ and $f$, $g \in \fml F_0(W,X)$.  If $f \simeq g$, then $f \circ h \simeq g \circ h$.
 \end{enumerate}
\end{prop}















\section{The Differential}
\begin{defn} Let $V$ and $W$ be normed linear spaces, $a \in V$, and $f \in \fml F_a(V,W)$. Define the function
 \index{delta@$\Delta f_a$ (a translate of~$f$)}%
$\Delta f_a$ by
   \[ \Delta f_a(h) := f(a+h) - f(a) \]
for all $h$ such that $a+h$ is in the domain of~$f$.  Notice that since $f$ is defined in a neighborhood of $a$, the function $\Delta f_a$ is defined in a
neighborhood of 0; that is, $\Delta f_a$ belongs to $\fml F_0(V,W)$. Notice also that $\Delta f_a(0) = 0$.
\end{defn}

\begin{prop}\label{C025214} If $V$ and $W$ are normed linear spaces and $a \in V$, then the function~$\Delta$ has the following properties.
 \begin{enumerate}
   \item[(a)] If $f \in \fml F_a(V,W)$ and $\alpha \in \R$, then
     \[ \Delta(\alpha f)_a = \alpha\,\Delta f_a\,. \]
   \item[(b)] If $f$, $g \in \fml F_a(V,W)$, then
     \[ \Delta(f + g)_a = \Delta f_a + \Delta g_a\,. \]
   \item[(c)] If $\phi \in \fml F_a(V,\R)$ and $f \in \fml F_a(V,W)$, then
     \[ \Delta(\phi f)_a = \phi(a)\cdot\Delta f_a\,+\, \Delta\phi_a\cdot f(a)
                       \,+\, \Delta\phi_a\cdot\Delta f_a\,. \]
   \item[(d)] If $f \in \fml F_a(V,W)$, $g \in \fml F_{f(a)}(W,X)$, and $g \circ f \in
\fml F_a(V,X)$, then
     \[ \Delta(g \circ f)_a = \Delta g_{{}_{\sst{f(a)}}} \circ \Delta f_a\,. \]
   \item[(e)] A function $f\colon V \sto W$ is continuous at the point $a$ in $V$  if and only
if $\Delta f_a$ is continuous at~$0$.
    \item[(f)] If $f \colon U \sto U_1$ is a bijection between subsets of arbitrary vector spaces,
then for each $a$ in $U$ the function $\Delta f_a\colon U - a \sto U_1 - f(a)$ is invertible and
     \[ \bigl(\Delta f_a\bigr)^{-1}=\Delta\bigl(f^{-1}\bigr)_{f(a)}\,. \]
 \end{enumerate}
\end{prop}

\begin{defn} Let $V$ and $W$ be normed linear spaces, $a \in V$, and $f \in \fml F_a(V,W)$. We say that $f$ is
 \index{differentiable}%
\df{differentiable at} $a$ if there exists a continuous linear map which is tangent at 0 to $\Delta f_a$. If such a map exists, it is called the
 \index{differential}%
\df{differential} of $f$ at $a$ and is denoted
 \index{differential@$df_a$ (the differential of $f$ at~$a$)}%
by~$df_a$. Thus $df_a$ is just a member of $\ofml B(V,W)$ which satisfies $df_a \simeq \Delta f_a$.  We denote by $\fml D_a(V,W)$ the family of all functions
in $\fml F_a(V,W)$ which are differentiable at~$a$. We often shorten this to~$\fml D_a$.
\end{defn}

We establish next that there can be at most one bounded linear map tangent to $\Delta f_a$.

\begin{prop}\label{C025221} Let $V$ and $W$ be normed linear spaces and $a \in V$. If $f \in \fml D_a(V,W)$, then its differential is unique.
\end{prop}

\begin{exer} Let
 \[f\colon \R^3 \sto \R^2\colon (x,y,z) \mapsto (x^2y - 7, 3xz + 4y)\]
and $a = (1,-1,0)$.  Use the \emph{definition} of ``differential'' to find $df_a$.
\emph{Hint.}  Work with the matrix representation of $df_a$.  Since the differential must
belong to $\ofml B(\R^3,\R^2)$, its matrix representation is a $2 \times 3$ matrix $M =
\begin{bmatrix}
             r  &  s  &  t \\
             u  &  v  &  w
     \end{bmatrix}$\,.  Use the
requirement that $\norm h^{-1}\,\norm{\Delta f_a(h) - Mh} \sto 0$ as $h \sto 0$ to
discover the identity of the entries in~$M$.
\end{exer}

\begin{exer} Let $\vc F\colon \R^2 \sto \R^4$ be defined by $\vc F(x,y) = (y,x^2,4-xy,7x)$, and
let $\vc p = (1,1)$.  Use the \emph{definition} of ``differentiable'' to show that $\vc
F$ is differentiable at~$\vc p$. Find the (matrix representation of the) differential of
$\vc F$ at~$p$.
\end{exer}

\begin{prop}\label{C025231} Let $V$ and $W$ be normed linear spaces and $a \in V$. If $f \in \fml D_a$, then $\Delta f_a \in \lobo O$; thus, every function
which is differentiable at a point is continuous there.
\end{prop}

\begin{prop} Let $V$ and $W$ be normed linear spaces and $a \in V$.  Suppose that $f$, $g \in \fml D_a(V,W)$ and that $\alpha \in \R$. Then
 \begin{enumerate}
  \item  $\alpha f$ is differentiable at $a$ and
     \[d(\alpha f)_a = \alpha\,df_a\,;\]
  \item also, $f + g$ is differentiable at $a$ and
     \[d(f + g)_a = df_a + dg_a\,.\]
 \end{enumerate}
Suppose further that $\phi \in \fml D_a(V,\R)$. Then
 \begin{enumerate}
  \item[(c)] $\phi f \in \fml D_a(V,W)$ and
      \[d(\phi f)_a = d\phi_a \cdot f(a) \,+\, \phi(a)\,df_a\,.\]
 \end{enumerate}
\end{prop}

It seems to me that the version of the \emph{chain rule} given in~\eqref{C025101i},  although (under appropriate hypotheses) a correct equation, really says
very little.  The idea that should be conveyed is that the best linear approximation to the composite of two smooth functions is the composite of their best
linear approximations.

\begin{thm}[The Chain Rule]\label{C025237} Let $V$, $W$, and $X$ be normed linear spaces with $a \in V$.  If $f \in \fml D_a(V,W)$ and
$g \in \fml D_{f(a)}(W,X)$, then $g \circ f \in \fml D_a(V,X)$ and
   \[ d(g \circ f)_a = dg_{{}_{\scriptstyle{f(a)}}} \circ df_a\,. \]
\end{thm}

\begin{proof} Our hypotheses are $\Delta f_a \simeq df_a$ and $\Delta g_{f(a)} \simeq dg_{f(a)}$.
By proposition~\ref{C025231} $\Delta f_a \in \lobo O$. Then by
proposition~\ref{C025124}(f)
  \begin{equation}\label{C025237i} \Delta g_{f(a)} \circ \Delta f_a
                                       \simeq dg_{f(a)}  \circ \Delta f_a
  \end{equation}
and by proposition~\ref{C025124}(e)
  \begin{equation}\label{C025237ii} dg_{f(a)} \circ \Delta f_a
                                       \simeq dg_{f(a)} \circ df_a\,.
  \end{equation}
According to proposition~\ref{C025214}(d)
  \begin{equation}\label{C025237iii} \Delta(g \circ f)_a
                               \simeq \Delta g_{f(a)} \circ \Delta f_a\,.
  \end{equation}
From \eqref{C025237i}, \eqref{C025237ii},\eqref{C025237iii}, and
proposition~\ref{C025124}(a) it is clear that
    \[ \Delta(g \circ f)_a \simeq dg_{f(a)} \circ df_a\,. \]
Since $dg_{f(a)} \circ df_a$ is a bounded linear transformation, the desired conclusion
is an immediate consequence of proposition~\ref{C025221}.
\end{proof}

\begin{exer} Derive (under appropriate hypotheses) equation~\eqref{C025101i} from
theorem~\ref{C025237}.
\end{exer}

\begin{exer} Let $\vc T$ be a linear map from $\R^n$ to $\R^m$ and $\vc p \in \R^n$. Find
$d\vc T_{\vc p}$.
\end{exer}

\begin{exam} Let $T$ be a symmetric $n \times n$ matrix and let $\vc p \in \R^n$.  Define a
function $f\colon \R^n \sto \R$ by $f(\vc x) = \langle T\vc x,\vc x\rangle$. Then
  \[ df_{\vc p}(\vc h) = 2\langle T\vc p,\vc h\rangle \]
for every $h \in \R^n$.
\end{exam}









\section{The Gradient of a Scalar Field in $\R^n$}

\begin{defn} A
 \index{field!scalar}%
 \index{scalar!field}%
\df{scalar field} on $\R^n$ is a scalar valued function on a subset of~$\R^n$.
\end{defn}

\begin{defn}  Let $U$ be an open subset of $\R^n$ and $\phi\colon U \sto \R$ be a scalar field.  If $\phi$ is differentiable at a point $a$ in $\intr U$, then
its differential $d\phi_a$ is a (continuous) linear map from $\R^n$ into~$\R$.  That is, $d\phi_a \in (\R^n)^*$.  Thus according to the
\emph{Riesz-Fr\'echet theorem}~\ref{riesz_frechet_thm} there exists a unique vector, which we denote by $\nabla\phi(a)$, representing the linear
functional $d\phi_a$.  That is,
 \index{<@$\nabla\phi(a)$ (gradient of $\phi$ at~$a$)}%
$\nabla\phi(a)$ is the unique vector in $\R^n$ such that
  \[ d\phi_a(x)  =  \langle x,\nabla\phi(a) \rangle \]
for all $x$ in $\R^n$.  The vector $\nabla\phi(a)$ is the
 \index{gradient!at a point}%
\df{gradient} of $\phi$ at~$a$.  If $U$ is an open subset of $\R^n$ and $\phi$ is
differentiable at each point of~$U$, then the function
  \[ \nabla\phi\colon U \sto \R^n\colon u \mapsto \nabla\phi(u) \]
is the
 \index{gradient!on an open set}%
\df{gradient} of~$\phi$.  Notice two things: first, the gradient of a scalar field is a vector
field (that is, a map from $\R^n$ into~$\R^n$); and second, the differential $d\phi_a$ is the zero linear functional if and only if the
gradient at~$a$, $\nabla\phi(a)$, is the zero vector in~$\R^n$.
\end{defn}

Perhaps the most useful fact about the gradient of a scalar field $\phi$ at a point $a$ in $\R^n$ is that it is the vector at $a$ which points in the direction
of the most rapid increase of~$\phi$, a fact to be proved in proposition~\ref{prop_max_dd}.

\begin{defn}  Let $f$ be a member of $\fml F_a(V,W)$ and $v$ be a nonzero vector in~$V$.  Then $D_vf(a)$, the
 \index{dvf@$D_vf$ (directional derivative)}%
 \index{derivative!directional}%
 \index{directional derivative}%
\df{derivative of $f$ at $a$ in the direction of $v$}, is defined by
  \[ D_vf(a) := \lim_{t \sto 0} \frac1t\,\Delta f_a(tv)\]
if this limit exists.  This directional derivative is also called the
 \index{G\^ateaux!differential}%
 \index{differential!G\^ateaux}%
\df{G\^ateaux differential} (or
 \index{G\^ateaux!variation}%
 \index{variation!G\^ateaux}%
\df{G\^ateaux variation}) of $f$, and is sometimes denoted by $\delta f(a;v)$. Many authors require that in the preceding definition $v$ be a unit vector.
We will \emph{not} adopt this convention.
\end{defn}

Recall that for $\vc 0 \ne v \in V$ the curve $\ell \colon \R \sto V$ defined by $\ell(t) = a + tv$ is the parametrized line through $a$ in the direction of~$v$.
In the following proposition, which helps illuminate our use of the adjective ``directional'', we understand the domain of $f \circ \ell$ to be the set of all
numbers $t$ for which the expression $f(\ell(t))$ makes sense; that is,
  \[ \dom(f \circ \ell) = \{t \in \R \colon  a + tv \in \dom f\}\,. \]
Since $a$ is an interior point of the domain of $f$, the domain of $f \circ \ell$ contains an open interval about~$0$.

\begin{prop}\label{prop_diff_dd}  If $f \in \fml D_a(V,W)$ and $\vc 0 \ne v \in V$, then the directional derivative $D_vf(a)$ exists and is the tangent vector
to the curve $f \circ \ell$ at $0$ (where $\ell$ is the parametrized line through $a$ in the direction of~$v$).  That is,
  \[ D_vf(a) = D(f \circ \ell)(0)\,. \]
\end{prop}

\begin{exam}\label{exam_dd1} Let $f(x,y) = \ln\bigl(x^2 + y^2\bigr)^{\frac12}$.  Then $D_vf(a) = \frac7{10}$ when $a=(1,1)$ and $v=(\frac35,\frac45)$.
\end{exam}

\begin{prop}\label{prop_dd_diff} If $f \in \fml D_a(V,W)$, then for every nonzero $v$ in $V$
  \[ D_vf(a) = df_a(v)\,. \]
\end{prop}

\begin{prop}\label{prop_max_dd}  Let $\phi\colon U \sto \R$ be a scalar field on a subset $U$ of~$\R^n$.  If $\phi$ is differentiable at a point $a$ in
$U$ and $d\phi_a$ is not the zero functional, then the maximum value of the directional derivative $D_u\phi(a)$, taken over all unit vectors $u$ in $\R^n$,
is achieved when $u$ points in the direction.of the gradient $\nabla\phi(a)$.  The minimum value is achieved when $u$ points in the opposite direction
$-\nabla\phi(a)$.
\end{prop}

What role do partial derivatives play in all this?  Conceptually, not much of one.  They are just directional derivatives in the directions of the standard basis
vector of $\R^n$.  They are, however, useful for computation.  For example, if $F$ is a mapping from $\R^n$ to $\R^m$ differentiable at a point $a$, then the matrix
representation of $dF_a$ is an $m \times n$ matrix (the so-called \emph{Jacobian matrix}) whose entry in the $j^{\text{th}}$ row and $k^{\text{th}}$ column is the
partial derivative $F^j_k = \dfrac{\partial F^j}{\partial x_k}$ (where $F^j$ is the $j^{\text{th}}$ coordinate function of~$F$).  And if $\phi$ is a differentiable
scalar field on $\R^n$. then its gradient can be represented as $\biggl(\dfrac{\partial\phi}{\partial x_1}, \dots, \dfrac{\partial\phi}{\partial x_n}\biggr)$.

\begin{exer} Take any elementary calculus text and derive every item called a \emph{chain rule} in that text from theorem~\ref{C025237}.
\end{exer}





\endinput
\chapter{DIFFERENTIAL FORMS ON MANIFOLDS}


\begin{center}
\fbox{\parbox{3.8in}{\textbf{In everything that follows all vector spaces
 \index{conventions!in chapters 10--11 all vector spaces are real, finite dimensional, and oriented}%
are assumed to be real, finite dimensional, and oriented; and all manifolds are smooth
oriented
 \index{conventions!in chapters 10--11 all manifolds are smooth and oriented}%
differentiable manifolds.}}}
\end{center}

 \vskip 20 pt



\section{Vector Fields}

\begin{defn} Let $M$ be a manifold.  The set
   \[ TM := \bigcup_{m \in M} T_m \]
is the
 \index{tangent!bundle}%
 \index{bundle!tangent}%
 \index{TM@$TM$ (tangent bundle)}%
\df{tangent bundle} of~$M$. The
 \index{projection!tangent bundle}%
 \index{tangent!bundle!projection}%
 \index{tauM@$\sbsb{\tau}M$ (tangent bundle projection)}%
\df{tangent bundle projection} is the map $\sbsb{\tau}M \colon TM \sto M$ such that
$\sbsb{\tau}M(w) = m$ whenever $w \in T_m$.
\end{defn}

\begin{defn} A
 \index{vector!field}%
 \index{field!vector}%
\df{vector field} on a manifold $M$ is a map $v\colon M \sto TM$ such that $\sbsb{\tau}M
\circ v = Id_M$ (the identity map on~$M$).  It is an equivalent formulation to say that
$v$ is a vector field if it is a right inverse of the tangent bundle projection or to say
that it is a
 \index{section}
\emph{section} of the tangent bundle.
\end{defn}

\begin{notn} For a manifold $M$ denote by
 \index{Cinfinity@$\fml C^\infty(M)$ (smooth real valued functions on~$M$)}%
$\fml C^\infty(M)$ the family of smooth real valued functions on~$M$. That is, a function
$f$ belongs to $\fml C^\infty(M)$ provided that it belongs to  $\fml C^\infty_m(M,\R)$
for every $m \in M$ (see definition~\ref{def_smooth_rv_fcn}).
\end{notn}

\begin{defn} If $v$ is a vector field on a manifold $M$ and $f \in \fml C^\infty(M)$,
define the
 \index{vf@$vf$ (action of a vector field on a function)}%
function $vf\colon M\sto \R$ by
   \[ (vf)(m) = v(m)(\wh f)\,. \]
We will say that $v$ is a
 \index{smooth!vector field}%
\df{smooth} vector field if $vf$ is smooth for every $f \in \fml C^\infty(M)$.
\end{defn}

\begin{exer} Let $M$ be an $n$-manifold and $\phi = (x^1, \dots, x^n)\colon U \sto \R^n$
be a chart.  Regard $U$ as a submanifold of $M$ in the obvious fashion.  For $1 \le k \le
n$ take $\pd{}{x^k}$ to be the vector field on $U$ defined by
   \[ \pd{}{x^k}\colon m \mapsto \pd{}{x^k}\biggr|_m \]
where, as before (see~\ref{cor_derivation_partial} and~\ref{notn_derivation_partial}),
   \[ \pd{}{x^k}\biggr|_m\bigl(\wh f\bigr) = \bigl(f \circ \phi^{-1}\bigr)_k(\phi(m)) \]
for all $f \in \fml C^\infty(U)$. Show that the vector field $\pd{}{x^k}$ is smooth.
\end{exer}

\begin{exer} Notation as in the preceding exercise.  Let $v$ be a smooth vector field
on~$U$. For every $m \in U$ there exist numbers $\alpha_1(m)$, \dots, $\alpha_n(m)$ such
that
   \[ v(m) = \sum_{k=1}^n \alpha_k(m)\pd{}{x^k}\biggr|_m \]
(see~\ref{cor_basis_tan_sp}).  Thus we may write
   \[ v = \sum_{k=1}^n \alpha_k\pd{}{x^k} \]
where $\alpha_k$ is the function $m \mapsto \alpha_k(m)$.  Show that each $\alpha_k$ is a
smooth function.
\end{exer}














\section{Differential $1$-forms}

\begin{defn}  Let $M$ be a manifold. The  set
   \[ T^*M := \bigcup_{m \in M} {T_m}^* \]
is the
 \index{cotangent!bundle}%
 \index{bundle!cotangent}%
 \index{TM@$T^*M$ (cotangent bundle)}%
\df{cotangent bundle} of~$M$. The
 \index{projection!cotangent bundle}%
 \index{cotangent!bundle!projection}%
 \index{rhoM@$\sbsb{\rho}M$ (cotangent bundle projection)}%
\df{cotangent bundle projection} is the map \mbox{$\sbsb{\rho}M\colon T^*M\sto M$} such
that $\sbsb{\rho}M(u) = m$ whenever $u \in {T_m}^*$.
\end{defn}

\begin{defn}\label{def_one_form} A
 \index{differential!one-form}%
 \index{one-form!differential}%
 \index{1@$1$-form}%
 \index{form!one-}%
\df{differential one-form} (or \df{differential $1$-form}) on a manifold $M$ is a map
$\omega\colon M \sto T^*M$ such that $\sbsb{\rho}M \circ \omega = Id_M$ (the identity map
on~$M$). Equivalently, it is a right inverse of the cotangent bundle projection, or it is
a section of the cotangent bundle.
\end{defn}

\begin{defn} If $\omega$ is a differential one-form on an $n$-manifold $M$ and $v$ is a
vector field on~$M$,
 \index{omegav@$\omega(v)$ (a one-form acting on a vector field)}%
we define
   \[ \omega(v)\colon M \sto \R\colon m \mapsto \bigl(\omega(m)\bigr)\bigl(v(m)\bigr). \]
\end{defn}

\begin{defn} A differential one-form $\omega$ is
 \index{smooth!differential one-form}%
\df{smooth} if $\omega(v) \in \fml C^\infty(M)$ whenever $v$ is a smooth vector field
on~$M$.
\end{defn}

\begin{prop} Let $M$ be an $n$-manifold and $f \in \fml C^\infty(M)$. Then the function
$df\colon m \mapsto df_m$ is a smooth differential one-form on~$M$.
\end{prop}

\begin{conv} From now on let's drop the words ``smooth'' and ``differential'' in the
phrase
 \index{conventions!all one-forms are smooth differential one-forms}%
\emph{smooth differential one-form}.  There is no other kind of ``one-form'' that we will
be interested in.
\end{conv}






















\section{Differential $k$-forms}

\begin{notn}\label{diff_forms_001notn} For an $n$-manifold $M$ and $k \ge 0$ let
 \index{<bigwedge@$\bigwedge(M)$, ${\bigwedge}^k(M)$ (differential forms)}%
  \begin{align*}
       {\bigwedge}^{\!k}(M) &= \bigcup\bigl\{\,{\bigwedge}^{\!k}({T_m}^*)\colon m \in M\bigr\} \\
    \intertext{and}
        \bigwedge(M)   &= \bigcup\bigl\{\,\bigwedge({T_m}^*)\colon m \in M\bigr\}.
  \end{align*}
\end{notn}

\begin{defn} A
 \index{differential!form}%
 \index{form!differential}%
\df{differential form} is a section of $\bigwedge(M)$.  Thus $\omega$ is a differential
form if $\omega(m) \in \bigwedge({T_m}^*)$ for every $m \in M$. Similarly, $\omega$ is a
 \index{differential!$k$-form}%
\df{differential $k$-form} (or just a
 \index{kform@$k$-form}%
\df{$k$-form}) if it a section of ${\bigwedge}^{\!k}(M)$.  Notice that for $1$-forms this
definition agrees with the one given in~\ref{def_one_form} since ${\bigwedge}^{\!1}(M) = T^*M$.  Also notice that a
 \index{zero!form}%
 \index{0@$0$-form}%
$0$-form is just a real valued function on~$M$ (because of our
identification of ${\bigwedge}^{\!0}({T_m}^*)$ with~$\R$---see~\ref{conv_wedge0_is_R}).
\end{defn}

\begin{exer} Let $M$ be a $3$-manifold and $\phi = (x,y,z)\colon U \sto \R^3$ be a chart.
Regard $U$ as a submanifold of~$M$. Exhibit bases for $\bigwedge^{\!0}(U)$, $\bigwedge^{\!1}(U)$,
$\bigwedge^{\!2}(U)$, $\bigwedge^{\!3}(U)$, and $\bigwedge(U)$.
\end{exer}

\begin{defn} Given a chart $\phi = (x^1, \dots, x^n)\colon U \sto \R^n$ on an
$n$-manifold~$M$, we may express a $k$-form $\omega$ locally (that is, on~$U$) by
   \[ \omega(m) = \sum_{j_1 < \dots < j_k} a_{j_1 \dots j_k}(m) \,
                      d{x^{j_1}}_m \wedge \dots \wedge d{x^{j_k}}_m \]
for all $m \in U$.  More succinctly,
   \[ \omega = \sum_{j_1 < \dots < j_k} a_{j_1 \dots j_k} \,
                      d{x^{j_1}} \wedge \dots \wedge d{x^{j_k}}\,. \]
It should be kept in mind that the ``coefficients'' $a_{j_1 \dots j_k}$ in this
expression are \emph{functions} and that they depend on the choice of coordinate system
(chart). The $k$-form $\omega$ is
 \index{smooth!locally}%
 \index{locally!smooth $k$-form}%
\df{smooth} with respect to the chart $\phi$ if all the coefficients $a_{j_1 \dots j_k}$
are smooth real valued functions on~$U$.  A $k$-form $\omega$ defined on all of $M$ is
 \index{smooth!$k$-form}%
 \index{kform@$k$-form!smooth}%
\df{smooth} if it is smooth with respect to every chart on~$M$.  A differential form is
 \index{smooth!differential form}%
 \index{differential!form!smooth}%
 \index{form!smooth differential}%
\df{smooth} if its component in $\bigwedge^{\!k}(M)$ is smooth for every~$k$. The set of
smooth differential forms on $M$ is denoted
 \index{Cinfinity@$\fml C^\infty\bigl(M,\bigwedge(M)\bigr)$, $\fml C^\infty\bigl(M,\bigwedge^k(M)\bigr)$  (smooth differential forms)}%
by $\fml C^\infty\bigl(M,\bigwedge(M)\bigr)$ and the set of smooth $k$-forms by $\fml
C^\infty\bigl(M,\bigwedge^{\!k}(M)\bigr)$.
\end{defn}

\begin{conv} In the sequel all $k$-forms are smooth differential $k$-forms
 \index{conventions!all $k$-forms are smooth differential $k$-forms}%
 \index{conventions!all differential forms are smooth}%
and all differential forms are smooth.
\end{conv}

The next theorem defines a mapping $d$ on differential forms called the
 \index{exterior!differentiation operator}%
 \index{differentiation!exterior}%
 \index{operator!exterior differentiation}%
\df{exterior differentiation operator}.

\begin{thm}\label{thm_exist_exterior_diff} If $M$ is an $n$-manifold, then there exists a
unique linear map
  \[ d \colon \fml C^\infty\bigl(M, \bigwedge(M)\bigr) \sto \fml
                                    C^\infty\bigl(M, \bigwedge(M)\bigr)\]
which satisfies
  \begin{enumerate}
    \item $d^{\sto}\bigl(\fml C^\infty\bigl(M, \bigwedge^{\!k}(M)\bigr)\bigr) \subseteq
             \fml C^\infty\bigl(M, \bigwedge^{\!k+1}(M)\bigr)$;
    \item $d(f) = df$ (the ordinary differential of~$f$) for every $0$-form~$f$;
    \item if $\omega$ is a $k$-form and $\mu$ is any differential form, then
       \[ d(\omega \wedge \mu) = (d\omega) \wedge \mu + (-1)^k \omega \wedge d\mu \text{; \quad and} \]
    \item $d^2 = 0$.
  \end{enumerate}
\end{thm}

\begin{proof} Proofs of the existence and uniqueness of such a function can be found in \cite{Lee:2003} (theorem 12.14), \cite{Walschap:2004}
(chapter 1, theorem 11.1), and \cite{BishopC:1964} (section~4.6).
\end{proof}

\begin{exer} Let $M$ be a $3$-manifold, $\phi = (x,y,z)\colon U \sto \R^3$ be a chart
on~$M$, and $f \colon U \sto \R$ be a $0$-form on~$U$.  Compute $d(f\,dy)$.  (If $f$ is a
$0$-form and $\omega$ is any differential form, it is conventional
 \index{conventions!when $f$ is a $0$-form, $f\omega$ means $f \wedge \omega$}
to write $f\omega$ for $f \wedge \omega$.)
\end{exer}

\begin{exam} Let $M$ be a $3$-manifold and $\phi = (x,y,z)\colon U \sto \R^3$ be a chart
on~$M$.  Then $d\bigl(\cos(xy^2)\,dx \wedge dz\bigr) = 2xy\sin(xy^2)\,dx \wedge dy \wedge dz$.
\end{exam}

\begin{exer} Let $M$ be a $3$-manifold and $\phi = (x,y,z)\colon U \sto \R^3$ be a chart
on~$M$.  Compute $d(x\,dy \wedge dz + y\,dz \wedge dx + z\,dx \wedge dy)$.
\end{exer}

\begin{exer} Let $M$ be a $3$-manifold and $\phi = (x,y,z)\colon U \sto \R^3$ be a chart
on~$M$.  Compute $d[(3xz\,dx + xy^2\,dy) \wedge (x^2y\,dx - 6xy\,dz)]$.
\end{exer}

\begin{exer} In beginning calculus texts some curious arguments are given for
replacing the expression $dx\,dy$ in the integral $\iint_R f\,dx\,dy$ by $r\,dr\,d\theta$
when we change from rectangular to polar coordinates in the plane. Show that if we
interpret $dx\,dy$ as the differential form $dx \wedge dy$, then this is a correct
substitution. (Assume additionally that $R$ is a region in the open first quadrant and
that the integral of $f$ over $R$ exists.)
\end{exer}

\begin{exer} Give an explanation similar to the one in the preceding exercise of
the change in triple integrals from rectangular to spherical coordinates.
\end{exer}

\begin{exer} Generalize the two preceding exercises.
\end{exer}

\begin{prop} If $f$ is a $0$-form and $\omega$ is a $k$-form on~$U$, then $*\,(f\omega) =
f(*\,\omega)$.
\end{prop}

\begin{prop} If $\omega$ and $\mu$ are $k$-forms on~$U$, then $*\,(\omega + \mu) =
*\,\omega + *\,\mu$.
\end{prop}

\begin{prop} If $\omega$ is a $k$-form on~$U$, then $**\,\omega = (-1)^{k(n-k)}\omega$.
\end{prop}

Notice that, in consequence of the preceding proposition, every $k$-form on a
$3$-manifold satisfies $**\,\omega = \omega$.

\begin{exer} For real valued functions $a$, $b$, and $c$ on $U$ compute
  \begin{enumerate}
    \item $*\,(a\vc 1)$,
    \item $*\,(a\,dx + b\,dy + c\, dz)$,
    \item $*\,(a\,dy \wedge dz + b\,dz \wedge dx + c\,dx\wedge dy)$, and
    \item $*\,a(dx \wedge dy \wedge dz)$.
  \end{enumerate}
\end{exer}












\section{Some Classical Vector Analysis}

\begin{defn} In beginning calculus we learn that the gradient of a smooth scalar field
$f$ on $\R^n$ can be represented at a point $m$ as the vector $\bigl(\pd f{x^1}(m),
\dots, \pd f{x^n}(m)\bigr)$.  For a smooth function $f$ on the domain $U$ of a chart
$\phi = (x^1, \dots, x^n)$ in an $n$-manifold we define the
 \index{gradient}%
\df{gradient} of $f$ at a point $m$ in $U$ to be the vector \emph{in the cotangent space}
${T_m}^*$ whose components with respect to the usual basis $\{d{x^1}_m, \dots,
d{x^n}_m\}$ for ${T_m}^*$ are just $\pd f{x^1}$, \dots, $\pd f{x^n}$.  We denote this
 \index{gradient@$\grad f(m)$ (gradient of $f$ at~$m$)}%
 \index{<@$\nabla f(m)$ (gradient of $f$ at~$m$)}%
vector by $\grad f(m)$ or $\nabla f(m)$.  Thus we make no distinction between the
$1$-forms $\grad f$ and $df$, since
   \[ \grad f = \pd f{x^1}\,dx^1 + \dots + \pd f{x^n}\,dx^n = df \]
(see proposition~\ref{prop_expansion_dffrntl}).
\end{defn}

\begin{defn} Let $\omega$ be a $1$-form on the domain of a chart on a manifold. The
 \index{curl}%
\df{curl} of $\omega$, denoted by
 \index{<@$\nabla \times \omega$ (curl of a $1$-form)}%
$\curl\omega$ or $\nabla \times \omega$ is defined by
   \[ \curl\omega = *\,d\omega . \]
(Notice that on a $3$-manifold the curl of a $1$-form is again a $1$-form.)
\end{defn}

\begin{exam} If $\omega = a\,dx + b\,dy + c\,dz$ is a $1$-form on a $3$-manifold, then
   \[ \curl\omega = \biggl(\pd cy - \pd bz\biggr)\,dx + \biggl(\pd az - \pd cx\biggr)\,dy
                       + \biggl(\pd bx - \pd ay\biggr)\,dz. \]
\end{exam}

\begin{rem} Some depraved souls who completely abandon all inhibitions concerning
notation have been known to write
   \[ \curl\omega = \det\begin{bmatrix}  dx   &   dy   &   dz   \\
                                       \pd{}x & \pd{}y & \pd{}z \\
                                          a   &    b   &    c \end{bmatrix}. \]
\end{rem}

\begin{defn} Let $\omega$ be a $1$-form on the domain of a chart on a manifold. The
 \index{divergence}%
\df{divergence} of $\omega$, denoted by
 \index{<@$\nabla\cdot\omega$ (divergence of a $1$-form)}%
 \index{divergence@$\divr\omega$ (divergence of a $1$-form)}%
$\divr\omega$ or $\nabla\cdot\omega$ is defined by
   \[ \divr\omega = *\,d\,*\,\omega . \]
(Notice that on a $3$-manifold the divergence of a $1$-form is a $0$-form; that is, a
real valued function.)
\end{defn}

\begin{exam} If $\omega = a\,dx + b\,dy + c\,dz$ is a $1$-form on a $3$-manifold, then
   \[ \divr\omega = \pd ax + \pd by + \pd cz\,. \]
\end{exam}

\begin{exer} If $f$ is a $0$-form on the domain of a chart, prove (without using partial
derivatives) that $\curl\grad f = 0$.
\end{exer}

\begin{exer} If $\omega$ is a $1$-form on the domain of a chart, prove (without using
partial derivatives) that $\divr\curl\omega = 0$.
\end{exer}

\begin{defn} Let $\omega$ and $\mu$ be $1$-forms on the domain of a chart.  Define the
 \index{cross-product}%
\df{cross-product} of $\omega$ and $\mu$, denoted by
 \index{binaryoperation@$\omega \times \mu$ (cross-product of $1$-forms)}%
$\omega \times \mu$, by
   \[ \omega \times \mu = *\,(\omega \wedge \mu). \]
\end{defn}

\begin{exam} If $\omega = a\,dx + b\,dy + c\,dz$ and $\mu = e\,dx + f\,dy + g\,dz$ are
$1$-forms on a $3$-manifold, then
   \[ \omega \times \mu = (by - cf)\,dx + (ce - ay)\,dy + (af - be)\,dz. \]
\end{exam}

\begin{rem} Occasionally as a memory aid some people write $\omega \times \mu = \det
   \begin{bmatrix}
       dx  &  dy  &  dz \\
       a   &  b   &  c  \\
       e   &  f   &  g
   \end{bmatrix}$.
\end{rem}

\begin{exer} Suppose we wish to define the \emph{dot product} $\langle \omega,\mu
\rangle$ of two $1$-forms $\omega = a\,dx + b\,dy + c\,dz$ and $\mu = e\,dx + f\,dy +
g\,dz$ on a $3$-manifold to be the $0$-form $ae + bf + cg$.  Rephrase this definition
without mentioning the components of $\omega$ and~$\mu$.
\end{exer}

\begin{exer} Suppose we wish to define the \emph{triple scalar product} $[\omega,\mu,
\eta]$ of the $1$-forms $\omega = a\,dx + b\,dy + c\,dz$, $\mu = e\,dx + f\,dy + g\,dz$,
and $\eta = j\,dx + k\,dy + l\,dz$ on a $3$-manifold to be the $0$-form $bgj - fcj + cek
- agk + afl - bel$. Rephrase this definition without mentioning the components of
$\omega$, $\mu$, and~$\eta$.
\end{exer}














\section{Closed and Exact Forms}

\begin{defn}Let $U$ be an open subset of a manifold and
 \[ \cdots \to {\bigwedge}^{\!k-1}(U) \to^{d_{k-1}} {\bigwedge}^{\!k}(U) \to^{d_k} {\bigwedge}^{\!k+1}(U) \to \cdots \]
where $d_{k-1}$ and $d_k$ are exterior differentiation operators.  Elements of
$\ker d_k$ are called
 \index{closed!differential form}%
 \index{differential!form!closed}%
 \index{form!closed }%
\df{closed} $k$-forms and elements of $\ran d_{k-1}$ are
 \index{exact!differential form}%
 \index{differential!form!exact}%
 \index{form!exact}%
\df{exact} $k$-forms.  In other words, a $k$-form $\omega$ is \df{closed} if $d\omega = 0$.  It is
\df{exact} if there exists a $(k-1)$-form $\eta$ such that $\omega = d\eta$.
\end{defn}

\begin{prop}\label{prop_exact_implies_closed} Every exact differential form is closed.
\end{prop}

\begin{prop}\label{closed_exact_prop001} If $\omega$ and $\mu$ are closed differential forms, so is $\omega \wedge \mu$.
\end{prop}

\begin{prop} If $\omega$ is an exact form and $\mu$ is a closed form, then $\omega \wedge \mu$ is exact.
\end{prop}

\begin{exam} Let $\phi = (x,y,z)\colon U \sto \R^3$ be a chart on a $3$-manifold and
$\omega = a\,dx + b\,dy + c\,dz$ be a $1$-form on~$U$.  If $\omega$ is exact, then $\pd
cy = \pd bz$, $\pd az = \pd cx$, and $\pd bx = \pd ay$.
\end{exam}

\begin{exer} Determine if each of the following $1$-forms is exact in~$\R^2$.  If it is,
specify the $0$-form of which it is the differential.
   \begin{enumerate}
    \item $ye^{xy}\,dx + xe^{xy}\,dy$;
    \item $x\sin y\,dx + x\cos y\,dy$; and
    \item $\displaystyle \biggl(\frac{\arctan y}{\sqrt{1 - x^2}} + \frac xy +3x^2\biggr)\,dx
            + \biggl(\frac{\arcsin x}{1 + y^2} - \frac{x^2}{2y^2} + e^y\biggr)\,dy$.
   \end{enumerate}
\end{exer}

\begin{exer} Explain why solving the initial value problem
   \[ e^x\cos y + 2x -e^x(\sin y)y' = 0,\qquad y(0) = \pi/3 \]
is essentially the same thing as showing that the $1$-form $(e^x\cos y + 2x)\,dx - e^x(\sin y)\,dy$ is exact. Do it.
\end{exer}

























\section{Poincar\'e's Lemma}

\begin{notn}[for the entire section] Let $n \in \N$, $U$ be a nonempty open convex subset
of $\R^n$, and $x = (x^1, \dots, x^n)$ be the identity map on~$U$.

Whenever $1 \le k \le n$ and $\nu = b\,dx^{i_1} \wedge \dots, \wedge\, dx^{i_k}$ is a
$k$-form on $U$, we define a $0$-form $g^\nu$ on $U$ by
   \[ g^\nu(x) := \int_0^1 b(tx)t^{k-1}\,dt \quad\text{if $\nu \ne 0$} \qquad \text{and}
                 \qquad g^0(x) = 0\,; \]
and we define $(k-1)$-forms $\mu^\nu$ and $h(\nu)$ by
    \[ \mu^\nu := \sum_{j=1}^k (-1)^{j-1} x^{i_j}\,dx^{i_1} \wedge \dots \wedge
                       \wh{dx^{i_j}} \wedge \dots \wedge\,dx^{i_k}  \]
and
   \[ h(\nu) := g^\nu \mu^\nu\,. \]
In the definition of $\mu^\nu$, the circumflex above the term $dx^{i_j}$ indicates that
the term is deleted.  For example, if $k = 3$, then
    \[ \mu^\nu = x^{i_1}\,dx^{i_2} \wedge\,dx^{i_3} - x^{i_2}\,dx^{i_1} \wedge\,dx^{i_3}
                                + x^{i_3}\,dx^{i_1} \wedge\,dx^{i_2}\,. \]
For each $k$ extend $h$ to all of $\bigwedge^{\!k}(U)$ by requiring it to be linear.  Thus
   \[ h^{\sto}\bigl({\bigwedge}^{\!k}(U)\bigr) \subseteq {\bigwedge}^{\!k-1}(U)\,. \]
\end{notn}

\begin{thm}[Poincar\'e's lemma]\label{thm_Poincare} If $U$ is a nonempty open
 \index{Poincar\'e's lemma}%
convex subset of~$\R^n$ and $p \ge 1$, then every closed $p$-form on $U$ is exact.
\end{thm}

\begin{proof}[Hint for proof] Let $p$ be a fixed integer such that $1 \le p \le n$.  Let
$i_1$, \dots, $i_p$ be distinct integers between $1$ and $n$, and let $a$ be a $0$-form
on~$U$. Define
  \begin{align*}
        \beta &:= dx^{i_1} \wedge \dots \wedge dx^{i_p},  \\
       \omega &:= a\beta,
   \end{align*}
and, for $1 \le k \le n$, define
   \[ \eta^k := a_k\,dx^k\wedge \beta. \]
(Here, $a_k = \pd a{x^k}$.) \ns

Now, do the following.
   \begin{enumerate}
    \item[(a)] Show that ${g^\omega}_k(x) = \int_0^1 a_k(tx)t^p\,dt$ for $1 \le k \le n$
and $x \in U$.
    \item[(b)] Show that $\mu^{\eta^k} = x^k\beta - dx^k \wedge \mu^\omega$ for $1 \le k \le n$.
    \item[(c)] Compute $h\bigl(\eta^k\bigr)$ for $1 \le k \le n$.
    \item[(d)] Show that $d\omega = \sum_{k=1}^n \eta^k$.\
    \item[(e)] Compute $h\,d\omega$.
    \item[(f)] Show that $d\bigl(\mu^\omega\bigr) = p\,\beta$.
    \item[(g)] Compute $d(h\omega)$.
    \item[(h)] Compute $\D\frac d{dt}\bigl(t^p\,a(tx)\bigr)$.
    \item[(i)] Show that $p g^\omega + \sum_{k=1}^n {g^\omega}_k x^k = a$.
    \item[(j)] Show that $(dh + hd)(\omega) = \omega$.
   \end{enumerate}     \ns
\end{proof}

\begin{prop} If $\omega$ is a $1$-form on a nonempty convex open subset $U$ of $\R^3$
with $\curl\omega = 0$, then there exists a $0$-form $f$ on $U$ such that $\omega = \grad f$.
\end{prop}

\begin{exer} \emph{Use the proof of Poincar\'e's lemma} to find a $0$-form on $\R^3$ whose gradient is
   \[ (2xyz^3 - y^2z)\,dx + (x^2z^3 - 2xyz)\,dy + (3x^2yz^2 - xy^2)\,dz.\]
\end{exer}

\begin{exer} Consider the $1$-form $\nu = e^z\,dx + x\,dy$ in~$\R^3$ and let $\omega = d\nu$.  \emph{Use the proof of Poincar\'e's lemma} to find another
$1$-form $\eta = a\,dx + b\,dy + c\,dz$ such that $\omega = d\eta$.  Explain carefully what happens at $z = 0$.
Find $\D\pd a{z^n}(0,0,0)$ for every integer $n \ge 0$.
\end{exer}

\begin{prop} If $\omega$ is a $1$-form on a nonempty convex open subset $U$ of $\R^3$
with $\divr\omega = 0$, then there exists a $1$-form $\eta$ on $U$ such that $\omega = \curl\eta$.
\end{prop}

\begin{exer} Let $\omega = 2xyz\,dx + x^3z^2\,dy - yz^2\,dz$.  Check that $\divr\omega =
0$.  \emph{Use the proof of Poincar\'e's lemma} to find a $1$-form $\eta$ whose curl is~$\omega$.
\end{exer}

\begin{prop} Every smooth real valued function $f$ on a nonempty convex open subset $U$
of $\R^3$ is $\divr\eta$ for some $1$-form~$\eta$ on~$U$.
\end{prop}

\begin{exer}  In the proof of the preceding proposition, what needs to be changed if $U$
lies in~$\R^2$?
\end{exer}

\begin{exer} \emph{Use the proof of Poincar\'e's lemma} to find a $1$-form $\eta$ on $\R^3$
whose whose divergence is the function
  \[ f\colon (x,y,z) \mapsto xy - y^2z + xz^3. \]
\end{exer}







\endinput
\chapter{DIFFERENTIAL MANIFOLDS}

The purpose of this chapter and the next is to examine an important and nontrivial example of a Grassmann algebra: the algebra of differential forms on
a differentiable manifold.  If your background includes a study of manifolds, skip this chapter.  If you are pressed for time, reading just the first
section of the chapter should enable you to make sense out of most of the ensuing material.  That section deals with familiar manifolds in low (three or less)
dimensional Euclidean spaces.  The major weakness of this presentation is that it treats manifolds in a non-coordinate-free manner as subsets of some larger
Euclidean space.  (A helix, for example, is a $1$-manifold embedded in $3$-space.)  The rest of the chapter gives a (very brief) introduction to a more
satisfactory coordinate-free way of viewing manifolds.  For a less sketchy view of the subject read one of the many splendid introductory books in the field.
I particularly like~\cite{BishopC:1964} and~\cite{Lee:2003}.






\section{Manifolds in $\R^3$}

A \df{$0$-manifold} is a point (or finite collection of points).

A function is
 \index{smooth!function}%
\df{smooth} if it is infinitely differentiable (that is, if it has derivatives of all orders).

A
 \index{curve}%
\df{curve} is a continuous image of a closed line segment in~$\R$.  If $C$ is a curve, the choice of an interval $[a,b]$ and a continuous
function $f$ such that $C = f^\sto([a,b])$ is a
 \index{curve!parametrization of a}%
 \index{parametrization!of a curve}%
\df{parametrization} of~$C$.  If the function $f$ is smooth, we say that $C$ is a
 \index{smooth!curve}%
 \index{curve!smooth}%
\df{smooth curve}.

A
 \index{1@$1$-manifold}%
 \index{manifold}%
\df{$1$-manifold} is a curve (or finite collection of curves).  A $1$-manifold is
 \index{flat}%
\df{flat}
if it is contained in some line in $\R^3$.  For example, the line segment connecting two points in $\R^3$ is a flat $1$-manifold.

A
 \index{surface}%
\df{surface} is a continuous image of a closed rectangular region in~$\R^2$.  If $S$ is a surface, the the choice of a rectangle
$R = [a_1,b_1] \times [a_2,b_2]$ and a continuous function $f$ such that $S = f^\sto(R)$ is a
 \index{surface!parametrization of a}%
 \index{parametrization!of a surface}%
\df{parametrization} of~$S$. If the function
$f$ is smooth, we say that $S$ is a
 \index{smooth!surface}%
 \index{surface!smooth}%
\df{smooth surface}.

A
 \index{2@$2$-manifold}%
 \index{manifold}%
\df{$2$-manifold} is a surface (or finite collection of surfaces).  A $2$-manifold is
 \index{flat}%
\df{flat} if it is contained in some plane in $\R^3$.  For example, the triangular region connecting the points $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$
is a flat $2$-manifold.

A
 \index{solid}%
\df{solid} is a continuous image of the $3$-dimensional region determined by a closed rectangular parallelepiped (to avoid a six-syllable word many people
say \emph{rectangular solid} or even just \emph{box}) in~$\R^3$.  If $E$ is a solid, then the choice of a rectangular parallelepiped $P = [a_1,b_1] \times
[a_2,b_2] \times [a_3,b_3]$ and a continuous function $f$ such that $E = f^\sto(P)$ is a
 \index{solid!parametrization of a}%
 \index{parametrization!of a solid}%
\df{parametrization} of~$E$. If the function $f$ is smooth, we say that $E$ is a
 \index{smooth!solid}%
 \index{solid!smooth}%
\df{smooth solid}.

A
 \index{3@$3$-manifold}%
 \index{manifold}%
\df{$3$-manifold} is a solid (or finite collection of solids).













\section{Charts, Atlases, and Manifolds}

\begin{defn} Let $M$ and $S$ be sets; $U$, $V \subseteq M$, $\phi\colon U \sto S$, and $\psi\colon V \sto S$ be injective maps. Then the
 \index{composite}%
\df{composite} $\psi \circ \phi^{-1}$, is taken to be the function
  \[ \psi \circ \phi^{-1} \colon \phi^{\sto}(U \cap V) \sto \psi^{\sto}(U \cap V). \]
These composite maps are called, variously,
 \index{overlap maps}%
 \index{transition maps}%
 \index{connecting maps}%
\df{transition maps} or \df{overlap maps} or \df{connecting maps}.
\end{defn}

\begin{prop} The preceding definition makes sense and the composite map $\psi \circ
\phi^{-1}$ is a bijection.
\end{prop}

\begin{defn} Let $m$, $n \in \N$, and $\open U{\R^m}$. A function $F\colon U \sto \R^n$ is
 \index{smooth}%
\df{smooth} (or
 \index{infinitely differentiable}%
 \index{differentiable!infinitely}%
\df{infinitely differentiable}, or
 \index{Cinfinity@$\fml C^\infty$ (smooth)}%
\df{$\fml C^\infty$}) if the differential $d^pF_a$ exists for every $p \in \N$ and every
$a \in U$.  We denote
 \index{Cinfinity@$\fml C^\infty(U,\R^n)$}%
by $\fml C^\infty(U,\R^n)$ the family of all smooth functions from $U$ into~$\R^n$.
\end{defn}

\begin{defn} Let $M$ be a topological space and $n \in \N$.  A pair $(U,\phi)$, where $U$
is an open subset of $M$ and $\phi\colon U \sto \wt U$ is a homeomorphism from $U$ to an
open subset $\wt U$ of~$\R^n$, is called an
 \index{coordinate chart}%
 \index{chart}%
\df{($n$-dimensional coordinate) chart}.  (The notation here is a bit redundant.  If we
know the function $\phi$, then we also know its domain~$U$. Indeed, do not be surprised
to see reference to \emph{the chart}~$\phi$ or to \emph{the chart}~$U$.)

Let $p \in M$.  A chart $(U,\phi)$ is said to \df{contain}~$p$ if $p \in U$ and is said
to be a chart
 \index{centered at}%
\df{(centered) at}~$p$ if $\phi(p) = \vc 0$.  A family of $n$-dimensional coordinate
charts whose domains cover $M$ is an
 \index{atlas}%
\df{($n$-dimensional) atlas} for~$M$.  If such an atlas exists, the space $M$ is said to
be
 \index{locally!Euclidean}%
 \index{Euclidean!locally}%
\df{locally Euclidean}.
\end{defn}

\begin{notn}\label{notn_charts} Let $n \in \N$.  For $1 \le k \le n$ the function
$\pi_k\colon \R^n \sto \R\colon x = (x_1, x_2, \dots, x_n) \mapsto x_k$ is the
 \index{coordinate!projection}%
 \index{projection!coordinate}%
$k^{\text{th}}$ \df{coordinate projection}. If $\phi\colon M \sto \R^n$ is a chart on a
topological space, one might reasonably expect the $n$ component functions of $\phi$
(that is, the functions $\pi_k \circ \phi$) to be called $\phi^1$, \dots, $\phi^n$. But
this is uncommon. People seem to like $\phi$ and $\psi$ as names for charts.  But then
the components of these maps have names such as $x^1$, \dots, $x^n$, or $y^1$, \dots,
$y^n$. Thus we usually end up with something like $\phi(p) = \bigl(x^1(p), \dots,
x^n(p)\bigr)$. The numbers $x^1(p)$, \dots, $x^n(p)$ are called the
 \index{coordinates, local}%
 \index{local!coordinates}%
\df{local coordinates} of~$p$.

Two common exceptions to this notational convention occur in the cases when $n = 2$ or $n
= 3$. In the former case you are likely to see things like $\phi = (x,y)$ and $\psi =
(u,v)$ for charts on $2$-manifolds.  Similarly, for $3$-manifolds expect to see notations
such as $\phi = (x,y,z)$ and $\psi = (u,v,w)$.
\end{notn}

\begin{defn} A second countable Hausdorff topological space $M$ equipped with an
$n$-dimensional atlas is a
 \index{manifold!topological}%
 \index{topological!manifold}%
 \index{nmanifold@$n$-manifold}%
\df{topological $n$-manifold} (or just a \df{topological manifold}).
\end{defn}

\begin{defn} Charts $\phi$ and $\psi$ of a topological $n$-manifold $M$ are said to be
 \index{compatible!charts}%
 \index{smoothly compatible!charts}%
 \index{charts!compatible}%
\df{(smoothly) compatible} if the transition maps $\psi \circ \phi^{-1}$ and $\phi \circ
\psi^{-1}$ are smooth. An atlas on $M$ is a
 \index{smooth!atlas}%
 \index{atlas!smooth}%
\df{smooth atlas} if every pair of its charts is smoothly compatible.  Two atlases on $M$
are
 \index{compatible!atlases}%
 \index{smoothly compatible!atlases}%
 \index{atlases!compatible}%
\df{(smoothly) compatible} (or
 \index{equivalent!atlases}%
 \index{atlases!equivalent}%
\df{equivalent}) if every chart of one atlas is smoothly compatible with every chart of
the second; that is, if their union is a smooth atlas on~$M$.
\end{defn}

\begin{prop} Every smooth atlas on a topological manifold is contained in a unique maximal
smooth atlas.
\end{prop}

\begin{defn} A maximal smooth atlas on a topological manifold $M$ is a
 \index{differential!structure}%
 \index{structure!differential}%
\df{differential structure} on~$M$. A topological $n$-manifold which has been given a
differential structure is a
 \index{smooth!manifold}%
 \index{manifold!smooth}%
\df{smooth $n$-manifold} (or a
 \index{manifold!differential}%
 \index{differential!manifold}%
\df{differential $n$-manifold}, or a
 \index{Cinfinity@$\fml C^\infty$ (smooth)!-manifold}%
\df{$\fml C^\infty$ $n$-manifold}).
\end{defn}

\noindent\textbf{NOTE:}\label{note_man_smooth} From now on we will be concerned only with
differential manifolds; so the modifier ``smooth'' will ordinarily be omitted when we
refer to charts, to atlases, or to manifolds.  Thus it will be understood that
 \index{conventions!all manifolds are smooth}%
 \index{conventions!all charts belong to the differential structure}%
by \emph{manifold} we mean a topological $n$-manifold (for some fixed~$n$) equipped with
a differential structure to which all the charts we mention belong.

\begin{exam} Let $U$ be an open subset of $\R^n$ for some $n \in \Z^+$ and $\iota\colon U
\sto \R^n\colon x \mapsto x$ be the inclusion map.  Then $\{\iota\}$ is a smooth atlas
for~$U$.  We make the convention that when an open subset of $\R^n$ is regarded as an
$n$-manifold we will suppose, unless the contrary is explicitly stated,
 \index{conventions!open subsets of $\R^n$ are manifolds in a canonical fashion}%
that the inclusion map $\iota$ is a chart in its differentiable structure.
\end{exam}

\begin{exam}[An atlas for $\Sp^1$] Let $\Sp^1 = \{(x,y) \in \R^2\colon x^2 + y^2 = 1 \}$
 \index{S1@$\Sp^1$ (unit circle)}%
be the unit circle in $\R^2$ and $U = \{(x,y) \in \Sp^1\colon y \ne -1 \}$.  Define $\phi
\colon U \sto \R$ to be the projection of points in $U$ from the point $(0,-1)$ onto the
$x$-axis; that is, if $p = (x,y)$ is a point in $U$, then $\bigl(\phi(p),0\bigr)$ is the
unique point on the $x$-axis which is collinear with both $(0,-1)$ and $(x,y)$.
 \begin{enumerate}
    \item Find an explicit formula for $\phi$.
    \item Let $V = \{(x,y)\in \Sp^1\colon y \ne 1 \}$. Find an explicit formula for the
projection $\psi$ of points in $V$ from $(0,1)$ onto the $x$-axis.
    \item The maps $\phi$ and $\psi$ are bijections between $U$ and $V$, respectively,
and~$\R$.
    \item The set $\{\phi,\psi\}$ is a (smooth) atlas for~$\Sp^1$.
 \end{enumerate}
\end{exam}

\begin{exam}[An atlas for the $n$-sphere]\label{exam_n_sphere} The previous example
can be generalized
 \index{S1n@$\Sp^n$ ($n$-sphere)}%
to the $n$-sphere $\Sp^n = \{ x \in \R^{n+1}\colon \norm x = 1\}$. The generalization of
the mapping $\phi$ is called the
 \index{stereographic projection}%
\df{stereographic projection from the south pole} and the generalization of $\psi$ is the
\df{stereographic projection from the north pole}. (Find a simple expression for the
transition maps.)
\end{exam}

\begin{exam}[Another atlas for $\Sp^1$] Let $U = \{(x,y)\in \Sp^1\colon x \ne 1 \}$. For
$(x,y) \in U$ let $\phi(x,y)$ be the angle (measured counterclockwise) at the origin from
$(1,0)$ to $(x,y)$. (So $\phi(x,y) \in (0,2\pi)$.)  Let $V = \{(x,y)\in \Sp^1\colon y \ne
1 \}$. For $(x,y) \in V$ let $\psi(x,y)$ be $\pi/2$ plus the angle (measured
counterclockwise) at the origin from $(0,1)$ to $(x,y)$. (So $\psi(x,y) \in
(\pi/2,5\pi/2)$.) Then $\{\phi,\psi\}$ is a (smooth) atlas for~$\Sp^1$.
\end{exam}

\begin{exam}[The projective plane $\Po^2$]\label{exam_proj_plane} Let
 \index{projective plane}%
 \index{p2@$\Po^2$ (projective plane)}%
$\Po^2$ be the set of all lines through the origin in~$\R^3$. Such a line is determined
by a nonzero vector lying on the line. Two nonzero vectors $x = (x^1,x^2,x^3)$ and $y=
(y^1,y^2,y^3)$ determine the same line if there exists $\alpha \in \R$, $\alpha \ne 0$,
such that $y = \alpha x$.  In this case we write $x \sim y$. It is clear that $\sim$ is
an equivalence relation.  We regard a member of $\Po^2$ as an equivalence class of
nonzero vectors. Let $U_k = \{\,\,[x] \in \Po^2 \colon x^k \ne 0\}$ for $k = 1,2,3$. Also
let
  \begin{align*}
       \phi &\colon U_1 \sto \R^2 \colon [(x,y,z)]
                         \mapsto \bigl(\tfrac yx, \tfrac zx\bigr); \\
       \psi &\colon U_2 \sto \R^2 \colon [(x,y,z)]
                         \mapsto \bigl(\tfrac xy, \tfrac zy\bigr);\text{ and} \\
       \eta &\colon U_3 \sto \R^2 \colon [(x,y,z)]
                         \mapsto \bigl(\tfrac xz, \tfrac yz\bigr).
  \end{align*}
The preceding sets and maps are well defined; and $\{ \phi, \psi, \eta\}$ is a (smooth)
atlas for~$\Po^2$.
\end{exam}

\begin{exam}[The general linear group] Let $G = GL(n,\R)$ be the group of nonsingular $n
\times n$ matrices of real numbers. If $\vc a = [a_{jk}]$ and $\vc b = [b_{jk}]$ are
members of $G$ define
   \[d(\vc a,\vc b) = \biggl[ \sum_{j,k = 1}^n (a_{jk} - b_{jk})^2\biggr]^\frac12.\]
The function $d$ is a metric on~$G$.  Define
   \[ \phi\colon G \sto \R^{n^2} \colon \vc a = [a_{jk}] \mapsto \bigl(a_{11}, \dots,
            a_{1n},a_{21}, \dots, a_{2n}, \dots, a_{n1}, \dots, a_{nn}\bigr). \]
Then $\{\phi\}$ is a (smooth) atlas on~$G$.  (Be a little careful here.  There is one
point that is not completely obvious.)
\end{exam}

\begin{exam} Let
   \begin{align*}
           I &\colon \R \sto \R \colon x \mapsto x\,, \\
           a &\colon \R \sto \R \colon x \mapsto \arctan x\,, \text{ and} \\
           c &\colon \R \sto \R \colon x \mapsto x^3.
   \end{align*}
Each of $\{I\}$, $\{a\}$, and $\{c\}$ is a (smooth) atlas for~$\R$. Which of these are
equivalent?
\end{exam}














\section{Differentiable Functions Between Manifolds}

\begin{defn} A function $F\colon M \sto N$ between (smooth) manifolds is \df{smooth at}
a point $m \in M$ if there exist charts $(U,\phi)$ containing $m$ and $(V,\psi)$
containing $F(m)$ such that $F^{\sto}(U) \subseteq V$ and the
 \index{local!representative}%
 \index{representative, local}%
 \index{Fpsiphi@$F_{\psi\phi}$, $F_\phi$ (local representatives)}%
\df{local representative} $F_{\psi\phi} := \psi \circ F \circ \phi^{-1}\colon
\phi^{\sto}(U) \sto \psi^{\sto}(V)$ is smooth at~$\phi(m)$. The map $F$ is
 \index{smooth!map between manifolds}%
\df{smooth} if it is smooth at every $m \in M$.

In the case that $N$ is a subset of some Euclidean space $\R^n$, it is the usual
convention to use the inclusion mapping $\iota\colon N \sto \R^n$ as the preferred chart
on~$N$. In this case the local representative of $F$ is written as $F_\phi$ rather
than~$F_{\iota\phi}$.
\end{defn}

\noindent\textbf{NOTE:} It now makes sense to say (and is true) that a single chart on a
manifold is a smooth map.

\begin{prop} Suppose that a map $F\colon M\sto N$ between manifolds is smooth at a
point~$m$ and that $(W,\mu)$ and $(X,\nu)$ are charts at $m$ and $F(m)$, respectively,
such that $F^{\sto}(W) \subseteq X$.  Then $\nu \circ F \circ \mu^{-1}$ is smooth
at~$\mu(m)$.
\end{prop}

\begin{prop} If $F\colon M \sto N$ and $G\colon N \sto P$ are smooth maps between
manifolds, then $G \circ F$ is smooth.
\end{prop}

\begin{prop} Every smooth map $F\colon M \sto N$ between manifolds is continuous.
\end{prop}

\begin{exam}\label{exam_map_s2_p2} Consider the $2$-manifold $\Sp^2$ with the
differential structure generated by the stereographic projections form the north and
south poles (see example~\ref{exam_n_sphere}) and the $2$-manifold $\Po^2$ with the
differentiable structure generated by the atlas given in example~\ref{exam_proj_plane}.
The map $F\colon \Sp^2 \sto \Po^2\colon (x,y,z) \mapsto [(x,y,z)]$ is smooth. (Think of
$F$ as taking a point $(x,y,z)$ in $\Sp^2$ to the line in $\R^3$ passing through this
point and the origin.)
\end{exam}
















\section{The Geometric Tangent Space}

\begin{defn} Let $J \subseteq \R$ be an interval with nonempty interior and $M$ be a
manifold.  A smooth function $c\colon J \sto M$ is a
 \index{curve}%
\df{curve} in~$M$. If, in addition, $0 \in J^\circ$ and $c(0) = m \in M$, then we say
that $c$ is a
 \index{curve!at a point}%
\df{curve at}~$m$.
\end{defn}

\begin{exam} The function $b\colon t \mapsto t^2$ is a curve at $0$ in~$\R$.  Sketch
the range of~$b$.
\end{exam}

The next example illustrates the point that the ranges of smooth curves may not ``look
smooth''.

\begin{exam} The function $c\colon t \mapsto (\cos^3t, \sin^3t)$ is a curve at $(1,0)$
in~$\R^2$.  Sketch its range.
\end{exam}

Let $V$ and $W$ be normed linear spaces. Recall that a $W$-valued function $f$ defined on
some neighborhood of $0$ in~$V$ is said to belong to the
 \index{oh@$\lobo o(V,W)$ (``little-oh'' functions)}%
family $\lobo o(V,W)$ provided that for every $\epsilon > 0$ there exists $\delta > 0$
such that $\norm{f(x)} \le \epsilon \norm x$ whenever $\norm x \le \delta$.  Recall also
that $W$-valued functions $f$ and $g$, each defined on a neighborhood of $0$ in~$V$, are
said to be
 \index{tangent!at $0$}%
\df{tangent} (at~$0$) if $f - g \in \lobo o(V,W)$.  In this case we
 \index{<relation@$f \simeq g$ ($f$ is tangent to~$g$)}%
write $f \simeq g$.

Thus in the special case when $b$ and $c$ are curves at a point $w$ in a normed linear
space~$W$, we \emph{should} say that $b$ and $c$ are tangent \emph{at}~$0$ if $b - c \in
\lobo o(\R,W)$. As a matter of fact, it is almost universal custom in this situation to
say that $b$ and $c$ are tangent \emph{at}~$w$.  This use of ``at~$w$'' for ``at~$0$''
results from a tendency to picture curves in terms of their ranges.  (For example, asked
to visualize the curve $c\colon t \mapsto (\cos t, \sin t)$, most people see a circle. Of
course, the circle is only the range of $c$ and not its graph, which is a helix
in~$\R^3$.)  We will follow convention and say that the curves $b$ and $c$ are tangent
\emph{at}~$w$.  This convention will also apply to curves in manifolds.

\begin{exam} It is important to note that it cannot be determined whether two curves are
tangent just by looking at their ranges.  The curves $b\colon t \mapsto (t,t^2)$ and
$c\colon t \mapsto (2t,4t^2)$ have identical ranges; they both follow the parabola $y =
x^2$ in~$\R^2$ (and in the same direction).  They are both curves at the origin.
Nevertheless, they are \emph{not tangent} at the origin.
\end{exam}

Definition~\ref{def_tan_curve} below says that curves in a manifold are \emph{tangent} if
their composites with a chart $\phi$ are tangent in the sense described above.  Before
formally stating this definition it is good to know that tangency thus defined does not
depend on the particular chart chosen.

\begin{prop} Let $m$ be a point in a manifold and $b$ and $c$ be curves at~$m$.  If $\phi
\circ b \simeq \phi \circ c$ for \emph{some} chart $\phi$ centered at~$m$, then $\psi
\circ b \simeq \psi \circ c$ for \emph{every} chart $\psi$ centered at~$m$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use proposition 25.4.7 and problem 25.4.10 in~\cite{Erdman:2005}.   \ns
\end{proof}

\begin{defn}\label{def_tan_curve} Let $m$ be a point in a manifold and $b$ and $c$ be
curves at~$m$.  Then $b$ and $c$ are
 \index{tangent!at $m$}%
 \index{<relation@$f \simeq g$ ($f$ is tangent to~$g$)}%
\df{tangent at}~$m$ (we write $b \simeq c$) if $\phi \circ b \simeq \phi \circ c$ for
some (hence all) charts $\phi$ centered at~$m$.
\end{defn}

It is useful to know that smooth mappings between manifolds preserve tangency.

\begin{prop}\label{prop_smooth_prsv_tan} If $F \colon M \sto N$ is a smooth mapping
between manifolds and $b$ and $c$ are curves tangent at a point $m \in M$, then the
curves $F \circ b$ and $F \circ c$ are tangent at~$F(m)$ in~$N$.
\end{prop}

\begin{defn} Since the family of ``little-oh'' functions is closed under addition it is
obvious that tangency at a point $m$ is an equivalence relation on the family of curves
at~$m$.  We denote the equivalence class containing the curve $c$
 \index{cm@$\wt c_m$ (tangency class of curves)}%
by $\wt c$ or, if we wish to emphasize the role of the point $m$, by $\wt c_m$.  Each
equivalence class $\wt c_m$ is a
 \index{geometric!tangent vector}%
 \index{vector!geometric tangent}%
 \index{tangent!vector!geometric}%
\df{geometric tangent vector} at~$m$ and the family of all such vectors is the
 \index{geometric!tangent space}%
 \index{space!geometric tangent}%
 \index{tangent!space!geometric}%
\df{geometric tangent space} at~$m$.  The geometric tangent space at $m$ is denoted
 \index{TmM@$\wt T_m(M)$ (geometric tangent space at~$m$)}%
by~$\wt T_m$ (or, if we wish to emphasize the role of the manifold~$M$, by~$\wt T_m(M)$).
\end{defn}

The language (involving the words ``vector'' and ``space'') in the preceding definition
is highly optimistic.  So far we have a \emph{set} of equivalence classes---with no
vector space structure.  The key to providing $\wt T_m$ with a such a structure is exercise~\ref{inv_lin_maps007}.
There we found that a set $S$ may be given a vector space structure by transferring the structure from a known vector space $V$ to the set $S$  by
means of a bijection $f\colon S \sto V$.  We will show, in particular, that if $M$ is an $n$-manifold, then for each $m \in M$ the geometric tangent
space $\wt T_m$ can be given the vector space structure of~$\R^n$.

\begin{defn} Let $\phi$ be a chart containing a point $m$ in an $n$-manifold~$M$ and let
$\vc u$ be a nonzero vector in~$\R^n$.  For every $t \in \R$ such that $\phi(m) + t\vc u$
belongs to the range of $\phi$, let
   \[ c_{\vc u}(t) = \phi^{-1}\bigl(\phi(m) + t\vc u\bigr). \]
Notice that since $c_{\vc u}$ is the composite of smooth functions and since $c_{\vc
u}(0) = m$, it is clear that $c_{\vc u}$ is a curve at $m$ in~$M$.
\end{defn}

\begin{exam}\label{exam_basis_curves} If $M$ is an $n$-manifold, then the curves
$c_{\vc e^1}, \dots, c_{\vc e^n}$ obtained by means of the preceding definition from the
standard basis vectors $\vc e^1, \dots, \vc e^n$ of $\R^n$ will prove to be very useful.
We will shorten the notation somewhat and write $\vc c_k$ for $c_{\vc e^k}$ ($1 \le k \le
n$).  We think of the curves $c_1, \dots, c_n$ as being ``linearly independent directions
in the tangent space at~$m$'' (see proposition~\ref{prop_basis_curves_basis}).  We call
these curves the
 \index{basis!curves at a point}%
 \index{standard!basis curves at a point}%
 \index{curve!standard basis}%
\emph{standard basis curves at $m$ determined by $\phi$}.  It is important to keep in
mind that these curves depend on the choice of the chart~$\phi$; the notation $c_1,
\dots, c_n$ fails to remind us of this.
\end{exam}

\begin{prop}\label{prop_map_C_phi} Let $\phi$ be a chart at a point $m$ in an
 \index{Cphi@$C_\phi$ (map from $\wt T_m$ to $\R^n$)}%
$n$-manifold.  Then the map
   \[ C_\phi\colon \wt T_m \sto \R^n \colon \wt c \mapsto D(\phi \circ c)(0) \]
is well-defined and bijective.
\end{prop}

Notice that initially we had no way of ``adding'' curves $b$ and $c$ at a point $m$ in a
manifold or of ``multiplying'' them by scalars.  Now, however, we can use the bijection
$C_\phi$ to transfer the vector space structure from $\R^n$ to the tangent space~$\wt
T_m$. Thus we add equivalence classes $\wt b$ and~$\wt c$ in the obvious fashion. The
formula is
 \begin{equation}
    \wt b + \wt c = {C_\phi}^{-1}\bigl(C_\phi(\wt b) + C_\phi(\wt c)\bigr).
 \end{equation}
Similarly, if $b$ is a curve at $m$ and $\alpha$ is a scalar, then
 \begin{equation}
    \alpha \wt b = {C_\phi}^{-1}\bigl(\alpha C_\phi(\wt b)\bigr).
 \end{equation}

\begin{cor} At every point $m$ in an $n$-manifold the geometric tangent space $\wt T_m$
may be regarded as a vector space isomorphic to~$\R^n$.
\end{cor}

As remarked previously, we have defined the vector space structure of the geometric
tangent space at $m$ in terms of the mapping $C_\phi$, which in turn depends on the
choice of a particular chart~$\phi$.  From this it might appear that addition and scalar
multiplication on the tangent space depend on~$\phi$.  Happily this is not so.

\begin{prop} Let $m$ be a point in an $n$-manifold.  The vector space structure of the
geometric tangent space $\wt T_m$ is independent of the particular chart $\phi$ used to
define it.
\end{prop}

\begin{prop}\label{prop_basis_curves_basis} Let $\phi$ be a chart containing a point
$m$ in an $n$-manifold. Let $\vc c_1$, \dots, $\vc c_n$ be the standard basis curves
determined by~$\phi$ (see example~\ref{exam_basis_curves}).  Then $\{ \wt{\vc c}_1,
\dots, \wt{\vc c}_n\}$ is a basis for the tangent space~$\wt T_m$.
\end{prop}

\begin{exer} We know from the preceding proposition that if $\wt c$ belongs to $\wt T_m$,
then there exist scalars $\alpha_1, \dots, \alpha_n$ such that $\wt c = \sum_{k=1}^n
\alpha_k \wt{\vc c}_k$.  Find these scalars.
\end{exer}

If $F\colon M \sto N$ is a smooth mapping between manifolds and $m$ is a point in~$M$, we
denote by $\wt dF_m$ the mapping that takes each geometric tangent vector $\wt c$ in $\wt
T_m$ to the corresponding geometric tangent vector $(F \circ c)^\sim$ in~$\wt T_{F(m)}$.
That $\wt dF_m$ is well-defined is clear from proposition~\ref{prop_smooth_prsv_tan}. The
point of proposition~\ref{prop_dFm_notn_plaus} below is to make the notation for this
particular map plausible.

\begin{defn}\label{def_geom_dffrntl} If $F\colon M \sto N$ is a smooth mapping between
manifolds and $m \in M$, then the function
  \[ \wt dF_m\colon \wt Tm \sto \wt T_{F(m)}\colon \wt c \mapsto (F \circ c)^\sim \]
 \index{dFm@$\wt dF_m$ (differential of $f$ at~$m$)}%
 \index{differential!of a smooth map between manifolds}%
is the \df{differential} of $F$ at~$m$.
\end{defn}

\begin{prop}\label{prop_dFm_notn_plaus} Let $F\colon M\sto N$ be a smooth map from an
$n$-manifold $M$ to a $p$-manifold~$N$. For every $m \in M$ and every pair of charts
$\phi$ at $m$ and $\psi$ at~$F(m)$ the following diagram commutes
  \[ \xy
          \Square[\wt T_m`\wt T_{F(m)}`\R^n`\R^p;\wt dF_m`C_\phi`C_\psi`
                    d\bigl(F_{\psi\phi}\bigr)_{\phi(m)}]
      \endxy \]
and consequently $\wt dF_m$ is a linear map. \emph{(The maps $C_\phi$ and $C_\psi$ are
defined as in proposition~\ref{prop_map_C_phi}.)}
\end{prop}

The map $C_\phi$ has been used to provide the geometric tangent space $\wt T_m$ at a
point $m$ on an $n$-manifold with the vector space structure of~$\R^n$. With equal ease
it may be used to provide $\wt T_m$ with a norm. By defining
  \[ \norm{\wt c} = \norm{C_\phi(\wt c)} \]
it is clear that we have made $\wt T_m$ into a normed linear space (but in a way that
\emph{does} depend on the choice of the chart~$\phi$). Furthermore, under this definition
$C_\phi$ is an isometric isomorphism between $\wt T_m$ and~$\R^n$.  Thus, in particular,
we may regard $T_m$ and $T_{F(m)}$ as Euclidean spaces.  From the preceding proposition
we see that the map $\wt dF_m$ is (continuous and) linear, being the composite of the
mapping $d\bigl(F_{\phi\psi}\bigr)_{\phi(m)}$ with two isometric isomorphisms
$\bigl(C_\psi\bigr)^{-1}$ and $C_\phi$.

If we use the mapping $C_\phi$ to identify $\wt T_m$ and $\R^n$ as Banach spaces and,
similarly, $C_\psi$ to identify $\wt T_{F(m)}$ and $\R^p$, then the continuous linear
maps $\wt dF_m$ and $d\bigl(F_{\phi\psi}\bigr)_{\phi(m)}$ are also identified.  The
notation which we have used for the mapping $\wt dF_m$ is thus a reasonable one since, as
we have just seen, this mapping can be identified  with the differential at $\phi(m)$ of
the local representative of $F$ and since its \emph{definition} does not depend on the
charts $\phi$ and~$\psi$.  To further strengthen the case for the plausibility of this
notation. consider what happens if $M$ and $N$ are open subsets of $\R^n$ and $\R^p$,
respectively (regarded as manifolds whose differential structure is generated in each
case by the appropriate identity map).  The corresponding local representative of $F$ is
$F$ itself, in which case the bottom map in the diagram for
proposition~\ref{prop_dFm_notn_plaus} is simply~$dF_m$.

The preceding definition also helps to justify the usual intuitive picture for familiar
$n$-manifolds of the tangent space being a copy of $\R^n$ placed at~$m$.  Suppose that
for a particular $n$-manifold $M$ there exists a smooth inclusion map of $M$ into a
higher dimensional Euclidean space~$\R^p$.  (For example, the inclusion map of the
$2$-manifold $\Sp^2$ into $\R^3$ is smooth.)  Then taking $F$ in the preceding discussion
to be this inclusion map, the function $\wt dF_m$ maps $\wt T_m$ into a subspace of $\wt
T_{F(m)} = \R^p$.  Picture this subspace being translated to the point $m = F(m)$
in~$\R^p$.  To see how this works it is best to work through the details of some
examples.

\begin{exam} Let $U = \{(x,y,z) \in \Sp^2\colon x > 0\}$.  The map $\phi\colon U \sto
\R^2\colon (x,y,z) \mapsto (y,z)$ is a chart for the $2$-manifold~$\Sp^2$. Let
$\iota\colon \Sp^2 \sto \R^3$ be the inclusion map of $\Sp^2$ into~$\R^3$ and let $m =
\bigl(\frac 12, \frac 1{\sqrt 2}, \frac 12\bigr) \in \Sp^2$.  Then the range of
$d\bigl(\iota_\phi\bigr)_{\phi(m)}$ is the plane in $\R^3$ whose equation is $x + \sqrt
2y + z = 0$.  If we translate this plane to the point $m$, we obtain the plane whose
equation is $x + \sqrt 2y + z = 2$, which is exactly the result we obtain using
techniques of beginning calculus.to ``find the equation of the tangent plane to the
surface $x^2 + y^2 + z^2 = 1$ at the point $\bigl(\frac 12, \frac 1{\sqrt 2}, \frac
12\bigr)$''.
\end{exam}

\begin{exam} Same example as the preceding except this time let $U = \{ (x,y,z) \in \Sp^2
\colon z~\ne~-1\}$ and $\phi\colon U \sto \R^2$ be the stereographic projection of
$\Sp^2$ from the south pole.  That is,
  \[ \phi(x,y,z) = \biggl(\frac x{1+z}\,, \frac y{1+z}\biggr). \]
\end{exam}

\begin{prop}[A Chain Rule for Maps Between Manifolds]\label{prop_cr_manif} If
$F\colon M \sto N$ and
 \index{chain rule}%
$G\colon N \sto P$ are smooth mappings between manifolds, then
   \begin{equation}\label{eqn_cr_manif}
     \wt d(G \circ F)_m = \wt dG_{F(m)} \circ \wt dF_m
   \end{equation}
for every $m \in M$.
\end{prop}














\section{The Algebraic Tangent Space}

There is another way of looking at the tangent space at a point $m$ in a manifold.
Instead of regarding tangent vectors as ``directions'' determined by equivalence classes
of curves at~$m$, we will now consider them to be ``directional derivatives'' of (germs
of) smooth functions defined in neighborhoods of~$m$.

\begin{defn}\label{def_smooth_rv_fcn} Let $m$ be a point in a manifold $M$ and $f$,
$g \in \fml C_m^\infty(M,\R)$
 \index{Cinfinitym@$\fml C_m^\infty(M,\R)$ (smooth functions defined in a neighborhood of~$m$)}%
(the family of all smooth real valued functions defined on a neighborhood of~$m$). We
 \index{<binary relation@$f \sim g$ (functions agree on some neighborhood)}%
write $f \sim g$ if there exists a neighborhood of $m$ on which $f$ and $g$ agree.  Then
$\sim$ is clearly an equivalence relation on $\fml C_m^\infty(M,\R)$.  The corresponding
equivalence classes are
 \index{germs}%
\df{germs} of smooth functions at~$m$.  If $f$ is a member of $\fml C_m^\infty(M,\R)$, we
denote the germ containing $f$
 \index{f@$\wh f$ (germ containing~$f$)}%
by~$\wh f$. The family of all germs of smooth real valued functions st $m$ is denoted
 \index{GmM@$\fml G_m(M)$ (germs of functions at~$m$)}%
by $\fml G_m(M)$ (or just $\fml G_m$).  Addition, multiplication, and scalar
multiplication of germs are defined as you would expect.  For $\wh f$, $\wh g \in \fml
G_m$ and $\alpha \in \R$ let
   \begin{align*}
       \wh f + \wh g &= (f + g)\wh{\phantom J} \\
         \wh f\,\, \wh g &= (fg)\wh{\phantom J} \\
         \alpha\wh f &= (\alpha f)\wh{\phantom J}
   \end{align*}
(As usual, the domain of $f + g$ and $fg$ is taken to be $\dom f \cap \dom g$.)
\end{defn}

\begin{prop} If $m$ is a point in a manifold, then the set $\fml G_m$ of germs of smooth
functions at $m$ is (under the operations defined above) a unital commutative algebra.
\end{prop}

\begin{defn}\label{def_alg_tan_sp} Let $m$ be a point in a manifold.  A
 \index{derivation}%
\df{derivation} on the algebra $\fml G_m$ of germs of smooth functions at $m$ is a linear
functional $v \in \fml G^*_m$ which satisfies
 \index{Leibniz's rule}%
\emph{Leibniz's rule}
   \[ v(\wh f\,\, \wh g) = f(m)v(\wh g) + v(\wh f) g(m) \]
for all $\wh f$, $\wh g \in \fml G_m$. Another name for a derivation on the algebra $\fml
G_m$ is an
 \index{algebraic!tangent vector}%
 \index{tangent!vector!algebraic}%
 \index{vector!algebraic tangent}%
\df{algebraic tangent vector} at~$m$.  The set of all algebraic tangent vectors at $m$
(that is, derivations on~$\fml G_m$) will be called the
 \index{algebraic!tangent space}%
 \index{tangent!space!algebraic}%
 \index{space!algebraic tangent}%
\df{algebraic tangent space} at~$m$ and will be denoted
 \index{TmM@$\wh T_m(M)$ (algebraic tangent space at~$m$)}%
by $\wh T_m(M)$ (or just $\wh T_m$).

The idea here is to bring to manifolds the concept of \emph{directional derivative}; as
the next example shows directional derivatives at points in a normed linear space are
derivations on the algebra of (germs of) smooth functions.
\end{defn}

\begin{exam} Let $V$ be a normed linear space and let $a$ and $v$ be vectors in~$V$.
Define $D_{v,a}$ on functions $f$ in $\fml G_a(V)$ by
   \[ D_{v,a}(\wh f) := D_vf(a). \]
(Here, $D_vf(a)$ is the usual directional derivative of $f$ at $a$ in the direction
of~$v$ from beginning calculus.) Then $D_{v,a}$ is well-defined and is a derivation
on~$\fml G_a$.  (Think of the operator $D_{v,a}$ as being ``differentiation in the
direction of the vector $v$ followed by evaluation at~$a$.'')
\end{exam}

\begin{prop} If $m$ is a point in a manifold, $v$ is a derivation on $\fml G_m$, and $k$
is a smooth real valued function constant in some neighborhood of $m$, then $v(\wh k) =
0$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Recall that $1 \cdot 1 = 1$. \ns
\end{proof}

It is an easy matter to show that the terminology ``algebraic tangent space'' adopted in
definition~\ref{def_alg_tan_sp} is not overly optimistic.  It \emph{is} a vector space.

\begin{prop} If $m$ is a point in a manifold, then the tangent space $\wh T_m$ is a
vector space under the usual pointwise definition of addition and scalar multiplication.
\end{prop}

We now look at an example which establishes a first connection between the geometric and
the algebraic tangent spaces.

 \begin{exam}\label{exam_gtv_to_atv} Let $m$ be a point in a manifold $M$ and $\wt c$ be
a vector in the geometric tangent space~$\wt T_m$.  Define
   \[ v_{\wt c}\colon \fml G_m \sto \R\colon \wh f \mapsto D(f \circ c)(0).\]
Then $v_{\wt c}$ is well-defined and belongs to the algebraic tangent space~$\wh T_m$.
 \end{exam}

The notation $v_{\wt c}(\wh f)$ of the preceding example is not particularly attractive.
In the following material we will ordinarily write just~$v_c(f)$.  Although strictly
speaking this is incorrect, it should not lead to confusion.  We have shown that $v_{\wt
c}(\wh f)$ depends \emph{only} on the equivalence classes $\wt c$ and $\wh f$, not on the
representatives chosen.  Thus we do not distinguish between $v_b(g)$ and $v_c(f)$
provided that $b$ and $c$ belong to the same member of $\wt T_m$ and $f$ and $g$ to the
same germ.

Example~\ref{exam_gtv_to_atv} is considerably more general than it may at first appear.
Later in this section we will show that the association $\wt c \mapsto v_{\wt c}$ is an
isomorphism between the tangent spaces $\wt T_m$ and~$\wh T_m$.  In particular, there are
no derivations on $\fml G_m$ other than those induced by curves at~$m$.

For the moment, however, we wish to make plausible for an $n$-manifold the use of the
notation $\pd{}{x^k}\bigr|_m$ for the derivation $v_{\vc c_k}$, where $\vc c_k$ is the
$k^{\text{th}}$ standard basis curve at the point $m$ determined by the chart $\phi =
(x^1, \dots, x^n)$ (see the notational convention in~\ref{notn_charts} and
example~\ref{exam_basis_curves}).  The crux of the matter is the following proposition,
which says that if $c_{\vc u} = \phi^{-1} \circ b_{\vc u}$, where $\phi$ is a chart at
$m$ and $b_{\vc u}$ is the parametrized line through $\phi(m)$ in the direction of the
vector $\vc u$, then the value at a germ $\wh f$ of the derivation $v_{c_{\vc u}}$ may be
found by taking the directional derivative in the direction $\vc u$ of the local
representative~$f_\phi$.

\begin{prop} Let $\phi$ be a chart containing the point $m$ in an $n$-manifold.  If
$\vc u \in \R^n$ and $b_{\vc u}\colon \R~\sto~\R^n\colon \\ t \mapsto \phi(m) + t\vc u$,
then $c_{\vc u} := \phi^{-1} \circ b_{\vc u}$ is a curve at $m$ and
   \[ v_{c_{\vc u}}(f) = D_{\vc u}(f_\phi)(\phi(m)) \]
for all $\wh f \in \fml G(m)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use proposition 25.5.9 in~\cite{Erdman:2005}.
%proposition~\ref{prop_dd_diff}%
\end{proof}

\begin{defn} In proposition~\ref{prop_basis_curves_basis} we saw that if $\phi$ is a
chart containing a point $m$ in an $n$-manifold, then the vectors $\wt{\vc c}_1,\dots,
\wt{\vc c}_n$ form a basis for the geometric tangent space~$\wt T_m$.  We call these
vectors the
 \index{basis!for the geometric tangent space}%
\df{basis vectors of $\wt T_m$ determined by~$\phi$}.
\end{defn}

\begin{cor}\label{cor_derivation_partial} Let $\phi$ be a chart containing the point
$m$ in an $n$-manifold.  If $\wt{\vc c}_1,\dots, \wt{\vc c}_n$ are the basis vectors for
$\wt T_m$ determined by $\phi$, then
  \begin{equation}\label{eqn_derivation_partial}
      v_{\vc c_k}(f) = \big(f_\phi\bigr)_k(\phi(m))
  \end{equation}
for every $\wh f$ in $\fml G_m$. \emph{(The subscript $k$ on the right side of the
equation indicates differentiation; $\big(f_\phi\bigr)_k$ is the $k^{\text{th}}$ partial
derivative of the local representative~$f_\phi$, in another notation the right hand side
is $\pd{f_\phi}{x^k} (\phi(m)$.)}
\end{cor}

\begin{notn}\label{notn_derivation_partial} The preceding corollary says that (in an
$n$-manifold~$M$) the action of the derivation $v_{\vc c_k}$ on a function $f$
(technically, on the germ~$\wh f$) is that of partial differentiation of the local
representative $f_\phi$ followed by evaluation at the point~$\phi(m)$.  In particular, if
$M$ happens to be an open subset of $\R^n$, then \eqref{eqn_derivation_partial} becomes
  \[ v_{\vc c_k}(f) = f_k(m) \]
so that the value of $v_{\vc c_k}$ at $f$ \emph{is} the $k^{\text{th}}$ partial
derivative of $f$ (evaluated at~$m$).  It is helpful for the notation to remind us of
this fact; so in the following material we will usually write $\pd{}{x^k}\bigr|_m$ for
$v_{\vc c_k}$, where $1 \le k \le n$ and $\phi = (x^1, \dots, x^n)$ is the chart
containing $m$ in terms of which the curves $\vc c_k$ are defined.  The value of this
derivation at $f$ will be denoted by $\pd f{x^k}(m)$. Thus
  \[ \pd f{x^k}(m) = \pd{}{x^k}\biggr|_m(f) = v_{\vc c_k}(f)
                                       = v_{\wt{\vc c}_k}(\wh f\,\,). \]
\end{notn}

Let $\phi = (x_1, \dots, x^n)$ and $\psi = (y^1, \dots, y^n)$ be charts containing a
point $m$ on an $n$-manifold.  It is perhaps tempting, but utterly wrong, to believe that
if $x^k = y^k$ for some $k$, then $\pd{}{x^k}\bigr|_m = \pd{}{y^k}\bigr|_m$. The formula
  \[ \pd{f}{x^k}(m) = (f \circ \phi^{-1})_k(\phi(m)) \]
(see~\ref{cor_derivation_partial} and \ref{notn_derivation_partial}) should make it clear
that $\pd{}{x^k}$ depends on \emph{all} the components of the chart $\phi$ and not just
on the single component~$x^k$.  In any case, here is a concrete counterexample.

\begin{exam} Consider $\R^2$ as a $2$-manifold (with the usual differential structure
generated by the atlas whose only member is the identity map on~$\R^2$). Let $\phi =
(x^1,x^2)$ be the identity map on $\R^2$ and $\psi = (y^1,y^2)$ be the map defined by
$\psi\colon \R^2 \sto \R^2\colon (u,v) \mapsto (u,u+v)$. Clearly $\phi$ and $\psi$ are
charts containing the point $m = (1,1)$ and $x^1 = y^1$.  We see, however, that
$\pd{}{x^1} \ne \pd{}{y^1}$ by computing $\pd{f}{x^1}(1,1)$ and $\pd{f}{y^1}(1,1)$ for
the function $f\colon \R^2 \sto \R\colon (u,v) \mapsto u^2v$.
\end{exam}

\begin{prop}[Change of Variables Formula]\label{prop_cvf} Let $\phi = (x^1, \dots, x^n)$
  \index{change of variables}%
and $\psi = (y^1, \dots, y^n)$ be charts at a point $m$ on an $n$-manifold.  Then
  \begin{equation}\label{eqn_cvf}
    \pd{}{x^k}\biggr|_m = \sum_{j=1}^n \pd{y^j}{x^k}(m)\, \pd{}{y^j}\biggr|_m
  \end{equation}
for $1 \le k \le n$.
\end{prop}

\begin{rem} If $\phi = (x^1, \dots, x^n)$ and $\psi = (y^1, \dots, y^n)$ are charts on an
$n$-manifold which overlap (that is, $\dom \phi \cap \dom \psi \ne \emptyset$), then the
preceding \emph{change of variables formula} holds for \emph{all} $m$ in $\dom \phi \cap
\dom \psi \ne \emptyset$.  Thus in the interests of economy of notation, symbols
indicating evaluation at $m$ are normally omitted.  Then~\eqref{eqn_cvf} becomes
  \begin{equation}\label{eqn_cvf2}
    \pd{}{x^k} = \sum_{j=1}^n \pd{y^j}{x^k}\, \pd{}{y^j}
  \end{equation}
with the understanding that it may be applied to any real valued function which is smooth
on~$\dom \phi \cap \dom \psi$.
\end{rem}

\begin{exer} Regard the $2$-sphere in $\R^3$ as a $2$-manifold whose differentiable
structure is is generated by the stereographic projections from the north and south poles
(see example~\ref{exam_n_sphere}). Let $\phi$ be the stereographic projection from the
south pole.  That is,
   \[ \phi(x,y,z) = \biggl( \frac x{1+z}, \frac y{1+z}\biggr) \]
for all $(x,y,z) \in \Sp^2$ such that $z \ne -1$.  Let $\psi$ be defined by
   \[ \psi(x,y,z) = (y,z) \]
for all $(x,y,z) \in \Sp^2$ such that $x > 0$. (It is easy to see that $\psi$ is a
chart.)  Also define a real valued function $f$ by
   \[ f(x,y,z) = \frac xy + \frac yz \]
for all $(x,y,z) \in \Sp^2$ such that $x$, $y$, $z > 0$.  Let $m =
\bigl(\frac12,\frac1{\sqrt 2}, \frac12\bigr)$.

For these data verify by explicit computation formula~\eqref{eqn_cvf}.  That is, by
computing both sides show that for $k=1$ and $k=2$ the formula
   \[ \pd{f}{x^k}(m) = \pd{y^1}{x^k}(m)\,\pd{f}{y^1}(m)
                           + \pd{y^2}{x^k}(m)\,\pd{f}{y^2}(m) \]
is correct for the functions $\phi$, $\psi$, and $f$ and the point $m$ given in the
preceding paragraph.
\end{exer}

\begin{prop}[Another version of the chain rule]\label{prop_cr_manif2} Let
 \index{chain rule}%
$G\colon M \sto N$ be a smooth map between manifolds of dimensions $n$ and $p$,
respectively, $\phi = (x^1, \dots, x^n)$ be a chart containing the point $m$ in~$M$, and
$\psi = (y^1, \dots, y^p)$ be a chart containing~$G(m)$ in~$N$. Then
   \begin{equation}\label{eqn_cr_manif2}
     \pd{(f \circ G)}{x^k}(m)
            = \sum_{j=1}^p\, \pd{(y^j \circ G)}{x^k}(m)\, \pd{f}{y^j}(G(m))
   \end{equation}
whenever $f \in \fml C_{G(m)}^\infty$ and $1 \le k \le n$.
\end{prop}

\begin{rem} If one is willing to adopt a sufficiently relaxed attitude towards notation
many complicated looking formulas can be put in ``simpler'' form. Convince yourself that
it is not beyond the realm of possibility for one to encounter
equation~\eqref{eqn_cr_manif2} written in the form
   \[ \pd{z}{x^k} = \sum_{j=1}^p \pd{z}{y^j}\,\pd{y^j}{x^k}.\]
\end{rem}

\begin{exer} Consider the $2$-manifold $\Sp^2$ with the differential structure generated
by the stereographic projections form the north and south poles (see
example~\ref{exam_n_sphere}) and the $2$-manifold $\Po^2$ with the differentiable
structure generated by the atlas given in example~\ref{exam_proj_plane}.  Recall from
example~\ref{exam_map_s2_p2} that the map $F\colon \Sp^2 \sto \Po^2\colon (x,y,z) \mapsto
[(x,y,z)]$ is smooth. Define
   \[ h\bigl(\,[(x,y,z)]\,\bigr) = \frac{x+y}{2z} \]
whenever $[(x,y,z)] \in \Po^2$ and $z \ne 0$; and let $m = \bigl(\frac12,\frac1{\sqrt 2},
\frac12\bigr)$. It is clear that $h$ is well-defined. (You may assume it is smooth in a
neighborhood of~$F(m)$.) Let $\phi = (u,v)$ be the stereographic projection of $\Sp^2$
from the south pole (see~\ref{exam_n_sphere}).
  \begin{enumerate}
    \item[(a)] Using the \emph{definitions} of $\pd{}{u}\bigl |_m$ and $\pd{}{v}\bigl |_m$
compute $\pd{(h \circ F)}{u}(m)$ and $\pd{(h \circ F)}{v}(m)$.
    \item[(b)] Let $\eta$ be the chart in $\Po^2$ defined in example~\ref{exam_proj_plane}.
Use this chart, which contains $F(m)$, and the version of the \emph{chain rule} given in
proposition~\ref{prop_cr_manif2} to compute (independently of part (a)\,) $\pd{(h \circ
F)}{u}(m)$ and $\pd{(h \circ F)}{v}(m)$.
   \end{enumerate}
\end{exer}

Let $m$ be a point in an $n$-manifold.  In example~\ref{exam_gtv_to_atv} we defined, for
each $\wt c$ in the geometric tangent space~$\wt T_m$, a function
   \[ v_{\wt c}\colon \fml G_m \sto \R\colon \wh f \mapsto D(f \circ c)(0) \]
and showed that $v_{\wt c}$ is (well-defined and) a derivation on the space $\fml G_m$ of
germs at~$m$.  Thus the map $v\colon \wt c \mapsto v_{\wt c}$ takes members of $\wt T_m$
to members of~$\wh T_m$.  The next few propositions lead to the conclusion that $v\colon
\wt T_m \sto \wh T_m$ is a vector space isomorphism and that consequently the two
definitions of ``tangent space'' are essentially the same.  Subsequently we will drop the
diacritical marks tilde and circumflex that we have used to distinguish the geometric and
algebraic tangent spaces and instead write just $T_m$ for the tangent space at~$m$.
 \index{conventions!the geometric and algebraic tangent spaces are ``the same''}%
 \index{Tm@$T_m$ (tangent space at~$m$)}%
We will allow context to dictate whether a tangent vector (that is, a member of the
tangent space) is to be interpreted as an equivalence class of curves or as a derivation.
Our first step is to show that the map $v$ is linear.

\begin{prop} Let $m$ be a point in an $n$-manifold and
   \[ v\colon \wt T_m \sto \wh T_m\colon \wt c \mapsto v_{\wt c} \]
be the map defined in example~\ref{exam_gtv_to_atv}.  Then
  \begin{enumerate}
    \item[(i)] $v_{\wt b + \wt c} = v_{\wt b} + v_{\wt c}$ \quad and
    \item[(ii)] $v_{\alpha \wt c} = \alpha v_{\wt c}$
   \end{enumerate}
for all $\wt b$, $\wt c \in \wt T_m$ and $\alpha \in \R$.
\end{prop}

\begin{prop} Let $m$ be a point in an $n$-manifold. Then the map
   \[ v\colon \wt T_m \sto \wh T_m\colon \wt c \mapsto v_{\wt c} \]
(defined in~\ref{exam_gtv_to_atv}) is injective.
\end{prop}

In order to show that the map $v\colon \wt T_m \sto \wh T_m$ is surjective we will need
to know that the tangent vectors $\pd{}{x^1}\bigr|_m$, \dots, $\pd{}{x^n}\bigr|_m$ span
the tangent space $\wh T_m$.  The crucial step in this argument depends on adapting the
(second order) \emph{Taylor's formula} so that it holds on finite dimensional manifolds.

\begin{lem} Let $\phi = (x^1, \dots, x^n)$ be a chart containing a point $m$ in an
$n$-manifold and $f$ be a member of~$\fml C_m^\infty$. Then there exist a neighborhood
$U$ of $m$ and smooth functions~$s^{jk}$ (for $1 \le j,k \le n$) such that
   \[ f = f(m) + \sum_{j=1}^n (x^j - a^j)\bigl(f_\phi\bigr)_j(a)
               + \sum_{j,k=1}^n (x^j - a^j)(x^k - a^k)s^{jk} \]
where $a = \phi(m)$.
\end{lem}

\begin{proof}[\emph{Hint for proof}] Apply \emph{Taylor's formula} to the local
representative~$f_\phi$.   \ns
\end{proof}

\begin{prop} If $\phi = (x^1, \dots, x^n)$ is a chart containing a point $m$ in an
$n$-manifold, then the derivations $\pd{}{x^1}\bigr|_m$, \dots, $\pd{}{x^n}\bigr|_m$ span
the algebraic tangent space~$\wh T_m$.  In fact, if $w$ is an arbitrary element of~$\wh
T_m$, then
   \[ w = \sum_{j=1}^n w \bigl(\wh{x^j}\bigr)\,\pd{}{x^j}\biggr|_m\,. \]
\end{prop}

\begin{prop} Let $m$ be a point in an $n$-manifold. Then the map
   \[ v\colon \wt T_m \sto \wh T_m\colon \wt c \mapsto v_{\wt c} \]
(defined in~\ref{exam_gtv_to_atv}) is surjective.
\end{prop}

\begin{cor}\label{prop_equiv_tan_sps} Let $m$ be a point in an $n$-manifold. Then the
map $v$ (of the preceding proposition) is an isomorphism between the tangent spaces $\wt
T_m$ and~$\wh T_m$.
\end{cor}

\begin{cor}\label{cor_basis_tan_sp} If $\phi = (x^1, \dots, x^n)$ is a chart containing
a point $m$ in an $n$-manifold, then the derivations $\pd{}{x^1}\bigr|_m$, \dots,
$\pd{}{x^n}\bigr|_m$ constitute a basis for the tangent space~$\wh T_m$.
\end{cor}

In \ref{def_geom_dffrntl} we defined the differential $\wt dF_m$ of a smooth map $F
\colon M \sto N$ between finite dimensional manifolds at a point $n \in M$.  This
differential between the geometric tangent spaces at $m$ and $F(m)$ turned out to be a
linear map (see~\ref{prop_dFm_notn_plaus}).  In a similar fashion $F$ induces a linear
map, which we denote by $\wh dF_m$, between the algebraic tangent spaces at $m$ and
$F(m)$. We define this new \emph{differential} and then show that it is essentially same
as the one between the corresponding geometric tangent spaces.

\begin{defn}\label{def_alg_dffrntl} Let $F\colon M \sto N$ be a smooth map between finite
dimensional manifolds, $m \in M$, and $w \in \wh T_m$.  Define
 \index{dFm@$\wh dF_m$ (differential of $f$ at~$m$)}%
 \index{differential!of a smooth map between manifolds}%
$\wh dF_m \colon \wh T_m \sto \wh T_{F(m)}$ by setting $\bigl(\wh dF_m(w)\bigr)(\wh g) =
w(\wh{g \circ F})$---or in somewhat less cluttered notation
   \[\wh dF_m(w)(g) = w(g \circ F) \]
for each $g \in \fml C_{F(m)}^\infty$.
\end{defn}

\begin{prop} The function $\wh dF_m(w)$, defined above, is well-defined and is a
derivation on~$\fml G_{F(m)}$.
\end{prop}

Now we show that this new differential is essentially the same as the one defined
in~\ref{def_geom_dffrntl}.

\begin{prop}\label{prop_equiv_dffrntls} Let $F \colon M \sto N$ be a smooth mapping
between finite dimensional manifolds and $m \in M$. Then the following diagram commutes.
  \[ \xy
          \Square[\wt T_m`\wt T_{F(m)}`\wh T_m`\wh T_{F(m)};\wt dF_m`v`v`\wh dF_m]
      \endxy \]
\end{prop}

In light of the isomorphism between the geometric and algebraic tangent spaces to a
manifold (see~\ref{prop_equiv_tan_sps}) and the equivalence of the respective
differential maps (proved in the preceding proposition), we will for the most part write
just $T_m$ for either type of tangent space
 \index{conventions!dfm@$dF_m = \wt dF_m = \wh dF_m$}%
and $dF_m$ for either differential of a smooth map.  In situations where the difference
is important context should make it clear which one is intended.

\begin{cor}\label{cor_dF_linear} If $F \colon M \sto N$ is a smooth map between finite
dimensional manifolds and $m \in M$, then $\wh dF_m$ is a linear transformation from the
algebraic tangent space $\wh T_m$ into $\wh T_{F(m)}$.
\end{cor}

\begin{prop}[Yet another chain rule] If $F \colon M \sto N$  and $G \colon N \sto P$
 \index{chain rule}%
are smooth maps between finite dimensional manifolds and $m \in M$, then
   \[ \wh d(G \circ F)_m = \wh dG_{F(m)} \circ \wh dF_m\,. \]
\end{prop}

In the next exercise we consider the tangent space $T_a$ at a point $a$ in~$\R$. Here, as
usual, the differential structure on $\R$ is taken to be the one generated by the atlas
whose only chart is the identity map $I$ on~$\R$. The tangent space is one-dimensional;
it is generated by the tangent vector~$\pd{}I\bigr|_a$.  This particular notation is, as
far as I know, never used; some alternative standard notations are $\dfrac
d{dx}\biggr|_a$, $\dfrac d{dt}\biggr|_a$, and $\dfrac d{dI}\biggr|_a$.  And, of course,
$\dfrac d{dx}\biggr|_a(f)$ is written as $\dfrac {df}{dx}(a)$.

\begin{exer} Let $a \in \R$ and $g \in \fml C_a^\infty$.  Find $\dfrac{dg}{dx}(a)$.
\end{exer}

\begin{exer}\label{exer_dfmw} If $f \in \fml C^\infty_a$, where $m$ is a point in some
$n$-manifold, and $w \in T_m$, then $df_m(w)$ belongs to $T_{f(m)}$.  Since the tangent
space at $f(m)$ is one-dimensional, there exists $\lambda \in \R$ such that $df_m(w) =
\lambda \dfrac d{dI}\biggr|_{f(m)}$.  Show that $\lambda = w(f)$.
\end{exer}

\begin{conv} Let $a$ be a point in~$\R$.  It is natural in the interests of simplifying
notation to make use of the isomorphism $\lambda \dfrac d{dI}\biggr|_a \mapsto \lambda$
 \index{conventions!identification of $\R$ with $1$-dimensional tangent spaces}%
between the tangent space $T_a$ and $\R$ to identify these one-dimensional spaces.  If $f
\in \fml C_m^\infty$ where $m$ is a point in an $n$-manifold and if we regard $T_{f(m)} =
\R$, then corollary~\ref{cor_dF_linear} says that $df_m$ is a linear map from $T_m$
 \index{conventions!notations associated with $1$-dimensional tangent spaces}%
into~$\R$; that is, $df_m$ belongs to the dual space of~$T_m$; that is
   \begin{equation}\label{eq_conv_1dim_tan_spi}
           df_m \in {T_m}^*\,.
   \end{equation}
Furthermore, under the identification $\lambda =\lambda \dfrac d{dI}\biggr|_{f(m)}$ we
conclude from exercise~\ref{exer_dfmw} that
   \begin{equation}\label{eq_conv_1dim_tan_spii}
           df_m(w) = w(f)\,.
   \end{equation}
for every $w \in T_m$.  From now on we adopt \eqref{eq_conv_1dim_tan_spi}
and~\eqref{eq_conv_1dim_tan_spii} even though they are, strictly speaking, abuses of
notation. They should cause little confusion and are of considerable help in reducing
notational clutter.
\end{conv}

\begin{defn} If $m$ is a point in an $n$-manifold, the dual space ${T_m}^*$ of the
tangent space at $m$ is called the
 \index{cotangent space}%
\df{cotangent space} at~$m$.
\end{defn}

Notice that in \eqref{eq_conv_1dim_tan_spi} we have adopted the convention that at every
point $m$ the differential of a smooth real valued function $f$ belongs to the cotangent
space at~$m$. In particular, if $\phi = (x^1, \dots, x^n)$ is a chart on an $n$-manifold,
then each of its components $x^k$ is a smooth real valued function and therefore
$d{x^k}_m$ belongs to the cotangent space ${T_m}^*$ for every $m$ in the domain
of~$\phi$.  The next proposition shows that in fact the set $\{d{x^1}_m, \dots,
d{x^n}_m\}$ of cotangent vectors is the basis for~${T_m}^*$ dual to the basis for $T_m$
given in corollary~\ref{cor_basis_tan_sp}.

\begin{prop} Let $\phi = (x^1, \dots, x^n)$ be a chart containing the point $m$ in an
$n$-manifold. Then $\{d{x^1}_m, \dots, d{x^n}_m\}$ is a basis for ${T_m}^*$; it is dual
to the basis $\bigl\{\pd{}{x^1}\bigr|_m, \dots, \pd{}{x^n}\bigr|_m\bigr\}$ for~$T_m$.
\end{prop}

\begin{prop}\label{prop_expansion_dffrntl} Let $m$ be a point in an $n$-manifold, $f
\in \fml C_m^\infty$, and $\phi = (x^1, \dots, x^n)$ be a chart containing~$m$. Then
   \[ df_m = \sum_{k=1}^n \pd{f}{x^k}(m)\,d{x^k}_m\,. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] There exist scalars $\alpha_1$, \dots $\alpha_n$
such that $df_m = \sum_{j=1}^n \alpha_j\,d{x^j}_m$. (Why?) Consider $ \sum_{j=1}^n
\alpha_j\,d{x^j}_m \bigl(\pd{}{x^k}\bigr|_m\bigr)$.   \ns
\end{proof}


Notice that this proposition provides some meaning (and justification) for the
conventional formula frequently trotted out in beginning calculus courses.
  \[ df = \pd fx\,dx + \pd fy\,dy + \pd fz\,dz\,. \]










\endinput
\chapter{GEOMETRIC ALGEBRA}

\section{Geometric Plane Algebra}

In this section we look at an interesting algebraic approach to plane geometry, one that keeps track of orientation of plane figures as well as
such standard items as magnitudes, direction, perpendicularity, and angle.  The object we start with is the Grassmann algebra $\bigwedge(\R^2)$
on which we define a new multiplication, called the \emph{Clifford product}, under which it will become, in the language of the next chapter, an
example of a \emph{Clifford algebra}.

\begin{notn} We make a minor notational change.  Heretofore the standard basis vectors for $\R^n$ were denoted by $e^1$, $e^2$, \dots $e^n$. For
our present purposes we will list them as $e_1$, $e_2$, \dots $e_n$. This allows us to use superscripts for powers.
\end{notn}

\begin{defn}\label{def_Cliff_alg} We start with the inner product space~$\R^2$.  On the corresponding Grassmann algebra
$\bigwedge(\R^2) = \bigoplus_{k=0}^2 \bigwedge^k(\R^2)$ we say that an element homogeneous of degree $0$ (that is, belonging to $\bigwedge^0(\R^2)$\,) is a
 \index{scalar}%
\df{scalar} (or real number).  An element homogeneous of degree $1$ (that is, belonging to $\bigwedge^1(\R^2)$\,) we will call a
 \index{vector}%
 \index{1@$1$-vector}%
\df{vector} (or a \df{$1$-vector}).
And an element homogeneous of degree $2$ (that is, belonging to $\bigwedge^2(\R^2)$\,) will be called a
 \index{bivector}%
 \index{2@$2$-blade}%
 \index{pseudoscalar}%
\df{bivector} (or a \df{2-blade}, or a \df{pseudoscalar}).  We will shorten the name of the bivector $e_1 \wedge e_2$ to~$e_{12}$. Similarly we write $e_{21}$
 \index{e@$e_{12}$ ($= e_1 \wedge e_2$)}%
for the bivector $e_2 \wedge e_1$.  Notice that $\{1, e_1, e_2, e_{12}\}$ is a basis for the $4$-dimensional Grassmann algebra $\bigwedge(\R^2)$, and that
$e_{21} = -e_{12}$.  Notice also that since $\bigwedge^2(\R^2)$ is $1$-dimensional, every bivector is a scalar multiple of~$e_{12}$.

We now introduce a new multiplication, called
 \index{Clifford!multiplication!in the plane}%
 \index{multiplication!Clifford}%
\df{Clifford multiplication} (or \df{geometric multiplication}) into $\bigwedge(\R^2)$ as follows: if $v$ and $w$ are $1$-vectors in $\bigwedge(\R^2)$, then their
 \index{Clifford!product!in the plane}%
 \index{product!Clifford}%
 \index{geometric!product}%
 \index{product!geometric}%
\df{Clifford product} $vw$ is defined by
    \begin{equation}\label{eqn_Cliff_mult}
              vw := \langle v,w \rangle + v \wedge w\,.
    \end{equation}
To make $\bigwedge(\R^2)$ into a unital algebra, we will extend this new multiplication to all elements of the space. The Clifford product of a scalar and
an arbitrary element of $\bigwedge(\R^2)$ is the same as it is in the Grassmann algebra~$\bigwedge(\R^2)$.  We must also specify the Clifford product of a
vector and a bivector.  Since we require the distributive law to hold for Clifford multiplication we need only specify the product $e_ie_{12}$ and
$e_{12}e_i$ for $i = 1$ and~$2$.  If we wish Clifford multiplication to be associative there is only one way to do this (see exercise~\ref{exer_Cliff_mult}):
set $e_1e_{12} = e_2$, $e_2e_{12} = -e_1$, $e_{12}e_1 = -e_2$, and $e_{12}e_2 = e_1$.  Similarly, if we are to have associativity, the product of bivectors
must be determined by $e_{12}e_{12} = -1$.
\end{defn}

\begin{cau} Now we have three different ``multiplications'' to consider: the \emph{inner} (or \emph{dot}) product $\langle v,w \rangle$,
the \emph{\emph{wedge}} (or \emph{exterior}, or \emph{outer}) product $v \wedge w$, and the \emph{Clifford} (or \emph{geometric}) product $vw$.
Equation~\eqref{eqn_Cliff_mult} holds only for \emph{vectors}.  Do \emph{not} use it for arbitrary members of $\bigwedge(\R^2)$.
(There is, of course, a possibility of confusion: Since $\bigwedge(\R^2)$ is indeed a vector space, any of its elements may in general usage be
called a ``vector''.  On the other hand, we have divided the homogeneous elements of $\bigwedge(\R^2)$ into three disjoint classes: scalars,
vectors, and bivectors.  When we wish to emphasize that we are using the word ``vector'' in the latter sense, we call it a $1$-vector.
\end{cau}

\begin{notn}\label{notn_cl20} The object we have just defined turns out to be a unital algebra and is an example of a \emph{Clifford algebra}.  
In the next chapter we will explain why we denote this
 \index{Cl@$\cl(2,0)$}%
particular algebra---the vector space $\bigwedge(\R^2)$ together with Clifford multiplication---by $\cl(2,0)$.
\end{notn}

\begin{prop} If $v$ is a $1$-vector in $\cl(2,0)$, then $v^2 = {\norm v}^2$. \emph{(Here, $v^2$ means $vv$.)}
\end{prop}

\begin{cor} Every nonzero $1$-vector $v$ in $\cl(2,0)$ has an inverse (with respect to Clifford multiplication).
 \index{inverse!in a Clifford algebra}%
As usual, the inverse of $v$ is denoted by $v^{-1}$.
\end{cor}

\begin{exer}\label{exer_Cliff_mult} Justify the claims made in the last two sentences of definition~\ref{def_Cliff_alg}.
\end{exer}

The magnitude (or norm) of a $1$-vector is just its length. We will also assign a
 \index{magnitude!of a bivector}%
 \index{norm!of a bivector}%
 \index{bivector!norm of a}%
\df{magnitude} (or \df{norm}) to bivectors.  The motivation for the following definition is that we wish a bivector $v \wedge w$
to represent an equivalence class of directed regions in the plane, two such regions being equivalent if they have the same area
and the same orientation (positive = counterclockwise or negative = clockwise). So we will take the magnitude of $v \wedge w$ to
be the area of the parallelogram generated by $v$ and~$w$.

\begin{defn} Let $v$ and $w$ be $1$-vectors in $\cl(2,0)$.
 \index{<norm@$\norm{v \wedge w}$ (norm of a bivector)}%
Define $\norm{v \wedge w} := \norm{v}\norm{w}\sin\theta$ where $\theta$ is the angle between $v$ and $w$ ($0 \le \theta \le \pi$).
\end{defn}

\begin{prop} If $v$ and $w$ are $1$-vectors in $\cl(2,0)$, then $\norm{v \wedge w}$ is the area of the parallelogram generated
by $v$ and~$w$.
\end{prop}

\begin{prop} If $v$ and $w$ are $1$-vectors in $\cl(2,0)$, then $v \wedge w = \norm{v \wedge w}e_{12} = \det(v,w)e_{12}$.
\end{prop}

\begin{exer} Suppose you know how to find the Clifford product of any two elements of $\cl(2,0)$.  Explain how to use equation~\eqref{eqn_Cliff_mult}
to recapture formulas defining the inner product and the wedge product of two $1$-vectors in~$\cl(2,0)$.
\end{exer}

A vector in the plane is usually regarded as an equivalence class of directed segments. (Two directed segments are taken to be equivalent if they lie
on parallel lines, have the same length, and point in the same direction.)  Each such equivalence class of directed segments has a
 \index{standard!representative!of a vector}%
\emph{standard representative}, the one whose tail is at the origin.  Two standard representatives are parallel if one is a nonzero multiple of the other.
By a common abuse of language, where we conflate the notions of directed segments and vectors, we say that two nonzero vectors in the plane are parallel
if one is a scalar multiple of the other.

\begin{prop} Two nonzero vectors in $\cl(2,0)$ are parallel if and only if their Clifford product commutes.
\end{prop}

\begin{defn} Two elements, $a$ and $b$, in a semigroup (or ring, or algebra)
 \index{anticommute}%
\df{anticommute} if $ba = -ab$.
\end{defn}

\begin{exam} The bivector $e_{12}$ anticommutes with all vectors in $\cl(2,0)$.
\end{exam}

\begin{prop} Two vectors in $\cl(2,0)$ are perpendicular if and only if their Clifford product anticommutes.
\end{prop}

Let $v$ and $w$ be nonzero vectors in $\cl(2,0)$. We can write $v$ as the sum of two vectors $v_{\|}$ and $v_{\perp}$,where $v_{\|}$ is parallel to $w$
and $v_{\perp}$ is perpendicular to~$w$.  The vector $v_{\|}$ is the
 \index{v@$v_\printparallel$ (parallel component of~$v$)}%
 \index{<subscript@$v_\printparallel$ (parallel component of~$v$)}%
 \index{v@$v_{\perp}$ (perpendicular component of~$v$)}%
 \index{<subscript@$v_{\perp}$ (perpendicular component of~$v$)}%
\df{parallel component} of $v$ and $v_{\perp}$ is the \df{perpendicular component} of $v$ (with respect to~$w$).

\begin{prop} Let $v$ and $w$ be nonzero vectors in $\cl(2,0)$. Then $v_{\|} = \langle v,w \rangle w^{-1}$ and $v_{\perp} = (v \wedge w)w^{-1}$.
\end{prop}

\begin{defn} If $v$ and $w$ are nonzero $1$-vectors in $\cl(0,2)$, the
 \index{reflection}%
\df{reflection} of $v$ across $w$ is the map $v = v_{\|} + v_{\perp} \mapsto  v_{\|} - v_{\perp}$.
\end{defn}

\begin{prop} If $v$ and $w$ are nonzero $1$-vectors in $\cl(0,2)$, then the reflection $v' = v_{\|} - v_{\perp}$ of $v$ across $w$ is given by
$v' = wvw^{-1}$.
\end{prop}

Although $\cl(2,0)$ is a real algebra, it contains a field isomorphic to the complex numbers. We start with the observation that ${e_{12}}^2 = -1$.
This prompts us to give a new name to~$e_{12}$.  We will call it~$\clim$. In the space $\bigwedge^0(\R^2) \oplus \bigwedge^2(\R^2)$, of scalars plus
pseudoscalars, this element serves the same role as the complex number $i$ does in the complex plane~$\C$.

\begin{defn} If $\zeta := a + b\clim$ is the sum of a scalar and a pseudoscalar, we define its
 \index{conjugate!in $\cl(2,0)$}%
\df{conjugate} $\bar{\zeta}$ as $a - b\clim$ and its
 \index{absolute value!in $\cl(2,0)$}%
\df{absolute value} $\abs\zeta$ as $\sqrt{a^2 + b^2}$.
\end{defn}

\begin{prop} If $\zeta \in \bigwedge^0(\R^2) \oplus \bigwedge^2(\R^2)$, then $\zeta\bar{\zeta} = {\abs{\zeta}}^2$ and, if $\zeta$ is not zero, it is
invertible with $\zeta^{-1} = \bar\zeta{\abs{\zeta}}^{-2}$.
\end{prop}

\begin{prop}\label{prop_02space_C} The space $\bigwedge^0(\R^2) \oplus \bigwedge^2(\R^2)$ (with Clifford multiplication) is a field and is isomorphic
to the field $\C$ of complex numbers.
\end{prop}

Complex numbers serve two purposes in the plane.  They implement rotations via their representation in polar form $ze^{i\theta}$ and, as points,
they represent vectors.  Recall that in the Clifford algebra $\cl(2,0)$ we have $\bigwedge^1(\R^2) = \R^2$.  So here vectors live in $\bigwedge^1(\R^2)$
while (the analogs of) complex numbers live in $\bigwedge^0(\R^2) \oplus \bigwedge^2(\R^2)$.  Since these are both two-dimensional vector spaces, they are
isomorphic.  It turns out that there exists a simple, natural, and very useful isomorphism between these spaces: left multiplication by~$e_1$.

\begin{notn} In the remainder of this section we will shorten $\bigwedge(\R^2)$ to $\bigwedge$ and $\bigwedge^k(\R^2)$ to $\bigwedge^k$ for $k = 0$, $1$, and~$2$.
\end{notn}

\begin{prop} The map from $\bigwedge^1$ to $\bigwedge^0 \oplus \bigwedge^2$ defined by $v \mapsto \hat v := e_1v$ is a
 \index{<over@$\hat v$ ($=e_1v$)}%
 \index{v@$\hat v$ ($=e_1v$)}%
vector space isomorphism. The mapping is also an isometry in the sense that $\abs{\hat v} = \norm v$.  The mapping is its own inverse;
that is, $e_1\hat v = v$.
\end{prop}

\begin{defn} For $\phi \in \R$ define $e^{\phi\clim}$ by the usual power series
   \[ e^{\phi\clim} := \sum_{k=0}^\infty \frac{(\phi\clim)^k}{k!} = \cos\phi + (\sin\phi)\clim. \]
\end{defn}

\begin{exer} Discuss the convergence problems (or lack thereof) that arise in the preceding definition.
\end{exer}

Next we develop a formula for the positive (counterclockwise) rotation of a nonzero vector $v$ through angle of $\phi$ radians. Let $v'$ be the vector
in its rotated position.  First move these $1$-vectors to their corresponding points in the ($\cl(2,0)$ analog of the) complex plane. Regarding these two
points, $\zeta = e_1v$, $\zeta' = e_1v' \in \bigwedge^0 \oplus \bigwedge^2$ as complex numbers (\emph{via} proposition~\ref{prop_02space_C}), we see that
$\zeta' = e^{\phi\clim}\zeta$. Then move $\zeta$ and $\zeta'$ back to~$\bigwedge^1$ to obtain the formula in the following proposition.

\begin{prop} Let $v$ be a nonzero $1$-vector in $\cl(2,0)$.  After being rotated by $\phi$ radians in the positive direction it becomes the vector~$v'$.
Then
   \[ v' = e^{-\phi\clim}v = v e^{\phi\clim}\,. \]
\end{prop}

In definition~\ref{def_Cliff_alg} and exercise~\ref{exer_Cliff_mult} we established that a necessary condition for $\cl(2,0)$ under Clifford
multiplication to be an algebra, is that certain relations involving the basis elements, $1$, $e_1$, $e_2$, and $e_{12}$ hold.  This doesn't prove,
however, that if these relations are satisfied, then $\cl(2,0)$ under Clifford multiplication is an algebra.  This lacuna is not hard to fill.

\begin{prop} The Clifford algebra $\cl(2,0)$ is, in fact, a unital algebra isomorphic to the matrix algebra $\mathbf M_2(\R)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Try mapping $e_1$ to $\begin{bmatrix} 1  &  0  \\ 0  & -1 \end{bmatrix}$ and $e_2$ to
$\begin{bmatrix} 0  &  1  \\ 1  &  0 \end{bmatrix}$.  \ns
\end{proof}

\begin{exam} The Clifford algebra $\cl(2,0)$ is \emph{not} a $\Z^+$-graded algebra.
\end{exam}

\begin{prop} The Clifford algebra $\cl(2,0)$ does \emph{not} have the cancellation property.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider $uv$ and $uw$ where $u = e_2 - e_{12}$, $v = e_1 + e_2$, and $w = 1 + e_2$.  \ns
\end{proof}




\vfill\eject










\section{Geometric Algebra in $3$-Space}

\begin{defn}\label{defn_Cliff_prod_3space} We now give the Grassmann algebra $\bigwedge(\R^3) = \bigoplus_{k=0}^3\bigwedge^k(\R^3)$
a new multiplication, called the \emph{Clifford product} (or \emph{geometric product}). The resulting Clifford algebra will be
denoted by $\cl(3,0)$.  Recall that the vector space $\bigwedge(\R^3)$ is $8$-dimensional. If we let $\{e_1, e_2, e_3\}$ be the
standard basis for $\R^3$, then
  \begin{align*} {\bigwedge}^0(\R^3) &= \spn\{\vc 1\}, \\
                 {\bigwedge}^1(\R^3) &= \spn\{e_1, e_2, e_3\}, \\
                 {\bigwedge}^2(\R^3) &= \spn\{e_2\wedge e_3, e_3 \wedge e_1, e_1 \wedge e_2\}, \text{ and } \\
                 {\bigwedge}^3(\R^3) &= \spn\{e_1 \wedge e_2 \wedge e_3\}.
  \end{align*}
As in $\cl(2,0)$ we use the abbreviations $e_{23} = e_2\wedge e_3$, $e_{31} =  e_3 \wedge e_1$, and $e_{12} = e_1 \wedge e_2$.  Additionally
we let $e_{123} = e_1 \wedge e_2 \wedge e_3$.  As before, members of $\bigwedge^0(\R^3)$ are called
\index{scalar}%
\df{scalars}.  Element homogeneous of degree $1$ (that is, belonging to $\bigwedge^1(\R^2)$\,) are called
 \index{vector}%
 \index{1@$1$-vector}%
\df{vectors} (or a \df{$1$-vectors}).  Elements homogeneous of degree $2$ (that is, belonging to $\bigwedge^2(\R^2)$\,) are called
 \index{bivector}%
 \index{2@$2$-blade}%
\df{bivectors}.  Decomposable elements of $\bigwedge^2(\R^2)$ are called \df{$2$-blades}. (Thus any bivector is a sum of $2$-blades.)
And elements homogeneous of degree $3$ (that is, scalar multiples of $e_{123}$) are called
 \index{trivector}%
\df{trivectors} (or
 \index{pseudoscalar}%
\df{pseudoscalars}, or
 \index{3@$3$-blade}%
\df{$3$-blades}.

We start by taking scalar multiplication to be exactly as it is in the Grassmann algebra~$\bigwedge(\R^3)$. Then we define the
 \index{Clifford!product}%
 \index{product!Clifford}%
\df{Clifford product} or
 \index{geometric!product}%
 \index{product!geometric}%
\df{geometric product} of two $1$-vectors, $v$ and $w$, exactly as we did for $\cl(2,0)$:
    \begin{equation}\label{eqn_Cliff_mult_3d}
              vw := \langle v,w \rangle + v \wedge w\,.
    \end{equation}
Additionally, we set $e_1e_2e_3 := e_1 \wedge e_2 \wedge e_3$.
\end{defn}


\begin{exer} Show that in order for definition~\ref{defn_Cliff_prod_3space} to produce an algebra we need the following multiplication table to hold
for the basis elements.
\vskip 10 pt
 \begin{center}
  \begin{tabular}{|c||c|c|c|c|c|c|c|c|}\hline
        {}    &    $1$    &   $e_1$   &   $e_2$   &   $e_3$   & $e_{23}$  & $e_{31}$  & $e_{12}$  & $e_{123}$ \\
    \hline\hline
       $1$    &    $1$    &   $e_1$   &   $e_2$   &   $e_3$   & $e_{23}$  & $e_{31}$  & $e_{12}$  & $e_{123}$ \\
    \hline
     $e_1$    &   $e_1$   &    $1$    &  $e_{12}$ & $-e_{31}$ & $e_{123}$ &  $-e_3$   &   $e_2$   & $e_{23}$  \\
    \hline
     $e_2$    &   $e_2$   & $-e_{12}$ &     $1$   &  $e_{23}$ &  $e_3$    & $e_{123}$ &  $-e_1$   & $e_{31}$  \\
    \hline
     $e_3$    &   $e_3$   &  $e_{31}$ & $-e_{23}$ &    $1$    & $-e_2$    &   $e_1$   & $e_{123}$ & $e_{12}$  \\
    \hline
    $e_{23}$  &  $e_{23}$ & $e_{123}$ &  $-e_3$   &   $e_2$   &   $-1$    & $-e_{12}$ & $e_{31}$  &  $-e_1$   \\
    \hline
    $e_{31}$  &  $e_{31}$ &   $e_3$   & $e_{123}$ &  $-e_1$   &  $e_{12}$ &    $-1$   & $-e_{23}$ &  $-e_2$   \\
    \hline
    $e_{12}$  &  $e_{12}$ &  $-e_2$   &   $e_1$   & $e_{123}$ & $-e_{31}$ &  $e_{23}$ &   $-1$    &  $-e_3$   \\
    \hline
    $e_{123}$ & $e_{123}$ &  $e_{23}$ &  $e_{31}$ &  $e_{12}$ &  $-e_1$   &  $-e_2$   &  $-e_3$   &   $-1$    \\
    \hline
  \end{tabular}
 \end{center}
\end{exer}

\vskip 10 pt

\begin{notn} The object defined in~\ref{defn_Cliff_prod_3space}, the vector space $\bigwedge(\R^3)$ together with Clifford multiplication,
turns out to be a unital algebra and is a second example of a \emph{Clifford algebra}.  We will see in the next chapter why this algebra
is denoted
 \index{Cl@$\cl(3,0)$}%
by~$\cl(3,0)$.
\end{notn}

In the preceding section we commented on the fact that, geometrically speaking, we picture a vector in the plane as an equivalence class of directed
intervals (that is, directed line segments), two intervals being equivalent if they lie on parallel lines, have the same length, and point in the same
direction.  In a similar fashion it is helpful to have a geometrical interpretation of a $2$-blade.  Let us say that an
 \index{directed interval}%
\emph{directed interval} in $3$-space is an oriented parallelogram generated by a pair of vectors.  Two such intervals will be said to be equivalent
if they lie in parallel planes, have the same area, and have the same orientation.  We may choose as a
 \index{standard!representative!of a $2$-blade}%
standard representative of the $2$-blade $v \wedge w$ the parallelogram formed by putting the tail of $v$ at the origin and the tail of $w$ at the
head of~$v$.  (Complete the parallelogram with by adding, successively, the vectors $-v$ and~$-w$.)  The standard representative of the $2$-blade
$w \wedge v = -v \wedge w$ is the same parallelogram with the opposite orientation: put the tail of $w$ at the origin and the tail of $v$ at the head of~$w$.

\begin{prop}\label{repr_cl30_1} The Clifford algebra $\cl(3,0)$ is indeed a unital algebra.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider the subalgebra of $\mathbf M_4(\R)$ generated by the matrices
  \[ \varepsilon_1 = \begin{bmatrix} 0 &  0 &  0 &  1 \\
                                     0 &  0 &  1 &  0 \\
                                     0 &  1 &  0 &  0 \\
                                     1 &  0 &  0 &  0
                     \end{bmatrix}, \quad
     \varepsilon_2 = \begin{bmatrix} 0 &  0 &  1 &  0 \\
                                     0 &  0 &  0 & -1 \\
                                     1 &  0 &  0 &  0 \\
                                     0 & -1 &  0 &  0
                     \end{bmatrix}, \quad \text{ and } \quad
     \varepsilon_3 = \begin{bmatrix} 1 &  0 &  0 &  0 \\
                                     0 &  1 &  0 &  0 \\
                                     0 &  0 & -1 &  0 \\
                                     0 &  0 &  0 & -1
                     \end{bmatrix}. \]    \ns
\end{proof}

\begin{exer}\label{repr_cl30_2} The proof of the preceding proposition provides a representation of the algebra $\cl(3,0)$ as an $8$-dimensional algebra of
matrices of real numbers. Another, perhaps more interesting, representation of the same algebra is by means of the
 \index{Pauli spin matrices}%
 \index{spin matrices}%
 \index{matrix!Pauli spin}%
\emph{Pauli spin matrices:}
  \[ \sigma_1 = \begin{bmatrix} 0 & 1  \\ 1 & 0  \end{bmatrix}, \quad
     \sigma_2 = \begin{bmatrix} 0 & -i \\ i & 0  \end{bmatrix}, \quad \text{ and }
     \sigma_3 = \begin{bmatrix} 1 & 0  \\ 0 & -1 \end{bmatrix}. \]

 Let $\C(2)$ be the \emph{real} vector space of $2 \times 2$-matrices with complex entries
 \begin{enumerate}
              \item[(a)] Show that although the \emph{complex} vector space $M_2(\C)$ is $4$-dimensional, $\C(2)$ is $8$-dimensional.
              \item[(b)] Show that $\{I , \sigma_1, \sigma_2, \sigma_3, \sigma_2\sigma_3, \sigma_3\sigma_1, \sigma_1\sigma_2, \sigma_1\sigma_2\sigma_3\}$
is a linearly independent subset of~$\C(2)$. (Here, $I$ is the $2 \times 2$ identity matrix.)
              \item[(c)] Find an explicit isomorphism from $\cl(3,0)$ to~$\C(2)$.
 \end{enumerate}
\end{exer}

As was the case in the plane we wish to assign magnitudes to $2$-blades.  We adopt the same convention: the magnitude of $v \wedge w$ is
the area of the parallelogram generated by $v$ and~$w$.

\begin{defn} Let $v$ and $w$ be $1$-vectors in $\cl(3,0)$.
 \index{<norm@$\norm{v \wedge w}$ (norm of a bivector)}%
Define $\norm{v \wedge w} := \norm{v}\norm{w}\sin\theta$ where $\theta$ is the angle between $v$ and $w$ ($0 \le \theta \le \pi$).
\end{defn}

\begin{prop} Let $u$ be a unit vector in $\R^3$ and $P$ be a plane through the origin perpendicular to~$u$. Then the reflection $\dot v$ of
a vector $v \in \R^3$ with respect to $P$ is given by
   \[ \dot v = -uvu\,. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] If $v_{\|}$ and $v_{\perp}$ are, respectively, the parallel and perpendicular components of $v$ with
respect to~$u$, then $\dot{v} = v_{\perp} - v_{\|}$.  \ns
\end{proof}

\begin{exer} Using the usual vector notation, how would you express $\dot{v}$ in the preceding proposition?
\end{exer}

\begin{exer} Find the area of the triangle in $\R^3$ whose vertices are the origin, $(5,-4,2)$, and~$(1,-4,-6)$.
\end{exer}

\begin{exam} To rotate a vector in $3$-space by $2\theta$ radians about an axis determined by a nonzero vector~$a$, choose unit vectors $u_1$
and $u_2$ perpendicular to $a$ so that the angle between $u_1$ and $u_2$ is $\theta$ radians.  Let $P_1$ and $P_2$ be the planes through the origin
perpendicular to $u_1$ and $u_2$, respectively.  Then the map $v \mapsto \ddot{v} = R^{-1}vR$ where $R = u_1u_2$ is the desired rotation.
\end{exam}

\begin{exam} It is clear that a $90^\circ$ rotation in the positive (counterclockwise) direction about the $z$-axis in $\R^3$ will take the point
$(0,2,3)$ to the point $(-2,0,3)$.  Show how the formula derived in the previous example gives this result when we choose $u_1 = e_1$,
$u_2 = \frac1{\sqrt2}e_1 + \frac1{\sqrt2}e_2$, $a = (0,0,1)$, and $v = 2e_2 + 3e_3$.
\end{exam}












\endinput



















\chapter{HOMOLOGY AND COHOMOLOGY}

\section{The de Rham Cohomology Group}

\begin{defn}\label{cohom_df014def} Let $M$ be an $n$-manifold.  We denote
 \index{ZkM@$Z^k(M)$ (closed $k$-forms)}%
by $Z^k(M)$ (or just $Z^k$) the vector space of all closed $k$-forms on~$M$. The ``Z'' is
for the German word ``Zyklus'', which means \emph{cycle}.  So in cohomological language
 \index{cocycles}%
closed forms are often called \emph{cocycles}.

Also we denote
 \index{BkM@$B^k(M)$ (exact $k$-forms)}%
by $B^k(M)$ (or just $B^k$) the vector space of exact $k$-forms on~$M$. Since there are
no differential forms of degree strictly less than $0$, we take $B^0 = B^0(M) = \{0\}$.
For convenience we also take $Z^k = \{0\}$ and $B^k = \{0\}$ whenever $k < 0$ or $k > n$.
The letter ``B'' refers to the word ``boundary''.  So exact forms in the context of
 \index{coboundaries}%
cohomology are often called \emph{coboundaries}.

It is a trivial consequence of proposition~\ref{prop_exact_implies_closed} that $B^k(M)$
is a vector subspace of $Z^k(M)$.  Thus it makes sense to define
  \[ H^k = H^k(M) := \frac{Z^k(M)}{B^k(M)}\,. \]
The quotient space
 \index{HkM@$H^k(M)$ (de Rham cohomology group)}%
 \index{de Rham cohomology group}%
 \index{cohomology group!de Rham}%
 \index{group!de Rham cohomology}%
$H^k(M)$ is the $k^{\text{th}}$ \df{de Rham cohomology group} of~$M$. (Yes, even though
it is a vector space, it is traditionally called a \emph{group}.)  The dimension of the
vector space $H^k(M)$ is the
 \index{Betti number}%
$k^{\text{th}}$ \df{Betti number} of the manifold~$M$.

Another (obviously equivalent) way of phrasing the definition of the $k^{\text{th}}$ de Rham cohomology group is in terms of the maps
  \[ \cdots \to {\bigwedge}^{\!k-1}(M) \to^{d_{k-1}} {\bigwedge}^{\!k}(M) \to^{d_k} {\bigwedge}^{\!k+1}(M) \to \cdots \]
where $d_{k-1}$ and $d_k$ are exterior differentiation operators.  Define
  \[ H^k(M) := \frac{\ker d_k}{\ran d_{k-1}} \]
for all $k$.

It is an interesting fact, but one that we shall not prove, that these cohomology groups are topological invariants.
That is, if two manifolds $M$ and $N$ are homeomorphic, then $H^k(M)$ and $H^k(N)$ are isomorphic.
\end{defn}

\begin{exam} If $M$ is a connected manifold, then $H^0(M) \cong \R$.
\end{exam}

\begin{exer} For $U$ an open subset of $\R^n$ give a very clear description of $H^0(U)$ and explain
why its dimension is the number of connected components of~$U$.  \emph{Hint.} A function is said to be
 \index{locally constant}%
 \index{constant!locally}%
\df{locally constant} if it is constant in some neighborhood of each point in its domain.
\end{exer}

\begin{defn}\label{cohom_df015def} Let $F\colon M \sto N$ be a smooth function between smooth manifolds.  For $k \ge 1$
 \index{<bigwedgek@${\bigwedge}^kF$}%
define
  \[ {\bigwedge}^{\!k} F\colon {\bigwedge}^{\!k}(N) \sto {\bigwedge}^{\!k}(M) \colon \omega \mapsto ({\bigwedge}^{\!k}F)(\omega) \]
where
 \begin{equation}\label{cohom_df015defi}
    \bigl(({\bigwedge}^{\!k}F)(\omega)\bigr)_m(v^1, \dots, v^k) = \omega_{F(m)}\bigl(dF_m(v^1), \dots, dF_m(v^k)\bigr)
 \end{equation}
for every $m \in M$ and $v^1$, \dots $v^k \in T_m$. Also define $\bigl(({\bigwedge}^{\!0}F)(\omega)\bigr)_m = \omega_{F(m)}$.
We simplify the notation in~\eqref{cohom_df015defi} slightly
 \begin{equation}
    ({\bigwedge}^{\!k}F)(\omega)(v^1, \dots, v^k) = \omega(dF(v^1), \dots, dF(v^k)).
 \end{equation}
Denote by $F^\ast$ the map induced by the maps ${\bigwedge}^{\!k}F$ which takes the $\Z$-graded algebra $\bigwedge(N)$ to the $\Z$-graded
algebra~$\bigwedge(M)$.
\end{defn}

\begin{exam}\label{cohom_df015z} For each $k \in \Z^+$ the pair of maps $M \mapsto \bigwedge^{\!k}(M)$ and $F \mapsto \bigwedge^{\!k}(F)$ (as
defined in~\ref{diff_forms_001notn} and~\ref{cohom_df015def}) is a contravariant functor from the category of smooth manifolds and smooth maps to
the category of vector spaces and linear maps.
\end{exam}

\begin{prop} If $F\colon M \sto N$ is a smooth function between smooth manifolds, $\omega \in \bigwedge^{\!j}(N)$, and $\mu \in \bigwedge^{\!k}(N)$,
then
 \[ ({\bigwedge}^{\!j+k}F)(\omega \land \mu) = ({\bigwedge}^{\!j}F)(\omega)\land({\bigwedge}^{\!k}F)(\mu). \]
\end{prop}

\begin{exer} In example~\ref{cohom_df015z} you showed that $\bigwedge^{\!k}$ was a functor for each $k$.  What about $\bigwedge$ itself?
Is it a functor?  Explain.
\end{exer}

\begin{prop} If $F\colon M \sto N$ is a smooth function between smooth manifolds, then
   \[ d \circ F^* = F^* \!\circ d\,. \]
\end{prop}

\begin{exer} Let $V = \{\vc 0\}$ be the $0$-dimensional Euclidean space. Compute the $k^{\text{th}}$ de Rham cohomology group $H^k(V)$ for all $k \in \Z$.
\end{exer}

\begin{exer} Compute $H^k(\R)$ for all $k \in \Z$.
\end{exer}

\begin{exer} Let $U$ be the union of $m$ disjoint open intervals in~$\R$.  Compute $H^k(U)$ for all $k \in \Z$.
\end{exer}

\begin{exer}\label{cohom_exer037} Let $U$ be an open subset of $\R^n$. For $[\omega] \in H^j(U)$ and $[\mu] \in H^k(U)$ define
    \[ [\omega][\mu] = [\omega \land \mu] \in H^{j+k}(U). \]
Explain why proposition~\ref{closed_exact_prop001} is necessary for this definition to make sense. Prove also that this definition does not depend on
the representatives chosen from the equivalence classes.  Show that this definition makes \smash[b]{$H^\ast(U) = \bigoplus\limits_{k \in \Z} H^k(U)$}
into a $\Z$-graded algebra.  This is the
 \index{de Rham cohomology algebra}%
 \index{cohomology!de Rham}%
 \index{algebra!de Rham cohomology}%
\df{de Rham cohomology algebra} of~$U$.
\end{exer}

\begin{defn}\label{cohom_df037def} Let $F\colon M \sto N$ be a smooth function between smooth manifolds. For each integer $k$ define
 \[ H^k(F)\colon H^k(N) \sto H^k(M)k \colon [\omega] \mapsto [{\bigwedge}^{\!k}(F)(\omega)]. \]
Denote by $H^\ast(F)$ the induced map which takes the $\Z$-graded algebra $H^\ast(N)$ into~$H^\ast(M)$.
\end{defn}

\begin{exam} With the definitions given in~\ref{cohom_exer037} and~~\ref{cohom_df037def} $H^\ast$ becomes a contravariant functor from the
category of open subsets of $\R^n$ and smooth maps to the category of $\Z$-graded algebras and their homomorphisms.
\end{exam}























\section{Cochain Complexes}

\begin{defn} A sequence
  \[ \cdots\to V_{k-1} \to^{d_{k-1}} V_k \to^{d_k} V_{k+1} \to\cdots\]
of vector spaces and linear maps is a
 \index{cochain!complex}%
 \index{complex!cochain}%
\df{cochain complex} if $d_k \circ d_{k-1} = \vc 0$ for all $k \in \Z$. Such a sequence may be denoted
 \index{<superscriptpair@$(V^\ast,d)$, $V^\ast$ (cochain complex)}%
by $(V^\ast,d)$ or just by~$V^\ast$.
\end{defn}

\begin{defn} We generalize definition~\ref{cohom_df014def} in the obvious fashion. If $V^\ast$ is a cochain complex, then the $k^{\text{th}}$
 \index{cohomology!group}%
 \index{group!cohomology}%
\df{cohomology group} $H^k(V^\ast)$ is defined to be $\ker d_k/ \ran d_{k-1}$. (As before, this ``group'' is actually a vector space.) In this
context the elements of $V_k$ are often called
 \index{cochain!$k$-}%
 \index{kcochain@$k$\,-cochain}%
\df{$k$-cochains}, elements of $\ker d_k$ are
 \index{cocycle!$k$-}%
 \index{kcocycle@$k$\,-cocycle}%
\df{$k$-cocycles}, elements of $\ran d_{k-1}$ are
 \index{coboundary!$k$-}%
 \index{kcoboundary@$k$\,-coboundary}%
\df{$k$-coboundaries}, and $d$ is the
 \index{coboundary!operator}%
\df{coboundary operator}.
\end{defn}

\begin{defn} Let $(V^\ast,d)$ and $(W^\ast,\delta)$ be cochain complexes. A
 \index{cochain!map}%
\df{cochain map} $G\colon V^\ast \sto W^\ast$ is a sequence of linear maps $G_k\colon V_k \sto W_k$ satisfying
  \[ \delta_k \circ G_k = G_{k+1} \circ d_k\]
for every $k \in \Z$. That is, the diagram
  \[\xymatrix{
       \dots\ar[r] & V_k\ar[d]^{G_k}\ar[r]^{d_k} & {V_{k+1}}\ar[d]^{G_{k+1}} \ar[r] & \dots \\
       \dots\ar[r] & W_k\ar[r]_{\delta_k} & W_{k+1}\ar[r] & \dots
     }\]
commutes.
\end{defn}

\begin{prop} Let $G\colon V^\ast \sto W^\ast$ be a cochain map between cochain complexes. For each $k \in \Z$ define
  \[ G^\ast_k\colon H^k(V^\ast) \sto H^k(W^\ast) \colon [v] \mapsto [G_k(v)] \]
whenever $v$ is a cocycle in~$V_k$. Then the maps $G^\ast_k$ are well defined and linear.
\end{prop}

\begin{proof}[\emph{Hint for proof}] To prove that $G^\ast_k$ is well-defined we need to show two things: that $G_k(v)$ is a cocycle in $W_k$
and that the definition does not depend on the choice of representative~$v$.  \ns
\end{proof}

\begin{defn} A sequence
  \[ \vc 0 \to U^\ast \to^F V^\ast \to^G W^\ast \to \vc 0\]
of cochain complexes and cochain maps is
 \index{exact sequence!of cochain complexes}%
 \index{short exact sequence}%
\df{(short) exact} if for every $k \in \Z$ the sequence
  \[ \vc 0 \to U_k \to^{F_k} V_k \to^{G_k} W_k \to \vc 0 \]
of vector spaces and linear maps is (short) exact.
\end{defn}

\begin{prop} If $\vc 0 \to U^\ast \to^F V^\ast \to^G W^\ast \to \vc 0$ is a short
exact sequence of cochain complexes, then
  \[ H^k(U^\ast) \to^{F^\ast_k} H^k(V^\ast) \to^{G^\ast_k} H^k(W^\ast) \]
is exact at $H^k(V^\ast)$ for every $k \in \Z$.
\end{prop}

\begin{prop} A short exact sequence
   \[ \vc 0 \to U^\ast \to^F V^\ast \to^G W^\ast \to \vc 0 \]
of cochain complexes induces a long exact sequence
  \[ \to H^{k-1}(W^\ast) \to^{\eta_{k-1}} H^k(U^\ast) \to^{F^\ast_k} H^k(V^\ast)
              \to^{G^\ast_k} H^k(W^\ast)\to^{\eta_k}  H^{k+1}(U^\ast) \to  \]
\end{prop}

\begin{proof}[\emph{Hint for proof}]  If $w$ is a cocycle in $W_k$, then, since $G_k$ is surjective, there exists $v \in V_k$ such that $w = G_k(v)$.
It follows that $dv \in \ker G_{k+1} = \ran F_{k+1}$ so that $dv = F_{k+1}(u)$ for some $u \in U_{k+1}$. Let $\eta_k([w]) = [u]$.   \ns
\end{proof}
















\section{Simplicial Homology}
\begin{defn} Let $V$ be a vector space. Recall that a \emph{linear combination} of a finite set $\{x_1, \dots, x_n\}$ of vectors in $V$ is a vector of the
form $\sum_{k=1}^n \alpha_k x_k$ where $\alpha_1, \dots, \alpha_n \in \R$.  If $\alpha_1 = \alpha_2 = \dots = \alpha_n = 0$, then the linear combination is
\emph{trivial}; if at least one $\alpha_k$ is different from zero, the linear combination is \emph{nontrivial}.  A linear combination
$\sum_{k=1}^n\alpha_k x_k$ of the vectors $x_1, \dots, x_n$ is a
 \index{convex!combination}%
 \index{combination!convex}%
\df{convex combination} if $\alpha_k \ge 0$ for each $k$ ($1 \le k \le n$) and if $\sum_{k=1}^n \alpha_k = 1$.
\end{defn}

\begin{defn}\label{smplx002def} If $a$ and $b$ are vectors in the vector space $V$, then the
 \index{closed!segment}%
 \index{segment, closed}%
\df{closed segment} between $a$ and $b$, denoted
 \index{<bracket@$[a,b]$ (closed segment in a vector space)}%
by~$[a,b]$, is $\{(1 - t)a + tb \colon 0 \le t \le 1\}$.
\end{defn}

\begin{cau} Notice that there is a slight conflict between this notation, when applied to the vector space $\R$ of real numbers, and the usual notation for closed
intervals on the real line. In $\R$ the closed segment $[a,b]$ is the same as the closed interval $[a,b]$ provided that $a \le b$. If $a > b$, however, the closed
segment $[a,b]$ is the same as the segment $[b,a]$, it contains all numbers $c$ such that $b \le c \le a$, whereas the closed interval $[a,b]$ is empty.
\end{cau}

\begin{defn} A subset $C$ of a vector space $V$ is
 \index{convex!set}%
\df{convex} if the closed segment $[a,b]$ is contained in $C$ whenever $a$, $b \in C$.
\end{defn}

\begin{defn}\label{smplx005def} Let $A$ be a subset of a vector space $V$.  The
 \index{convex!hull}%
 \index{hull, convex}%
\df{convex hull} of $A$ is the smallest convex subset of $V$ which contain~$A$.
\end{defn}

\begin{exer}\label{smplx006} Show that definition~\ref{smplx005def} makes sense by showing that the intersection of a family of convex subsets of a vector space
is itself convex.  Then show that a ``constructive characterization'' is equivalent; that is, prove that the convex hull of $A$ is the set of all convex
combinations of elements of~$A$.
\end{exer}

\begin{defn} A set $S = \{v_0,v_1,\dots,v_p\}$ of $p+1$ vectors in a vector space $V$ is
 \index{convex!independent}%
 \index{independent!convex}%
\df{convex independent} if the set $\{v_1 - v_0\,,\, v_2 - v_0\,,\, \dots,\, v_p - v_0\}$ is linearly independent in~$V$.
\end{defn}

\begin{defn} An
 \index{affine subspace}%
 \index{subspace!affine}%
\df{affine subspace} of a vector space $V$ is any translate of a linear subspace of~$V$.
\end{defn}

\begin{exam} The line whose equation is $y = 2x - 5$ is not a linear subspace of~$\R^2$.  But it is an affine subspace: it is the line determined by the equation
$y = 2x$ (which \emph{is} a linear subspace of $\R^2$) translated downwards parallel to the $y$-axis by 5 units.
\end{exam}

\begin{defn} Let $p \in \Z^+$.  The closed convex hull of a convex independent set $S = \{v_0, \dots, v_p\}$ of $p+1$ vectors in some vector space is a
 \index{psimplex@$p$\,-simplex}%
 \index{simplex!closed}%
 \index{closed!simplex}%
 \index{<list@$[s] = [v_0,\dots,v_p]$ (closed $p\,$-simplex)}%
\df{closed $p$\,-simplex}.  It is denoted by $[s]$ or by $[v_0. \dots, v_p]$. The integer $p$ is the
 \index{simplex!dimension of a}%
 \index{dimension!of a simplex}%
\df{dimension} of the simplex. The
 \index{simplex!open}%
 \index{open!simplex}%
 \index{<list@$(s) = (v_0,\dots,v_p)$ (open $p$\,-simplex)}%
\df{open $p$\,-simplex} determined by the set $S$ is the set of all convex combinations $\sum_{k=0}^p \alpha_k v_k$ of elements of $S$ where each
$\alpha_k > 0$. The open simplex will be denoted by~$(s)$ or by $(v_0, \dots, v_p)$.  We make the special convention that a single vector $\{v\}$ is
both a closed and an open $0$\,-simplex.

If $[s]$ is a simplex in $\R^n$ then the
 \index{simplex!plane of a}%
 \index{plane!of a simplex}%
\df{plane} of $[s]$ is the affine subspace of $\R^n$ having the least dimension which contains~$[s]$.  It turns out that the open simplex $(s)$ is the
interior of $[s]$ in the plane of~$[s]$.
\end{defn}

\begin{defn} Let $[s] = [v_0, \dots, v_p]$ be a closed $p$\,-simplex in~$\R^n$ and $\{j_0, \dots, j_q\}$ be a nonempty subset of $\{0,1,\dots,p\}$.
Then the closed $q$\,-simplex $[t] = [v_{j_0}, \dots, v_{j_q}]$ is a
 \index{closed!face of a simplex}%
 \index{simplex!face of a}%
 \index{face}%
\df{closed $q$-face} of~$[s]$. The corresponding open simplex $(t)$ is an
 \index{open!face of a simplex}%
\df{open $q$-face} of~$[s]$. The $0$\,-faces of a simplex are called the
 \index{vertex!of a simplex}%
 \index{simplex!vertex of a}%
\df{vertices} of the simplex.
\end{defn}

Note that distinct open faces of a closed simplex $[s]$ are disjoint and that the union of all the open faces of $[s]$ is $[s]$ itself.

\begin{defn} Let $[s] = [v_0, \dots, v_p]$ be a closed $p$\,-simplex in~$\R^n$. We say that two orderings $(v_{i_0}, \dots, v_{i_p})$ and
$(v_{j_0}, \dots, v_{j_p})$ of the vertices are
 \index{equivalent!orderings of vertices}%
\df{equivalent} if $(j_0, \dots, j_p)$ is an even permutation of $(i_0, \dots, i_p)$.  (This \emph{is} an equivalence relation.) For $p \ge 1$
there are exactly two equivalence classes; these are the
 \index{orientation!of a simplex}%
\df{orientations} of~$[s]$. An
 \index{oriented!simplex}%
 \index{simplex!oriented}%
\df{oriented simplex} is a simplex together with one of these orientations.  The oriented simplex determined by the ordering $(v_0, \dots, v_p)$
will be denoted
 \index{<list@$\langle s \rangle = \langle v_0,\dots,v_p \rangle$ (closed $p$\,-simplex)}%
by $\langle v_0, \dots, v_p \rangle$. If, as above, $[s]$ is written as $[v_0, \dots, v_p]$, then we may shorten $\langle v_0, \dots, v_p \rangle$
to~$\langle s \rangle$.

Of course, none of the preceding makes sense for $0$\,-simplexes.  We arbitrarily assign them two orientations, which we denote by $+$ and~$-$.  Thus
$\langle s \rangle$ and $-\langle s \rangle$ have opposite orientations.
\end{defn}

\begin{defn} A finite collection $K$ of open simplexes in $\R^n$ is a
 \index{simplicial!complex}%
 \index{complex!simplicial}%
\df{simplicial complex} if the following conditions are satisfied:
  \begin{enumerate}
    \item if $(s) \in K$ and $(t)$ is an open face of $[s]$, then $(t) \in K$; and
    \item if $(s)$, $(t) \in K$ and $(s) \ne (t)$, then $(s) \cap (t) = \emptyset$.
  \end{enumerate}
The
 \index{dimension!of a simplicial complex}%
 \index{simplicial!complex!dimension of a}%
 \index{complex!simplicial!dimension of a}%
 \index{dimK@$\dim K$ (dimension of a simplicial complex~$K$)}%
\df{dimension} of a simplicial complex $K$, denoted by $\dim K$, is the maximum dimension of the simplexes constituting~$K$.  If $r \le \dim K$, then the
 \index{rskeleton@$r$\,-skeleton}%
 \index{skeleton of a complex}%
\df{$r$-skeleton} of $K$, denoted by $K^r$, is the set of all open simplexes in $K$ whose dimensions are no greater than~$r$.  The
 \index{polyhedron}%
 \index{<bracket@$\abs K$ (polyhedron of a complex)}%
\df{polyhedron}, $\abs K$, of the complex $K$ is the union of all the simplexes in~$K$.
\end{defn}

\begin{defn} Let $K$ be a simplicial complex in $\R^n$.  For $0 \le p \le \dim K$ let
$A_p(K)$ (or just $A_p$) denote the free vector space generated by the set of all
oriented $p$\,-simplexes belonging to~$K$. For $1 \le p \le \dim K$ let $W_p(K)$ (or just
$W_p$) be the subspace of $A_p$ generated by all elements of the form
   \[ \langle v_0, v_1, v_2, \dots, v_p \rangle +
                           \langle v_1, v_0, v_2, \dots, v_p \rangle \]
and
 \index{cp@$C_p(K)$ ($p$\,-chains)}%
let $C_p(K)$ (or just $C_p$) be the resulting quotient space~$A_p/W_p$.For $p = 0$ let
$C_p = A_p$ and for $p < 0$ or $p > \dim K$ let $C_p = \{ \vc 0\}$.  The elements of
$C_p$ are the
 \index{pchains@$p$\,-chains}%
 \index{chains}%
\df{$p$\,-chains} of~$K$.

Notice that for any $p$ we have
  \[ [\langle v_0, v_1, v_2, \dots, v_p \rangle] = -[\langle v_1, v_0, v_2, \dots, v_p \rangle]\,. \]
To avoid cumbersome notation we will not distinguish between the $p$\,-chain $[\langle v_0, v_1, v_2, \dots, v_p \rangle]$ and its representative
$\langle v_0, v_1, v_2, \dots, v_p \rangle$.
\end{defn}

\begin{defn} Let $\langle s \rangle = \langle v_0, v_1, \dots, v_{p+1} \rangle$ be an oriented $(p+1)$\,-simplex. We define the
 \index{boundary!of a simplex}%
 \index{<@$\partial\langle s \rangle$ (boundary of a simplex)}%
\df{boundary} of $\langle s \rangle$, denoted by $\partial\langle s \rangle$, by
  \[ \partial\langle s \rangle  = \sum_{k=0}^{p+1}(-1)^k \langle v_0, \dots, \wh v_k, \dots, v_{p+1} \rangle\,. \]
(The caret above the $v_k$ indicates that that term is missing; so the boundary of a $(p+1)$\,-simplex is an alternating sum of $p$\,-simplexes.)
\end{defn}

\begin{defn} Let $K$ be a simplicial complex in~$\R^n$. For $1 \le p \le \dim K$ define
   \[ \partial_p = \partial \colon C_{p+1}(K) \sto C_p(K)\colon \]
as follows. If $\sum a(s) \langle s \rangle$ is a $p$\,-chain in $K$, let
   \[ \partial \bigl(\sum a(s) \langle s \rangle\bigr)
                        = \sum a(s) \partial\langle s \rangle\,. \]
For all other $p$ let $\partial_p$ be the zero map. The maps $\partial_p$ are called
 \index{boundary!maps}%
 \index{<@$\partial_p$ (boundary map)}%
\df{boundary maps}.  Notice that each $\partial_p$ is a linear map.
\end{defn}

\begin{prop}\label{smplx016} If $K$ is a simplicial complex in~$\R^n$, then $\partial^{\,2}\colon C_{p+1}(K) \sto C_{p-1}(K)$ is identically zero.
\end{prop}

\begin{proof}[\emph{Hint for proof}] It suffices to prove this for generators $\langle v_0, \dots, v_{p+1} \rangle$.   \ns
\end{proof}

\begin{defn}  Let $K$ be a simplicial complex in~$\R^n$ and $0 \le p \le \dim K$.  Define
 \index{zpk@$Z_p(K)$ (space of simplicial $p$\,-cycles)}%
$Z_p(K) = Z_p$ to be the kernel of $\partial_{p-1}\colon C_p \sto C_{p-1}$ and
 \index{bpk@$B_p(K)$ (space of simplicial $p$\,-boundaries)}%
$B_p(K) = B_p$ to be the range of $\partial_p\colon C_{p+1} \sto C_p$. The members of
$Z_p$ are
 \index{cycles}%
 \index{pcycles@$p$\,-cycles}%
\df{$p$\,-cycles} and the members of $B_p$ are
 \index{boundaries}%
 \index{pboundaries@$p$\,-boundaries}%
\df{$p$\,-boundaries}.

It is clear from proposition~\ref{smplx016} that $B_p$ is a subspace of the vector space~$Z_p$. Thus we may
 \index{hpk@$H_p(K)$ (the $p^{\text{th}}$ simplicial homology group)}%
 \index{simplicial homology group}%
 \index{homology}%
 \index{group!simplicial homology}%
define $H_p(K) = H_p$ to be $Z_p/B_p$. It is the $p^{\text{th}}$ \df{simplicial homology group} of~$K$. (And, of course, $Z_p$, $B_p$, and $H_p$
are the trivial vector space whenever $p < 0$ or $p > \dim K$.)
\end{defn}

\begin{exer}\label{smplx018} Let $K$ be the topological boundary (that is, the $1$\,-skeleton) of an oriented $2$\,-simplex in $\R^2$.  Compute
$C_p(K)$, $Z_p(K)$, $B_p(K)$, and~$H_p(K)$ for each~$p$.
\end{exer}

\begin{exer} What changes in exercise~\ref{smplx018} if $K$ is taken to be the oriented $2$\,-simplex itself?
\end{exer}

\begin{exer} Let $K$ be the simplicial complex in $\R^2$ comprising two triangular regions similarly oriented with a side in common. For all $p$
compute $C_p(K)$, $Z_p(K)$, $B_p(K)$, and~$H_p(K)$.
\end{exer}

\begin{defn} Let $K$ be a simplicial complex. The number $\beta_p := \dim H_p(K)$ is the
 \index{Betti number}%
 \index{betap@$\beta_p$ (Betti number)}%
$p^{\text{th}}$ \df{Betti number} of the complex~$K$. And $\chi(K) := \sum_{p=0}^{\dim K}
(-1)^p\beta_p$ is the
 \index{Euler characteristic}%
 \index{characteristic!Euler}%
 \index{chik@$\chi(K)$ (Euler characteristic)}%
\df{Euler characteristic} of~$K$.
\end{defn}

\begin{prop} Let $K$ be a simplicial complex.  For $0 \le p \le \dim K$ let $\alpha_p$ be
the number of $p$\,-simplexes in~$K$.  That is, $\alpha_p = \dim C_p(K)$.  Then
   \[ \chi(K) = \sum_{p=0}^{\dim K} (-1)^p\alpha_p. \]
\end{prop}























\section{Simplicial Cohomology}
\begin{defn} Let $K$ be a simplicial complex. For each $p \in \Z$
 \index{cpk@$C^p(K)$ ($p$\,-cochains)}%
let $C^p(K) = \bigl(C_p(K)\bigr)^*$. The elements of $C^p(K)$ are
 \index{pcochain@$p$\,-cochain}%
 \index{cochain}%
\df{(simplicial) $p$\,-cochains}. Then the adjoint ${\partial_p}^*$ of the boundary map
   \[ \partial_p\colon C_{p+1}(K) \sto C_p(K) \]
is the linear map
   \[ {\partial_p}^* = \partial^*\colon C^p(K) \sto C^{p+1}(K)\,. \]
(Notice that $\partial^* \circ \partial^* = 0$.)

Also define
 \begin{enumerate}
    \item $Z^p(K) := \ker {\partial_p}^*$;
  \vskip 5 pt
    \item $B^p(K) := \ran {\partial_{p-1}}^*$; and
  \vskip 5 pt
    \item $H^p(K) := Z^p(K)/B^p(K)$.
 \end{enumerate}
Elements of $Z^p(K)$ are
 \index{zpk@$Z^p(K)$ (space of simplicial $p$\,-cocycles)}%
 \index{pcocycle@$p$\,-cocycle!simplicial}%
 \index{cocycle!simplicial}%
 \index{simplicial!cocycle}%
\df{(simplicial) $p$\,-cocycles} and elements of $B^p(K)$ are
 \index{bpk@$B^p(K)$ (space of simplicial $p$\,-coboundaries)}%
 \index{pcoboundary@$p$\,-coboundary!simplicial}%
 \index{coboundary!simplicial}%
 \index{simplicial!coboundary}%
\df{(simplicial) $p$\,-coboundaries}. The vector space $H^p(K)$ is the
 \index{hpk@$H^p(K)$ (the $p^{\text{th}}$ simplicial cohomology group)}%
 \index{cohomology!group!simplicial}%
 \index{simplicial!cohomology group}%
 \index{group!simplicial cohomology}%
\df{$p^{\text{th}}$ simplicial cohomology group} of~$K$.
\end{defn}

\begin{prop} If $K$ is a simplicial complex in $\R^n$, then $H^p(K) \cong \bigl(H_p(K)\bigr)^*$ for every integer~$p$.
\end{prop}

\begin{defn} Let $F\colon N \sto M$ be a smooth injection between smooth manifolds. The pair $(N,F)$ is a
 \index{submanifold}%
 \index{smooth!submanifold}%
\df{smooth submanifold} of $M$ if $dF_n$ is injective for every $n \in N$.
\end{defn}

\begin{defn} Let $M$ be a smooth manifold, $K$ be a simplicial complex in $\R^n$, and $h \colon [K] \sto M$ be a homeomorphism. The triple $(M,K,h)$ is a
 \index{smooth!triangulation}%
 \index{triangulated manifold}%
 \index{manifold!smoothly triangulated}%
\df{smoothly triangulated manifold} if for every open simplex $(s)$ in $K$ the map $\sbsb{h\bigl|}{[s]} \colon [s] \sto M$ has an extension $h_s\colon U \sto M$
to a neighborhood $U$ of $[s]$ lying in the plane of~[s] such that $(U,h_s)$ is a smooth submanifold of~$M$.
\end{defn}

\begin{thm} A smooth manifold can be triangulated if and only if it is compact.
\end{thm}

The proof of this theorem is tedious enough that very few textbook authors choose to include it in their texts.  You can find a ``simplified''
proof in~\cite{Cairns:1961}.

\begin{thm}[de Rham's theorem] If $(M,K,\phi)$ is a smoothly triangulated manifold, then
   \[ H^p(M) \cong H^p(K) \]
for every $p \in \Z$.
\end{thm}

\begin{proof} See~\cite{Hu:1969}, chapter IV, theorem 3.1; \cite{Lee:2003}, theorem 16.12; \cite{SingerT:1967}, pages 164--173; and
\cite{Warner:1983}, theorem 4.17.   \ns
\end{proof}

\begin{prop}[pullbacks of differential forms]\label{smplx036prop} Let $F\colon M \sto N$ be a smooth mapping between smooth manifolds.  Then there exists
an algebra homomorphism $F^*\colon \bigwedge(N) \sto \bigwedge(M)$, called the
 \index{pullback of differential forms}%
 \index{differential!form!pullback of}%
\df{pullback} associated with~$F$ which satisfies the following conditions:
 \begin{enumerate}
    \item $F^*$ maps $\bigwedge^{\!p}(N)$ into $\bigwedge^{\!p}(M)$ for each~$p$;
    \item $F^*(g) = g \circ F$ for each $0$-form $g$ on~$N$; and
    \item $(F^*\mu)_m(v) = \mu_{F(m)}(dF_m(v))$ for every $1$-form $\mu$ on $N$, every $m \in M$, and every $v \in T_m$.
 \end{enumerate}
\end{prop}

\begin{prop} If $F\colon M \sto N$ is a smooth map between $n$-manifolds, then $F^*$ is a cochain map from the cochain complex $(\bigwedge^{\!*}(N),\,d\,)$
to the cochain complex $(\bigwedge^{\!*}(M),\,d\,)$. That is, the diagram
 \[\xy
   \square<1000,700>[\bigwedge^{\!p}(N)`\bigwedge^{\!p+1}(N)`\bigwedge^{\!p}(M)`\bigwedge^{\!p+1}(M);d`F^*`F^*`d]
 \endxy\]
commutes for every $p \in \Z$.
\end{prop}










\endinput
\chapter{THE SPECTRAL THEOREM FOR INNER PRODUCT SPACES}

\textbf{In this chapter all vector spaces (and algebras) have complex or real scalars.}
 \index{conventions!in chapter 5 all vector spaces are real or complex}%


\section{Inner Products}
\begin{defn}\label{inner_product_defn} Let $V$ be a complex (or a real) vector space.  A function which associates to each pair of vectors $x$ and
$y$ in $V$ a complex number (or,  in the case of a real vector space, a real number)
 \index{<bracketspointy@$\langle x,y \rangle$ (inner product)}%
$\langle x,y \rangle$ is an
 \index{inner product}%
 \index{product!inner}%
\df{inner product} (or a \df{dot product}) on $V$ provided that the following four
conditions are satisfied:
 \begin{enumerate}
   \item[(a)] If $x$, $y$, $z \in V$, then
       \[ \langle x + y , z \rangle = \langle x , z \rangle + \langle y , z \rangle. \]
   \item[(b)] If $x$, $y \in V$, then
       \[ \langle \alpha x , y \rangle = \alpha \langle x , y \rangle. \]
   \item[(c)] If $x$, $y \in V$, then
       \[ \langle x , y \rangle = \overline{\langle y , x \rangle}. \]
   \item[(d)] For every nonzero $x$ in $V$ we have $\langle x,x \rangle > 0$.
 \end{enumerate}

\noindent Conditions (a) and (b) show that an inner product is linear in its first variable.  Conditions (a) and (b) of proposition~\ref{ip002prop}
say that an inner product is
 \index{conjugate!linear}%
 \index{linear!conjugate}%
\df{conjugate linear} in its second variable.  When a mapping is linear in one variable and conjugate linear in the other, it is often called
 \index{sesquilinear}%
 \index{linear!sesqui-}%
\df{sesquilinear} (the prefix ``sesqui-'' means ``one and a half''). Taken together conditions (a)--(d) say that the inner product is a \emph{positive
definite conjugate symmetric sesquilinear form}.  Of course, in the case of a real vector space, the complex conjugation indicated in~(c) has no effect
and the inner product is a \emph{positive definite symmetric bilinear form}.
 \index{inner product!space}%
 \index{space!inner product}%
A vector space on which an inner product has been defined is an

\df{inner product space}.
\end{defn}

\begin{prop}\label{ip002prop} If $x$, $y$, and $z$ are vectors in an inner product space
and $\alpha \in \C$, then
 \begin{enumerate}
   \item[(a)] $\langle x,y + z \rangle
                = \langle x,y \rangle
                + \langle x,z \rangle$,
   \item[(b)] $\langle x , \alpha y \rangle
                = \conj\alpha \langle x,y \rangle$, and
   \item[(c)] $\langle x,x \rangle = 0$ if and only if $x = 0$.
 \end{enumerate}
\end{prop}

\begin{exam}\label{ip005x} For vectors $x = (x_1,x_2,\dots,x_n)$ and $y = (y_1,y_2,\dots,y_n)$
belonging to $\C^n$ define
   \[ \langle x,y \rangle = \sum_{k=1}^n x_k \conj{y_k}\,. \]
Then $\C^n$ is an inner product space.
\end{exam}

\begin{exam}\label{inner_prod_X_Cab} For $a < b$ let $\fml C([a,b],\C)$ be the family of all continuous complex valued functions on the interval $[a,b]$.
For every $f$, $g \in \fml C([a,b],\C)$ define
   \[ \langle f,g \rangle = \int_a^b f(x) \conj{g(x)}\,dx. \]
Then $\fml C([a,b],\C)$ is a complex inner product space. In a similar fashion, $\fml C([a,b]) = \fml C([a,b],\R)$ is made into a real inner product space.
\end{exam}

\begin{defn}\label{norm_notn} When a (real or complex) vector space has been equipped with an inner product we define the
 \index{norm}%
 \index{vector!norm of a}%
\df{norm} of a vector $x$ by
  \[ \norm x := \sqrt{\langle x,x \rangle}; \]
(This somewhat optimistic terminology is justified in proposition~\ref{ip009prop} below.)
\end{defn}

\begin{thm}\label{ip008thm} In every inner product space the
 \index{Schwarz inequality}%
 \index{inequality!Schwarz}%
\emph{Schwarz inequality}
   \[ \abs{\langle x,y \rangle} \le \norm x \,\norm y. \]
holds for all vectors $x$ and~$y$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Let $V$ be an inner product space and fix vectors $x$, $y \in V$.  For every scalar $\alpha$ we know that
  \begin{equation}\label{Schwarz_ineq}
      0 \le \langle x - \alpha y, x - \alpha y \rangle.
  \end{equation}
Expand the right hand side of~\eqref{Schwarz_ineq} into four terms and write $\langle y,x \rangle$ in polar form: $\langle y,x \rangle = r e^{i\theta}$,
where $r > 0$ and $\theta \in \R$.  Then in the resulting inequality consider those $\alpha$ of the form $te^{-i\theta}$ where $t \in \R$.
Notice that now the right side of~\eqref{Schwarz_ineq} is a quadratic polynomial in~$t$.  What can you say about its discriminant?  \ns
\end{proof}

\begin{exer} If $a_1, \dots ,a_n > 0$, then
  \[ \biggl(\sum_{j=1}^n a_j\biggr)\biggl(\sum_{k=1}^n \frac1{a_k}\biggr) \ge n^2. \]
The proof of this is obvious from the Schwarz inequality if we choose $x$ and $y$ to be what?
\end{exer}

\begin{exer} In this exercise notice that part (a) is a special case of part~(b).
 \begin{enumerate}
  \item[(a)] Show that if $a$, $b$, $c > 0$, then $\bigl(\frac12 a + \frac13 b + \frac16 c\bigr)^2 \le \frac12 a^2 + \frac13 b^2 + \frac16 c^2$.

\vskip 3 pt

  \item[(b)] Show that if $a_1, \dots,a_n,w_1, \dots,w_n > 0$ and $\sum_{k=1}^n w_k = 1$, then
     \[ \biggl(\sum_{k=1}^n a_kw_k\biggr)^2 \le \sum_{k=1}^n {a_k}^2w_k. \]
 \end{enumerate}
\end{exer}

\begin{exer} Show that if $\sum_{k=1}^\infty{a_k}^2$ converges, then $\sum_{k=1}^\infty k^{-1}a_k$ converges absolutely.
\end{exer}

\begin{exam}\label{ip006x} A sequence $(a_k)$ of (real or) complex numbers is said to be \df{square summable}
 \index{square summable sequence}%
 \index{summable!square}%
if $\sum_{k=1}^\infty \abs{a_k}^2 < \infty$.  The vector space of all square summable sequences of real numbers (respectively, complex numbers) is denoted
 \index{l2@$l_2(\R)$, $l_2(\C)$ (square summable sequences)}%
by $l_2(\R)$ (respectively, $l_2(\C)$). When no confusion will result, both are denoted by~$l_2$.  If $a$, $b \in l^2$, define
   \[ \langle a,b \rangle = \sum_{k=1}^\infty a_k\conj{b_k}. \]
(It must be shown that this definition makes sense and that it makes $l_2$ into an inner product space.)
\end{exam}

\begin{defn} Let $V$ be a complex (or real) vector space. A function
 \index{<norm@$\norm x$ (norm of a vector)}%
$\norm{\hphantom{x}} \colon V \sto \R \colon x \mapsto \norm{x}$ is a
 \index{norm}%
\df{norm} on $V$ if
 \begin{enumerate}
  \item[(i)] $\norm{x + y} \le \norm{x} + \norm{y}$ \qquad  for all $x$, $y \in V$;
  \item[(ii)] $\norm{\alpha x} = \abs{\alpha}\,\norm{x}$\qquad for all $x \in V$ and $\alpha \in \C \textrm{ (or $\R$)}$; and
  \item[(iii)] if $\norm{x} = 0$, then $x = \vc 0$.
 \end{enumerate}
The expression $\norm{x}$ may be read as ``the \emph{norm} of $x$'' or ``the
 \index{length!of a vector}%
\emph{length} of $x$''.

A vector space on which a norm has been defined is a
 \index{normed!linear space}%
 \index{normed!vector space|seeonly{normed linear space}}%
 \index{space!normed linear}%
\df{normed linear space} (or
 \index{vector!space!normed}%
\df{normed vector space}). A vector in a normed linear space which has norm $1$ is a
 \index{unit!vector}%
 \index{vector!unit}%
\df{unit vector}.
\end{defn}

\begin{prop} If $\norm{\hphantom{O}}$ is norm on a vector space $V$, then $\norm x \ge 0$ for every $x \in V$ and $\norm{\vc 0} = 0$.
\end{prop}

As promised in definition~\ref{norm_notn} we can verify the (somewhat obvious) fact that every inner product space is a normed linear space
(and therefore a topological---in fact, a metric---space).

\begin{prop}\label{ip009prop} Let $V$ be an inner product space. The map $x \mapsto \norm
x$ defined on $V$ in~\ref{norm_notn} is a norm on~$V$.
\end{prop}

\begin{prop}[The parallelogram law]  If $x$ and $y$ are vectors in an inner product
 \index{parallelogram law}%
space, then
   \[ \norm{x + y}^2 + \norm{x - y}^2 = 2\norm x^2 + 2\norm y^2\,. \]
\end{prop}

\enlargethispage{\baselineskip}

\begin{exam}\label{00015061} Consider the space $\fml C([0,1])$ of continuous complex valued functions defined on $[0,1]$.  Under the
 \index{uniform!norm}%
 \index{norm!uniform}%
\df{uniform norm} $\norm f_u := \sup\{\abs{f(x)}\colon 0 \le x \le 1\}$ the vector space $\fml C([0,1])$ is a normed linear space, but there is no inner product on
$\fml C([0,1])$ which induces this norm.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Use the preceding proposition. \ns
\end{proof}

\begin{prop}[The polarization identity]\label{0001507} If $x$ and $y$ are vectors
 \index{polarization identity}%
in a complex inner product space, then
 \[ \langle x,y \rangle = \tfrac14 (\norm{x + y}^2 - \norm{x - y}^2 + i\,\norm{x + iy}^2 - i\,\norm{x - iy}^2)\,. \]
\end{prop}

\begin{exer} What is the corresponding formula for the \emph{polarization identity} in a real inner product space?
\end{exer}























\section{Orthogonality}
\begin{defn} Vectors $x$ and $y$ in an inner product space $H$ are
 \index{orthogonal}%
\df{orthogonal} (or
 \index{perpendicular}%
\df{perpendicular}) if $\langle x,y \rangle = 0$.  In this case we write
 \index{<binrelperp@$x \perp y$ (orthogonal vectors)}%
$x \perp y$.  Subsets $A$ and $B$ of $H$ are \df{orthogonal} if $a \perp b$ for every $a
\in A$ and $b \in B$.  In this case we write $A \perp B$.
\end{defn}

\begin{prop} Let $a$ be a vector in an inner product space $H$.  Then $a \perp x$ for every
$x \in H$ if and only if $a = 0$.
\end{prop}

\begin{prop}[The Pythagorean theorem]\label{00015037}  If $x \perp y$ in an inner
 \index{Pythagorean theorem}%
product space, then
   \[ \norm{x + y}^2 = \norm x^2 + \norm y^2\,. \]
\end{prop}

\begin{defn} If $M$ and $N$ are subspaces of an inner product space $H$ we use the notation
 \index{<binop@$M \oplus N$ (inner product space direct sum of $M$ and~$N$)}%
$H = M \oplus N$ to indicate not only that $H$ is the sum of $M$ and $N$ but also that $M$ and $N$ are orthogonal. Thus we say that $H$ is the
 \index{internal!orthogonal direct sum}
 \index{direct sum!internal orthogonal}%
(\df{internal}) \df{orthogonal direct sum} of $M$ and~$N$.
\end{defn}

\begin{prop} If $M$ and $N$ are subspaces of an inner product space $H$ and $H$ is the orthogonal direct sum of $M$ and $N$, then it is also
the vector space direct sum of $M$ and~$N$.
\end{prop}

As is the case with vector spaces in general, we make a distinction between internal and external direct sums.

\begin{defn}\label{0001504} Let $V$ and $W$ be inner product spaces.  For $(v,w)$ and
$(v',w')$ in $V \times W$ and $\alpha \in \C$ define
 \begin{align*}
      (v,w) + (v',w') &= (v + v', w + w') \\
\intertext{and}
         \alpha(v,w) &= (\alpha v,\alpha w)\,.
 \end{align*}
This results in a vector space, which is the \emph{(external) direct sum} of $V$ and~$W$.
To make it into an inner product space define
   \[ \langle (v,w),(v',w') \rangle = \langle v,v' \rangle + \langle w,w' \rangle. \]
This makes the direct sum of $V$ and $W$ into an inner product space.  It is the
(\df{external orthogonal})
 \index{external!direct sum}%
 \index{direct sum!external orthogonal}
 \index{<binop@$M \oplus N$ (inner product space direct sum of $M$ and~$N$)}%
\df{direct sum} of $V$ and $W$ and is denoted by $V \oplus W$.
\end{defn}

\begin{cau} Notice that the same notation $\oplus$ is used for both internal and external direct sums \emph{and} for both vector space direct sums (see
definitions~\ref{vsp_int_dir_sum} and~\ref{prods031def}) and orthogonal direct sums. So when we see the symbol $V \oplus W$ it is important
to be alert to context, to know which category we are in: vector spaces or inner product spaces, especially as it is common practice to omit
the word ``orthogonal'' as a modifier to ``direct sum'' even in cases when it is intended.
\end{cau}

\begin{exam} In $\R^2$ let $M$ be the $x$-axis and $L$ be the line whose equation is $y = x$. If we think of $\R^2$ as a (real) vector space,
then it is correct to write $\R^2 = M \oplus L$.  If, on the other hand, we regard $\R^2$ as a (real) inner product space, then
$\R^2 \ne M \oplus L$ (because $M$ and $L$ are not perpendicular).
\end{exam}

\begin{notn}\label{0001511} Let $V$ be an inner product space, $x \in V$, and $A$, $B \subseteq V$. If $x \perp a$ for every $a \in A$,
we write $x \perp A$; and if $a \perp b$ for every $a \in A$ and $b \in B$, we write $A \perp B$.  We define $A^\perp$, the
 \index{orthogonal!complement}%
 \index{complement!orthogonal}%
 \index{<superscript@$A^\perp$ (orthogonal complement of a set)}%
\df{orthogonal complement} of $A$, to be $\{x \in V\colon x \perp A\}$.  We write $A^{\perp\perp}$ for $\left(A^\perp\right)^\perp$.
\end{notn}

\begin{cau} The superscript $\perp$ is here used quite differently than in our study of vector spaces (see~\ref{annihilators_perp_notn}).  These two uses are,
however, by no means unrelated!  It is an instructive exercise to make explicit exactly what this relationship is.
\end{cau}

\begin{prop}\label{ip025prop} If $A$ is a subset of an inner product space $V$, then $A^\perp$ is a subspace of~$V$ and $A^\perp = (\spn A)^\perp$.
Furthermore, if $A \subseteq B \subseteq V$, then $B^\perp \preceq A^\perp$.
\end{prop}

\begin{defn} When a nonzero vector $x$ in an inner product space $V$ is divided by its norm the resulting vector $u = \dfrac x{\norm x}$ is clearly a unit vector.
 \index{normalizing a vector}%
We say that $u$ results from \df{normalizing} the vector~$x$. A subset $E$ of $V$ is
 \index{orthonormal}%
\df{orthonormal} if every pair of distinct vectors in $E$ are orthogonal and every vector in $E$ has length one.  If, in addition, $V$ is the span of $E$, then
 \index{orthonormal!basis}%
 \index{basis!orthonormal}%
$E$ is an \df{orthonormal basis} for~$V$.
\end{defn}

\begin{defn}\label{orthog_fourier_defn} Let $V$ be an inner product space and $E = \{e^1, e^2, \dots, e^n\}$ be a finite orthonormal subset of~$V$.  For each
$k \in \N_k$ let $x_k := \langle x, e^k \rangle$.  This scalar is called the
 \index{Fourier!coefficient}%
 \index{coefficient!Fourier}%
\df{Fourier coefficient} of $x$ with respect to~$E$. The vector $s := \sum_{k=1}^n x_ke^k$ is the
 \index{Fourier!sum}%
 \index{sum!Fourier}%
\df{Fourier sum} of $x$ with respect to~$E$.
\end{defn}

\begin{prop} Let notation be as in~\ref{orthog_fourier_defn}.  Then $s = x$ if and only if $x \in \spn E$.
\end{prop}

\begin{prop} Let notation be as in~\ref{orthog_fourier_defn}.  Then $x - s \perp e^k$ for $k = i, \dots, n$ and therefore $x - s \perp s$.
\end{prop}

The next result gives us a recipe for converting a finite linearly independent subset of an inner product space into an orthonormal basis for the span of that set.

\begin{thm}[Gram-Schmidt Orthonormalization] Let $A= \{a^1, a^2, \dots, a^n\}$ be a finite linearly independent subset of an inner product space $V$. Define
vectors $e^1$, \dots, $e^n$ recursively by setting
   \[ e^1 := {\norm {a^1}}^{-1}a^1 \]
and for $2 \le m \le n$
   \[ e^m := {\norm{a^m - s^m}}^{-1}(a^m - s^m) \]
where $s^m := \sum_{k=1}^{m-1} \langle a^m, e^k \rangle e^k$ is the Fourier sum for $a^m$ with respect to $E_{m-1} :=  \{e^1, \dots, e^{m-1}\}$.  Then
$E_n$ is an orthonormal basis for the span of~$A$.
\end{thm}

It should be clear from the proof of the preceding theorem that finiteness plays no essential role.  The theorem remains true for countable linearly independent sets
(as does its proof).

\begin{cor} Every finite dimensional inner product space has an orthonormal basis.
\end{cor}

\begin{exam} Let $\R[x]$ be the inner product space of real polynomials whose inner product is defined by
    \[ \langle p,q \rangle := \int_{-1}^1 \wt p(x) \wt q(x)\,dx \]
for all $p$, $q \in \R[x]$.  Application of the \emph{Gram-Schmidt} process to the set $\{1,x,x^2,x^3, \dots\}$ of real polynomials produces an orthonormal sequence
of polynomials known as the \emph{Legendre polynomials}.  Compute the first four of these.
\end{exam}

\begin{exam} Let $\R[x]$ be the inner product space of real polynomials whose inner product is defined by
    \[ \langle p,q \rangle := \int_0^\infty \wt p(x) \wt q(x)e^{-x}\,dx \]
for all $p$, $q \in \R[x]$.  Application of the \emph{Gram-Schmidt} process to the set $\{1,x,x^2,x^3, \dots\}$ of real polynomials produces an orthonormal sequence
of polynomials known as the \emph{Laguerre polynomials}.  Compute the first four of these.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Integration by parts or familiarity with the gamma function allows us  to conclude that $\int_0^\infty x^n e^{-x}\,dx = n!$
for each $n \in \N$. \ns
\end{proof}

\begin{prop}\label{ip028prop} If $M$ is a subspace of a finite dimensional inner product space $V$ then $V = M \oplus M^\perp$.
\end{prop}

\begin{exam} The subspace $l_c(\N,\R)$ of the inner product space $l_2(\R)$ (see example~\ref{ip006x}) shows that the preceding proposition does not hold
for infinite dimensional spaces.
\end{exam}

\begin{prop} Let $M$ be a subspace of an inner product space~$V$. Then
 \begin{enumerate}
  \item[(a)] $M \subseteq M^{\perp\perp}$;
  \item[(b)] equality need not hold in \emph{(a)}; but
  \item[(c)] if $V$ is finite dimensional, then $M = M^{\perp\perp}$.
 \end{enumerate}
\end{prop}

\begin{prop} If $S$ is a set of mutually perpendicular vectors in an inner product space and $\vc 0 \notin S$, then the set $S$ is linearly independent.
\end{prop}

\begin{prop}\label{orthog009prop} Let $M$ and $N$ be subspaces of an inner product space~$V$.  Then
 \begin{enumerate}
  \item[(a)] $(M + N)^\perp = (M \cup N)^\perp = M^\perp \cap N^\perp$ and
  \item[(b)] if $V$ is finite dimensional, then $(M \cap N)^\perp = M^\perp + N^\perp$.
 \end{enumerate}
\end{prop}

\begin{prop} Let $S$, $T\colon H \sto K$ be linear maps between inner product spaces $H$ and~$K$. If $\langle Sx,y \rangle = \langle Tx,y \rangle$ for every
$x \in H$ and $y \in K$, then $S = T$.
\end{prop}


\begin{prop}\label{C0635105} If $H$ is a complex inner product space and $T \in \ofml L(V)$ satisfies $\langle T\vc z, \vc z\rangle = 0$ for all 
$\vc z \in H$, then $T = 0$. 
\end{prop}

\begin{proof}[\emph{Hint for proof}] In the hypothesis replace $\vc z$ first by $\vc x + \vc y$ and then by $\vc x + i\vc y$.  \ns 
\end{proof}

\begin{exam} Give an example to show that the preceding result does not hold for real inner product spaces.  
\end{exam}

\begin{exam} Let $H$ be a complex inner product space and $a \in H$.  Define $\psi_a\colon H \sto \C$ by $\psi_a(x) = \langle x,a \rangle$
for all $x \in H$.  Then $\psi_a$ is a linear functional on~$H$.
\end{exam}

\begin{thm}[Riesz-Fr\'echet Theorem]\label{riesz_frechet_thm} If $f$ is a linear functional on a finite dimensional inner product space $H$,
 \index{Riesz-Fr\'echet theorem!for inner product spaces}%
then there exists a unique vector $a \in H$ such that
  \[f(x) = \langle x,a \rangle\]
for every $x \in H$.
\end{thm}

\begin{exam} Consider the function $\phi \colon l_c(\N) \sto \C \colon x \mapsto \sum_{k=1}^\infty \alpha_k$ where
$x = \sum_{k=1}^\infty \alpha_k e^k$, the $e^k$'s being the standard basis vectors for $l_c(\N)$.  This linear functional provides an example which
shows that the \emph{Riesz-Fr\'echet theorem} does not hold (as stated) in infinite dimensional spaces.
\end{exam}

Here is another example of the failure of this result in infinite dimensional spaces.

\begin{exam}  On the vector space $H$ of polynomials over~$\C$ define an inner product by $\langle p,q \rangle = \int_0^1 p(t) \overline{q(t)}\,dt$.
For a fixed $z \in \C$ define the functional $E_z$, called \emph{evaluation at $z$}, by $E_z(p) = p(z)$ for every $p \in H$.  Then $E_z$ belongs
to $H^*$ but that there is no polynomial $p$ such that $E_z(q) = \langle q, p \rangle$ for every $q \in H$.
\end{exam}

\begin{cau} It is important not to misinterpret the two preceding examples.  There is indeed a (very important!) version of the \emph{Riesz-Fr\'echet theorem}
which does in  fact hold for infinite dimensional spaces.  If we restrict our attention to \emph{continuous} linear functionals on \emph{complete} inner product
spaces, then the conclusion of theorem~\ref{riesz_frechet_thm} does indeed hold for infinite dimensional spaces.
\end{cau}

\begin{exer} Use vector methods (as described in~\ref{vector_spaces_rem_vgeom}---no coordinates, no major results from Euclidean geometry) to show that the
midpoint of the hypotenuse of a right triangle is equidistant from the vertices. \emph{Hint.} Let $\bigtriangleup ABC$ be a right triangle and $O$ be the
midpoint of the hypotenuse $AB$. What can you say about $\langle\,\overrightarrow{AO} +\overrightarrow{OC} , \overrightarrow{CO} + \overrightarrow{OB}\,\rangle$?
\end{exer}

\begin{exer} Use vector methods (as described in~\ref{vector_spaces_rem_vgeom}) to show that an angle inscribed in a semicircle is a right angle.
\end{exer}

\begin{exer} Use vector methods (as described in~\ref{vector_spaces_rem_vgeom}) to show that if a parallelogram has perpendicular diagonals, then it is a
rhombus (that is, all four sides have equal length).  \emph{Hint.} Let $ABCD$ be a parallelogram. Express the inner product of the diagonals
$\overrightarrow{AC}$ and $\overrightarrow{DB}$ in terms of the lengths of the sides $\overrightarrow{AB}$ and $\overrightarrow{BC}$.
\end{exer}

\begin{exer} Use vector methods (as described in~\ref{vector_spaces_rem_vgeom}) to show that the diagonals of a rhombus are perpendicular.
\end{exer}






















\section{Involutions and Adjoints}
\begin{defn}\label{involution_defn001}  An \df{involution} on a complex (or real) algebra $A$ is a map
 \index{involution}%
$x \mapsto x^\ast$ from $A$ into $A$ which satisfies
 \begin{enumerate}
   \item[(i)] $(x + y)^\ast = x^\ast + y^\ast$,
   \item[(ii)] $(\alpha x)^* = \conj\alpha\, x^*$,
   \item[(iii)] $x^{\ast\ast} = x$, and
   \item[(iv)] $(xy)^\ast = y^\ast x^\ast$
 \end{enumerate}
for all $x,y \in A$ and $\alpha \in \C \textrm{ (or $\R$)}$.  An algebra on which an involution has been
defined is a
 \index{<Z@$*\,$-algebra (algebra with involution)}%
 \index{star-algebra}%
 \index{involution!algebra with}%
\df{$*\,$-algebra } (pronounced ``star algebra''). An algebra homomorphism $\phi$
between $*\,$-algebras which preserves involution (that is, such that $\phi(a^*) = (\phi(a))^*$) is a
 \index{<Z@$*\,$-homomorphism (star homomorphism)}%
 \index{star-homomorphism}%
\df{$*\,$-homomorphism} (pronounced ``star homomorphism''.  A $*\,$-homomorphism
$\phi\colon A \sto B$ between unital algebras is said to be \df{unital} if $\phi(\vc 1_A)
= \vc 1_B$.
\end{defn}

\begin{prop} If $a$ and $b$ are elements of a $*\,$-algebra, then $a$ commutes with $b$ if and only if $a^*$ commutes with $b^*$.
\end{prop}

\begin{prop} In a unital $*\,$-algebra $\vc 1^* = \vc 1$.
\end{prop}

\begin{prop}\label{prop_left_iden} If a $*\,$-algebra $A$ has a left multiplicative identity $e$, then $A$ is unital and $e = \vc 1_A$.
\end{prop}

\begin{prop} An element $a$ of a unital $*\,$-algebra is invertible if and only if $a^*$ is.  And when $a$ is invertible we have
  \[ (a^*)^{-1} = \bigl(a^{-1}\bigr)^* . \]
\end{prop}

\begin{prop} Let $a$ be an element of a unital $*\,$-algebra.  Then $\lambda \in \sigma(a)$ if and only if $\conj\lambda \in \sigma(a^*)$.
\end{prop}

\begin{defn} An element $a$ of a complex $*\,$-algebra $A$ is
 \index{normal}%
\df{normal} if $a^*a = aa^*$.  It is
 \index{self-adjoint}%
\df{self-adjoint} (or
 \index{Hermitian}%
\df{Hermitian}) if $a^* = a$.  It is
 \index{skew-Hermitian}%
\df{skew-Hermitian} if $a^* = -a$.  And it is
 \index{unitary}%
\df{unitary} if $a^*a = aa^* = \vc 1$. The set of all self-adjoint elements of $A$ is
denoted
 \index{HA@$\fml H(A)$ (self-adjoint elements of~$A$)}%
by $\fml H(A)$, the set of all normal elements
 \index{NA@$\fml N(A)$ (normal elements of~$A$)}%
by~$\fml N(A)$, and the set of all unitary elements
 \index{UA@$\fml U(A)$ (unitary elements of~$A$)}%
by~$\fml U(A)$.

Oddly, and perhaps somewhat confusingly, history has dictated an alternative, but parallel, language for real algebras---especially algebras of
matrices and linear maps.  An element $a$ of a real $*\,$-algebra $A$ is
 \index{symmetric!element of a real $*\,$-algebra}%
\df{symmetric} if $a^* = a$.  It is
 \index{skew-symmetric}%
\df{skew-symmetric} if $a^* = -a$.  And it is
 \index{orthogonal!element of a real $*\,$-algebra}%
\df{orthogonal} if $a^*a = aa^* = \vc 1$.
\end{defn}

\begin{exam} Complex conjugation is an involution on the algebra $\C$ of complex numbers.
\end{exam}

\begin{exam} Transposition (see definition~\ref{transpose_defn}) is an involution on the real algebra $\mathbf M_n$ of $n \times n$ matrices.
\end{exam}

\begin{exam} Let $a < b$.  The map $f \mapsto \conj f$ of a function to its complex conjugate is an involution on the complex algebra~$\fml C([a,b],\C)$
of continuous complex valued functions on~$[a,b]$
\end{exam}

\begin{prop} For every element $a$ of a $*\,$-algebra $A$ there exist unique self-adjoint elements $u$ and $v$ in $A$ such that $a = u + iv$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] The self-adjoint element $u$ is called the
 \index{real!part of a $*\,$-algebra element}%
 \index{imaginary!part of a $*\,$-algebra element}%
\emph{real part} of~$a$ and $v$ the \emph{imaginary part} of~$a$.  \ns
\end{proof}

\begin{cor} An element of a $*\,$-algebra is normal if and only if its real part and its imaginary part commute.
\end{cor}

\begin{defn} Let $H$ and $K$ be complex inner product spaces and $T\colon H \sto K$ be a linear map.  If there exists a
 \index{<superscript@$T^*$ (inner product space adjoint of $T$)}%
function $T^*\colon K \sto H$ which satisfies
  \[\langle Tx, y \rangle = \langle x, T^*y \rangle\]
for all $x \in H$ and $y \in K$, then $T^*$ is the
 \index{adjoint}%
\df{adjoint} of~$T$.  If a linear map $T$ has an adjoint we say that $T$ is
 \index{adjointable}%
\df{adjointable}.  Denote the set of all adjointable maps from $H$ to $K$
 \index{AH@$\ofml A(H,K)$, $\ofml A(H)$ (adjointable maps)}%
by $\ofml A(H,K)$ and write $\ofml A(H)$ for~$\ofml A(H,H)$.

When $H$ and $K$ are real vector spaces, the adjoint of~$T$ is usually called the
 \index{transpose}%
\df{transpose} of~$T$ and the
 \index{<superscript@$T^t$ (transpose of $T$)}%
notation $T^t$ is used (rather than~$T^*$).
\end{defn}

\begin{prop} Let $T\colon H \sto K$ be a linear map between complex inner product spaces.  If the adjoint of $T$ exists, then it is unique.
(That is, there is at most one function $T^*\colon K \sto H$ that satisfies $\langle Tx, y \rangle = \langle x, T^*y \rangle$ for all $x \in H$
and $y \in K$.)
\end{prop}

Similarly, of course, if $T\colon H \sto K$ is a linear map between real inner product spaces and if the transpose of $T$ exists, then it is unique.

\begin{exam} Let $\fml C = \fml C([0,1])$ be the inner product space defined in example~\ref{inner_prod_X_Cab} and $J_0 = \{f \in \fml C\colon f(0) = 0\}$.
Then the inclusion map $\iota\colon J_0 \sto \fml C$ is an example of a map which is \emph{not} adjointable.
\end{exam}

\begin{exam} Let $U$ be the unilateral shift operator on $l_2$ (see example~\ref{ip006x})
 \index{unilateral shift!is adjointable on~$l_2$}%
 \[ U\colon l_2 \sto l_2\colon
      (x_1,x_2,x_3,\dots) \mapsto (0,x_1,x_2,\dots), \]
then its adjoint is given by
 \index{adjoint!of the unilateral shift}%
 \[ U^*\colon l_2 \sto l_2\colon
      (x_1,x_2,x_3,\dots) \mapsto (x_2,x_3,x_4,\dots). \]
\end{exam}

\begin{exam}[Multiplication operators] Let $\phi$ be a fixed continuous complex valued function on the interval~$[a,b]$.  On the inner product space
$\fml C = \fml C([a,b],\C)$ (see example~\ref{inner_prod_X_Cab}) define
   \[ M_\phi\colon \fml C \sto \fml C\colon f \mapsto \phi f\,. \]
Then $M_\phi$ is an adjointable operator on~$\fml C$.
\end{exam}

\begin{prop} Let $T\colon H \sto K$ be a linear map between complex inner product spaces. If the adjoint of $T$ exists, then it is linear.
\end{prop}

And, similarly, of course, if the transpose of a linear map between real inner product spaces exists, then it is linear.  In the sequel we will forgo the dubious
helpfulness of mentioning every obvious real analog of results holding for complex inner product spaces.

\begin{prop} Let $T\colon H \sto K$ be a linear map between complex inner product spaces. If the adjoint of $T$ exists, then so does the adjoint
of~$T^*$ and $T^{**} = T$.
\end{prop}

\begin{prop} Let $S\colon H \sto K$ and $T\colon K \sto L$ be linear maps between complex inner product space. Show that if $S$ and $T$ both have
adjoints, then so does their composite $TS$ and
  \[(TS)^* = S^*T^*.\]
\end{prop}

\begin{prop} If $T\colon H \sto K$ is an invertible linear map between complex inner product spaces and both $T$ and $T^{-1}$ have adjoints, then
$T^*$ is invertible and $(T^*)^{-1} = (T^{-1})^*$.
\end{prop}

\begin{prop} Let $S$ and $T$ be operators on a complex inner product space $H$. Then $(S + T)^* = S^* + T^*$ and $(\alpha T)^* = \conj\alpha T^*$ for
every $\alpha \in \C$.
\end{prop}

\begin{exam} If $H$ is a complex inner product space, then $\ofml A(H)$ is a unital complex $*\,$-algebra.  It is a unital subalgebra of $\ofml L(H)$.
\end{exam}

\begin{thm} Let $T$ be an adjointable operator on an inner product space~$H$. Then
  \begin{enumerate}
    \item[(a)] $\ker T = (\ran T^*)^\perp$ and
    \item[(b)] $\ran T^* \subseteq (\ker T)^\perp$.
If $H$ is finite dimensional, then equality holds in \emph{(b)}.
  \end{enumerate}
\end{thm}

\begin{thm} Let $T$ be an adjointable operator on an inner product space~$H$. Then
  \begin{enumerate}
    \item[(a)] $\ker T^* = (\ran T)^\perp$ and
    \item[(b)] $\ran T \subseteq (\ker T^*)^\perp$.
If $H$ is finite dimensional, then equality holds in \emph{(b)}.
  \end{enumerate}
\end{thm}

\begin{prop} Every linear map between finite dimensional complex inner product spaces is adjointable.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use the \emph{Riesz-Fr\'echet theorem}.  \ns
\end{proof}

\begin{cor} If $H$ is a finite dimensional inner product space then $\ofml L(H)$ is a
  \index{L@$\ofml L(V)$!as a unital $*\,$-algebra}%
  \index{algebra!with involution!$\ofml L(V)$ as a}%
unital $*\,$-algebra.
\end{cor}

\begin{prop} An adjointable operator $T$ on a complex inner product space $H$ is unitary if and only if it preserves inner products (that is,
if and only if $\langle Tx,Ty \rangle = \langle x,y \rangle$ for all $x$, $y \in H$).  Similarly, an adjointable operator on a real inner
product space is orthogonal if and only if it preserves inner products.  
\end{prop}

\begin{exer} Let $T\colon H \sto K$ be a linear map between finite dimensional complex inner product spaces.  Find the matrix representation of $T^*$
in terms of the matrix representation of~$T$. Also, for a linear map $T\colon H \sto K$ between finite dimensional real inner product spaces find the
matrix representation of $T^t$ in terms of the matrix representation of~$T$.
\end{exer}

\begin{prop} Every eigenvalue of a Hermitian operator on a complex inner product space is real.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $x$ be an eigenvector associated with an eigenvalue~$\lambda$ of an operator~$A$.  Consider
$\lambda \norm{x}^2$.  \ns
\end{proof}

\begin{prop} Let $A$ be a Hermitian operator on a complex inner product space. Prove that eigenvectors associated with distinct
eigenvalues of $A$ are orthogonal.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $x$ and $y$ be eigenvectors associated with distinct eigenvalues $\lambda$ and $\mu$ of~$A$.
Start your proof by showing that $\lambda \langle x,y \rangle = \mu \langle x,y \rangle$. \ns
\end{proof}

\begin{prop} Let $N$ be a normal operator on a complex inner product space~$H$.  Then $\norm{Nx} = \norm{N^*x}$ for every $x \in H$.
\end{prop}





















\section{Orthogonal Projections}
\begin{prop} Let $H$ be an inner product space and $M$ and $N$ be subspaces of $H$ such that $H = M + N$ and $M \cap N = \{\vc 0\}$. (That is, suppose
that $H$ is the \emph{vector space} direct sum of $M$ and~$N$.) Also let $P = \sbsb E{NM}$ be the projection of $H$ along $N$ onto~$M$.  Prove that $P$
is self-adjoint ($P^*$ exists and $P^* = P$) if and only if $M \perp N$.
\end{prop}

\begin{defn} A \df{projection} in a $*\,$-algebra $A$ is an element
 \index{projection!in a $*\,$-algebra}%
 \index{P@$\fml P(A)$ (projections on a $*\,$-algebra)}%
$p$ of the algebra which is idempotent ($p^2 = p$) and self-adjoint ($p^*=p$).  The set of all projections in $A$ is denoted by~$\fml P(A)$.

An operator $P$ on a complex inner product space $H$ is an
 \index{orthogonal!projection}%
 \index{projection!orthogonal}%
\df{orthogonal projection} if it is self-adjoint and idempotent; that is, if it is a projection in the $*\,$-algebra $\ofml A(H)$ of adjointable
operators on~$H$. (On a real inner product space, of course, the appropriate adjectives are \emph{symmetric} and \emph{idempotent}.)

Notice that a vector space projection $\sbsb E{MN}$ is, in general, \emph{not} a projection in any $*\,$-algebra.
\end{defn}

\begin{conv} It is standard practice to refer to orthogonal projections on inner product spaces simply as ``projections''.  This clearly invites confusion
 \index{conventions!in inner product spaces ''projection'' means ''orthogonal projection''}%
with the vector space notion of projection.  So one must be careful: just as the symbols $\oplus$ and $\perp$ have different meanings depending on context
(vector spaces or inner product spaces), so does the word ``projection''.  In these notes and elsewhere, when the context is inner product spaces the word
''projection'' frequently means ''orthogonal projection''
\end{conv}

\begin{prop} If $P$ is an orthogonal projection on an inner product space, then the space is the orthogonal direct sum of the range of $P$ and the kernel of~$P$.
\end{prop}

\begin{prop} Let $p$ and $q$ be projections in a $*\,$-algebra. Then the following are equivalent:
 \begin{enumerate}
  \item[(a)] $pq = \vc 0$;
  \item[(b)] $qp = \vc 0$;
  \item[(c)] $qp = -pq$;
 \index{projections!sum of}%
  \item[(d)] $p + q$ is a projection.
 \end{enumerate}
\end{prop}

\begin{defn} Let $p$ and $q$ be projections in a $*\,$-algebra.  If any of the conditions in the preceding result holds, then we say that $p$ and $q$ are
 \index{projection!orthogonal}%
 \index{projections!orthogonality of}%
 \index{orthogonal!projections!in a $*\,$-algebra}%
\df{orthogonal} and write
 \index{<binrelperp@$p \perp q$ (orthogonality of projections)}%
$p \perp q$.  (Thus for operators on an inner product space we can correctly speak of orthogonal orthogonal projections!)
\end{defn}

\begin{prop} Let $P$ and $Q$ be projections on an inner product space~$H$. Then $P \perp Q$
 \index{orthogonal!inner product projections}%
if and only if $\ran P \perp \ran Q$. In this case $P + Q$ is an orthogonal projection
whose kernel is $\ker P \cap \ker Q$ and whose range is $\ran P + \ran Q$.
\end{prop}

\begin{exam} On an inner product space projections need not commute. For example, let $P$ be the projection of the (real) inner product space $R^2$ onto the line
$y = x$ and $Q$ be the projection of $R^2$ onto the $x$-axis.  Then $PQ \ne QP$.
\end{exam}

\begin{prop} Let $p$ and $q$ be projections in a $*\,$-algebra. Then $pq$ is a
 \index{projections!product of}%
projection if and only if $pq = qp$.
\end{prop}

\begin{prop} Let $P$ and $Q$ be projections on an inner product space~$H$. If $PQ = QP$, then $PQ$ is a projection whose kernel is $\ker P + \ker Q$ and whose
range is $\ran P \cap \ran Q$.
\end{prop}

\begin{prop} Let $p$ and $q$ be projections in a $*\,$-algebra. Then the following are equivalent:
 \begin{enumerate}
  \item[(a)] $pq = p$;
  \item[(b)] $qp = p$;
 \index{projections!difference of}%
  \item[(c)] $q - p$ is a projection.
 \end{enumerate}
\end{prop}

\begin{defn}\label{ortho_projs_order_defn} Let $p$ and $q$ be projections in a $*\,$-algebra.  If any of the conditions in the preceding result holds, then we
 \index{<binrel@$p \preceq q$ (ordering of projections in a $*\,$-algebra)}%
 \index{projections!ordering of}%
 \index{ordering!partial!of projections}%
 \index{partial!ordering!of projections}%
write $p \preceq q$.  In this case we say that $p$ is a
 \index{subprojection}%
\df{subprojection} of~$q$ or that $p$ is \df{smaller} than~$q$.
\end{defn}

\begin{prop} If $A$ is a $*\,$-algebra, then the relation $\preceq$ defined in~\ref{ortho_projs_order_defn} is a partial ordering on~$\fml P(A)$.
If $A$ is unital, then $\vc 0 \preceq p \preceq \vc 1$ for every $p \in \fml P(A)$.
\end{prop}

\begin{notn} If $H$, $M$, and $N$ are subspaces of an inner product space, then the assertion
 \index{<binopdiff@$H \ominus N$}%
$H = M \oplus N$, may be rewritten as $M = H \ominus N$ (or $N = H \ominus M$).
\end{notn}

\begin{prop} Let $P$ and $Q$ be projections on an inner product space~$H$. Then the following are equivalent:
 \begin{enumerate}
  \item[(a)] $P \preceq Q$;
  \item[(b)] $\norm{Px} \le \norm{Qx}$ for all $x \in H$; and
  \item[(c)] $\ran P \subseteq \ran Q$.
 \end{enumerate}
In this case $Q - P$ is a projection whose kernel is $\ran P + \ker Q$ and whose range is
$\ran Q \ominus \ran P$.
\end{prop}

The next two results are optional: they will not be used in the sequel.

\begin{prop} Suppose $p$ and $q$ are projections on a $*\,$-algebra~$A$. If $pq = qp$, then
the infimum of $p$ and $q$, which we denote
 \index{<binop@$p \curlywedge q$ (infimum of projections)}%
 \index{infimum!of projections in a $*\,$-algebra}%
 \index{projections!infimum of}%
by $p \curlywedge q$, exists with respect to the partial ordering $\preceq$ and $p
\curlywedge q = pq$.  The infimum $p \curlywedge q$ may exist even when $p$ and $q$ do
not commute.  A necessary and sufficient condition that $p \perp q$ hold is that both $p
\curlywedge q = \vc 0$ and $pq = qp$ hold.
\end{prop}

\begin{prop} Suppose $p$ and $q$ are projections on a $*\,$-algebra~$A$. If $p \perp q$, then
the supremum of $p$ and $q$, which we denote
 \index{<binop@$p \curlyvee q$ (supremum of projections)}%
 \index{supremum!of projections in a $*\,$-algebra}%
 \index{projections!supremum of}%
by $p \curlyvee q$, exists with respect to the partial ordering $\preceq$ and $p
\curlyvee q = p + q$.  The supremum $p \curlyvee q$ may exist even when $p$ and $q$ are
not orthogonal.
\end{prop}





























\section{The Spectral Theorem for Inner Product Spaces}
\begin{defn} Two elements $a$ and $b$ of a ${}^*$-algebra $A$ are
 \index{unitarily!equivalent}%
 \index{equivalent!unitarily}%
\df{unitarily equivalent} if there exists a unitary element $u$ of $A$ such that $b =
u^*au$.
\end{defn}

\begin{defn} An operator $T$ on a complex inner product space $V$ is
 \index{unitarily!diagonalizable}%
 \index{diagonalizable!unitarily}%
\df{unitarily diagonalizable} if there exists an orthonormal basis for $V$ consisting of
eigenvectors of~$T$.
\end{defn}

\begin{prop} Let $A$ be an $n \times n$ matrix $A$ of complex numbers. Then $A$, regarded as an operator on $\C^n$, is unitarily diagonalizable if
and only if it is unitarily equivalent to a diagonal matrix.
\end{prop}


\begin{defn}  Let $M_1 \oplus \dots \oplus M_n$ be an orthogonal direct sum decomposition
of an inner product space~$H$. For each $k$ let $P_k$ be the orthogonal projection onto
$M_k$.  The projections $P_1$, \dots $P_n$ are the
 \index{orthogonal!projection!associated with an orthogonal direct sum decomposition}%
 \index{orthogonal!direct sum!projections associated with}%
\df{orthogonal projections associated with the orthogonal direct sum decomposition} $H =
M_1 \oplus \dots \oplus M_n$.  The family $\{P_1, \dots, P_n\}$ is an
 \index{resolution of the identity!orthogonal}%
 \index{orthogonal!resolution of the identity}%
\df{orthogonal resolution of the identity}.  (Compare this with
definitions~\ref{po013def} and~\ref{po010def}.)
\end{defn}


\begin{thm}[Spectral Theorem: Complex Inner Product Space Version] An operator $N$
 \index{spectral theorem!for complex inner product spaces}%
 \index{inner product!space!spectral theorem for}%
on a finite dimensional complex inner product space~$V$ is normal if and only if it is
unitarily diagonalizable in which case it can be written as
  \[ N = \sum_{k=1}^n \lambda_kP_k \]
where $\lambda_1$, \dots, $\lambda_n$ are the (distinct) eigenvalues of $N$ and $\{P_1,
\dots, P_n\}$ is the orthogonal resolution of the identity whose orthogonal projections
are associated with the corresponding eigenspaces $M_1$, \dots, $M_n$.
\end{thm}

\begin{proof} See~\cite{Roman:2005}, theorems~10.13 and~10.21; or~\cite{HoffmanK:1971}, chapter~8, theorems~20 and~22, and chapter~9. theorem~9. \ns
\end{proof}

\begin{exer} Let $N$ be the operator on $\C^2$ whose matrix representation is
  \[ \begin{bmatrix}  0  &  1  \\ -1  &  0  \end{bmatrix}. \]

\vskip 3 pt

 \begin{enumerate}
  \item[(a)] The eigenspace $M_1$ associated with the eigenvalue $-i$ is the
span of (~1~,~\underbar{\hphantom{OOO}}~).

\vskip 3 pt

  \item[(b)] The eigenspace $M_2$ associated with the eigenvalue $i$ is the
span of (~1~,~\underbar{\hphantom{OOO}}~).

\vskip 3 pt

  \item[(c)] The (matrix representations of the) orthogonal projections
$P_1$ and $P_2$ onto the eigenspaces $M_1$ and $M_2$,
respectively, are
        $P_1 = \begin{bmatrix}
                     a &  b \\
                    -b &  a
               \end{bmatrix}$; and
        $P_2 = \begin{bmatrix}
                     a & -b \\
                     b &  a
               \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OOO}} and
$b$~=~\underbar{\hphantom{OOO}}~.


\vskip 3 pt

  \item[(d)] Write $N$ as a linear combination of the projections found in~(c).

\vskip 3 pt

Answer: $[N]$ = \underbar{\hphantom{OOO}} $P_1 + {}$
\underbar{\hphantom{OOO}} $P_2 $.

\vskip 3 pt

  \item[(e)] A unitary matrix $U$ which diagonalizes $[N]$ is
    \smash[b]{$\begin{bmatrix}
                  a &  a \\
                 -b &  b
               \end{bmatrix}$} where $a$~=~\underbar{\hphantom{OOO}} and
$b$~=~\underbar{\hphantom{OOO}}~.

The associated diagonal form
$\Lambda = U^*\,[N]\,U$ of~$[N]$ is $\begin{bmatrix}
                        \hphantom{jO}  \hphantom{jOO} \\
                        \hphantom{jO}  \hphantom{jOO} \\
                        \hphantom{jO}  \hphantom{jOO}
                     \end{bmatrix}$.
 \end{enumerate}
\end{exer}

\begin{exer} Let $H$ be the self-adjoint matrix
 $\begin{bmatrix}
        2  & 1+i \\
      1-i  &   3 \end{bmatrix}$.

\vskip 3 pt

 \begin{enumerate}
  \item[(a)] Use the \emph{spectral theorem} to write $H$ as a linear
combination of orthogonal projections.

\vskip 3 pt

Answer: $H = \alpha P_1 + \beta P_2$ where $\alpha$ =
\underbar{\hphantom{OOO}} , $\beta$ = \underbar{\hphantom{OOO}} ,
$P_1 = \dfrac13
  \begin{bmatrix}
               2       &  -1 - i          \\
       \hphantom{OOOO} &  \hphantom{OOOO}
  \end{bmatrix}$, and $P_2 = \dfrac13
  \begin{bmatrix}
               1       &   1 + i          \\
       \hphantom{OOOO} &  \hphantom{OOOO}

 \end{bmatrix}$.

\vskip 3 pt

  \item[(b)] Find a square root of $H$.


\vskip 3 pt

Answer: $\sqrt H = \dfrac13
    \begin{bmatrix}
               4       &   1 + i          \\
       \hphantom{OOOO} &  \hphantom{OOOO}
    \end{bmatrix}$.
 \end{enumerate}
\end{exer}

\begin{exer} Let $N = \dfrac13\begin{bmatrix}
                       4+2i & 1-i  & 1-i  \\
                       1-i  & 4+2i & 1-i  \\
                       1-i  & 1-i  & 4+2i
                              \end{bmatrix}$.

\vskip 3 pt

 \begin{enumerate}
  \item[(a)] The matrix $N$ is normal because $NN^* = N^*N =
    \begin{bmatrix}
          a & b & b \\
          b & a & b \\
          b & b & a
    \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OOOO}} and
$b$~=~\underbar{\hphantom{OOOO}}~.


\vskip 3 pt

  \item[(b)] According to the \emph{spectral theorem} $N$ can be written
as a linear combination of orthogonal projections.  Written in
this form $N = \lambda_1 P_1 + \lambda_2 P_2$ where $\lambda_1$ =
\underbar{\hphantom{OOO}}~, \\ $\lambda_2$ =
\underbar{\hphantom{OOOOOO}}~,
      $P_1 = \begin{bmatrix}
                  a & a & a \\
                  a & a & a \\
                  a & a & a \\
             \end{bmatrix}$, and
      $P_2 = \begin{bmatrix}
                  b & -a & -a \\
                 -a &  b & -a \\
                 -a & -a &  b
             \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OOO}} and
$b$~=~\underbar{\hphantom{OOO}}~.

 \item[(c)] A unitary matrix $U$ which diagonalizes $N$ is
            $\begin{bmatrix}
                  a & -b & -c \\
                  a &  b & -c \\
                  a &  d & 2c
               \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OOOO}}~,
$b$~=~\underbar{\hphantom{OOOO}}~,
$c$~=~\underbar{\hphantom{OOOO}}~, and
$d$~=~\underbar{\hphantom{OOOO}}~.

The associated diagonal form
$\Lambda = U^*NU$ of~$N$ is $\begin{bmatrix}
                               \hphantom{jOOO}  \hphantom{jOO} \\
                               \hphantom{jOOO}  \hphantom{jOO} \\
                               \hphantom{jOOO}  \hphantom{jOO} \\
                               \hphantom{jOOO}  \hphantom{jOO}
                              \end{bmatrix}$.

 \end{enumerate}
\end{exer}



\endinput
\chapter{THE LANGUAGE OF CATEGORIES}

\section{Objects and Morphisms}
\begin{defn} Let $\sfml A$ be a class, whose members we call
 \index{objects}%
\df{objects}. For every pair $(S,T)$ of objects we associate a set $\mor ST$, whose
members we call
 \index{morphisms}%
 \index{morphism@$\mor ST$ (morphisms from $S$ to $T$)}%
\df{morphisms} from $S$ to $T$. We assume that $\mor ST$ and $\mor UV$ are disjoint
unless $S = U$ and $T = V$.

We suppose further that there is an operation $\circ$ (called
 \index{composition!of morphisms}%
 \index{morphisms!composition of}%
\df{composition}) that associates with every $\alpha \in \mor ST$ and every $\beta \in
\mor TU$ a morphism $\beta \circ \alpha \in \mor SU$ in such a way that:
 \begin{enumerate}
  \item[(1)] $\gamma \circ (\beta \circ \alpha) = (\gamma \circ
\beta) \circ \alpha$ whenever $\alpha \in \mor ST$, $\beta \in \mor TU$, and $\gamma \in
\mor UV$;
  \item[(2)] for every object $S$ there is a morphism $I_S \in
\mor SS$ satisfying $\alpha \circ I_S = \alpha$ whenever $\alpha \in \mor ST$ and $I_S
\circ \beta = \beta$ whenever $\beta \in \mor RS$.
 \end{enumerate}
Under these circumstances the class $\sfml A$, together with the associated families of
morphisms, is a
 \index{category}%
\df{category}.

We will reserve the notation
 \index{<arrow@$S \to^\alpha T$ (morphism in a category)}%
$S \to^\alpha T$ for a situation in which $S$ and $T$ are objects in some category and
$\alpha$ is a morphism belonging to $\mor ST$. As is the case with groups and vector
spaces we usually omit the composition symbol $\circ$ and write
  \index{<concat@$\beta\alpha$ (notation for composition of morphisms)}%
  \index{conventions!notation for composition!of morphisms}%
$\beta\alpha$ for $\beta \circ \alpha$.
\end{defn}

\begin{exam} The category
 \index{category!$\cat{SET}$ as a}%
 \index{sets@$\cat{SET}$!the category}%
$\cat{SET}$ has sets for objects and functions (maps) as morphisms.
\end{exam}

\begin{exam} The category
 \index{category!$\cat{AbGp}$ as a}%
 \index{abgp@$\cat{AbGp}$!the category}%
$\cat{AbGp}$ has Abelian groups for objects and group homomorphisms as morphisms.  (See
proposition~\ref{abgp011}.)
\end{exam}

\begin{exam} The category
 \index{category!$\cat{VEC}$ as a}%
 \index{vectorspaces@$\cat{VEC}$!the category}%
$\cat{VEC}$ has vector spaces for objects and linear transformations as morphisms. (See
proposition~\ref{lintran011}.)
\end{exam}

\begin{exam} Let $S$ and $T$ be partially ordered sets.  A function $f\colon S \sto T$ is
 \index{order!preserving}%
\df{order preserving} if $f(x) \le f(y)$ in~$T$ whenever $x \le y$ in~$S$. The category
 \index{category!$\cat{POSET}$ as a}%
 \index{posets@$\cat{POSET}$!the category}%
$\cat{POSET}$ has partially ordered sets for objects and order preserving maps as
morphisms.
\end{exam}

The preceding examples are examples of
 \index{concrete category}%
 \index{category!concrete}%
 \index{conventions!all categories are concrete}%
\emph{concrete categories}---that is, categories in which the objects are sets (together,
usually, with additional structure) and the morphism are functions (usually preserving,
in some sense, this extra structure).  In these notes the categories of interest are
concrete ones.  Even so, it may be of interest to see an example of a category that is
\emph{not} concrete.

\begin{exam}\label{obj_mor001} Let $G$ be a monoid. Consider a category $\cat C_G$ having
exactly one object, which we call~$\star$. Since there is only one object there is only
one family of morphisms $\mor{\star}{\star}$, which we take to be~$G$. Composition of
morphisms is defined to be the monoid multiplication.  That is, $a \circ b := ab$ for all
$a$, $b \in G$. Clearly composition is associative and the identity element of $G$ is the
identity morphism. So $\cat C_G$ is a category.
\end{exam}

\begin{defn} In any concrete category we will call an injective morphism a
 \index{monomorphism}%
\df{monomorphism} and a surjective morphism an
 \index{epimorphism}%
\df{epimorphism}.
\end{defn}

\begin{cau} The definitions above reflect the original Bourbaki use of the term and are
the ones most commonly adopted by mathematicians outside of category theory where
``monomorphism'' means ``left cancellable'' and ``epimorphism'' means ``right
cancellable''.  (Notice that the terms \emph{injective} and \emph{surjective} may not
make sense when applied to morphisms in a category that is not concrete.)

A morphism $B \to^g C$ is
 \index{left!cancellable}%
 \index{cancellable!left}%
\df{left cancellable} if whenever morphisms $A \to^{f_1} B$ and $A \to^{f_2} B$ satisfy
$gf_1 = gf_2$, then $f_1 = f_2$.  Saunders Mac Lane suggested calling left cancellable
morphisms
 \index{monic}%
\df{monic} morphisms.  The distinction between monic morphisms and monomorphisms turns
out to be slight.  In these notes almost all of the morphisms we encounter are monic if
and only if they are monomorphisms. As an easy exercise prove that any injective morphism
in a (concrete) category is monic.  The converse sometimes fails.

In the same vein Mac Lane suggested calling a
 \index{right!cancellable}%
 \index{cancellable!right}%
\emph{right cancellable} morphism (that is, a morphism $A \to^f B$ such that whenever
morphisms $B \to^{g_1} C$ and $B \to^{g_2} C$ satisfy $g_1f = g_2f$, then $g_1 = g_2$) an
 \index{epic}%
\df{epic} morphism.  Again it is an easy exercise to show that in a (concrete) category
any epimorphism is epic.  The converse, however, fails in some rather common categories.
\end{cau}

\begin{defn} The terminology for inverses of morphisms in categories is essentially the same
as for functions.  Let $S \to^\alpha T$ and $T \to^\beta S$ be morphisms in a category.
If $\beta \circ \alpha = I_S$, then $\beta$ is a
 \index{left!inverse!of a morphism}%
 \index{inverse!left}%
\df{left inverse} of $\alpha$ and, equivalently, $\alpha$ is a
 \index{right!inverse!of a morphism}%
 \index{inverse!right}%
\df{right inverse} of $\beta$. We say that the morphism $\alpha$ is an
 \index{isomorphism!in a category}%
\df{isomorphism} (or is
 \index{invertible}%
\df{invertible}) if there exists a morphism $T \to^\beta S$ which is both a left and a
right inverse for~$\alpha$.  Such a function is denoted by $\alpha^{-1}$ and is called
the
 \index{<@$\alpha^{-1}$ (inverse of a morphism $\alpha$)}%
 \index{inverse!of a morphism}%
\df{inverse} of~$\alpha$.
\end{defn}

\begin{prop} If a morphism in some category has both a left and a right inverse, then it
is invertible.
\end{prop}

In any concrete category one can inquire whether every bijective morphism (that is, every
map which is both a monomorphism and an epimorphism) is an isomorphism.  We saw in
proposition~\ref{inv_lin_maps004} that in the category $\cat{VEC}$ the answer is
\emph{yes}.  In the next example the answer is~\emph{no}.

\begin{exam} In the category $\cat{POSET}$
 \index{bijective morphisms!need not be invertible!in $\cat{POSET}$}%
 \index{poset@$\cat{POSET}$!bijective morphism in}%
 \index{poset@$\cat{POSET}$!the category}%
of partially ordered sets and order preserving maps not every bijective morphism is an isomorphism.
\end{exam}

\begin{exam} If in the category $\cat C_G$ of example~\ref{obj_mor001} the monoid $G$ is
a group, then every morphism in $\cat C_G$ is an isomorphism.
\end{exam}





















\section{Functors}
\begin{defn} If $\cat{A}$ and $\cat{B}$ are categories a
 \index{functor!covariant}%
 \index{covariant}%
(\df{covariant}) \df{functor} $F$ from $\cat A$ to $\cat B$ (written ${\cat A}\to^F{\cat
B}$) is a pair of maps: an
 \index{object!map}%
\df{object map} $F$ which associates with each object $S$ in $\cat A$ an object $F(S)$ in
$\cat B$ and a
 \index{morphism!map}%
\df{morphism map} (also denoted by $F$) which associates with each morphism $f \in \mor
ST$ in $\cat A$ a morphism $F(f) \in \mor {F(S)}{F(T)}$ in $\cat B$, in such a way that
 \begin{enumerate}
  \item[(1)] $F(g \circ f) = F(g) \circ F(f)$
whenever $g \circ f$ is defined in $\cat A$; and
  \item[(2)] $F(\id S) = \id{F(S)}$ for every object $S$ in $\cat A$.
 \end{enumerate}


The definition of a
 \index{functor!contravariant}%
 \index{contravariant}%
\df{contravariant functor} ${\cat A}\to^F{\cat B}$ differs from the preceding definition
only in that, first, the morphism map associates with each morphism $f \in \mor ST$ in
$\cat A$ a morphism $F(f) \in \mor {F(T)}{F(S)}$ in $\cat B$ and, second, condition (1)
above is replaced by
    \begin{enumerate}
     \item[($1'$)] $F(g \circ f) = F(f) \circ F(g)$ whenever $g \circ f$
is defined in~$\cat A$.
    \end{enumerate}
 \end{defn}

\begin{exam}\label{functors005exam} A
 \index{forgetful functor}%
 \index{functor!forgetful}%
\df{forgetful functor} is a functor that maps objects and morphisms from a category $\cat
C$ to a category $\cat C'$ with less structure or fewer properties.  For example, if $V$
is a vector space, the functor $F$ which ``forgets'' about the operation of scalar
multiplication on vector spaces would map $V$ into the category of Abelian groups. (The
Abelian group $F(V)$ would have the same set of elements as the vector space $V$ and the
same operation of addition, but it would have no scalar multiplication.)  A linear map
$T\colon V \sto W$ between vector spaces would be taken by the functor $F$ to a group
homomorphism $F(T)$ between the Abelian groups $F(V)$ and~$F(W)$.

Forgetful functor can ``forget'' about properties as well.  If $G$ is an object in the
category of Abelian groups, the functor which ``forgets'' about commutativity in Abelian
groups would take $G$ into the category of groups.

It was mentioned in the preceding section that all the categories that are of interest in
these notes are concrete categories (ones in which the objects are sets with additional
structure and the morphisms are maps which preserve, in some sense, this additional
structure). We will have several occasions to use a special type of forgetful
functor---one which forgets about all the structure of the objects except the underlying
set and which forgets any structure preserving properties of the morphisms. If $A$ is an
object in some concrete category $\cat C$, we denote by
 \index{<unop@$\abs{A}$ (the forgetful functor acting on~$A$)}%
$\abs{A}$ its underlying set. And if \mbox{$A~\to^f~B$} is a morphism in $\cat C$ we
denote by $\abs f$ the map from $\abs A$ to $\abs B$ regarded simply as a function
between sets.  It is easy to see that $\abs{\hphantom{M}}$\,, which takes objects in
$\cat C$ to objects in $\cat{SET}$ (the category of sets and maps) and morphisms in $\cat
C$ to morphisms in $\cat{SET}$, is a covariant functor.

In the category $\cat{VEC}$ of vector spaces and linear maps, for example,
$\abs{\hphantom{M}}$ causes a vector space $V$ to ``forget'' about both its addition and
scalar multiplication ($\abs V$ is just a set). And if $T\colon V \sto W$ is a linear
transformation, then $\abs T\colon \abs V \sto \abs W$ is just a map between sets---it
has ``forgotten'' about preserving the operations.
\end{exam}

\begin{notn} Let $f \colon S \sto T$ be a function between sets. Then we define
$f^{\sto}(A) = \{f(x)\colon x \in A\}$ and
 \index{<superscript@$f^{\sto}(A)$ (image of $A$ under $f$)}%
 \index{<superscript@$f^{\gets}(B)$ (preimage of $B$ under $f$)}%
$f^{\gets}(B) = \{ x \in S\colon f(x) \in B\}$. We say that $f^{\sto}(A)$ is the
 \index{image}%
\df{image of $A$ under $f$} and that $f^{\gets}(B)$ is the
 \index{preimage}%
\df{preimage of $B$ under~$f$}.
\end{notn}

\begin{defn} A partially ordered set is
 \index{order!complete}%
 \index{complete!order}%
\df{order complete} if every nonempty subset has a supremum (that is, a least upper
bound) and an infimum (a greatest lower bound).
\end{defn}

\begin{defn} Let $S$ be a set.  Then the
 \index{power set}%
 \index{P@$\sfml P(S)$ (power set of $S$)}%
\df{power set} of $S$, denoted by $\sfml P(S)$, is the family of all subsets of~$S$.
\end{defn}

\begin{exam}[The power set functors] Let $S$ be a nonempty set.
 \begin{enumerate}
  \item[(a)] The power set $\sfml P(S)$ of $S$ partially ordered by $\subseteq$ is
order complete.
  \item[(b)] The class of order complete partially ordered sets and order
preserving maps is a category.
 \index{functor!power set}%
 \index{power set!functor}%
  \item[(c)] For each function $f$ between sets let $\sfml P(f) = f^{\sto}$. Then
$\sfml P$ is a covariant functor from the category of sets and functions to the category
of order complete partially ordered sets and order preserving maps.
  \item[(d)] For each function $f$ between sets let $\sfml P(f) = f^{\gets}$. Then
$\sfml P$ is a contravariant functor from the category of sets and functions to the
category of order complete partially ordered sets and order preserving maps.
 \end{enumerate}
\end{exam}

\begin{defn} Let $T\colon V \sto W$ be a linear map between vector spaces. For every
$g \in W^*$ let $T^*(g) = g\,T$.  Notice that $T^*(g) \in V^*$.  The map $T^*$ from the
vector space $W^*$ into the vector space~$V^*$ is the (vector space)
 \index{adjoint!vector space}%
 \index{vector!space!adjoint map}%
 \index{<superscript@$T^*$ (vector space adjoint of $T$)}%
\df{adjoint} map of~$T$.
\end{defn}

\vskip 5 pt

\begin{cau} In inner product spaces we will use the same notation $T^*$ for a
different map. If $T\colon V \sto W$ is a linear map between inner product spaces, then
the (inner product space) adjoint transformation $T^*$ maps $W$ to~$V$ (not $W^*$
to~$V^*$).
\end{cau}

\begin{exam}[The vector space duality functor] Let $T \in \ofml L(V,W)$ where $V$ and
$W$ are vector spaces over a field~$\F$. Then the pair of maps $V \mapsto V^*$ and $T
\mapsto T^*$
 \index{functor!vector space duality}%
 \index{duality functor!for vector spaces}%
is a contravariant functor from the category of vector spaces and linear maps into
itself. Show that (the morphism map of) this functor is linear.  (That is, show that $(S
+ T)^* = S^* + T^*$ and $(\alpha T)^* = \alpha T^*$ for all $S$, $T \in \ofml L(V,W)$ and
$\alpha \in \F$.)
\end{exam}

There are several quite different results that in various texts are labeled as \emph{the
fundamental theorem of linear algebra}.  Many of them seem to me not to be particularly
``fundamental'' because they apply only to finite dimensional inner product spaces or,
what amounts to the same thing, matrices.  I feel the following result deserves the name
because it holds for arbitrary linear maps between arbitrary vector spaces.

\begin{thm}[Fundamental Theorem of Linear Algebra]\label{functors_FTLA} For every linear
map $T\colon V \sto W$ between vector spaces the following hold.
    \begin{enumerate}
        \item $\ker T^* = (\ran T)^\perp$;
        \item $\ran T^* = (\ker T)^\perp$;
        \item $\ker T = (\ran T^*)_\perp$; and
        \item $\ran T = (\ker T^*)_\perp$.
    \end{enumerate}
\end{thm}

\begin{proof}[\emph{Hint for proof}] Showing in (2) that $(\ker T)^\perp \subseteq \ran T^*$ takes some thought.  Let $f \in (\ker T)^\perp$.
We wish to find a $g \in W^*$ such that $f = T^*g$.  By propositions~\ref{lintran014} and~\ref{bases029} there exists a subspace $M$ of $V$
such that $V = \ker T \oplus M$. Let $\iota\colon M \sto V$ be the inclusion mapping of $M$ into $V$ and $T|_M$ be the restriction of $T$ to~$M$.
Use proposition~\ref{inv_lin_maps012} to show that $T|_M$ has a left inverse $S\colon W \sto M$.  Let $g := f \iota S$.  Use
theorem~\ref{subspace008}.   \ns
\end{proof}

\begin{exer} What is the relationship between a linear map $T$ being injective and its
adjoint $T^*$ being surjective? between $T$ being surjective and $T^*$ being injective?
\end{exer}




\vskip 10 pt













\section{Universal Mapping Properties}
Much of mathematics involves the construction of new objects from old ones---things such
as products, coproducts, quotients, completions, compactifications, and unitizations.
Often it is possible---and highly desirable---to characterize such a construction by
means of a diagram which describes what the constructed object ``does'' rather than
telling what it ``is'' or how it is constructed.  Such a diagram is a
 \index{universal!mapping!diagram}%
 \index{mapping!universal}%
\df{universal mapping diagram} and it describes the
 \index{universal!mapping!property}%
 \index{mapping!universal}%
\df{universal mapping property} of the object being constructed.

\vskip 5 pt

Here is a first example of such a property.

\begin{defn} Let $F$ be an object in a concrete category $\cat C$ and
$\iota\colon S \sto \abs F$ be an inclusion map whose domain is a nonempty set~$S$.  We say that the
object $F$ is
 \index{free!object}%
 \index{object!free}%
\df{free on} the set $S$ (or that $F$ is the \df{free object generated by}~$S$) if for
every object $A$ in $\cat C$ and every map $f\colon S \sto \abs A$ there exists a unique
morphism $\widetilde f_\iota\colon F \sto A$ in $\cat C$ such that $\abs{\wt f_\iota}
\circ \iota = f$.
 \[\xy
     \qtriangle/{>}`{>}`{-->}/[S`\abs F`\abs A;\iota`f`\abs{\wt f_\iota}]
     \morphism(1000,500)|r|/{-->}/<0,-500>[F`A;\widetilde f_\iota]
   \endxy\]
We will be interested in
 \index{free!vector space}%
 \index{vector!space!free}%
\df{free vector spaces}; that is, free objects in the category $\cat{VEC}$ of vector
spaces and linear maps.  Naturally, merely \emph{defining} a concept does not guarantee
its existence. It turns out, in fact, that free vector spaces exist on arbitrary sets.
(See exercise~\ref{ump001z}.)
\end{defn}

\vskip 5 pt

\begin{exer} In the preceding definition reference to the forgetful functor is often
omitted and the accompanying diagram is often drawn as follows:
 \[\xy
     \qtriangle/{>}`{>}`{-->}/[S`F`A;\iota`f`\wt f_\iota]
   \endxy\]
It certainly looks a lot simpler.  Why do you suppose I opted for the more complicated
version?
\end{exer}

\vskip 5 pt

\begin{prop} If two objects in some concrete category are free on the same set,
then they are isomorphic.
\end{prop}

\vskip 5 pt

\begin{defn} Let $A$ be a subset of a nonempty set~$S$ and $\F$ be a field. Define
$\sbsb\chi A \colon S \sto \F$, the
 \index{characteristic!function}%
 \index{<@$\sbsb{\chi}{A}$ (characteristic function of $A$)}%
 \index{function!characteristic}%
\df{characteristic function} of~$A$, by
  \[ \sbsb{\chi}{A}(x) = \begin{cases}
                                  1, &\text{if $x \in A$} \\
                                  0, &\text{otherwise}
                              \end{cases} \]
\end{defn}

\begin{exam}\label{ump001z} If $S$ is an arbitrary nonempty set and $\F$ is a field,
then there exists a vector space $V$ over~$\F$ which is free on~$S$.  This vector space
is unique (up to isomorphism).
\end{exam}

\begin{proof}[\emph{Hint for proof}] Given the set $S$ let $V$ be the set of all
$\F$-valued functions on $S$ which have finite support. Define addition and scalar
multiplication pointwise. The map $\iota\colon s \mapsto \sbsb \chi {\{s\}}$ of each
element $s \in S$ to the characteristic function of~$\{s\}$ is the desired injection. To
verify that $V$ is free over $S$ it must be shown that for every vector space $W$ and
every function \hbox{$S~\to^{f}~\abs W$} there exists a unique linear map $V \to^{\wt f}
W$ which makes the following diagram commute.
 \[\xy
     \qtriangle/{>}`{>}`{-->}/[S`\abs V`\abs W;\iota`f`\abs{\wt f}]
     \morphism(1100,500)|r|/{-->}/<0,-500>[V`W;\wt f]
   \endxy\]
\ns \end{proof}

\begin{prop} Every vector space is free.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Of course, part of the problem is to specify a set
$S$ on which the given vector space is free. \ns
\end{proof}

\begin{exer} Let $S = \{a,*,\#\}$.  Then an expression such as
  \[ 3a - \tfrac12* + \,\sqrt{2\,}\,\# \]
is said to be a
 \index{formal linear combination}%
 \index{linear!combination!formal}%
 \index{combination!formal linear}%
\emph{formal linear combination} of elements of~$S$.  Make sense of such expressions.
\end{exer}


















\section{Products and Coproducts}
In this section we define products and coproducts of vector spaces in terms of universal
mapping properties.

\begin{defn}\label{direct_sum035def} Let $A_1$ and $A_2$ be objects in a category~$\cat C$.
We say that a triple $(P,\pi_1,\pi_2)$, where $P$ is an object and $\pi_k\colon P \sto
A_k$ ($k = 1$, $2$) are morphisms, is a
 \index{product!in a category}%
\df{product} of $A_1$ and $A_2$ if for every object $B$ in $\cat C$ and every pair of
morphisms $f_k\colon B \sto A_k$ ($k = 1$, $2$) there exists a unique map $g\colon B \sto
P$ such that $f_k = \pi_k \circ g$ for $k = 1$, $2$.
 \[\xy
    \Atrianglepair/>`-->`>`<-`>/[B`A_1`P`A_2;f_1`g`f_2`\pi_1`\pi_2]
 \endxy\]

A triple $(P,j_1,j_2)$, where $P$ is an object and $j_k\colon A_k \sto P$, ($k = 1$, $2$)
are morphisms, is a
 \index{coproduct!in a category}%
\df{coproduct} of $A_1$ and $A_2$ if for every object $B$ in $\cat C$ and every pair of
morphisms $F_k\colon A_k \sto B$ ($k = 1$, $2$) there exists a unique map $G\colon P \sto
B$ such that $F_k = G \circ j_k$ for $k = 1$,~$2$.
 \[\xy
    \Atrianglepair/<-`<--`<-`>`<-/[B`A_1`P`A_2;F_1`G`F_2`j_1`j_2]
 \endxy\]
\end{defn}

\begin{prop} In an arbitrary category products and coproducts (if they exist) are
 \index{uniqueness!of products, coproducts}%
 \index{product!uniqueness of}%
 \index{coproduct!uniqueness of}%
essentially unique.
\end{prop}

``Essentially unique'' means unique up to isomorphism.  Thus in the preceding
 \index{uniqueness!essential}%
 \index{essential!uniqueness}%
proposition the claim is that if $(P,\pi_1,\pi_2)$ and $(Q,\rho_1,\rho_2)$ are both
products of two given objects, then $P \cong Q$.

\begin{defn}\label{prods031def}  Let $V$ and $W$ be vector spaces over the same
field~$\F$. To make the Cartesian product $V \times W$ into a vector space we define
addition by
    \[ (v,w) + (v',w') = (v + v', w + w')\]
(where $v$, $v' \in V$ and $w$, $w' \in W$), and we define scalar multiplication by
    \[ \alpha(v,w) = (\alpha v,\alpha w)\]
(where $\alpha \in \F$, $v \in V$, and $w \in W$).  The resulting vector space we call
the
 \index{external!direct sum}%
 \index{direct sum!external}%
\df{(external) direct sum} of $V$ and~$W$.  It is conventional to use the same notation
 \index{<binop@$M \oplus N$ (vector space direct sum of $M$ and~$N$)}%
$V \oplus W$ for external direct sums that we use for internal direct sums.
\end{defn}

\begin{exam} The external direct sum of two vector spaces (as defined
in~\ref{prods031def}) is a vector space.
\end{exam}

\begin{exam}\label{prods035z} In the category of vector spaces and linear maps the
 \index{direct sum!as a product}%
 \index{direct sum!as a coproduct}%
external direct sum is both a product but also a coproduct.
\end{exam}

\begin{exam} In the category of sets and maps (functions) the product and the coproduct
are \emph{not} the same.
\end{exam}

\begin{prop} Let $U$, $V$, and $W$ be vector spaces. If $U \cong W$, then
$U \oplus V \cong W \oplus V$.
\end{prop}

\begin{exam} The converse of the preceding proposition is not true.
\end{exam}

\begin{defn} Let $V_0$, $V_1$, $V_2$, \dots be vector spaces (over the same field). Then
their
 \index{external!direct sum}%
 \index{direct sum!external}%
 \index{sum!direct}%
 \index{<binaryoperationz@$\bigoplus_{k=0}^\infty V_k$ (direct sum)}%
\df{(external) direct sum}, which is denoted by $\bigoplus\limits_{k=0}^\infty V_k$, is
defined to be the set of all functions $v\colon \Z^+ \sto \bigcup_{k=0}^\infty V_k$ with
finite support such that $v(k) = v_k \in V_k$ for each $k \in \Z^+$. The usual pointwise
addition and scalar multiplication make this set into a vector space.
\end{defn}





















\section{Quotients}
\begin{defn}\label{quo001def} Let $A$ be an object in a concrete category $\cat C$.
A surjective morphism $A \to^\pi B$ in $\cat C$ is a
 \index{quotient!map}%
\df{quotient map} for $A$ if a function $g\colon B \sto C$ (in $\cat{SET}$) is a morphism
(in $\cat C$) whenever $g \circ \pi$ is a morphism. An object $B$ in $\cat C$ is a
 \index{quotient!object}%
\df{quotient object} for $A$ if it is the range of some quotient map for~$A$.
\end{defn}

\begin{prop} In the category of vector spaces and linear maps every surjective linear
 \index{quotient!map}%
map is a quotient map.
\end{prop}

The next item shows how a particular quotient object can be generated by ``factoring out
a subspace''.

\begin{defn}\label{quo002def} Let $M$ be a subspace of a vector space $V$.  Define an
equivalence relation $\sim$ on $V$ by
  \[x \sim y \qquad \text{if and only if} \qquad y - x \in M.\]
For each $x \in V$ let $[x]$ be the equivalence class containing~$x$.
 \index{<unop@$[x]$ (equivalence class containing~$x$)}%
 \index{<binop@$V/M$ (quotient of $V$ by $M$)}%
Let $V/M$ be the set of all equivalence classes of elements of~$V$.  For $[x]$ and $[y]$
in $V/M$ define
  \[ [x] + [y] := [x+y]\]
and for $\alpha \in \R$ and $[x] \in V/M$ define
  \[\alpha[x] := [\alpha x].\]
Under these operations $V/M$ becomes a vector space.  It is the
 \index{quotient!vector space}%
\df{quotient space} of $V$ by~$M$.  The notation $V/M$ is usually read ``$V$ mod~$M$''.
The linear map
 \[\pi\colon V \sto V/M\colon x \mapsto [x]\]
is called the
 \index{quotient!map}%
\df{quotient map}.
\end{defn}

\begin{exer} Verify the assertions made in definition~\ref{quo002def}. In particular,
show that $\sim$ is an equivalence relation, that addition and scalar multiplication of
the set of equivalence classes is well defined, that under these operations $V/M$ is a
vector space, and that the quotient map is linear.
\end{exer}

The following result is called the \emph{fundamental quotient theorem} or the
 \index{fundamental quotient theorem}%
 \index{first isomorphism theorem}%
{first isomorphism theorem} for vector spaces.

\begin{thm}\label{quo005} Let $V$ and $W$ be vector spaces and $M \preceq V$. If
$T \in \ofml L(V,W)$ and $\ker T \supseteq M$, then there exists a unique $\wt T \in
\ofml L(V/M\,,W)$ which makes the following diagram commute.
 \[\xymatrix@+30pt{V \ar[d]_*+{\pi} \ar[dr]^*+{T} & \\
            V/M \ar[r]_*+{\widetilde T} & W  }\]
Furthermore, $\wt T$ is injective if and only if $\ker T = M$; and $\wt T$ is surjective
if and only if $T$ is.
\end{thm}

\begin{cor} If $T\colon V \sto W$ is a linear map between vector spaces, then
$\ran T \cong V/\ker T$.
\end{cor}

For obvious reasons the next result is usually called the
 \index{rank-plus-nullity theorem}%
\emph{rank-plus-nullity theorem}.  (It is also sometimes listed as part of the
\emph{fundamental theorem of linear algebra}.)

\begin{prop} Let $T\colon V \sto W$ be a linear map between vector spaces. If $V$ is
finite dimensional, then
  \[ \textrm{rank }T + \textrm{nullity }T = \dim V. \]
\end{prop}

\begin{cor} If $M$ is a subspace of a finite dimensional vector space $V$,
then $\dim V/M = \dim V - \dim M$.
\end{cor}










\section{Exact Sequences}
\begin{defn} A sequence of vector spaces and linear maps
  \[ \cdots \to V_{n-1} \to^{j_n}  V_n \to^{j_{n+1}}  V_{n+1} \to \cdots\]
is said to be
 \index{exact sequence!of vector spaces}%
 \index{sequence!exact}%
\df{exact at} $V_n$ if $\ran j_n = \ker j_{n+1}$. A sequence is \df{exact} if it is exact
at each of its constituent vector spaces.  A sequence of vector spaces and linear maps of
the form
 \[ \vc 0 \to U \to^j  V \to^k  W  \to \vc 0\]
is a
 \index{exact sequence!short}%
 \index{short exact sequence}%
\df{short exact sequence}.  (Here $\vc 0$ denotes the trivial $0$-dimensional vector
space, and the unlabeled arrows are the obvious linear maps.)
\end{defn}

\begin{prop} The sequence
   \[ \vc 0 \to U \to^j  V \to^k W \to \vc 0 \]
of vector spaces is exact at $U$ if and only if $j$ is injective. It is exact at $W$ if
and only if $k$ is surjective.
\end{prop}

\begin{exer} Suppose $a < b$.  Let $\fml K$ be the family of constant functions on the
interval $[a,b]$, $\fml C^1$ be the family of all continuously differentiable functions
on $[a,b]$, and $\fml C$ be the family of all continuous functions on $[a,b]$.  (A
function $f$ is said to be
 \index{continuously differentiable}%
 \index{differentiable!continuously}%
\df{continuously differentiable} if its derivative $f'$ exists and is continuous.)

Specify linear maps $j$ and $k$ so that the following sequence is short exact:
  \[ \vc 0 \to \fml K \to^j  \fml C^1 \to^k  \fml C \to \vc 0. \]
\end{exer}

\begin{exer} Let $\fml C$ be the family of all continuous functions on the interval
$[0,2]$.  Let $E_1$ be the mapping from $\fml C$ into $\R$ defined by $E_1(f) = f(1)$.
(The functional $E_1$ is called \emph{evaluation at~$1$}.)

Find a subspace $\fml F$ of $\fml C$ such that the following sequence is short exact.
  \[ \vc 0 \to \fml F \to^\iota  \fml C \to^{E_1} \R \to \vc 0. \]
\end{exer}

\begin{exer} If $j\colon U \sto V$ is an injective linear map between vector spaces,
then the sequence
  \[ \vc 0 \to U \to^j  V \to^\pi V/ \ran j \to \vc 0 \]
is exact.
\end{exer}

\begin{exam} Let $U$ and $V$ be vector spaces. Then the following sequence is short exact:
 \[\vc 0 \to U \to^{\iota_1}  U \oplus V \to^{\pi_2}  V \to \vc 0.\]
The indicated linear maps are the obvious ones:
  \[\iota_1\colon U \sto U \oplus V\colon u \mapsto (u,0)\]
and
  \[\pi_2\colon U \oplus V \sto V\colon (u,v) \mapsto v.\]
\end{exam}

\begin{prop} Consider the following diagram in the category of vector spaces and linear
maps.
 \[\xy
   \square(-475,0)/>```>/<400,500>[0``0`;```]
   \square|almb|[U`V`U\,'`V\,';j`f`g`j\,']
   \square(500,0)/>``-->`>/[V`W`V\,'`W\,';k``h`k']
   \square(1075,0)/>```>/<400,500>[`0``0;```]
  \endxy\]
If the rows are exact and the left square commutes, then there exists a unique linear map
$h\colon W \sto W\,'$ which makes the right square commute.
\end{prop}

\begin{prop}[The Short Five Lemma]\label{es020zprop} Consider the following diagram of vector
spaces and linear maps
  \[\xy
    \square(-475,0)/>```>/<400,500>[0``0`;```]
    \square|almb|[U`V`U\,'`V\,';j`f`g`j\,']
    \square(500,0)/>``>`>/[V`W`V\,'`W\,';k``h`k']
    \square(1075,0)/>```>/<400,500>[`0``0;```]
   \endxy\]
where the rows are exact and the squares commute.  Then the following hold.
 \begin{enumerate}
  \item[(a)] If $g$ is surjective, so is $h$.
  \item[(b)] If $f$ is surjective and $g$ is injective, then $h$ is injective.
  \item[(c)] If $f$ and $h$ are surjective, so is $g$.
  \item[(d)] If $f$ and $h$ are injective, so is $g$.
 \end{enumerate}
\end{prop}

\begin{prop} Show that if $\vc 0 \to U \to^j V \to^k W \to \vc 0$ is an exact sequence
of vector spaces and linear maps, then $V \cong U \oplus W$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider the following diagram and use
proposition~\ref{es020zprop}.
 \[\xymatrix{
     0\ar[r] & U\ar[r]^j\ar@{=}[d] & V\ar[r]^k\ar@{-->}[d]^g
             & W\ar[r]\ar@{=}[d] & 0 \\
     0\ar[r] & U\ar[r]_(0.4){i_1} & U\oplus W\ar@<2pt>[r]^(0.6){\pi_2}
                                              & W\ar[r]\ar@<2pt>[l]^(0.3){i_2} & 0
 }.\]
The trick is to find the right map~$g$. \ns
\end{proof}

\begin{exer} Prove the converse of the preceding exercise.  That is, suppose that $U$,
$V$, and $W$ are vector spaces and that $V \cong U \oplus W$; prove that there exist
linear maps $j$ and $k$ such that the sequence  $\vc 0 \to U \to^j V \to^k W \to \vc 0$
is exact. \emph{Hint.} Suppose $g\colon U \oplus W \sto V$ is an isomorphism. Define $j$
and $k$ in terms of~$g$.
\end{exer}

\begin{prop} Let $M$ and $M'$ be subspaces of vector spaces $V$ and $V'$, respectively.
Prove the following.
 \begin{enumerate}
  \item[(a)] Every linear map $T\colon V \sto V'$ which satisfies $T(M) \subseteq M'$
induces a linear map $\wh T$ from $V/M$ into $V'/M'$ which satisfies $\wh T(v + M) = (Tv)
+ M'$ for all $v \in V$.
  \item[(b)] If $T$ is an isomorphism and $T(M) = M'$, then $V/M \cong V'/M'$.
 \end{enumerate}
\end{prop}

\begin{prop}\label{es020} If $\vc 0 \to U \to^j V \to^k W \to \vc 0$ is an exact sequence
of vector spaces and linear maps, then $W \cong V/\ran j$.  Thus, if $U \preceq V$ and
$j$ is the inclusion map, then $W \cong V/U$.
\end{prop}

Give two different proofs of the preceding result: one using theorem~\ref{quo005} and the
other using proposition~\ref{es020z}.

\begin{prop}\label{es020z} The converse of proposition~\ref{es020} is also true. That is,
if $j\colon U \sto V$ is an injective linear map between vector spaces and $W \cong
V/\ran j$, then there exists a linear map $k$ which makes the sequence $\vc 0 \to U \to^j
V \to^k W \to \vc 0$ exact.
\end{prop}

\begin{prop} If $V_0, V_1, \dots, V_n$ are finite dimensional vector spaces
and the sequence
  \[ \vc 0 \to V_n \to^{d_n} V_{n-1} \to \dots \to V_1 \to^{d_1}  V_0 \to \vc 0 \]
is exact, then $\D\sum_{k=0}^n (-1)^k\dim V_k = 0$.
\end{prop}

















\section{Some Miscellaneous Results}
\begin{defn} Let $T \in \ofml L(V,W)$, where $V$ and $W$ are vector spaces.  Define
$\coker T$, the
 \index{cokernel}%
 \index{cokernel@$\coker T$ (the cokernel of $T$)}%
\df{cokernel} of~$T$, to be $W/\ran T$.
\end{defn}

\begin{prop} Let $U$, $V$, and $W$ be vector spaces.  If $S \in \ofml L(U,V)$,
$T \in \ofml L(V,W)$, then the sequence
 \[  \vc 0 \to \ker S \to \ker TS \to \ker T \to \coker S \to \coker TS
                           \to \coker T \to \vc 0 \]
is exact.
\end{prop}

\begin{prop} Let $W$ be a vector space and $M \preceq V \preceq W$.  Then
   \[ (W/M)\bigl/(V/M)  \cong W/V. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Proposition~\ref{es020}.  \ns
\end{proof}

\begin{prop} Let $V$ be a vector space and $M$, $M\,' \preceq V$ Then
   \[ (M + M\,')/M \cong M\,'/(M \cap M\,')\,.  \]
\end{prop}

\begin{prop} Let $M$ be a subspace of a vector space~$V$.  Then the following
are equivalent:
 \begin{enumerate}
  \item[(a)] $\dim V/M < \infty$\,;
  \item[(b)] there exists a finite dimensional subspace $F$ of $V$
such that $V = M \oplus F$\,; and
  \item[(c)] there exists a finite dimensional subspace $F$ of $V$
such that $V = M + F$.
 \end{enumerate}
\end{prop}

\begin{exer} Suppose that a vector space $V$ is the direct sum of subspaces $U$ and~$W$.
Some authors define the
 \index{codimension}%
\df{codimension} of $U$ to be $\dim W$.  Others define it to be $\dim V/U$.  Show that
these are equivalent.
\end{exer}





\endinput
\chapter{LINEAR TRANSFORMATIONS}

\section{Linearity}
\begin{defn} Let $V$ and $W$ be vector spaces over the same field~$\F$.  A function
$T\colon V \sto W$ is
 \index{linear}%
\df{linear} if $T(x + y) = Tx + Ty$ and $T(\alpha x) = \alpha Tx$ for all $x$, $y \in V$
and $\alpha \in \F$.  For linear functions it is a matter of convention to
 \index{conventions!write $Tx$ for $T(x)$ when $T$ is linear}%
write $Tx$ instead of $T(x)$ whenever it does not cause confusion.  (Of course, we would
not write $Tx + y$ when we intend $T(x+y)$.)  Linear functions are frequently called
\emph{linear transformations} or \emph{linear maps}.
\end{defn}

\begin{notn} If $V$ and $W$ are vector spaces (over the same field~$\F$) the family of
all linear functions from $V$ into $W$ is denoted by
 \index{L@$\ofml L(V,W)$, $\ofml L(V)$ (family of linear functions)}%
$\ofml L(V,W)$. Linear functions are frequently called
 \index{linear!transformations}%
\emph{linear transformations}, \emph{linear maps}, or \emph{linear mappings}.  When $V =
W$ we condense the notation $\ofml L(V,V)$ to $\ofml L(V)$ and we call the members of
$\ofml L(V)$
 \index{operator}%
\emph{operators}.
\end{notn}

\begin{exam} Let $V$ and $W$ be vector spaces over a field~$\F$. For $S$, $T \in
\ofml L(V,W)$ define $S + T$ by
   \[ (S + T)(x) := Sx + Tx \]
for all $x \in V$.  For $T \in \ofml L(V,W)$ and $\alpha \in \F$ define $\alpha T$ by
   \[ (\alpha T)(x) := \alpha(Tx) \]
for all $x \in V$.  Under these operations $\ofml L(V,W)$ is a vector space.
\end{exam}

\begin{prop}\label{lintran011} If $S\colon V \sto W$ and $T\colon W \sto X$ are linear
maps between vector spaces, then the composite of these two functions, nearly always
written as $TS$ rather than $T \circ S$,
 \index{conventions!for linear maps write $TS$ for $T \circ S$}%
is a linear map from $V$ into~$X$.
\end{prop}

\begin{conv} If $S\colon V \sto W$ and $T\colon W \sto X$ are linear maps between vector
spaces, then the composite linear map of these two functions is nearly always written as
$TS$ rather than $T \circ S$.
 \index{conventions!for linear maps write $TS$ for $T \circ S$}%
In the same vein, $T^2 = T \circ T$, $T^3 = T \circ T \circ T$, and so on.
\end{conv}

\begin{exer} Use the notation of definition~\ref{vector_space01} and suppose that
$(V,+,M)$ and $(W,+,M)$ are vector spaces over a common field $\F$ and that $T \in
\Hom(V,W)$ is such that the diagram
 \[ \xy
    \square[V`W`V`W;T`M_\alpha`M_\alpha`T]
 \endxy \]
commutes for every $\alpha \in \F$.  What can be said about the homomorphism~$T$?
\end{exer}

\begin{exam} Let $a < b$ and $\fml C = \fml C([a,b])$ be the vector space of all
continuous real valued functions on the interval~$[a,b]$.
 \index{Cab@$\fml C([a,b])$ (continuous real valued functions)}%
 \index{integration!is linear}%
Then integration
   \[ T\colon \fml C \sto \R \colon f \mapsto \int_a^b f(t)\,dt \]
is a linear map.
\end{exam}

\begin{exam} Let $a < b$ and $\fml C^1 = \fml C^1([a,b])$ be the set of all
continuously differentiable real valued functions on the interval~$[a,b]$. (Recall that a
function is
 \index{continuous!differentiability}%
 \index{differentiable!continuously}%
\df{continuously differentiable} if it has a derivative and the derivative is
continuous.)
 \index{Cab1@$\fml C^1([a,b])$ (continuously differentiable functions)}%
 \index{differentiation!is linear}%
Then differentiation
   \[ D\colon \fml C^1 \sto \fml C \colon f \mapsto f\,' \]
is linear.
\end{exam}

\begin{exam}\label{linearity_uni_shift} Let $\R^\infty$ be the vector space of all
sequences of real numbers and define
   \[ S\colon \R^\infty \sto \R^\infty\colon (x_1,x_2,x_3, \dots)
                         \mapsto (0,x_1,x_2,\dots). \]
Then $S$ is a linear operator.  It is called the
 \index{unilateral shift}%
 \index{shift!unilateral}%
\df{unilateral shift operator}.
\end{exam}

\begin{defn} Let $T\colon V \sto W$ be a linear transformation between vector spaces.
Then $\ker T$, the
 \index{kernel}%
 \index{kernel@$\ker T$ (the kernel of $T$)}%
\df{kernel} (or
 \index{nullspace}%
\df{nullspace}) of~$T$ is defined to be the set of all $x$ in $V$ such that $Tx = \vc 0$.
Also, $\ran T$, the
 \index{range}%
 \index{range@$\ran T$ (the range of $T$)}%
\df{range} of~$T$ (or the
 \index{image}%
\df{image} of~$T$), is the set of all $y$ in $W$ such that $y = Tx$ for some $x$ in~$V$.
The
 \index{rank}%
\df{rank} of $T$ is the dimension of its range and the
 \index{nullity}%
\df{nullity} of $T$ is the dimension of its kernel.
\end{defn}

\begin{defn} Let $T\colon V \sto W$ be a linear transformation between vector spaces
and let $A$ be a subset of~$V$.  Define
 \index{<image@$T^{\sto}(A)$ (direct image of $A$ under $T$)}%
$T^{\sto}(A) := \{Tx\colon x \in A\}$.  This is the
 \index{image!direct}%
 \index{direct image}%
\df{(direct) image of $A$ under~$T$}.
\end{defn}

\begin{prop} Let $T\colon V \sto W$ be a linear map between vector spaces and
$M \preceq V$. Then $T^\sto(M)$ is a subspace of~$W$. In particular, the range of a
linear map is a subspace of the codomain of the map.
\end{prop}

\begin{defn}  Let $T\colon V \sto W$ be a linear transformation between vector spaces
and let $B$ be a subset of~$W$.  Define
 \index{<image@$T^{\gets}(B)$ (inverse image of $B$ under $T$)}%
$T^{\gets}(B) := \{x \in V\colon  Tx \in B\}$.  This is the
 \index{inverse!image}%
 \index{image!inverse}%
\df{inverse image of $B$ under~$T$}.
\end{defn}

\begin{prop}\label{lintran014} Let $T\colon V \sto W$ be a linear map between vector spaces and
$M \preceq W$. Then $T^\gets(M)$ is a subspace of~$V$. In particular, the kernel of a
linear map is a subspace of the domain of the map.
\end{prop}

\begin{exer} Let $T\colon \R^3 \sto \R^3\colon x = (x_1,x_2,x_3) \mapsto (x_1 + 3x_2 -
2x_3, x_1 - 4x_3, x_1 + 6x_2)$.
 \begin{enumerate}
  \item[(a)] Identify the kernel of $T$ by describing it geometrically and by
giving its equation(s).
  \item[(b)] Identify the range of $T$ by describing it geometrically and by
giving its equation(s).
 \end{enumerate}
\end{exer}

\begin{exer} Let $T$ be the linear map from $\R^3$ to $\R^3$ defined by
  \[ T(x,y,z) = (2x + 6y - 4z, 3x + 9y - 6z, 4x + 12y - 8z). \]
Describe the kernel of $T$ geometrically and give its equation(s). Describe the range of
$T$ geometrically and give its equation(s).
\end{exer}

\begin{exer} Let $\fml C = \fml C[a,b]$ be the vector space of all continuous real
valued functions on the interval $[a,b]$ and $\fml C^1 = \fml C^1[a,b]$ be the vector
space of all continuously differentiable real valued functions on $[a,b]$. Let $D\colon
\fml C^1 \sto \fml C$ be the linear transformation defined by
 \[Df = f\,'\]
and let $T\colon \fml C \sto \fml C^1$ be the linear transformation defined by
 \[(Tf)(x) = \int_a^x f(t)\,dt\]
for all $f \in \fml C$ and $x \in [a,b]$.
 \begin{enumerate}
  \item[(a)] Compute (and simplify) $(DTf)(x)$.
  \item[(b)] Compute (and simplify) $(TDf)(x)$.
  \item[(c)] Find the kernel of $T$.
  \item[(d)] Find the range of $T$.
 \end{enumerate}
\end{exer}

\begin{prop} A linear map $T\colon V \sto W$ between vector spaces is injective if and
only if $\ker T = \{0\}$.
\end{prop}














\section{Invertible Linear Maps}
\begin{notn} In the sequel we will usually denote the identity operator $x \mapsto x$
on a vector space $V$ by $I_V$, or just $I$,
 \index{iden@$I_V$ (identity operator on a vector space)}%
 \index{identity!operator on a vector space}%
rather than by $\id{V}$.
\end{notn}

\begin{defn} A linear map $T\colon V \sto W$ between vector spaces
is
 \index{left!invertible linear map}%
 \index{invertible!left}%
\df{left invertible} (or has a \df{left inverse}, or is a
 \index{section}%
\df{section}) if there exists a linear map $L\colon W \sto V$ such that $LT = I_V$.

The map $T$ is
 \index{right!invertible linear map}%
 \index{invertible!right}%
\df{right invertible} (or has a \df{right inverse}, or is a
 \index{retraction}%
\df{retraction}) if there exists a linear map $R\colon W \sto V$ such that $TR = I_W$. We
say that $T$ is
 \index{invertible!linear map}%
 \index{linear!transformation!invertible}%
\df{invertible} (or has an \df{inverse}, or is an
 \index{isomorphism}%
\df{isomorphism}) if there exists a linear map $T^{-1}\colon W \sto V$ which is both a
left and a right inverse for~$T$. If there exists an isomorphism between two vector
spaces $V$ and $W$, we say that the spaces are
 \index{<binrel@$V \cong W$ (isomorphism of vector spaces)}%
\df{isomorphic} and we write $V \cong W$.
\end{defn}

\begin{exer} Show that an operator $T \in \ofml L(V)$ is invertible if it
satisfies the equation
  \[ T^2 - T + I_V = \vc 0. \]
\end{exer}

\begin{exam} The unilateral shift operator $S$ on the vector space $\R^\infty$ of
all sequences of real numbers (see~\ref{linearity_uni_shift}) is injective but not
surjective. It is left invertible but not right invertible.
\end{exam}

\begin{prop}\label{inv_lin_maps001} A linear map $T\colon V \sto W$ between vector spaces
is invertible if and only if has both a left inverse and a right inverse.
\end{prop}

\begin{prop}\label{inv_lin_maps004} A linear map between vector spaces is invertible if
and only if it is bijective.
\end{prop}

\begin{prop}\label{inverse_product_ops} Let $T\colon V \sto W$ and $S\colon W \sto X$ be linear maps between vector spaces.
If $T$ and $S$ are invertible, then so is $ST$ and $(ST)^{-1} = T^{-1}S^{-1}$.
\end{prop}

\begin{prop} An operator $T$ on a vector space $V$ is invertible if and only if it has a
unique right inverse.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider $ST + S - I_V$, where $S$ is the unique
right inverse for~$T$.  \ns
\end{proof}

\begin{exam} Every $n$-dimensional real vector space is isomorphic to~$\R^n$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Recall the notational conventions made
in~\ref{bases115}.   \ns
\end{proof}

\begin{exam}\label{inv_lin_maps005} Let $B$ be a basis for a vector space $V$ over a
field $\F$.  Then $V \cong l_c(B,\F)$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Recall the notational conventions made
in~\ref{bases111}.   \ns
\end{proof}

\begin{prop} Let $S$, $T \in \ofml L(V,W)$ where $V$ and $W$ are vector spaces over a
field $\F$; and let $B$ be a basis for~$V$.  If $S(e) = T(e)$ for every $e \in B$, then
$S = T$.
\end{prop}


\begin{prop} Let $V$ and $W$ be a vector spaces. If $V = M \oplus N$ and $T\colon M
\sto W$ is a linear map, then there exists $\wh T \in \ofml L(V,W)$ such that $\wh
T\,\bigr|_M = T$ and $\wh T\,\bigr |_N = \vc 0$.
\end{prop}

We can now make a considerable improvement on proposition~\ref{inv_lin_maps004}.

\begin{prop}\label{inv_lin_maps012} A linear transformation has a left inverse if and only if it is injective.
\end{prop}

\begin{prop}\label{inv_lin_maps013} A linear transformation has a right inverse if and only if it is surjective.
\end{prop}

\begin{prop}\label{inv_lin_maps006} Let $V$ and $W$ be vector spaces over a field $\F$
and $B$ be a basis for~$V$. If $f\colon B \sto W$, then there exists a unique linear map
$T_f\colon V \sto W$ which is an extension of~$f$.
\end{prop}

\begin{exer}\label{inv_lin_maps007} Let $S$ be a set, $V$ be a vector space over a field
$\F$, and $f\colon S \sto V$ be a bijection.  Explain how to use $f$ to make $S$ into a
vector space isomorphic to~$V$.
\end{exer}














\section{Matrix Representations}
\begin{prop} Let $T \in \ofml L(V,W)$ where $V$ is an $n$-dimensional vector space and $W$
is an $m$-dimensional vector space and let $\{e^1,e^2, \dots,e^n\}$ be an ordered basis
for~$V$.  Define an $m \times n$-matrix $[T]$ whose $k^{\text{th}}$ column ($1 \le k \le
n$) is the column vector $Te^k$ (see~\ref{bases115}).  Then for each $x \in V$ we have
   \[ Tx = [T]x. \]
\end{prop}

The displayed equation above requires a little interpretation. The left side is $T$
evaluated at $x$; the right side is an $m \times n$ matrix multiplied by an $n \times 1$
matrix (that is, a column vector).  Then the asserted equality is of two $m \times 1$
matrices (column vectors).

\begin{defn} If $V$ and $W$ are finite dimensional vector spaces with ordered bases and
$T \in \ofml L(V,W)$, then the matrix $[T]$ in the preceding proposition is the
 \index{matrix!representation of an linear map}%
 \index{representation!matrix}%
 \index{<unop@$[T]$ (matrix representation of~$T$)}%
\df{matrix representation} of~$T$.
\end{defn}

\begin{exer} Let $T:\R^4 \sto \R^3$ be defined by
 \[Tx = (x_1 - 3x_3 + x_4, 2x_1 + x_2 + x_3 + x_4, 3x_2 - 4x_3 + 7x_4)\]
for every $x = (x_1,x_2,x_3,x_4) \in \R^4$.
 \begin{enumerate}
  \item[(a)] Find $[T]$.
  \item[(b)] Find $T(1,-2,1,3)$.
  \item[(c)] Independently of part (b) calculate the matrix product \smash{$[T]
\begin{bmatrix} 1 \\ -2 \\ 1 \\ 3 \end{bmatrix}$}.
  \item[(d)] Find $\ker T$.
  \item[(e)] Find $\ran T$.
 \end{enumerate}
\end{exer}

\begin{exer} Let $\fml P_4(\R)$ be the vector space of polynomial functions of degree strictly
less than~4 on~$\R$. Consider the linear transformation $D^2\colon \fml P_4 \sto \fml
P_4\colon f \mapsto f\,''$.
 \begin{enumerate}
  \item[(a)] Find the matrix representation of $D^2$ (with respect to the standard
basis for~$\fml P^4(\R)$.
  \item[(b)] Find the kernel of $D^2$.
  \item[(c)] Find the range of $D^2$.
 \end{enumerate}
\end{exer}

\begin{exer} Let $T\colon \fml P_4(\R) \sto \fml P_5(\R)$ be the linear transformation
defined by $(Tp)(t) = (2 + 3t)p(t)$ for every $p \in \fml P_4(\R)$ and $t \in \R$.  Find
the matrix representation of $T$ with respect to the standard bases for $\fml P_4(\R)$
and $\fml P_5(\R)$.
\end{exer}

\begin{exer} Let $\fml P_n(\R)$ be the vector space of all polynomial functions on $\R$
with degree strictly less than~$n$. Define $T\colon \fml P_3(\R) \sto \fml P_5(\R)$ by
  \[ Tf(x) = \int_0^x\int_0^u p(t)\,dt\,du \]
for all $x$, $u \in \R$.
 \begin{enumerate}
  \item[(a)] Find the matrix representation of the linear map $T$ (with respect to the
standard bases for $\fml P_3(\R)$ and $\fml P_5(\R)$).
  \item[(b)] Find the kernel of $T$.
  \item[(c)] Find the range of $T$.
 \end{enumerate}
\end{exer}













\section{Spans, Independence, and Linearity}
\begin{prop} Let $T\colon V \sto W$ be an injective linear map between vector spaces. If $A$ is a
subset of $V$ such that $T^\sto(A)$ is linearly independent, then $A$ is linearly independent.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be an injective linear map between vector spaces. If $A$ is a linearly independent
subset of~$V$, then $T^\sto(A)$ is a linearly independent subset of~$W$.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be a linear map between vector spaces and $A \subseteq V$.
Then $T^\sto(\spn A) = \spn T^\sto(A)$.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be an injective linear map between vector spaces. If $B$ is a basis
for a subspace $U$ of~$V$, then $T^\sto(B)$ is a basis for~$T^\sto(U)$.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be an injective linear map between vector spaces. If $V$ is
spanned by a set $B$ of vectors and $T^\sto(B)$ is a basis for~$W$, then $B$ is a basis
for~$V$ and $T$ is an isomorphism.
\end{prop}

\begin{exer} Prove that a linear transformation $T\colon \R^3 \sto \R^2$ cannot be
one-to-one and that a linear transformation $S\colon \R^2 \sto \R^3$ cannot be onto. What
is the most general version of these assertions that you can invent (and prove)?
\end{exer}

\begin{prop} Suppose that $V$ and $W$ are finite dimensional vector spaces of the same
finite dimension and that $T\colon V \sto W$ is a linear map. Then the following are
equivalent;
 \begin{enumerate}
  \item[(a)] $T$ is injective;
  \item[(b)] $T$ is surjective; and
  \item[(c)] $T$ is invertible.
 \end{enumerate}
\end{prop}










\section{Dual Spaces}
\begin{defn} Let $V$ be a vector space over a field~$\F$. A linear map $f\colon V \sto
\F$ is a
 \index{linear!functional}%
 \index{functional!linear}%
\df{linear functional} on~$V$.  The set of linear functionals on $V$ is denoted
 \index{V@$V^*$ (dual space of~$V$)}%
by~$V^*$; that is, $V^* = \ofml L(V,\F)$.  The vector space $V^*$ is the
 \index{dual space}%
\df{dual space} of~$V$.
\end{defn}

\begin{conv}\label{dual022conv} Let $B$ be a basis for a vector space $V$ over a field $\F$.
Recall that in \ref{bases111} we adopted the notation
   \[ x = \sum_{e \in B} x_e e \]
where $x$ denotes both an element of $V$ and a scalar valued function on $B$ with finite
 \index{conventions!identification of a vector with a scalar valued function}%
support. In example~\ref{inv_lin_maps005} we justified this identification by
establishing that the vector spaces $V$ and $l_c(B,\F)$ are isomorphic.  Notice that this
is an extension of the usual notation in $\R^n$ where we write a vector $\vc v$ in terms
of its components:
    \[ x = \sum_{k=1}^n x_k\,e^k. \]
\end{conv}

\begin{exer} According to convention~\ref{dual022conv} above, what is the value of
$f_e$ when $e$ and $f$ are elements of the basis~$B$?
\end{exer}

\begin{prop}\label{dual023} Let $V$ be a vector space with basis $B$.  For every
$v \in V$ define a function
 \index{<superscript@$v^*$}%
$v^*$ on $V$ by
 \[ v^*(x) = \sum_{e \in B} x_e\,v_e \qquad \text{for all $x \in V$}.\]
Then $v^*$ is a linear functional on~$V$.
\end{prop}

\begin{notn}\label{dual023not} In the preceding proposition~\ref{dual023} the value
$v^*(x)$ of $v^*$ at $x$ is often written as
 \index{<bilinearform@$\langle x,v \rangle$ (alternative notation for $v^*(x)$)}%
$\langle x,v \rangle$.
\end{notn}

\begin{exer} Consider the notation~\ref{dual023not} above in the special case that
the scalar field $\F = \R$. Then $\langle \hphantom{x}, \hphantom{x}\rangle$ is an inner
product on the vector space~$V$. (For a definition of \emph{inner product}
see~\ref{inner_product_defn},)
\end{exer}

\begin{exer}\label{dual023z2} In the special case that the scalar field $\F = \C$,
things above are usually done a bit differently.  For $v \in V$ the function $v^*$ is
defined by
 \[ v^*(x) = \langle x,v \rangle = \sum_{e \in B} x_e\,\conj{v_e}\,. \]
Why do you think things are done this way?
\end{exer}

\begin{prop}\label{dual004} Let $v$ be a nonzero vector in a vector space~$V$ and $B$ be
a basis for $V$ which contains the vector~$v$.  Then there exists a linear functional $f
\in V^*$ such that $f(v) = 1$ and $f(e) = 0$ for every $e \in B \setminus \{v\}$.
\end{prop}

\begin{cor}\label{dual005} Let $M$ be a subspace of a vector space $V$ and $v$ a vector
in $V$ which does not belong to~$M$.  Then there exists $f \in V^*$ such that $f(v) = 1$
and $f^\sto(M) = \{0\}$.
\end{cor}

\begin{cor} If $v$ is a vector in a vector space~$V$ and $f(v) = 0$ for every $f \in V^*$,
then $v = \vc 0$.
\end{cor}

\begin{defn} Let $\F$ be a field.  A family $\fml F$ of $\F$-valued functions on a set $S$
containing at least two points
 \index{separates points}%
\df{separates points of}~$S$ if for every $x$, $y \in S$ such that $x \ne y$ there exists
$f \in \fml F$ such that $f(x) \ne f(y)$.
\end{defn}

\begin{cor} For every nontrivial vector space $V$, the dual space $V^*$ separates points
of~$V$.
\end{cor}

\begin{prop}\label{dual024} Let $V$ be a vector space with basis $B$. The map
$\Phi\colon V \sto V^* \colon v \mapsto v^*$ (see proposition~\ref{dual023}) is linear
and injective.
\end{prop}

The next result is the \emph{Riesz-Fr\'echet theorem for finite dimensional vector spaces
with basis}. It is important to keep in mind that the result does \emph{not} hold for
infinite dimensional vector spaces (see proposition~\ref{dual025}) and that the mapping
$\Phi$ depends on the basis which has been chosen for the vector space.

\begin{thm}\label{dual024z} Let $V$ be a finite dimensional vector space with basis $B$.
 \index{Riesz-Fr\'echet theorem!for vector spaces}%
Then the map $\Phi$ defined in the preceding proposition~\ref{dual024} is an isomorphism.
Thus for every $f \in V^*$ there exists a unique vector $a \in V$ such that $a^* = f$.
\end{thm}

\begin{defn} Let $V$ be a vector space with basis $\{e^\lambda\colon \lambda \in
\Lambda\}$. A basis $\{\varepsilon^\lambda\colon \lambda \in \Lambda\}$ for $V^*$ is the
 \index{dual basis}%
 \index{basis!dual}%
\df{dual basis} for $V^*$ if it satisfies
   \[ \varepsilon^\mu(e^\lambda) = \left\{%
      \begin{array}{ll}
          1, & \hbox{if $\mu = \lambda$;} \\
          0, & \hbox{if $\mu \ne \lambda$.} \\
      \end{array}%
      \right. \]
\end{defn}

\begin{thm} Every finite dimensional vector space $V$ with a basis has a unique dual
basis for its dual space. In fact, if $\{e^1, \dots, e^n\}$ is a basis for $V$, then
$\{(e^1)^*, \dots, (e^n)^*\}$ is the dual basis for~$V^*$.
\end{thm}

\begin{cor}\label{dual024z2} If a vector space $V$ is finite dimensional, then so is its
dual space and $\dim V = \dim V^*$.
\end{cor}

In proposition~\ref{dual024} we showed that the map
   \[ \Phi\colon V \sto V^*\colon v \mapsto v^* \]
is always an injective linear map.  In corollary~\ref{dual024z2} we showed that if $V$ is
finite dimensional, then so is $V^*$ and $\Phi$ is an isomorphism between $V$ and~$V^*$.
This is never true in infinite dimensional spaces.

\begin{prop}\label{dual025} If $V$ is infinite dimensional, then $\Phi$ is \emph{not} an
isomorphism.
\end{prop}

\enlargethispage{\baselineskip}

\begin{proof}[\emph{Hint for proof}] Let $B$ be a basis for~$V$.  Is there a functional
$g \in V^*$ such that $g(e) = 1$ for every $e \in B$?  Could such a functional be
$\Phi(x)$ for some $x \in V$?    \ns
\end{proof}

\begin{prop}\label{dual002} Let $V$ be a vector space over a field~$\F$.
For every $x$ in $V$
 \index{<over@$\wh x$ ($= \Gamma(x)$)}%
define
   \[ \wh x\colon V^* \sto \F\colon \phi \mapsto \phi(x)\,. \]
 \begin{enumerate}
  \item[(a)] The vector $\wh x$ belongs to $V^{**}$ for each $x \in V$.
  \item[(b)] Let $\sbsb{\Gamma}V$ be the map from $V$ to $V^{**}$ which takes $x$
 \index{gamma@$\Gamma$ (the map from $V$ to $V^{**}$ taking $x$ to $\wh x$)}%
to $\wh x$. (When no confusion is likely we write $\Gamma$ for $\sbsb{\Gamma} V$, so that
$\Gamma(x) = \wh x$ for each $x \in V$.) The function $\Gamma$ is linear.
  \item[(c)] The function $\Gamma$ is injective.
 \end{enumerate}
\end{prop}

\begin{prop} If $V$ is a finite dimensional vector space, then the map
$\Gamma\colon V \sto V^{**}$ defined in the preceding proposition~\ref{dual002} is an
isomorphism.
\end{prop}

\begin{prop} If $V$ is infinite dimensional, then the mapping $\Gamma$ (defined
in~\ref{dual002}) is \emph{not} an isomorphism.
\end{prop}

\begin{proof}[\emph{Hint for proof}]  Let $B$ be a basis for $V$ and $\psi \in V^*$ be
as in proposition~\ref{dual025}. Show that if we let $C_0$ be $\{e^*\colon e \in B\}$,
then the set $C_0 \cup \{\psi\}$ is linearly independent and can therefore be extended to
a basis $C$ for~$V^*$. Find an element $\tau$ in $V^{**}$ such that $\tau(\psi) = 1$ and
$\tau(\phi) = 0$ for every other $\phi \in C$. Can $\tau$ be $\Gamma x$ for some $x \in
V$?      \ns
\end{proof}











\section{Annihilators}
\begin{notn}\label{annihilators_perp_notn} Let $V$ be a vector space and $M \subseteq V$. Then
  \[ M^\perp := \{f \in V^*\colon  f(x)=0 \text { for all } x \in M\}\]
We say that $M^\perp$ is the
 \index{<superscript@$M^\perp$ (annihilator of $M$)}%
 \index{annihilator}%
\df{annihilator} of~$M$.  (The reasons for using the familiar ``orthogonal complement''
notation $M^\perp$ (usually read ``M perp'') will become apparent when we study inner
product spaces, where ``orthogonality'' actually makes sense.)
\end{notn}

\begin{exer} Find the annihilator in $\bigl(\R^2\bigr)^*$ of the vector
$(1,1)$ in~$\R^2$. (Express your answer in terms of the standard dual basis
for~$\bigl(\R^2\bigr)^*$.)
\end{exer}

\begin{prop}\label{annihilators113z} Let $M$ and $N$ be subsets of a vector space~$V$.
Then
 \begin{enumerate}
  \item[(a)] $M^\perp$ is a subspace of $V^*$.
  \item[(b)] If $M \subseteq N$, then $N^\perp \preceq M^\perp$.
  \item[(c)] $(\spn M)^\perp = M^\perp$.
  \item[(d)] $(M \cup N)^\perp = M^\perp \cap N^\perp$.
 \end{enumerate}
\end{prop}

\begin{prop}\label{annihilators113z2} If $M$ and $N$ be subspaces of a vector space~$V$,
then
  \[ (M + N)^\perp = M^\perp \cap N^\perp. \]
\end{prop}

\begin{exer} Explain why it is necessary in the preceding proposition to assume that $M$
and $N$ are subspaces of $V$ and not just subsets of~$V$.
\end{exer}

\begin{notn} Let $V$ be a vector space and $F \subseteq V^*$.  Then
  \[ F_\perp := \{x \in V \colon f(x)=0 \text { for all } f \in F\}\]
We say
 \index{<subscript1@$F_\perp$ (pre-annihilator of $F$)}%
 \index{pre-annihilator}%
that $F_\perp$ is the \df{pre-annihilator} of~$F$.
\end{notn}

\begin{prop}\label{annihilators113z3} If $M$ is a subspace of a vector space $V$, then $\bigl(M^\perp\bigr)_ \perp = M$.
\end{prop}

\begin{exer} Propositions \ref{annihilators113z} and \ref{annihilators113z2} asserted
some properties of the annihilator mapping $M \mapsto M^\perp$.  See to what extent you
can prove similar results about the pre-annihilator mapping $F \mapsto F_\perp$. What can
you say about the set $\bigl(F_\perp\bigr)^\perp$\,?
\end{exer}

\begin{prop} Let $V$ be a finite dimensional vector space and $F$ be a subspace of~$V^*$.  If $F_\perp = \{0\}$, then
$F = V^*$.
\end{prop}




\endinput
 \documentclass[11pt,reqno]{amsbook}
 \usepackage[dvips]{graphicx}
 \usepackage{amssymb}
 \usepackage{amscd}
 \usepackage[all]{xy}
 \usepackage{url}
 \usepackage{srcltx}
 \usepackage{showkeys}
 \usepackage[plainpages=false,pagebackref=true,hypertex]{hyperref}
     %plainpages=false [corrects page numbers in index]
     %pagebackref=true [provides back references from bibliography]



\makeindex


 \setlength{\textwidth}{6.5in}
 \setlength{\oddsidemargin}{0in}
 \setlength{\evensidemargin}{0in}
 \setlength{\textheight}{9.25in}
 \setlength{\topmargin}{-0.35in}

 \input{table}
 \input diagxy


%\includeonly{}


\swapnumbers \theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ax}[thm]{Axiom}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{notn}[thm]{Notation}
\newtheorem{conv}[thm]{Convention}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{cau}[thm]{CAUTION}
\newtheorem{fact}[thm]{Fact}
\newtheorem{exam}[thm]{Example}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{prob}[thm]{Problem}




\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}


\numberwithin{equation}{chapter}





 \newcommand{\field}[1]{\mathbb{#1}}
 \newcommand{\C}{\field{C}}
 \newcommand{\E}{\field{E}}
 \newcommand{\F}{\field{F}}
 \newcommand{\h}{\field{H}}
 \newcommand{\Ha}{\field{H}}
 \newcommand{\N}{\field{N}}
 \newcommand{\Po}{\field{P}}
 \newcommand{\Q}{\field{Q}}
 \newcommand{\R}{\field{R}}
 \newcommand{\Sp}{\field{S}}
 \newcommand{\T}{\field{T}}
 \newcommand{\Z}{\field{Z}}



 \newcommand{\sto}{\rightarrow}
      %Diagxy redefined \to . This shortens the arrow.
 \newcommand{\ofml}[1]{\mathfrak{#1}}
      % for families of operators
 \newcommand{\fml}[1]{\mathcal{#1}}
      % for families of functions
 \newcommand{\sfml}[1]{\mathfrak{#1}}
      % for families of sets
 \newcommand{\ftr}[1]{\mathsf{#1}}
      % for functors
 \newcommand{\vc}[1]{\mathbf{#1}}
      % for vectors
 \newcommand{\df}[1]{\textsc{#1}}
      % for terms being defined
 \newcommand{\D}{\displaystyle}
 \newcommand{\ns}{\renewcommand{\qed}{}}
 \newcommand{\abs}[1]{\lvert#1\rvert}
      % absolute value
 \newcommand{\bigabs}[1]{\left\lvert#1\right\rvert}
      % large absolute value
 \newcommand{\cat}[1]{\mathbf{#1}}
      % for categories
 \newcommand{\clim}{\,\boldsymbol{\hat\iota}}
      % for imaginary element if Clifford algebra
 \newcommand{\lobo}[1]{\mathfrak{#1}}
      % for big-oh and little-oh functions
\newcommand{\mor}[2]{\operatorname{\mathfrak{Mor}}(#1,#2)}
      % Morphisms from #1 to #2
 \newcommand{\norm}[1]{\lVert#1\rVert}
      % norm
 \newcommand{\bignorm}[1]{\bigl\lVert#1\bigr\rVert}
      % large norm
 \newcommand{\trinorm}[1]{\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert#1\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert}
      % 3-bar norm
 \newcommand{\intr}[1]{#1^{\circ}}
      % interior
 \newcommand{\clo}[1]{\overline{#1}}
      % closure
 \newcommand{\conj}[1]{\overline{#1}}
      % complex conjugate
 \newcommand{\id}[1]{\operatorname{id}_{#1}}
      % Identity on #1
 \newcommand{\open}[2]{#1\overset{\thickspace{}_\circ}{\subseteq}#2}
      % #1 is an open subset of #2
 \newcommand{\pd}[2]{\frac{\partial#1}{\partial#2}}
      % partial derivative of #1 wrt #2
 \newcommand{\printparallel}{\|}
      % allows makeindex to print \|
 \newcommand{\sbsb}[2]{#1_{{}_\sst{#2}}}
      % makes smaller, lower subscript
 \newcommand{\sst}[1]{{\scriptstyle{#1}}}
      % sub- and superscript size type
 \newcommand{\ssst}[1]{{\scriptscriptstyle{#1}}}
      % second level sub- and superscripts
 \newcommand{\ten}{\mathcal T}
 \newcommand{\wh}[1]{\widehat{#1}}
 \newcommand{\wt}[1]{\widetilde{#1}}




 \DeclareMathOperator{\ad}{Ad}
 \DeclareMathOperator{\alt}{Alt}
 \DeclareMathOperator{\ba}{ba}
 \DeclareMathOperator{\ca}{ca}
 \DeclareMathOperator{\card}{card}
 \DeclareMathOperator{\cl}{Cl}
 \DeclareMathOperator{\codim}{codim}
 \DeclareMathOperator{\coker}{coker}
 \DeclareMathOperator{\curl}{curl}
 \DeclareMathOperator{\diag}{diag}
 \DeclareMathOperator{\diam}{diam}
 \DeclareMathOperator{\divr}{div}
 \DeclareMathOperator{\dom}{dom}
 \DeclareMathOperator{\fin}{Fin}
 \DeclareMathOperator{\grad}{grad}
 \DeclareMathOperator{\Hom}{Hom}
 \DeclareMathOperator{\im}{im}
 \DeclareMathOperator{\ind}{ind}
 \DeclareMathOperator{\intrin}{int}
 \DeclareMathOperator{\inv}{inv}
 \DeclareMathOperator{\lat}{Lat}
 \DeclareMathOperator{\mx}{Max}
 \DeclareMathOperator{\ran}{ran}
 \DeclareMathOperator{\rca}{rca}
 \DeclareMathOperator{\sgn}{sgn}
 \DeclareMathOperator{\spn}{span}
 \DeclareMathOperator{\supp}{supp}
 \DeclareMathOperator{\tr}{tr}
 \DeclareMathOperator{\vol}{vol}



\input{table}


\begin{document}


 \frontmatter
 \title{ELEMENTS OF LINEAR AND MULTILINEAR ALGEBRA}
 \author{John M. Erdman \\
      Portland State University \\
      \mbox{\hphantom{P}} \\
      Version June 3, 2014  \\
      \mbox{\hphantom{P}} \\
      \copyright 2010 John M. Erdman \\
      \mbox{\hphantom{P}} \\
            \mbox{\hphantom{P}} \\
                  \mbox{\hphantom{P}} }


\vskip 2 in

 \email{erdman@pdx.edu}

\maketitle


\vskip 2 in

 \begin{figure}[h]
  \includegraphics{by-sa}
 \end{figure}

\vskip 1 in

This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/.



\vfill\eject


\maketitle

\include{Wiener_quote}


\tableofcontents


\include{preface}

 \cleardoublepage
 \phantomsection
        %allows TOC links to find preface correctly



\mainmatter

    \include{algebraic_objects}
    \include{vector_spaces}
    \include{linear_transformations}
    \include{language_categories}
    \include{spectral_thm_vsps}
    \include{inner_product}
    \include{differential_calculus}
    \include{multilinear_maps}
    \include{tensor_algebras}
    \include{differential_manifolds}
    \include{differential_forms}
    \include{homology_cohomology}
    \include{stokes_theorem}
    \include{geometric_algebra}
    \include{clifford_algebras}







\backmatter


 \clearpage
 \phantomsection
        %allows TOC links to find bibliography correctly
 \bibliographystyle {amsplain}
 \bibliography{multilinear_algebra_bib}
     %\addcontentsline{toc}{chapter}{Bibliography}


 \cleardoublepage
 \phantomsection
         %allows TOC links to find index correctly
 \printindex
 \addcontentsline{toc}{chapter}{Index}


\end{document}
\documentclass[11pt,reqno]{amsbook}
\usepackage[dvips]{graphicx}
 \usepackage{amssymb}
 \usepackage{amscd}
 \usepackage[all]{xy}
 \usepackage{url}
 \usepackage{srcltx}
% \usepackage{showkeys}
  \usepackage[pdftex,
            pdfauthor={John M. Erdman},
            pdftitle={Elements of Linear and Multilinear Algebra},
            pdfsubject={linear algebra, multilinear algebra},
            pdfkeywords={linear, algebra, multilinear, vector, transformation, matrix,
            spectrum, projections, dependence, basis, tensor, span, rank, nullity,
            similarity, eigenvalue, eigenvector, isomorphism, annihilator, Jordan
            decomposition, inner product, bilinear, convexity, Clifford algebras},
            plainpages=false,pdftex,pagebackref=true,colorlinks,bookmarks=false]{hyperref}
     %plainpages=false corrects page numbers in index
     %pagebackref=true provides back references from bibliography
     %bookmarks=false prevents PDF bookmarks and the accompanying
     %     error messages about tokens (like $)



\makeindex


 \setlength{\textwidth}{6.5in}
 \setlength{\oddsidemargin}{0in}
 \setlength{\evensidemargin}{0in}
 \setlength{\textheight}{9.25in}
 \setlength{\topmargin}{-0.35in}

 \input{table}
 \input diagxy


% \includeonly{}


\swapnumbers \theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ax}[thm]{Axiom}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{notn}[thm]{Notation}
\newtheorem{conv}[thm]{Convention}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{cau}[thm]{CAUTION}
\newtheorem{fact}[thm]{Fact}
\newtheorem{exam}[thm]{Example}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{prob}[thm]{Problem}




\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}


\numberwithin{equation}{chapter}






 \newcommand{\field}[1]{\mathbb{#1}}
 \newcommand{\C}{\field{C}}
 \newcommand{\E}{\field{E}}
 \newcommand{\F}{\field{F}}
 \newcommand{\h}{\field{H}}
 \newcommand{\Ha}{\field{H}}
 \newcommand{\N}{\field{N}}
 \newcommand{\Po}{\field{P}}
 \newcommand{\Q}{\field{Q}}
 \newcommand{\R}{\field{R}}
 \newcommand{\Sp}{\field{S}}
 \newcommand{\T}{\field{T}}
 \newcommand{\Z}{\field{Z}}



 \newcommand{\sto}{\rightarrow}
      %Diagxy redefined \to . This shortens the arrow.
 \newcommand{\ofml}[1]{\mathfrak{#1}}
      % for families of operators
 \newcommand{\fml}[1]{\mathcal{#1}}
      % for families of functions
 \newcommand{\sfml}[1]{\mathfrak{#1}}
      % for families of sets
 \newcommand{\ftr}[1]{\mathsf{#1}}
      % for functors
 \newcommand{\vc}[1]{\mathbf{#1}}
      % for vectors
 \newcommand{\df}[1]{\textsc{#1}}
      % for terms being defined
 \newcommand{\D}{\displaystyle}
 \newcommand{\ns}{\renewcommand{\qed}{}}
 \newcommand{\abs}[1]{\lvert#1\rvert}
       % absolute value
 \newcommand{\bigabs}[1]{\left\lvert#1\right\rvert}
       % large absolute value
 \newcommand{\cat}[1]{\mathbf{#1}}
       % for categories
 \newcommand{\clim}{\,\boldsymbol{\hat\iota}}
       % for imaginary element if Clifford algebra
 \newcommand{\lobo}[1]{\mathfrak{#1}}
       % for big-oh and little-oh functions
 \newcommand{\mor}[2]{\operatorname{\mathfrak{Mor}}(#1,#2)}
       % Morphisms from #1 to #2
 \newcommand{\norm}[1]{\lVert#1\rVert}
       % norm
 \newcommand{\bignorm}[1]{\left\lVert#1\right\rVert}
       % large norm
 \newcommand{\trinorm}[1]{\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert#1\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert}
       % 3-bar norm
 \newcommand{\intr}[1]{#1^{\circ}}
       % interior
 \newcommand{\clo}[1]{\overline{#1}}
       % closure
 \newcommand{\conj}[1]{\overline{#1}}
       % complex conjugate
 \newcommand{\id}[1]{\operatorname{id}_{#1}}
       % Identity on #1
 \newcommand{\open}[2]{#1\overset{\thickspace{}_\circ}{\subseteq}#2}
       % #1 is an open subset of #2
 \newcommand{\pd}[2]{\frac{\partial#1}{\partial#2}}
       % partial derivative of #1 wrt #2
 \newcommand{\printparallel}{\|}
       % allows makeindex to print \|
 \newcommand{\sbsb}[2]{#1_{{}_\sst{#2}}}
       % makes smaller, lower subscript
 \newcommand{\sst}[1]{{\scriptstyle{#1}}}
       % sub- and superscript size type
 \newcommand{\ssst}[1]{{\scriptscriptstyle{#1}}}
       % second level sub- and superscripts
 \newcommand{\ten}{\mathcal T}
 \newcommand{\wh}[1]{\widehat{#1}}
 \newcommand{\wt}[1]{\widetilde{#1}}




 \DeclareMathOperator{\ad}{Ad}
 \DeclareMathOperator{\alt}{Alt}
 \DeclareMathOperator{\ba}{ba}
 \DeclareMathOperator{\ca}{ca}
 \DeclareMathOperator{\card}{card}
 \DeclareMathOperator{\cl}{Cl}
 \DeclareMathOperator{\codim}{codim}
 \DeclareMathOperator{\coker}{coker}
 \DeclareMathOperator{\curl}{curl}
 \DeclareMathOperator{\diag}{diag}
 \DeclareMathOperator{\diam}{diam}
 \DeclareMathOperator{\divr}{div}
 \DeclareMathOperator{\dom}{dom}
 \DeclareMathOperator{\fin}{Fin}
 \DeclareMathOperator{\grad}{grad}
 \DeclareMathOperator{\Hom}{Hom}
 \DeclareMathOperator{\im}{im}
 \DeclareMathOperator{\ind}{ind}
 \DeclareMathOperator{\intrin}{int}
 \DeclareMathOperator{\inv}{inv}
 \DeclareMathOperator{\lat}{Lat}
 \DeclareMathOperator{\mx}{Max}
 \DeclareMathOperator{\ran}{ran}
 \DeclareMathOperator{\rca}{rca}
 \DeclareMathOperator{\sgn}{sgn}
 \DeclareMathOperator{\spn}{span}
 \DeclareMathOperator{\supp}{supp}
 \DeclareMathOperator{\tr}{tr}
 \DeclareMathOperator{\vol}{vol}




\begin{document}

\frontmatter
\title{ELEMENTS OF LINEAR AND MULTILINEAR ALGEBRA}
\author{John M. Erdman \\
      Portland State University \\
      \mbox{\hphantom{P}} \\
      Version June 3, 2014  \\
      \mbox{\hphantom{P}} \\
      \copyright 2010 John M. Erdman \\
  %    This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. \\
      \mbox{\hphantom{P}} \\
            \mbox{\hphantom{P}} \\
                  \mbox{\hphantom{P}}
      }


\vskip 2 in

 \email{erdman@pdx.edu}

\maketitle


 \begin{figure}[h]
  \includegraphics[scale=0.4]{by-sa.pdf}
 \end{figure}


This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/.



\vfill\eject



\maketitle


\include{Wiener_quote}


\tableofcontents



%\vskip 3 in
%\include{Wiener_quote}

\include{preface}

 \cleardoublepage
 \phantomsection
        %allows TOC links to find preface correctly



\mainmatter

    \include{algebraic_objects}
    \include{vector_spaces}
    \include{linear_transformations}
    \include{language_categories}
    \include{spectral_thm_vsps}
    \include{inner_product}
    \include{differential_calculus}
    \include{multilinear_maps}
    \include{tensor_algebras}
    \include{differential_manifolds}
    \include{differential_forms}
    \include{homology_cohomology}
    \include{stokes_theorem}
    \include{geometric_algebra}
    \include{clifford_algebras}




\backmatter

 \clearpage
 %\cleardoublepage
 \phantomsection
        %allows TOC links to find bibliography correctly
 \bibliographystyle{amsplain}
 \bibliography{multilinear_algebra_bib}
     %\addcontentsline{toc}{chapter}{Bibliography}


 \cleardoublepage
 \phantomsection
         %allows TOC links to find index correctly
 \printindex
 \addcontentsline{toc}{chapter}{Index}


\end{document}
\chapter{MULTILINEAR MAPS AND DETERMINANTS}

\section{Permutations}
A bijective map $\sigma\colon X \sto X$ from a set $X$ onto itself is a
 \index{permutation}%
\df{permutation} of the set.  If $x_1$, $x_2$, \dots, $x_n$ are distinct
elements of a set $X$, then the permutation of $X$ that maps $x_1 \mapsto x_2$,
$x_2 \mapsto x_3$, \dots, $x_{n-1} \mapsto x_n$, $x_n \mapsto x_1$ and leaves
all other elements of $X$ fixed is a
 \index{cycle}%
\df{cycle} (or
 \index{permutation!cyclic}%
\df{cyclic permutation}) of
 \index{cycle!length of a}%
 \index{length!of a cycle}%
\df{length}~$n$.  A cycle of length $2$ is a
 \index{transposition}%
\df{transposition}. Permutations $\sigma_1$, \dots, $\sigma_n$ of a set $X$ are
 \index{disjoint!permutations}%
\df{disjoint} if each $x \in X$ is moved by at most one $\sigma_j$; that is, if
$\sigma_j(x) \ne x$ for at most one $j \in \N_n := \{1,2, \dots, n\}$.

\begin{prop}  If $X$ is a nonempty set, the set of permutations of $X$ is a group
under composition.
\end{prop}

Notice that if $\sigma$ and $\tau$ are disjoint permutations of a set $X$, then
$\sigma\tau = \tau\sigma$. If $X$ is a set with $n$ elements, then the group of
permutations of $X$ (which we may identify with the group of permutations of
the set~$\N_n$) is the
 \index{symmetric!group}%
 \index{group!symmetric}%
\df{symmetric group on $n$ elements} (or \df{on $n$ letters}); it is denoted by~$S_n$.

\begin{prop} Any permutation $\sigma \ne \id X$ of a finite set $X$ can be written as
a product (composite) of cycles of length at least~$2$.
\end{prop}

\begin{proof} See\cite{Pinter:1990}, chapter 8, theorem 1.  \ns
\end{proof}

A permutation of a finite set $X$ is
 \index{even!permutation}%
 \index{permutation!even}%
\df{even} if it can be written as the product of an even number of transpositions, and it
is
 \index{odd!permutation}%
 \index{permutation!odd}%
\df{odd} if it can be written as a product of an odd number of transpositions.

\begin{prop} Every permutation of a finite set is either even or odd, but not both.
\end{prop}

\begin{proof} See\cite{Pinter:1990}, chapter 8, theorem 3.  \ns
\end{proof}

\begin{defn} The
 \index{sign!of a permutation}%
 \index{permutation!sign of a}%
\df{sign} of a permutation $\sigma$, denoted by $\sgn\sigma$, is $+1$ if $\sigma$ is even
and $-1$ if $\sigma$ is odd.
\end{defn}





\vskip .3 in





\section{Multilinear Maps}
\begin{defn} Let $V_1$, $V_2$, \dots, $V_n$, and $W$ be vector spaces over a field~$\F$.  We say that a function $f\colon V_1\times \dots \times V_n \sto W$ is
 \index{multilinear!map}%
 \index{nlinear@$n$-linear function}%
\df{multilinear} (or \df{$n$-linear}) if it is linear in each of its $n$
variables. We ordinarily call $2$-linear maps
 \index{bilinear!map}%
\df{bilinear} and $3$-linear maps
 \index{trilinear map}%
\df{trilinear}.  We denote
 \index{L@$\ofml L^n(V_1, \dots, V_n)$ (family of $n$-linear functions)}%
by $\ofml L^n(V_1, \dots, V_n;W)$ the family of all $n$-linear maps from $V_1 \times
\dots \times V_n$ into $W$. A multilinear map from the product $V_1 \times \dots \times
V_n$ into the scalar field $\F$ is a
 \index{multilinear!form}%
 \index{form!multilinear}%
\df{multilinear form} (or a
 \index{multilinear!functional}%
 \index{functional!multilinear}%
\df{multilinear functional}.
\end{defn}

\begin{exer} Let $V$ and $W$ be vector spaces over a field $\F$; $u$, $v$, $x$, $y \in V$; and $\alpha \in \F$.
 \begin{enumerate}
  \item[(a)] Expand $T(u + v,x + y)$ if $T$ is a bilinear map from $V \times V$ into $W$.
  \item[(b)] Expand $T(u + v,x + y)$ if $T$ is a linear map from $V \oplus V$ into $W$.
  \item[(c)] Write $T(\alpha x,\alpha y)$ in terms of $\alpha$ and $T(x,y)$ if $T$ is a bilinear map from $V \times V$ into $W$.
  \item[(d)] Write $T(\alpha x,\alpha y)$ in terms of $\alpha$ and $T(x,y)$ if $T$ is a linear map from $V \oplus V$ into $W$.
 \end{enumerate}
\end{exer}

\begin{exam} Composition of operators on a vector space $V$ is a bilinear map on~$\ofml L(V)$.
\end{exam}

\begin{prop}\label{mlm004z} If $U$, $V$, and $W$ are vector spaces over a field $\F$, then so is $\ofml L^2(U,V;W)$.  Furthermore the spaces
$\ofml L(U, \ofml L(V,W))$ and $\ofml L^2(U,V;W)$ are (naturally) isomorphic.
\end{prop}

\begin{proof}[\emph{Hint for proof}] The isomorphism is implemented by the map
   \[ F\colon \ofml L(U, \ofml L(V,W)) \sto \ofml L^2(U,V;W)\colon \phi \mapsto \hat\phi \]
where $\hat\phi(u,v) := (\phi(u))(v)$ for all $u \in U$ and $v \in V$.   \ns
\end{proof}

\begin{defn} A multilinear map $f\colon V^n \sto W$ from the $n$-fold product $V \times \dots \times V$ of a vector space $V$ into a vector space $W$ is
 \index{alternating}%
\df{alternating} if $f(v_1,\dots,v_n) = 0$ whenever $v_i = v_j$ for some $i \ne j$.
\end{defn}

\begin{exer} Let $V = \R^2$ and $f\colon V^2 \sto \R\colon (v,w) \mapsto v_1w_2$. Is $f$ bilinear? Is it alternating?
\end{exer}

\begin{exer} Let $V = \R^2$ and $g\colon V^2 \sto \R\colon (v,w) \mapsto v_1 + w_2$. Is $g$ bilinear? Is it alternating?
\end{exer}

\begin{exer} Let $V = \R^2$ and $h\colon V^2 \sto \R\colon (v,w) \mapsto v_1w_2 - v_2w_1$. Is $h$ bilinear?  Is it alternating?  If $\{e^1,e^2\}$ is the
usual basis for $\R^2$, what is $h(e^1,e^2)$?
\end{exer}

\begin{defn}  If $V$ and $W$ are vector spaces, a multilinear map $f\colon V^n \sto W$ is
 \index{skew-symmetric}%
 \index{symmetric!skew-}%
\df{skew-symmetric} if
  \[ f(v^1, \dots, v^n) = (\sgn \sigma) f\bigl(v^{\sigma(1)}, \dots, v^{\sigma(n)}\bigr) \qquad \text{for all $\sigma \in S_n$}. \]
\end{defn}

\begin{prop}\label{mlm009} Suppose that $V$ and $W$ be vector spaces. Then every alternating multilinear map $f\colon V^n \sto W$ is skew-symmetric.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider $f(u + v, u + v)$ in the bilinear case.  \ns
\end{proof}

\begin{rem}\label{mlm001rem} If a function $f\colon \R^n \sto \R$ is differentiable, then at each point $a$ in $\R^n$ the differential of $f$ at $a$ is a
linear map from $\R^n$ into $\R$. Thus we regard $df\colon a \mapsto df_a$ (the \df{differential of} $f$) as a map from $\R^n$ into $\ofml L(\R^n,\R)$. It
is natural to inquire whether the function $df$ is itself differentiable. If it is, its differential at $a$ (which we denote by $d^{\,2}\!f_a$) is a linear
map from $\R^n$ into $\ofml L(\R^n,\R)$; that is
  \[ d^{\,2}\!f_a \in \ofml L(\R^n, \ofml L(\R^n,\R)). \]
In the same vein, since $d^{\,2}\!f$ maps $\R^n$ into $\ofml L(\R^n, \ofml L(\R^n,\R))$, its differential (if it exists) belongs to
$\ofml L(\R^n,\ofml L(\R^n, \ofml L(\R^n,\R)))$. It is moderately unpleasant to contemplate what an element of
$\ofml L(\R^n, \ofml L(\R^n,\R))$ or of $\ofml L(\R^n,\ofml L(\R^n, \ofml L(\R^n,\R)))$ might ``look like''. And clearly as we pass to even higher order
differentials things look worse and worse. It is comforting to discover that an element of $\ofml L(\R^n, \ofml
L(\R^n,\R))$ may be regarded as a map from $(\R^n)^2$ into $\R$ which is bilinear (that is, linear in both of its variables), and that
$\ofml L(\R^n,\ofml L(\R^n, \ofml L(\R^n,\R)))$ may be thought of as a map from $(\R^n)^3$ into $\R$ which is linear in each of its three variables. More
generally, if $V_1$, $V_2$, $V_3$, and $W$ are arbitrary vector spaces it will be possible to identify the vector space $\ofml L(V_1, \ofml L(V_2,W)))$
with the space of bilinear maps from $V_1 \times V_2$ to~$W$, the vector space $\ofml L(V_1,\ofml L(V_2, \ofml L(V_3,W)))$ with the trilinear maps from
$V_1 \times V_2 \times V_3$ to~$W$, and so on (see, for example, proposition~\ref{mlm004z}).
\end{rem}




















\section{Determinants}\label{section_dets}
\begin{defn} A field $\F$ is \df{of characteristic zero} if $n\vc 1 = \vc 0$
 \index{characteristic!zero}%
for \emph{no} $n \in \N$.
\end{defn}

\begin{conv} \label{mlm011zconv} In the following material on determinants, we will assume that the scalar fields underlying all the vector spaces and algebras we
 \index{conventions!in section 7.3 all fields are of characteristic zero}%
encounter are of characteristic zero.  Thus multilinear functions will be alternating if and only if they are skew-symmetric. (See exercises~\ref{mlm009}
and~\ref{mlm011z}.)
\end{conv}

\begin{rem} Let $A$ be a unital commutative algebra. In the sequel we identify the algebra $\bigl(A^n\bigr)^n = A^n \times \dots \times A^n \text{ ($n$ factors)}$
with the algebra
 \index{M@$\mathbf M_n(A)$ ($n \times n$ matrices of members of~$A$)}%
$\mathbf M_n(A)$ of $n \times n$ matrices of elements of $A$ by regarding the term $a^k$
in $(a^1, \dots, a^n) \in \bigl(A^n\bigr)^n$ as the $k^{\text{th}}$ column vector of an
$n \times n$ matrix of elements of~$A$.  There are many standard notations for the same
thing: $\mathbf M_n(A)$, $A^n \times \dots \times A^n \text{ ($n$ factors)}$,
$\bigl(A^n\bigr)^n$, $A^{n \times n}$, and $A^{n^2}$, for example.

The identity matrix, which we usually denote by $I$, in $\mathbf M_n(A)$ is $(e^1, \dots,
e^n)$, where $e^1$, \dots, $e^n$ are the standard basis vectors for $A^n$; that is, $e^1
= (\vc 1_A,0 ,0, \dots)$, $e^2 = (0, \vc 1_A,0 ,0, \dots)$, and so on.
\end{rem}

\begin{defn} Let $A$ be a unital commutative algebra.  A
 \index{determinant!function}%
\df{determinant function} is an alternating multilinear map $D\colon \mathbf M_n(A) \sto A$ such that $D(I) = \vc 1_A$.
\end{defn}

\begin{prop} Let $V = \R^n$.  Define
 \[ \Delta\colon V^n \sto \R \colon (v^1, \dots, v^n) \mapsto \sum_{\sigma \in S_n} (\sgn\sigma)v^1_{\sigma(1)} \dots v^n_{\sigma(n)}.\]
Then $\Delta$ is a determinant function which satisfies $\Delta(e^1, \dots, e^n) = 1$.
\end{prop}

\emph{Note:} If $A$ is an $n \times n$ matrix of real numbers we define $\det A$, the
 \index{determinant}%
\df{determinant} of $A$, to be $\Delta(v^1, \dots, v^n)$ where $v^1$, \dots, $v^n$ are the column vectors of the matrix~$A$.

\begin{exer} Let $A = \begin{bmatrix} 1 & 3 & 2 \\ -1 & 0 & 3 \\ -2 & -2 & 1 \end{bmatrix}$. Use the definition above to find $\det A$.
\end{exer}

\begin{prop}\label{mlm011z} If $V$ and $W$ are vector spaces over a field $\F$ of characteristic zero and $f\colon V^n \sto W$ is a skew-symmetric
multilinear map, then $f$ is alternating.
\end{prop}

\begin{prop} Let $V$ and $W$ be vector spaces (over a field of characteristic $0$) and $f\colon V^n \sto W$ be a multilinear map. If $f(v^1, \dots, v^n) = 0$
whenever two consecutive terms in the $n$-tuple $(v^1, \dots, v^n)$ are equal, then $f$ is skew-symmetric and therefore alternating.
\end{prop}

\begin{prop} Let $f \colon V^n \sto W$ be an alternating multilinear map, $j \ne k$ in $\N_n$, and $\alpha$ be a scalar. Then
 \[ f(v^1, \dots, v^j \underset{j}{\underset{\uparrow}{+}} \alpha v^k, \dots, v^n) = f(v^1, \dots, \underset{j}{\underset{\uparrow}{v^j}} ,\dots, v^n). \]
\end{prop}

\begin{prop} Let $A$ be a unital commutative algebra (over a field of characteristic zero) and $n \in \N$.  A determinant function exists on~$\mathbf M_n(A)$.
\emph{Hint.}  Consider
   \[ \det\colon \mathbf M_n(A) \sto A \colon (a^1, \dots a^n) \mapsto \sum_{\sigma \in S_n} (\sgn\sigma)a^1_{\sigma(1)} \dots a^n_{\sigma(n)}.\]
\end{prop}

\begin{prop} Let $D$ be an alternating multilinear map on $\mathbf M_n(A)$ where $A$ is a unital commutative algebra and $n \in \N$. For every $C \in \mathbf M_n(A)$
    \[ D(C) = D(I)\det C. \]
\end{prop}

\begin{prop} Show that the determinant function on $\mathbf M_n(A)$, where $A$ is a unital commutative algebra, is unique.
\end{prop}

\begin{prop} Let $A$ be a unital commutative algebra and $B$, $C \in \mathbf M_n(A)$.  Then
    \[ \det(BC) = \det B \det C. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider the function $D(C) = D(c^1, \dots, c^n) := \det(Bc^1, \dots, Bc^n)$, where $Bc^k$ is the product of the
$n \times n$ matrix $B$ and the $k^{\text{th}}$ column vector of~$C$.   \ns
\end{proof}

\begin{prop} For an $n\times n$ matrix $B$ let $B^{\,t}$, the
 \index{transpose}%
\df{transpose} of $B$, be the matrix obtained from $B$ by interchanging its rows and columns; that is, if $B = \bigl[b^j_i\bigr]$, then $B^{\,t} = \bigl[b^i_j\bigr]$.
Then $\det B^{\,t} = \det B$.
\end{prop}
























\section{Tensor Products of Vector Spaces}

For a modern and very careful exposition of tensor products, which is more extensive than given here, I recommend chapter 14 of~\cite{Roman:2005}.)

\begin{defn}\label{tpvs006def}  Let $U$ and $V$ be vector spaces over a field~$\F$.  A vector space
 \index{<binop@$U \otimes V$ (tensor product)}%
$U \otimes V$ together with a bilinear map $\tau\colon U \times V \sto U \otimes V$ is a
 \index{tensor!product}%
 \index{product!tensor}%
\df{tensor product} of $U$ and~$V$ if for every vector space $W$ and every bilinear map $B\colon U \times V \sto W$, there exists a unique linear map
$\wt B\colon U \otimes V \sto W$ which makes the following diagram commute.
  \[ \xy
    \qtriangle[U \times V`U \otimes V`W;\tau`B`\wt B]
  \endxy \]
\end{defn}

\begin{prop} In the category of vector spaces and linear maps if tensor products exist, then they are unique (up to isomorphism).
\end{prop}

\begin{prop} In the category of vector spaces and linear maps tensor products exist.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $U$ and $V$ be vector spaces over a field~$\F$.
Consider the free vector space $l_c(U \times V) = l_c(U \times V\,,\,\F)$. Define
  \[ \ast\colon U \times V \sto l_c(U \times V)\colon (u,v) \mapsto \sbsb{\chi}{\{(u,v)\}}. \]
Write $u \ast v$ instead of $\ast(u,v)$. Then let
  \begin{align*}
     S_1 &= \{ (u_1 + u_2)\ast v - u_1 \ast v - u_2 \ast v \colon
                \text{$u_1$, $u_2 \in U$ and $v \in V$} \}, \\
     S_2 &= \{ (\alpha u)\ast v - \alpha (u \ast v) \colon
                \text{$\alpha \in \F$, $u \in U$, and $v \in V$} \}, \\
     S_3 &= \{ u \ast (v_1 + v_2) - u \ast v_1 - u \ast v_2 \colon
                \text{$u \in U$ and $v_1$, $v_2 \in V$} \}, \\
     S_4 &= \{  u \ast (\alpha v) - \alpha (u \ast v) \colon
                \text{$\alpha \in \F$, $u \in U$, and $v \in V$} \},  \\
       S &= \spn(S_1 \cup S_2 \cup S_3 \cup S_4), \text{ and}  \\
     U \otimes V &= l_c(U \times V)/S\,.
  \end{align*}
Also define
  \[ \tau\colon U \times V \sto U \otimes V\colon (u,v) \mapsto [u \ast v]. \]
Then show that $U \otimes V$ and $\tau$ satisfy the conditions stated in definition~\ref{tpvs006def}.  \ns
\end{proof}

\begin{notn} It is conventional to write $u \otimes v$ for $\tau\bigl(\,(u,v)\,\bigr) = [u \ast v]$. Tensors of the form $u \otimes v$ are called
 \index{tensor!elementary}%
 \index{elementary tensor}%
\df{elementary tensors} (or
 \index{tensor!decomposable}%
 \index{decomposable!tensor}%
\df{decomposable tensors} or
 \index{tensor!homogeneous}%
 \index{homogeneous!tensor}%
\df{homogeneous tensors}). Keep in mind that
\end{notn}

\begin{prop} Let $u$ and $v$ be elements of finite dimensional vector spaces $U$ and $V$, respectively. If $u \otimes v = \vc 0$, then either
$u = \vc 0$ or $v = \vc 0$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Argue by contradiction. Suppose there exist $u' \ne \vc 0$ in $U$ and $v' \ne \vc 0$ in V such that
$u' \otimes v' = \vc 0$.  Use propositon~\ref{dual004} to choose linear functionals $f \in U^*$ and $g \in V^*$ such that $f(u') = g(v') = 1$.
Consider the map $B$ defined on $U \times V$ by $B(u,v) = f(u)g(v)$.  \ns
\end{proof}

\begin{cau} One needs to exercise some care in dealing with elementary tensors: keep in mind that
   \begin{enumerate}
     \item not every member of $U \otimes V$ is of the form $u \otimes v$;
     \item the representation of a tensor as an elementary tensor, even when it is possible, fails to be unique; and
     \item the family of elementary tensors (although it spans $U \otimes V$) is by no means linearly independent.
   \end{enumerate}
We do, however, have the following useful result.
\end{cau}

\begin{prop} Let $e^1, \dots, e^n$ be linearly independent vectors in a vector space $U$ and $v^1, \dots, v^n$ be arbitrary vectors in a vector space~$V$.
Then $\sum_{j=1}^n e^j \otimes v^j = \vc 0$ if and only if $v^k = \vc 0$ for each $k \in \N_n$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Extend $\{e^1, \dots, e^n\}$ to a basis $E$ for~$U$.  Fix $k \in \N_n$.  Let $\phi \in V^*$.  Consider the scalar
valued function $B$ defined on $U \times V$ by $B(u,v) = \bigl(e^k\bigr)^*(u)\phi(v)$, where $\bigl(e^k\bigr)^*$ is as defined in proposition~\ref{dual023}.
Prove that $\phi(v^k) = 0$.  \ns
\end{proof}

\begin{prop} If $\{e^i\}_{i=1}^m$ and $\{f^j\}_{j=1}^n$ are bases for the finite dimensional vector spaces $U$ and $V$, respectively, then the family
$\{e^i \otimes f^j\}_{i=1,\,\,j=1}^{\;m\;\;\;\;\;n}$ is a basis for $U \otimes V$.
\end{prop}

\begin{cor} If $U$ and $V$ are finite dimensional vector spaces, then so is $U \otimes V$ and
   \[ \dim(U \otimes V) = (\dim U)(\dim V). \]
\end{cor}

\begin{prop} Let $U$ and $V$ be finite dimensional vector spaces and $\{f^j\}_{j=1}^n$ be a basis for~$V$. Then for every element $t \in U \otimes V$ there
exist unique vectors $u^1$, \dots $u^n \in U$ such that
     \[ t = \sum_{j=1}^n u^j \otimes f^j. \]
\end{prop}

\begin{prop} If $U$ and $V$ are vector spaces, then
   \[ U \otimes V \cong V \otimes U. \]
\end{prop}

\begin{prop} If $V$ is a vector space over a field $\F$, then
   \[ V \otimes \F \cong V \cong \F \otimes V. \]
\end{prop}

\begin{prop} Let $U$, $V$, and $W$ be vector spaces. For every vector space $X$ and every trilinear map $k\colon U \times V \times W \sto X$ there exists a
unique linear map $\wt k\colon U \otimes (V \otimes W) \colon \sto X$ such that
   \[ \wt k\bigl(u \otimes (v \otimes w)\bigr) = k(u,v,w) \]
for all $u \in U$, $v \in V$, and $w \in W$.
\end{prop}

\begin{prop} If $U$, $V$, and $W$ are vector spaces, then
  \[ U \otimes (V \otimes W) \cong (U \otimes V) \otimes W. \]
\end{prop}







\begin{prop} Let $U$ and $V$ be vector spaces.  Then there exists an injective linear map from $U \otimes V^\ast$ into $\ofml L(U,V)$.
If $U$ and $V$ are finite dimensional, then $U \otimes V^\ast \cong \ofml L(V,U)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}]  Consider the map $ B\colon U \times V^\ast \sto \ofml L(V,U) \colon (u,g) \mapsto B(u,g)$ where
     \[ B(u,g)(v) = g(v)\,u \]
for every $v \in V$.   \ns
\end{proof}









\begin{prop} If $U$, $V$, and $W$ are vector spaces, then
   \[ U \otimes (V \oplus W) \cong (U \otimes V) \oplus (U \otimes W). \]
\end{prop}

\begin{prop} If $U$ and $V$ are finite dimensional vector spaces, then
   \[ (U \otimes V)^* \cong U^* \otimes V^*. \]
\end{prop}

\begin{prop} If $U$, $V$, and $W$ are finite dimensional vector spaces, then
  \[ \ofml L(U \otimes V,W) \cong \ofml L(U, \ofml L(V,W)) \cong \ofml L^2(U,V;W). \]
\end{prop}

\begin{prop} Let $u^1$, $u^2 \in U$ and $v^1$, $v^2 \in V$ where $U$ and $V$ are finite dimensional vector spaces. If $u^1 \otimes v^1 = u^2 \otimes v^2
\ne \vc 0$, then $u^2 = \alpha u^1$ and $v^2 = \beta v^1$ where $\alpha \beta = 1$.
\end{prop}






















\section{Tensor Products of Linear Maps}
\begin{defn}\label{tpop010def} Let $S\colon U \sto W$ and $T\colon V \sto X$ be linear maps between vector spaces. We define the
 \index{tensor!product!of linear maps}%
 \index{linear!transformations!tensor products of}%
 \index{<binaryoperation@$S \otimes T$ (tensor products of linear maps)}%
\df{tensor product} of the linear maps $S$ and $T$ by
   \[ S \otimes T \colon U \otimes V \sto W \otimes X \colon u \otimes v \mapsto S(u) \otimes T(v)\,. \]
\end{defn}

\begin{exer} Definition~\ref{tpop010def} defines the tensor product $S \otimes T$ of two maps only for homogeneous elements of $U \otimes V$.  Explain exactly what
is needed to convince ourselves that $S \otimes T$ is well defined on all of~$U \otimes V$.  Then prove that $S \otimes T$ is a linear map.
\end{exer}

\begin{exer} Some authors hesitate to use the notation $S \otimes T$ for the mapping defined in~\ref{tpop010def} on the (very reasonable) grounds that
$S \otimes T$ already has a meaning; it is a member of the vector space $\ofml L(U,W) \otimes \ofml L(V,X)$.  Discuss this problem and explain, in particular, why
the use of the notation $S \otimes T$ in~\ref{tpop010def} is not altogether unreasonable.
\end{exer}

\begin{prop} Suppose that $R$, $S \in \ofml L(U,W)$ and that $T \in \ofml L(V,X)$ where $U$, $V$, $W$, and $X$ are finite dimensional vector spaces.  Then
  \[ (R + S) \otimes T = R \otimes T + S \otimes T. \]
\end{prop}

\begin{prop} Suppose that $R \in \ofml L(U,W)$ and that $S$, $T \in \ofml L(V,X)$ where
$U$, $V$, $W$, and $X$ are finite dimensional vector spaces. Then
  \[ R  \otimes (S + T) = R \otimes S + R \otimes T. \]
\end{prop}

\begin{prop} Suppose that $S \in \ofml L(U,W)$ and that $T \in \ofml L(V,X)$ where $U$, $V$, $W$, and $X$ are finite dimensional vector spaces. Then for all
scalars $\alpha$ and $\beta$
  \[ (\alpha S)  \otimes (\beta T) = \alpha \beta (S \otimes T). \]
\end{prop}

\begin{prop} Suppose that $Q \in \ofml L(U,W)$, $R \in \ofml L(V,X)$, $S \in \ofml L(W,Y)$, and that $T \in \ofml L(X,Z)$ where $U$, $V$, $W$, $X$, $Y$, and $Z$
are finite dimensional vector spaces. Then
  \[ (S \otimes T)(Q \otimes R) = SQ \otimes TR. \]
\end{prop}

\begin{prop} If $U$ and $V$ are finite dimensional vector spaces, then
   \[ I_U \otimes I_V = I_{U \otimes V}. \]
\end{prop}

\begin{prop} Suppose that $S \in \ofml L(U,W)$ and that $T \in \ofml L(V,X)$ where $U$, $V$, $W$, and $X$ are finite dimensional vector spaces. If $S$ and $T$ are
invertible, then so is $S \otimes T$ and
  \[ (S \otimes T)^{-1} = S^{-1} \otimes T^{-1}. \]
\end{prop}

\begin{prop} Suppose that $S \in \ofml L(U,W)$ and that $T \in \ofml L(V,X)$ where $U$, $V$, $W$, and $X$ are finite dimensional vector spaces. If $S \otimes T =
0$, then either $S = 0$ or $T = 0$.
\end{prop}

\begin{prop} Suppose that $S \in \ofml L(U,W)$ and that $T \in \ofml L(V,X)$ where $U$, $V$, $W$, and $X$ are finite dimensional vector spaces. Then
  \[ \ran(S \otimes T) = \ran S \otimes \ran T. \]
\end{prop}

\begin{prop} Suppose that $S \in \ofml L(U,W)$ and that $T \in \ofml L(V,X)$ where $U$, $V$, $W$, and $X$ are finite dimensional vector spaces. Then
   \[ \ker(S \otimes T) = \ker S \otimes V\, +\, U \otimes \ker T. \]
\end{prop}

%\begin{prop} Suppose that $S \in \ofml L(U,W)$ and that $T \in \ofml L(V,X)$ where $U$, $V$, $W$, and $X$ are finite dimensional vector spaces. Then
%   \[ (S \otimes T)^{\,t} = S^{\,t} \otimes T^{\,t}. \]
%\end{prop}













\endinput
\chapter*{PREFACE}

This set of notes is an activity-oriented introduction to the study of linear and
multilinear algebra. The great majority of the results in beginning linear and
multilinear are straightforward and can be verified by the thoughtful student. Indeed,
that is the main point of these notes---to convince the beginner that the subject is
accessible.  In the material that follows there are numerous indicators that suggest
activity on the part of the reader: words such as ``proposition'', ``example'',
``exercise'', and ``corollary'', if not followed by a proof or a reference to a proof,
are invitations to verify the assertions made. When the proof of a theorem appears to me
to be too difficult for the average student to (re)invent and I have no improvements to
offer to the standard proofs, I provide references to standard treatments. These notes
were written for a 2-term course in linear/multilinear algebra for seniors and first year
graduate students at Portland State University.

The prerequisites for working through this material are quite modest. Elementary
properties of the real number system, the arithmetic of matrices, ability to solve
systems of linear equations, and the ability to evaluate the determinant of a square
matrix are assumed.  A few examples and exercises depend on differentiation and/or
integration of real valued functions, but no particular skill with either is required.

There are of course a number of advantages and disadvantages in consigning a document to
electronic life. One advantage is the rapidity with which links implement
cross-references.  Hunting about in a book for \emph{lemma 3.14.23} can be time-consuming
(especially when an author engages in the entirely logical but utterly infuriating
practice of numbering lemmas, propositions, theorems, corollaries, and so on,
separately).  A perhaps more substantial advantage is the ability to correct errors, add
missing bits, clarify opaque arguments, and remedy infelicities of style in a timely
fashion.  The correlative disadvantage is that a reader returning to the web page after a
short time may find everything (pages, definitions, theorems, sections) numbered
differently.  (\LaTeX is an amazing tool.) I will change the date on the title page to
inform the reader of the date of the last nontrivial update (that is, one that affects
numbers or cross-references).

The most serious disadvantage of electronic life is impermanence.  In most cases when a
web page vanishes so, for all practical purposes, does the information it contains.  For
this reason (and the fact that I want this material to be freely available to anyone who
wants it) I am making use of a ``Share Alike'' license from \emph{Creative Commons}. It
is my hope that anyone who finds this material useful will correct what is wrong, add
what is missing, and improve what is clumsy.  For more information on creative commons
licenses see \url{http://creativecommons.org/}.  Concerning the text itself, please send
corrections, suggestions, complaints, and all other comments to the author at
   \[ \textrm{erdman@pdx.edu} \]


\vfill\eject




\endinput
\chapter{The Spectral Theorem for Vector Spaces}
\section{Projections}

Much of mathematical research consists analyzing complex objects by writing them as a
combination of simpler objects.  In the case of vector space operators the simpler
objects, the fundamental building blocks, are \emph{projection operators}.

\begin{defn}  Let $V$ be a vector space.  An operator $E \in \ofml
L(V)$ is a
 \index{projection!operator}%
 \index{operator!projection}%
\df{projection operator} if it is
 \index{idempotent}%
\df{idempotent}; that is, if $E^2 = E$.
\end{defn}

\begin{prop} If $E$ is a projection operator on a vector space $V$, then
   \[ V = \ran E \oplus \ker E. \]
\end{prop}

\begin{prop} Let $V$ be a vector space and $E$, $F \in \ofml L(V)$.  If $E + F = I_V$
and $EF = \vc 0$, then $E$ and $F$ are projection operators and $V = \ran E \oplus \ran
 F$.
\end{prop}

\begin{prop}\label{proj_003_prop} Let $V$ be a vector space and $E_1$, \dots, $E_n \in \ofml L(V)$. If
$\sum_{k=1}^nE_k = I_V$ and $E_iE_j = \vc 0$ whenever $i \neq j$, then each $E_k$ is a
projection operator and $V = \bigoplus_{k=1}^n \ran{E_k}$.
\end{prop}

\begin{prop} If $E$ is a projection operator on a vector space~$V$, then $\ran E =
\{x \in V\colon Ex = x\}$.
\end{prop}

\begin{prop} Let $E$ and $F$ be projection operators on a vector space~$V$. Then $E + F
= I_V$ if and only if $EF = FE = \vc 0$ and $\ker E = \ran F$.
\end{prop}

\begin{defn}\label{PO007def} Let $V$ be a vector space and suppose that $V = M \oplus N$.
We know from an earlier theorem~\ref{subspace008} that for each $v \in V$ there exist
unique vectors $m \in M$ and $n \in N$ such that $v = m + n$.  Define a function $\sbsb
E{NM}\colon V \sto V$ by $\sbsb E{NM}v = m$. The function $\sbsb E{NM}$ is called the
 \index{projection!along one subspace onto another}%
 \index{EMN@$\sbsb E{NM}$ (projection along $N$ onto $M$)}%
\df{projection of} $V$ \df{along} $N$ \df{onto}~$M$.  (This terminology is, of course,
optimistic.  We must \emph{prove} that $\sbsb E{NM}$ is in fact a projection operator.)
\end{defn}

\begin{prop} If $M \oplus N$ is a direct sum decomposition of a vector space~$V$,
then the function $\sbsb E{NM}$ defined in~\ref{PO007def} is a projection operator whose
range is $M$ and whose kernel is~$N$.
\end{prop}

\begin{prop} If $M \oplus N$ is a direct sum decomposition of a vector space~$V$,
then $\sbsb E{NM} + \sbsb E{MN} = I_V$ and $\sbsb E{NM}\sbsb E{MN} = \vc 0$.
\end{prop}

\begin{prop} If $E$ is a projection operator on a vector space $V$, then there exist
$M$, $N \preccurlyeq V$ such that $E = \sbsb E{NM}$.
\end{prop}

\begin{exer} Let $M$ be the line $y = 2x$ and $N$ be the $y$-axis in $\R^2$. Find
$[\sbsb E{MN}]$ and $[\sbsb E{NM}]$.
\end{exer}

\begin{exer} Let $E$ be the projection of $\R^3$ onto the plane $3x - y + 2z = 0$
along the $z$-axis.  Find the matrix representation $[E]$ (of $E$ with respect to the
standard basis of~$\R^3$).
\end{exer}

\begin{exer} Let $F$ be the projection of $\R^3$ onto the $z$-axis along the plane
$3x - y + 2z = 0$.  Where does $F$ take the point $(4,5,1)$?
\end{exer}

\begin{exer} Let $P$ be the plane in $\R^3$ whose equation is $x + 2y - z = 0$ and $L$ be
the line whose equations are $\D \frac x3 = y = \frac z2$.  Let $E$ be the projection of
$\R^3$ along $L$ onto~$P$ and $F$ be the projection of $\R^3$ along $P$ onto~$L$.  Then
  \[ [E] = \frac13 \begin{bmatrix}
                      a & -b    & c \\
                     -d & d     & d \\
                   a-2d & -b+2d & c+2d
                \end{bmatrix}
\qquad \text{ and } \qquad [F] = \frac13 \begin{bmatrix}
                     3d & 3e & -3d \\
                      d &  e &  -d \\
                     2d & 2e & -2d
               \end{bmatrix} \]
where $a$ = \underbar{\hphantom{OO}}~, $b$ = \underbar{\hphantom{OO}}~, $c$ =
\underbar{\hphantom{OO}}~, $d$ = \underbar{\hphantom{OO}}~, and $e$ =
\underbar{\hphantom{OO}}~.
\end{exer}

\begin{exer}\label{PO011} Let $T\colon V \sto W$ be linear and $S\colon W \sto V$ a left
inverse for~$T$. Then
 \begin{enumerate}
  \item[(a)] $W = \ran T \oplus \ker S$, and
  \item[(b)] $TS$ is the projection along $\ker S$ onto $\ran T$.
 \end{enumerate}
\end{exer}

% \begin{exer} In light of exercise~\ref{PO011} reconsider exercise~\ref{lin_transf013z}.
%\end{exer}














\section{Algebras}
\begin{defn} Let $(A,+,M)$ be a vector space over a field $\F$ which is equipped with
another binary operation $\cdot \colon A \times A \sto A \colon (a,b) \mapsto ab$ in such
a way that $(A,+,\cdot\,)$ is a ring. If additionally the equations
  \begin{equation}\label{eq_def_algebra}
     \alpha (ab) = (\alpha a)b = a(\alpha b)
  \end{equation}
hold for all $a$, $b \in A$ and $\alpha \in \F$, then $(A,+,M,\,\cdot\,)$ is an
 \index{algebra}%
\df{algebra} over the field~$\F$ (sometimes referred to as a \df{linear associative
algebra}).  We abuse notation in the usual way by writing such things as, ``Let $A$ be an
algebra.'' We say that an algebra $A$ is
 \index{unital!algebra}%
 \index{algebra!unital}%
\df{unital} if its underlying ring $(A,+,\cdot\,)$ is.  And it is
 \index{commutative!algebra}%
 \index{algebra!commutative}%
\df{commutative} if its ring is.
\end{defn}

\begin{exam} A field may be regarded as an algebra over itself.
\end{exam}

\begin{exam}\label{alg001} If $S$ is a nonempty set, then the vector space $\fml F(S,\F)$
(see example~\ref{vector_space02}) is a commutative unital algebra under
 \index{pointwise!multiplication}%
 \index{multiplication!pointwise}%
 \index{f(s)@$\fml F(S)$!as a commutative unital algebra}%
\df{pointwise multiplication}, which is defined for all $f$, $g \in \fml F(S,\F)$ by
  \[ (f \cdot g)(s) = f(s) \cdot g(s) \]
for all $s \in S$. The constant function $\vc 1$ (that is, the function whose value at
each $s \in S$ is~$1$) is the multiplicative identity.
\end{exam}

\begin{exam}\label{alg002} If $V$ is a vector space, then the set $\ofml L(V)$
 \index{L@$\ofml L(V)$!as a unital algebra}%
of linear operators on $V$ is a unital algebra under pointwise addition, pointwise scalar
multiplication, and composition.
\end{exam}

\begin{notn} In the following material we make the notational convention that if $B$
and $C$ are subsets of (a ring or) an algebra $A$, then $BC$ denotes the set of all sums
of
 \index{<binop@$BC$ (sums of products of elements in $B$ and ~$C$)}%
 \index{conventions!$BC$ denotes sums of products of elements in $B$ and ~$C$}%
products of elements in $B$ and~$C$.  That is
   \[ BC := \{b_1c_1 + \cdots + b_nc_n\colon \text{$n \in \N$; $b_1$, \dots, $b_n \in B$;
                 and $c_1$, \dots, $c_n \in C$}\}. \]
And, of course, if $b \in A$, then $bC = \{b\}C$.
\end{notn}

\begin{defn} A map $f\colon A \sto B$ between algebras is an
 \index{homomorphism!of algebras}%
 \index{algebra!homomorphism}%
\df{(algebra) homomorphism} if it is a linear map between $A$ and $B$ as vector spaces which preserves multiplication (in the
sense of equation~\eqref{rings015ii}.  In other words, an algebra homomorphism is a linear ring homomorphism. It is a
 \index{unital!algebra homomorphism}%
 \index{algebra!homomorphism!unital}%
 \index{homomorphism!of algebras!unital}%
\df{unital (algebra) homomorphism} if it preserves identities (as in~\eqref{rings015iii}). The
 \index{kernel!of an algebra homomorphism}%
\df{kernel} of an algebra homomorphism $f\colon A \sto B$ is, of course, $\{a \in A
\colon f(a) = \vc 0\}$.

If $f^{-1}$ exists and is also an algebra homomorphism, then $f$ is an
 \index{isomorphism!of algebras}%
\df{isomorphism} from $A$ to $B$.  If an isomorphism from $A$ to $B$ exists, then $A$ and
$B$ are
 \index{isomorphic}%
\df{isomorphic}.
\end{defn}

Here are three essentially obvious facts about algebra homomorphisms.

\begin{prop}\label{alg6824} Every bijective algebra (or ring) homomorphism
 \index{bijective morphisms!are invertible!in $\cat{ALG}$}%
 \index{algebras@$\cat{ALG}$!bijective morphisms in}%
is an isomorphism.
\end{prop}

\begin{prop} If $f\colon A \sto B$ is an isomorphism between algebras (or rings) and $A$ is
unital, then so is $B$ and $f$ is a unital homomorphism.
\end{prop}

\begin{prop}\label{alg6831} Let $A$, $B$, and $C$ be algebras (or rings). If $f\colon A \sto B$
and $g\colon B \sto C$ are homomorphisms, so is $gf\colon A \sto C$.  (As is the case
with
 \index{conventions!notation for composition!of algebra homomorphisms}%
group homomorphism and linear maps, $gf$ denotes the composite function $g \circ f$.) If
$f$ and $g$ are unital, so is~$gf$.
\end{prop}

Here is an example of another important algebra.

\begin{exam}\label{alg003} We have seen in example~\ref{vector_space02c} that the set
$\mathbf M_n$ of $n \times n$ matrices of real numbers is a vector space. If $a = \bigl[
a_{ij}\bigr]$ and $b = \bigl[ b_{kl}\bigr]$ are $n \times n$ matrices of real numbers,
then the \df{product} of $a$ and $b$ is the $n \times n$ matrix $c = ab$ whose entry in
the $i^{\text{th}}$ row and $k^{\text{th}}$ column is $c_{ik} = \sum_{j=1}^n
a_{ij}b_{jk}$\,.
 \index{M@$\mathbf M_n$!as a unital algebra}%
This definition makes $\mathbf M_n$ into a unital algebra.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Proving associativity of matrix multiplication can
be something of a nuisance if one charges ahead without thinking.  As an alternative to
brute calculation look at exercise~\ref{inv_lin_maps007}. \ns
\end{proof}


\begin{defn} A subset of an algebra $A$ which is closed under the operations of addition,
multiplication, and scalar multiplication is a
 \index{subalgebra}%
\df{subalgebra} of~$A$.  If $A$ is a unital algebra and $B$ is a subalgebra of $A$ which
contains the multiplicative identity of $A$, then $B$ is a
 \index{unital!subalgebra}%
 \index{subalgebra!unital}%
\df{unital subalgebra} of~$A$.
\end{defn}

\begin{cau} Be very careful with the preceding definition. It is possible for $B$ to be a
subalgebra of an algebra $A$ and to be a unital algebra but still not be a unital
subalgebra of~$A$!  The definition requires that for $B$ to be a unital subalgebra of $A$
the identity of $B$ must be the same as the identity of~$A$. \emph{Example:} Under
pointwise operations $A = \R^2$ is a unital algebra. The set $B = \{(x,0)\colon x \in
\R\}$ is a subalgebra of~$A$. And certainly $B$ is unital (the element $(1,0)$ is the
multiplicative identity of~$B$). But $B$ is \emph{not} a unital subalgebra of $A$ because
it does not contain the multiplicative identity $(1,1)$ of~$A$.
\end{cau}

\begin{exam}\label{alg011} Let $S$ be a nonempty set. The family $\fml B(S)$ of all
bounded real valued functions on $S$ is a
 \index{bounded@$\fml B(S)$!as a unital algebra}%
unital subalgebra of the algebra $\fml F(S)$ of all real valued functions on~$S$.
\end{exam}

\begin{defn} A
 \index{left!ideal}%
 \index{ideal!left}%
\df{left ideal} in an algebra $A$ is a vector subspace $J$ of $A$ such that $AJ \subseteq
J$. (For
 \index{right!ideal}%
 \index{ideal!right}%
\df{right ideals}, of course, we require $JA \subseteq J$.)  We say that $J$ is an
 \index{ideal!in an algebra}%
 \index{conventions!ideals are two-sided}%
\df{ideal} if it is a two-sided ideal; that is, both a left and a right ideal.  A
 \index{proper!ideal}%
 \index{ideal!proper}%
\df{proper} ideal is an ideal which is a proper subset of~$A$.

The ideals $\{0\}$ and $A$ are often referred to as the
 \index{trivial!ideal}%
 \index{ideal!trivial}%
\df{trivial ideals} of~$A$. The algebra $A$ is
 \index{simple!algebra}%
 \index{algebra!simple}%
\df{simple} if it has no nontrivial ideals.
\end{defn}

\begin{exam} If $\phi\colon A \sto B$ is an algebra homomorphism, then the kernel of
$\phi$ is an ideal in $A$ and the range of $\phi$ is a subalgebra of~$B$.
\end{exam}

\begin{defn} An element $a$ of a unital algebra $A$ is
 \index{invertible!element of an algebra}%
\df{invertible} if there exists an element $a^{-1}\in A$ such that $aa^{-1} = a^{-1}a =
\vc 1_A$.
\end{defn}

\begin{prop} If $a$ and $b$ are invertible elements in a unital algebra, then $ab$ is also invertible and $(ab)^{-1} = b^{-1}a^{-1}$.
\end{prop}

\begin{prop} No invertible element in a unital algebra can belong to a proper ideal.
\end{prop}

\begin{prop}\label{alg553} Let $a$ be an element of a commutative algebra~$A$.
Then $aA$ is an ideal in~$A$. If $A$ is unital and $a$ is not invertible, then $aA$ is a
proper ideal in~$A$.
\end{prop}

\begin{exam} Let $\sfml J$ be a family of ideals in an algebra $A$.  Then the
$\bigcap \sfml J$ is an ideal in~$A$.
\end{exam}

\begin{defn}\label{alg554} Let $a$ be an element of an algebra~$A$. Then the
intersection of all the ideals of $A$ which contain $a$ is the
 \index{principal ideal}%
 \index{ideal!principal}%
\df{principal ideal} generated by~$a$.
\end{defn}

\begin{prop}\label{alg555} Let $a$ be an element of a commutative algebra~$A$. The ideal
$aA$ in proposition~\ref{alg553} is the principal ideal generated by~$a$.
\end{prop}















\section{Quotients and Unitizations}
\begin{defn}\label{quo15441} Let $J$ be a proper ideal in an algebra~$A$.  Define an
equivalence relation $\sim$ on $A$ by
  \[ a \sim b \qquad \text{if and only if } \qquad b-a \in J. \]
For each $a \in A$ let $[a]$ be the equivalence class containing~$a$.  Let $A/J$ be the
set of all equivalence classes of elements of~$A$.  For $[a]$ and $[b]$ in $A/J$ define
  \[ [a] + [b] := [a+b] \qquad \text{ and } \qquad [a][b] := [ab] \]
and for $\alpha \in \C$ and $[a] \in A/J$ define
  \[\alpha[a] := [\alpha a] \,. \]
Under these operations $A/J$ becomes an algebra.  It is the
 \index{quotient!algebra}%
 \index{algebra!quotient}%
\df{quotient algebra} of $A$ by~$J$.  The notation $A/J$ is usually read ``$A$ mod~$J$''.
The surjective algebra homomorphism
  \[ \pi\colon A \sto A/J\colon a \mapsto [a]\]
is called the
 \index{quotient map, the}%
\df{quotient map}.
\end{defn}

\begin{exer} Verify the assertions made in the preceding definition.
\end{exer}

\begin{defn}Let $A$ be an algebra over a field~$\F$.  The
 \index{unitization}%
\df{unitization} of~$A$ is the unital algebra $\tilde A = A \times \F$ in which addition
and scalar multiplication are defined pointwise and multiplication is defined by
 \[ (a,\lambda)\cdot(b,\mu)
        = (ab + \mu a + \lambda b, \lambda\mu).\]
\end{defn}

\begin{exer} Prove that the unitization $\tilde A$ of an algebra $A$ is in fact a unital
algebra with $(0,1)$ as its identity. Prove also that $A$ is (isomorphic to) a subalgebra
of $\tilde A$ with codimension~$1$.
\end{exer}


















\section{The Spectrum}
\begin{defn} Let $a$ be an element of a unital algebra~$A$ over a field~$\F$.  The
 \index{spectrum}%
\df{spectrum} of $a$, denoted by
 \index{sigma@$\sigma(a)$, $\sigma_A(a)$ (the spectrum of~$a$)}%
$\sigma_A(a)$ or just $\sigma(a)$, is the set of all $\lambda \in \F$ such that $a -
\lambda\vc 1$ is not invertible.

If the algebra $A$ is not unital we will still speak of
 \index{conventions!spectrum of elements of nonunital algebras}%
\emph{the spectrum} of the element $a$ with the understanding that we are speaking of the
spectrum of $a$ in the unitization of~$A$.
\end{defn}

\begin{exam} If $z$ is an element of the algebra $\C$ of complex numbers, then
$\sigma(z) = \{z\}$.
\end{exam}

\begin{exam} Let $f$ be an element of the algebra $\fml C([a,b])$ of continuous
complex valued functions on the interval $[a,b]$. Then the spectrum of~$f$ is its range.
\end{exam}

\begin{exam} The operator which rotates (the real vector space) $\R^2$ by $\frac{\pi}2$
radians has empty spectrum.
\end{exam}

For the next example you may assume that a square matrix of real or complex numbers is
invertible if and only if its determinant is nonzero.

 \begin{exam} The family $\mathbf M_3(\C)$ of $3 \times 3$ matrices of complex numbers
is a unital algebra under the usual matrix operations. The spectrum of the matrix
$\begin{bmatrix} 5 & -6 & -6 \\ -1 & 4 & 2 \\ 3 & -6 & -4 \end{bmatrix}$ is $\{1,2\}$.
\end{exam}

\begin{exam} Let $a$ be an element of a unital complex algebra such that $a^2 = \vc 1$.
Then either
   \begin{enumerate}
    \item[(i)] $a = \vc 1$, in which case $\sigma(a) = \{1\}$, or
    \item[(ii)] $a = -\vc 1$, in which case $\sigma(a) = \{-1\}$, or
    \item[(iii)] $\sigma(a) = \{-1,1\}$.
   \end{enumerate}
\end{exam}

\begin{proof}[\emph{Hint for proof}] In (iii) to prove $\sigma(a) \subseteq \{-1,1\}$,
consider $\D\frac1{1 - \lambda^2}(a + \lambda \vc 1)$. \ns
\end{proof}

\begin{defn} An element $a$ of an algebra is
 \index{idempotent}%
\df{idempotent} if $a^2 = a$.
\end{defn}

\begin{exam} Let $a$ be an idempotent element of a unital complex algebra. Then
either
   \begin{enumerate}
    \item[(i)] $a = \vc 1$, in which case $\sigma(a) = \{1\}$, or
    \item[(ii)] $a = \vc 0$, in which case $\sigma(a) = \{0\}$, or
    \item[(iii)] $\sigma(a) = \{0,1\}$.
   \end{enumerate}
\end{exam}

\begin{proof}[\emph{Hint for proof}] In (iii) to prove $\sigma(a) \subseteq \{0,1\}$,
consider $\D \frac1{\lambda - \lambda^2}\bigl(a + (\lambda - 1)\vc 1\bigr)$. \ns
\end{proof}

















\section{Polynomials}
\begin{notn} If $S$ is a set and $A$ is an algebra,
 \index{l@$l(S,A)$ (functions from $S$ into $A$)}%
$l(S,A)$ denotes the vector space of all functions from $S$ into~$A$ with pointwise
operations of addition and scalar multiplication, and
 \index{l@$l_c(S,A)$ (functions from $S$ into $A$ with finite support)}%
$l_c(S,A)$ denotes the subspace of functions with finite support.
\end{notn}

\begin{defn}\label{poly001def} Let $A$ be a unital commutative algebra.  On the vector
space $l(\Z^+,A)$ define a binary operation $\ast$ (often called
 \index{convolution}%
\df{convolution}) by $(f \ast g)_n = \sum\limits_{j+k=n} f_{\!j}\,g_{\,k} =
\sum\limits_{j=0}^n f_j\,g_{\,n-j}$ (where $f$, $g \in l(\Z^+,A)$ and $n \in \Z^+$.  An
element of $l(\Z^+,A)$ is a
 \index{formal power series}%
 \index{power series!formal}%
 \index{series!formal power}%
\df{formal power series} (with coefficients in~$A$) and an element of $l_c(\Z^+,A)$ is a
 \index{polynomial}%
\df{polynomial} (with coefficients in~$A$).  The reason for using the expression ``with \emph{coefficients} in~$A$'' may seem at first a bit
mysterious.  For an explanation look at example~\ref{exam_std_form_poly}.
\end{defn}

\begin{prop} If $A$ is a unital commutative algebra, then under the operations defined
in~\ref{poly001def} $l(\Z^+,A)$ is a unital commutative algebra (whose multiplicative
identity is the sequence $(\vc 1_A,0,0,0,\dots)$) and $l_c(\Z^+,A)$ is a unital
subalgebra of $l(\Z^+,A)$.
\end{prop}

\begin{prop} If $\phi\colon A \sto B$ is a unital algebra homomorphism between unital
commutative algebras, then the map
 \[ l(\Z^+,\phi)\colon l(\Z^+,A) \sto l(\Z^+,B)\colon f
                          \mapsto \bigl(\phi(f_n)\bigr)_{n=0}^\infty\]
is also a unital homomorphism of unital commutative algebras. The pair of maps $A \mapsto
l(\Z^+,A)$ and $\phi \mapsto l(\Z^+,\phi)$ is a covariant functor from the category of
unital commutative algebras and unital algebra homomorphisms to itself.
\end{prop}

\begin{rem} We regard the algebra $A$ as a subset of $l(\Z^+,A)$ by identifying the
element $a \in A$ with the element $(a,0,0,0,\dots) \in l(\Z^+,A)$.  Thus the map $a
\mapsto (a,0,0,0,\dots)$ becomes an inclusion map. (Technically speaking, of course, the
map $\psi\colon a \mapsto (a,0,0,0,\dots)$ is an injective unital homomorphism and $A
\cong \ran\psi$.)
\end{rem}

\begin{conv} In the algebra $l(\Z^+,A)$ we will normally write
 \index{conventions!convolution $a \ast b$ written as a product $ab$}%
$ab$ for $a \ast b$.
\end{conv}

\begin{defn} Let $A$ be a unital commutative algebra.  In the algebra
$l(\Z^+,A)$ of formal power series the special sequence $x = (0,\vc 1_A,0,0,0,\dots)$ is
called the
 \index{indeterminant}%
\df{indeterminant} of $l(\Z^+,A)$.  Notice that the sequence $x^2 = x\cdot x =
(0,0,1,0,0,0,\dots)$, the sequence $x^3 = x\cdot x \cdot x = (0,0,0,1,0,0,0,\dots)$, and
so on. For each $n \in \N$ the sequence $x^n = x\cdot x\cdot x \cdots x$ ($n$ factors)
has the property that its $n^{\text{th}}$ coordinate ${x^n}_n$ is $1$ while its
$k^{\text{th}}$ coordinate ${x^n}_k$ is $0$ whenever $k \ne n$. It is conventional to
take $x^0$ to be the multiplicative identity $(\vc 1_A,0,0,0,\dots)$ in $l(\Z^+,A)$.
\end{defn}

\begin{rem} The algebra $l(\Z^+,A)$ of formal power series with coefficients in a unital
commutative algebra $A$ is frequently denoted by
 \index{<bracketsax@$A\bigl[[x]\bigr]$ (algebra of formal power series)}%
$A\bigl[[x]\bigr]$ and the subalgebra $l_c(\Z^+,A)$ of polynomials is denoted
 \index{<bracketsax@$A[x]$ (algebra of polynomials)}%
by $A[x]$.

For many algebraists scalar multiplication is of little interest so $A$ is taken to be a
unital commutative ring, so that $A\bigl[[x]\bigr]$ is \emph{ring} of formal power series
(with coefficients in~$A$) and $A[x]$ is the \emph{polynomial ring} (with coefficients
in~$A$).  We will be primarily interested in the case where $A$ is a field~$\F$. Since a
field can be regarded as a one-dimensional vector space over itself, it is also an
algebra. Thus we will take
 \index{<bracketsfx@$\F[x]$ (polynomial algebra with coefficients in~$\F$)}%
 \index{polynomial!algebra}%
 \index{algebra!polynomial}%
$\F[x]$ to be the \emph{polynomial algebra} with coefficients in~$\F$; it has as its
basis $\{x^n\colon n= 0,1,2,\dots\}$.
\end{rem}

\begin{defn} A nonzero polynomial $p$, being an element of $l_c(\Z^+,A)$, has
finite support. So there exists $n_0 \in \Z^+$ such that $p_n = 0$ whenever $n
> n_0$.  The smallest such $n_0$ is the
 \index{degree!of a polynomial}%
 \index{polynomial!degree of a}%
 \index{deg@$\deg p$ (degree of a polynomial)}%
\df{degree} of the polynomial. We denote it by $\deg p$.  A polynomial of degree $0$
is a
 \index{constant!polynomial}%
 \index{polynomial!constant}%
\df{constant polynomial}.  The zero polynomial (the additive identity of $l(\Z^+,A)$) is a special case;
while it is also a constant polynomial some authors assign it no degree whatever, while others let its degree be $-\infty$.

If $p$ is a polynomial of degree $n$, then $p_n$ is the
 \index{leading coefficient}%
 \index{coefficient!leading}%
\df{leading coefficient} of~$p$.  A polynomial is
 \index{monic polynomial}%
 \index{polynomial!monic}%
\df{monic} if its leading coefficient is~$1$.
\end{defn}

\begin{exam}\label{exam_std_form_poly} Let $A$ be a unital commutative algebra. If $p$ is a nonzero
polynomial in $l_c(\Z^+,A)$, then
 \[ p = \sum_{k=0}^n \sbsb pk x^k \qquad\text{where $n = \deg p$}. \]
This is the
 \index{standard!form of a polynomial}%
 \index{polynomial!standard form of a}%
\df{standard form} of the polynomial~$p$.  Keep in mind that each coefficient $\sbsb pk$ belongs to the algebra~$A$.
Also notice that it does not really matter whether we write $p$ as
$\sum_{k=0}^n \sbsb pk x^k$ or as $\sum_{k=0}^\infty \sbsb pk x^k$; so frequently we write just $\sum\sbsb pk x^k$.
\end{exam}

\begin{rem} Recall that there is occasionally a slight ambiguity in notation for sets.  For example, if we consider
the (complex) solutions to an algebraic equation $E$ of degree $n$, we know that, \emph{counting multiplicities},
there are $n$ solutions to the equation.  So it is common practice to write, ``Let $\{x_1,x_2, \dots, x_n\}$ be the
set of solutions to~$E$.''  Notice that in this context there may be repeated elements of the set.  The cardinality
of the set may be strictly less than~$n$.  However, when we encounter the expression, ``Let $\{x_1,x_2, \dots, x_n\}$
be a set of \dots,'' it is usually the intention of the author that the elements are distinct, that the cardinality
of the set is~$n$.

A similar ambiguity arises in polynomial notation.  If, for example, $p = \sum_{k=0}^n \sbsb pk x^k$ and $q =
\sum_{k=0}^n \sbsb qk x^k$ are both polynomials of degree $n$, we ordinarily write their sum as $p + q =
\sum_{k=0}^n (\sbsb pk + \sbsb qk)x^k$ even though the resulting sum may very well have degree strictly less than~$n$.
On the other hand when one sees, ``Consider a polynomial $p = \sum_{k=0}^n \sbsb pk x^k$ such that \dots,'' it is usually
intended that $p$ have degree~$n$; that is, that $p$ is written in standard form.
\end{rem}

\begin{prop} If $p$ and $q$ are polynomials with coefficients in a unital commutative
algebra $A$, then
 \begin{enumerate}
  \item[(i)] $\deg(p + q) \le \max\{\deg p, \deg q\}$, and
  \item[(ii)] $\deg(pq) \le \deg p + \deg q$.
 \end{enumerate}
If $A$ is a field, then equality holds in (ii).
\end{prop}

\begin{exam} If $A$ is a unital commutative algebra, then so is $l(A,A)$ under pointwise
operations of addition, multiplication, and scalar multiplication.
\end{exam}

\begin{defn}\label{poly011def} Let $A$ be a unital commutative algebra over a field~$\F$.  For
each polynomial $p = \sum_{k=0}^n \sbsb pk x^k$ with coefficients in $\F$ define
 \[ \widetilde p \colon A \sto A\colon a \mapsto \sum_{k=0}^n \sbsb pk a^k.\]
Then $\widetilde p$ is the
 \index{polynomial!function}%
\df{polynomial function} on $A$ determined by the polynomial~$p$. Also for fixed $a \in A$ define
 \[ \tau_a\colon \F[x] \sto A\colon p \mapsto \widetilde p(a).\]
The mapping $\tau_a$ is the
 \index{polynomial!functional calculus}%
 \index{functional!calculus!polynomial}%
 \index{calculus!polynomial functional}%
\df{polynomial functional calculus} determined by the element~$a$.
\end{defn}

It is important to distinguish between the concepts of polynomials with coefficients in
an algebra and polynomial functions.  Also important is the distinction between the
indeterminant $x$ in $l(\Z^+,A)$ and $x$ used as a variable for a polynomial function. (See~\ref{poly027exam}.)

\begin{exer} Let $A$ be a unital commutative algebra over a field~$\F$. Then for each $a \in A$
the polynomial functional calculus $\tau_a \colon \F[x] \sto A$ defined in~\ref{poly011def}
is a unital algebra homomorphism.
\end{exer}

\begin{prop}\label{poly012prop} Let $A$ be a unital commutative algebra over a field~$\F$. The map
    \[ \Psi \colon \F[x] \sto l(A,A)\colon p \mapsto \widetilde p \]
is a unital algebra homomorphism.
\end{prop}

\begin{exer} Under the homomorphism  $\Psi$ (defined in~\ref{poly012prop}) what is the image of
the indeterminant~$x$? Under the homomorphism $\tau_a$ (defined in~\ref{poly011def}) what is the image
of the indeterminant~$x$?
\end{exer}

The following example is intended to illustrate the importance of distinguishing between polynomials and
polynomial functions.

\begin{exam}\label{poly027exam} Let $\F = \{0,1\}$ be the two-element field.  The polynomials $p = x + x^2 + x^3$
and $q = x$ in the polynomial algebra $\F[x]$ show that the homomorphism $\Psi$ (defined in~\ref{poly012prop}) need
not be injective.
\end{exam}

\begin{prop}\label{poly013prop} Let $A$ be a unital algebra of finite dimension~$m$ over a field~$\F$. For every
$a \in A$ there exists a polynomial $p \in \F[x]$ such that $1 \le \deg p \le m$ and $\widetilde p(a) = 0$.
\end{prop}





































\section{Minimal Polynomials}
\begin{defn} Let $V$ be a vector space over a field~$\F$ and $T \in \ofml L(V)$. A
nonzero polynomial $p \in \F[x]$ such that $\widetilde p(T) = 0$ is an
 \index{annihilating polynomial}%
 \index{polynomial!annihilating}%
\df{annihilating polynomial} for~$T$.  A monic polynomial of smallest degree that
annihilates $T$ is a
 \index{minimal!polynomial}%
 \index{polynomial!minimal}%
\df{minimal polynomial} for~$T$.
\end{defn}

\begin{prop} Let $V$ be a finite dimensional vector space over a field~$\F$. Then
every $T \in \ofml L(V)$
 \index{minimal!polynomial!existence of}%
 \index{polynomial!minimal!existence of}%
has a minimal polynomial.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use example~\ref{poly013prop}. \ns
\end{proof}

\begin{thm}[Division Algorithm]\label{thm_div_alg} Let $f$ and $d$ be polynomials with coefficients in a field~$\F$ with $d \ne 0$.
 \index{division algorithm}%
Then there exist unique polynomials $q$ and $r$ in $\F[x]$ such that
 \begin{enumerate}
  \item[(i)] $f = dq + r$ and
  \item[(ii)] $r = 0$ or $\deg r < \deg d$.
 \end{enumerate}
\end{thm}

\begin{proof}[\emph{Hint for proof}] Let $f = \sum_{j=0}^k f_j\,x^j$ and $d =
\sum_{j=0}^m d_j\,x^j$ be in standard form.  The case $k < m$ is trivial. For $k \ge m$
suppose the result to be true for all polynomials of degree strictly less than $k$. What
can you say about $\widehat f = f - p$ where $p = (f_k\,d_m^{-1})\,x^{k-m}d$? \ns
\end{proof}

\begin{notn} If $T$ is an operator on a finite dimensional vector space over a field
$\F$, we denote its
 \index{mt@$\sbsb mT$ (minimal polynomial for~$T$)}%
minimal polynomial in $\F[x]$ by~$\sbsb mT$.
\end{notn}

\begin{prop} Let $V$ be a finite dimensional vector space over a field $\F$ and $T
\in \ofml L(V)$. Then the minimal polynomial $\sbsb mT$ for $T$ is unique.
\end{prop}

\begin{prop}\label{prop_minpoly_nasc_invertible} An operator $T$ on a finite dimensional vector space is invertible if and
only if the constant term of its minimal polynomial is not zero.
\end{prop}

\begin{exer}  Explain how, for an invertible operator~$T$ on a finite dimensional vector
space, we can write its inverse as a polynomial in~$T$.
\end{exer}

\begin{defn} If $\F$ is a field and $p$, $p_1 \in \F[x]$, we say that $p_1$
 \index{divides}%
\df{divides} $p$ if there exists $q \in \F[x]$ such that $p = p_1q$.
\end{defn}

\begin{prop}\label{prop_min_poly_divides} Let $T$ be an operator on a finite dimensional vector space over a field~$\F$.
If $p \in \F[x]$ and $\widetilde p(T) = 0$, then $\sbsb mT$ divides~$p$.
\end{prop}

\begin{defn} A polynomial $p \in \F[x]$ is
 \index{reducible}%
 \index{polynomial!reducible}%
\df{reducible} over $\F$ if there exist polynomials $f$, $g \in \F[x]$ both of degree at
least one such that $p = fg$.  A polynomial $p$ of degree at least one is
 \index{irreducible}%
 \index{polynomial!irreducible}%
\df{irreducible} (or
 \index{prime polynomial}%
 \index{polynomial!prime}%
\df{prime}) over~$\F$ provided that whenever $p = fg$ with $f$, $g \in \F[x]$, then
either $f$ or $g$ is constant.  That is, a polynomial $p$ of degree at least one is
irreducible if and only if it is not reducible.
\end{defn}

\begin{exam} Let $T$ be the operator on the real vector space $\R^2$ whose matrix
representation (with respect to the standard basis) is $\begin{bmatrix} 0 & -1
\\ 1 & 0 \end{bmatrix}$.  Find the minimal polynomial $\sbsb mT$ of $T$ and show
that it is irreducible (over~$\R$).
\end{exam}

\begin{exam} Let $T$ be the operator on the complex vector space $\C^2$ whose matrix
representation (with respect to the standard basis) is $\begin{bmatrix} 0 & -1
\\ 1 & 0 \end{bmatrix}$.  Find the minimal polynomial $\sbsb mT$ of $T$ and show
that it is \emph{reducible} (over~$\C$).
\end{exam}

\begin{defn} A field $\F$ is
 \index{field!algebraically closed}%
 \index{algebraically closed}%
\df{algebraically closed} if every prime polynomial in $\F[x]$ has degree~$1$.
\end{defn}

\begin{exam} The field $\R$ of real numbers is not algebraically closed.
\end{exam}

\begin{prop} Let $\F$ be a field and $p$ be a polynomial of degree $m \ge 1$ in $\F[x]$.
If $J_p$ is the principal ideal generated by $p$ in $\F[x]$, then $\dim \F[x]/J_p = m$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] See proposition~\ref{alg555}.  Show that $B =
\{\,[x^k] \colon k = 0, 1, \dots, m-1\}$ is a basis for the vector space $\F[x]/J_p$. \ns
\end{proof}

\begin{cor} Let $T$ be an operator on a finite dimensional vector space $V$ over a
field $\F$, $\Phi\colon \F[x] \sto \ofml L(V)$ be its associated polynomial functional
calculus, and $J_{\sbsb mT}$ be the principal ideal generated by its minimal polynomial. Then the sequence
   \[ \vc 0 \to J_{\sbsb mT} \to \F[x] \to^\Phi \ran \Phi \to \vc 0 \]
is exact.  Furthermore, the dimension of the range of the functional calculus associated with the operator $T$
is the degree of its minimal polynomial.
\end{cor}

\begin{defn}\label{minpoly030def} Let $t_0$, $t_1$, \dots , $t_n$ be distinct elements
of a field~$\F$.  For $0 \le k \le n$ define $p_k \in \F[x]$ by
 \[ p_k = \prod_{\substack{j=0\\j \ne k}}^n \frac{x - t_j}{t_k - t_j}.\]
\end{defn}

\begin{prop}[Lagrange Interpolation Formula]\label{minpoly_LIF}  The polynomials
defined
 \index{Lagrange interpolation formula}%
 \index{interpolation!Lagrange}%
in~\ref{minpoly030def} form a basis for the vector space $V$ of all polynomials with
coefficients in $\F$ and degree less than or equal to~$n$ and that for each polynomial $q
\in V$
   \[ q = \sum_{k=0}^n q(t_k)p_k.\]
\end{prop}

\begin{exer} Use the \emph{Lagrange Interpolation Formula} to find the polynomial with
coefficients in $\R$ and degree no greater than $3$ whose values at $-1$, $0$, $1$, and
$2$ are, respectively, $-6$, $2$, $-2$, and~$6$.
\end{exer}

\begin{prop} Let $\F$ be a field and $p$, $q$, and $r$ be polynomials in $\F[x]$.
If $p$ is a prime in $\F[x]$ and $p$ divides $qr$, then $p$ divides $q$ or $p$
divides~$r$.
\end{prop}


\begin{prop}  Let $\F$ be a field.  Then every nonzero ideal in $\F[x]$ is principal.
\end{prop}

\begin{proof}[\emph{Hint for proof}] If $J$ is a nonzero ideal in $\F[x]$ consider the
principal ideal generated by any member of $J$ of smallest degree. \ns
\end{proof}

\begin{defn} Let $p_1$, \dots, $p_n$ be polynomials, not all zero, with coefficients in
a field~$\F$. A monic polynomial $d$ such that $d$ divides each $p_k$ ($k = 1, \dots, n$)
and such that any polynomial which divides each $p_k$ also divides~$d$ is the
 \index{greatest common divisor}%
 \index{divisor!greatest common}%
\df{greatest common divisor} of the~$p_k$'s.  The polynomials $p_k$ are
 \index{relatively prime}%
 \index{prime!relatively}%
\df{relatively prime} if their greatest common divisor is~$1$.
\end{defn}

\begin{prop} Any finite set of polynomials (not all zero), with coefficients in a
field $\F$ has a greatest common divisor.
\end{prop}

\begin{thm}[Unique Factorization] Let $\F$ be a field.
 \index{factorization}%
 \index{unique factorization theorem}%
A nonconstant monic polynomial in $\F[x]$ can be factored in exactly one way (except for
the order of the factors) as a product of monic primes in~$\F[x]$ .
\end{thm}

\begin{defn}
  Let $\F$ be a field and $p(x) \in \F[x]$. An element $r \in \F$ is a
 \index{root}%
\df{root} of $p(x)$ if $p(r) = 0$.
\end{defn}

\begin{prop} Let $\F$ be a field and $p(x) \in \F[x]$. Then $r$ is a root of $p(x)$ if and only if $x - r$ is a factor of~$p(x)$.
\end{prop}





















\section{Invariant Subspaces}
\begin{defn} Let $T$ be an operator on a vector space $V$.  A subspace $M$ of $V$ is
 \index{invariant subspace}%
 \index{subspace!invariant}%
\df{invariant under} $T$ (or
 \index{Tinvariant@$T$-invariant subspace}%
\df{$T$-invariant}) if $T^\sto(M) \subseteq M$. Since the subspaces $\{0\}$ and $V$ are invariant under any operator
on~$V$, they are called the
 \index{trivial!invariant subspaces}%
 \index{invariant subspace!trivial}%
\df{trivial} invariant subspaces.
\end{defn}

\begin{exer} Let $S$ be the operator on $\R^3$ whose matrix representation is
$\begin{bmatrix}   3 &   4 &   2 \\
                   0 &   1 &   2 \\
                   0 &   0 &   0 \end{bmatrix}$. Find three one dimensional subspaces
$U$, $V$, and $W$ of $\R^3$ which are invariant under~$S$.
\end{exer}

\begin{exer} Let $T$ be the operator on $\R^3$ whose matrix representation is
$\begin{bmatrix}   0 &   0 &   2 \\
                   0 &   2 &   0 \\
                   2 &   0 &   0 \end{bmatrix}$. Find a two dimensional subspace
$U$ of $\R^3$ which is invariant under~$T$.
\end{exer}

\begin{exer} Find infinitely many subspaces of the vector space of polynomial functions
on $\R$ which are invariant under the differentiation operator.
\end{exer}

\begin{defn} An operator $T$ on a vector space $V$ is
 \index{reducing subspaces}%
 \index{subspace!reducing}%
\df{reduced} by a pair $(M,N)$ of subspaces $M$ and $N$ of $V$ if $V = M \oplus N$ and both $M$ and $N$
are invariant under~$T$. In this case $M$ and $N$ are \df{reducing subspaces} for~$T$.
\end{defn}

\begin{exer} Let $T$ be the operator on $\R^3$ whose matrix representation is
\smash[b]{$\begin{bmatrix}   2 &   0 &   0 \\
                            -1 &   3 &   2 \\
                             1 &  -1 &   0 \end{bmatrix}$}. Find a plane and a line in $\R^3$
which reduce~$T$.
\end{exer}

\begin{prop} Let $M$ be a subspace of a vector space $V$ and $T \in \ofml L(V)$. If $M$ is invariant under $T$, then
$ETE = TE$ for every projection $E$ onto~$M$. And if $ETE = TE$ for some projection $E$ onto~$M$, then $M$ is
invariant under~$T$.
\end{prop}

\begin{prop} Suppose a vector space $V$ has the direct sum decomposition $V = M \oplus N$. Then an operator $T$ on $V$
is reduced by the pair $(M,N)$ if and only if $ET = TE$, where $E = \sbsb E{MN}$ is the projection along $M$ onto~$N$.
\end{prop}

\begin{prop} Suppose a finite dimensional vector space $V$ has the direct sum decomposition $V = M \oplus N$ and that
$E = \sbsb E{MN}$ is the projection along $M$ onto~$N$. Show that $E^\ast$ is the projection in $\ofml L(V^\ast)$ along
$N^\perp$ onto~$M^\perp$.
\end{prop}

\begin{prop} Let $M$ and $N$ be complementary subspaces of a vector space $V$ (that is, $V$ is the direct sum of $M$ and
$N$) and let $T$ be an operator on~$V$.  If $M$ is invariant under $T$, then $M^\perp$ is invariant under~$T^\ast$
and if $T$ is reduced by the pair $(M,N)$, then $T^\ast$ is reduced by the pair $(M^\perp,N^\perp)$.
\end{prop}



















\section{Burnside's Theorem}
\begin{notn} Let $V$ be a vector space. For $T \in \ofml L(V)$ let
   \[ \lat T := \{M \preceq V\colon\text{$M$ is invariant under $T$}\}. \]
If $\ofml T \subseteq \ofml L(V)$ let
   \[ \lat \ofml T := \bigcap_{T \in \ofml T} \lat T.  \]
 \index{L@$\lat T$, $\lat\ofml T$ (collection of invariant subspaces)}%
We say that $\lat T$ (or $\lat \ofml T$) is
 \index{trivial!$\lat T$ or $\lat \ofml T$}%
\df{trivial} if it contains only the trivial invariant subspaces $\{0\}$ and~$V$.
\end{notn}

\begin{exam}
  If $V$ is a vector space, then $\lat {\ofml L(V)}$ is trivial.
\end{exam}

\begin{proof}[\emph{Hint for proof}]  For $\dim V \ge 2$ let $M$ be a nonzero proper subspace of~$V$.  Choose nonzero
vectors $x \in M$ and $y \in M^c$.  Define $T\colon V \sto V\colon v \mapsto f(v)y$ where $f$ is a functional in $V^*$
such that $f(x) = 1$.   \ns
\end{proof}

\begin{exam} Let $\ofml A$ be the subalgebra of $\ofml L(\R^2)$ whose members have matrix representations of the form
$\begin{bmatrix} a & b \\ -b & a \end{bmatrix}$.  Then $\lat\ofml A$ is trivial.
\end{exam}

\begin{exam} Let $\ofml A$ be the subalgebra of $\ofml L(\C^2)$ whose members have matrix representations of the form
$\begin{bmatrix} a & b \\ -b & a \end{bmatrix}$.  Then $\lat\ofml A$ is not trivial.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Try $\spn\{(1,-i)\}$.   \ns
\end{proof}

\begin{defn} Let $V$ be a vector space.  A subalgebra $\ofml A$ of $\ofml L(V)$ is
 \index{transitive!algebra}%
\df{transitive} if for every $x \ne 0$ and $y$ in $V$ there exists an operator $T$ in $\ofml A$ such that $y = Tx$.
\end{defn}

\begin{prop} Let $V$ be a vector space. A subalgebra $\ofml A$ of $\ofml L(V)$ is transitive if and only if
$\lat \ofml A$ is trivial.
\end{prop}

\begin{defn} A field $\F$ is
 \index{algebraically closed}%
 \index{closed!algebraically}%
\df{algebraically closed} if every nonconstant polynomial in $\F[x]$ has a root in~$\F$.
\end{defn}

\begin{exam} The field $\C$ of complex numbers is algebraically closed; the field $\R$ of real numbers is not.
\end{exam}

\begin{thm}[Burnside's Theorem] Let $V$ be a finite dimensional vector space over an algebraically closed field.
Then $\ofml L(V)$ has no proper subalgebra which is transitive.
\end{thm}

\begin{proof}
  See \cite{Farenick:2001}, theorem 3.15.  \ns
\end{proof}

\begin{cor} Let $V$ be a finite dimensional complex vector space of dimension at least~$2$. Then every proper subalgebra
of $\ofml L(V)$ has a nontrivial invariant subspace.
\end{cor}

\begin{exam} The preceding result does not hold for real vector spaces.
\end{exam}





















\section{Eigenvalues and Eigenvectors}
\begin{defn}\label{po013def} Suppose that on a vector space $V$ there exist projection
operators $E_1$, \dots, $E_n$ such that
 \begin{enumerate}
    \item[(i)] $I_V = E_1 + E_2 + \dots + E_n$ and
    \item[(ii)] $E_iE_j = 0$ whenever $i \ne j$.
 \end{enumerate}
Then we say that the family $\{E_1, E_2, \dots, E_n\}$ of projections is a
 \index{resolution of the identity!in vector spaces}%
 \index{identity!resolution of the}%
\df{resolution of the identity}.
\end{defn}

Recall that it was shown in proposition~\ref{proj_003_prop} that if $\{E_1, E_2, \dots, E_n\}$ is a resolution
of the identity on a vector space $V$, then $V = \bigoplus_{k=1}^n \ran E_k$.

\begin{defn}\label{po010def}  Let $M_1 \oplus \dots \oplus M_n$ be a direct sum
decomposition of a vector space~$V$.  For each $k \in \N_n$ let $N_k$ be the following
subspace of $V$ complementary to~$M_k$:
 \[ N_k := M_1 \oplus \dots \oplus M_{k-1} \oplus M_{k+1}
        \oplus \dots \oplus M_n.\]
Also (for each $k$) let
 \[ E_k := \sbsb E{N_kM_k} \]
be the projection onto $M_k$ along the complementary subspace~$N_k$.  The projections
$E_1$, \dots $E_n$ are the
 \index{projection!assocuated with direct sum decomposition}%
 \index{direct sum!projections associated with}%
\df{projections associated with the direct sum decomposition} $V =
M_1 \oplus \dots \oplus M_n$.
\end{defn}

\begin{prop} If $M_1 \oplus \dots \oplus M_n$ is a direct sum decomposition of a vector space $V$, then the family
$\{E_1, E_2, \dots, E_n\}$ of the associated projections is a resolution of the identity.
\end{prop}

In the following definition we make use of the familiar notion of the \emph{determinant} of a matrix even though we have not
yet developed the theory of determinants.  We will eventually do this.

\begin{defn} Let $V$ be a vector space over a field~$\F$ and $T \in \ofml L(V)$.  An element $\lambda \in \F$ is an
 \index{eigenvalue}%
\df{eigenvalue} of $T$ if $\ker(T - \lambda I) \ne \{\vc 0\}$. The collection of all eigenvalues of $T$ is its
 \index{point spectrum}%
 \index{spectrum!point}%
 \index{sigmaA@$\sigma_p(A)$, $\sigma_p(T)$ (point spectrum)}%
\df{point spectrum}, denoted by $\sigma_p(T)$.
\end{defn}

\begin{defn} If $\F$ is a field and $A$ is an $n \times n$ matrix of elements of $\F$, we define the
 \index{characteristic!polynomial}%
 \index{polynomial!characteristic}%
\df{characteristic polynomial} $\sbsb cA$ of $A$ to be the determinant of $A - xI$. (Note
that $\det(A - xI)$ is a polynomial.)  Some authors prefer the characteristic polynomial to be monic,
and consequently define it to be the determinant of $xI - A$.  As you would expect, the characteristic polynomial
$\sbsb cT$ of an operator $T$ on a finite dimensional space (with basis $B$) is the characteristic polynomial of the
matrix representation of that operator (with respect to~$B$).  Making use of some standard facts (which we have not yet
proved) about determinants (see section~\ref{section_dets}) we see that $\lambda \in \F$ is an eigenvalue of the matrix $A$
(or of its associated linear transformation) if and only if it is a root of the characteristic polynomial $\sbsb cA$.
\end{defn}

\begin{prop} If $T$ is an operator on a finite dimensional vector space, then $\sigma_p(T) = \sigma(T)$.
\end{prop}

\begin{exer}Let $A = \begin{bmatrix}
                  1  &  1  &  1 \\
                  1  &  1  &  1 \\
                  1  &  1  &  1 \end{bmatrix}$.
 \vskip 8 pt
\noindent The characteristic polynomial of $A$ is
$\lambda^p(\lambda - 3)^q$ where $p =$ \underbar{\hphantom{OOO}}
\,\,and $q =$ \underbar{\hphantom{OOO}}~.
 \vskip 8 pt
\noindent The minimal polynomial of~$A$ is $\lambda^r(\lambda -
3)^s$ where $r =$ \underbar{\hphantom{OOO}} \,\,and $s =$
\underbar{\hphantom{OOO}}~.
\end{exer}

\begin{exer} Let $T$ be the operator on $\R^4$ whose matrix representation is
\smash[b]{$\begin{bmatrix}
            0  &  1  &  0  & -1 \\
           -2  &  3  &  0  & -1 \\
           -2  &  1  &  2  & -1 \\
            2  & -1  &  0  &  3 \end{bmatrix}$}.

 \vskip 8 pt

\noindent The characteristic polynomial of $T$ is $(\lambda -
2)^p$ where $p =$ \underbar{\hphantom{OOO}}~.

 \vskip 8 pt

\noindent The minimal polynomial of~$T$ is $(\lambda - 2)^r$
where $r =$ \underbar{\hphantom{OOO}}~.
\end{exer}

\begin{exer} Choose $a$, $b$ and $c$ in the matrix $\smash[b]
                 {A = \begin{bmatrix}
                             0  &  1  &  0 \\
                             0  &  0  &  1 \\
                             a  &  b  &  c \end{bmatrix}}$
so that the characteristic polynomial of $A$ is $-\lambda^3 + 4\lambda^2 + 5\lambda + 6$.
\end{exer}

\begin{prop} Let $V$ be a finite dimensional vector space over a field~$\F$.  An operator $T$ on $V$ is invertible if
and only if $T$ is not a zero divisor in~$\ofml L(V)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] In one direction the proof is very easy. For the other, use proposition~\ref{prop_minpoly_nasc_invertible}. \ns
\end{proof}

\begin{cor}  Let $V$ be a finite dimensional vector space over a field~$\F$, $T \in \ofml L(V)$, and $\lambda \in \F$.
Then $T - \lambda I$ fails to be invertible if and only if $\lambda$ is a root of the minimal polynomial of~$T$.
\end{cor}

\begin{cor} If $T$ is an operator on a finite dimensional vector space, then its minimal polynomial and characteristic
polynomial have the same roots.
\end{cor}

\begin{thm}[Cayley-Hamilton Theorem]\label{eigenvals014thm} If $T$ is an operator on a finite dimensional vector space,
 \index{Cayley-Hamilton theorem}%
then the characteristic polynomial of $T$ annihilates~$T$.  Moreover, the minimal polynomial of $T$ is a factor of the
characteristic polynomial of~$T$.
\end{thm}

\begin{proof} See~\cite{Farenick:2001}, proposition 3.19.  \ns
\end{proof}

\begin{defn} Let $V$ be a vector space, $T$ be an operator on $V$,
and $\lambda$ be an eigenvalue of~$T$.  A nonzero vector $x$ in
the kernel of $T - \lambda I$ is an
 \index{eigenvector}%
\df{eigenvector of $T$ associated with} (or \df{corresponding to}, or \df{belonging to})
the eigenvalue~$\lambda$.
\end{defn}

\begin{defn} Let $V$ be a vector space, $T$ be an operator on $V$, and $\lambda$ be an eigenvalue of~$T$.  The
 \index{eigenspace}%
\df{eigenspace associated with} (or \df{corresponding to}, or \df{belonging to})
the eigenvalue~$\lambda$ is the kernel of $T - \lambda I$.
\end{defn}

\begin{exer}
Let $T$ be the operator on $\R^3$ whose matrix representation is
\smash[b]{$\begin{bmatrix}
            3  &  1  & -1 \\
            2  &  2  & -1 \\
            2  &  2  &  0 \end{bmatrix}$.}

\vskip 5 pt

 \begin{enumerate}
  \item[(a)] Find the characteristic polynomial of $T$.
  \item[(b)] Find the minimal polynomial of $T$.
  \item[(c)] Find the eigenspaces $V_1$ and $V_2$ of $T$.
 \end{enumerate}
\end{exer}

\begin{exer} Let $T$ be the operator on $\R^5$ whose matrix representation is
\smash[b]{$\begin{bmatrix}
            1 &   0 &   0 &   1 & -1  \\
            0 &   1 &  -2 &   3 & -3  \\
            0 &   0 &  -1 &   2 & -2  \\
            1 &  -1 &   1 &   0 &  1  \\
            1 &  -1 &   1 &  -1 &  2  \end{bmatrix}$.}

\vskip 5 pt

 \begin{enumerate}
  \item[(a)] Find the characteristic polynomial of $T$.
  \item[(b)] Find the minimal polynomial of $T$.
 \end{enumerate}
\end{exer}


\begin{prop} If $\lambda_1 \ne \lambda_2$ are eigenvalues of an operator $T$, then the eigenspaces $M_1$ and $M_2$
associated with $\lambda_1$ and $\lambda_2$, respectively, have only $\vc 0$ in common.
\end{prop}

\begin{prop} Let $V$ be a vector space over a field~$\F$.  If $v$ is an eigenvector associated with an eigenvalue $\lambda$
of an operator $T \in \ofml L(V)$ and $p$ is a polynomial in $\F[x]$, then $p(T)v = p(\lambda)v$.
\end{prop}

\begin{defn} Two operators on a vector space (or two $n \times n$ matrices) $R$ and $T$ are
 \index{similar}%
 \index{matrix!similarity}%
 \index{operator!similarity}%
\df{similar} if there exists an invertible operator (or matrix) $S$ such that $R = S^{-1}TS$.
\end{defn}

\begin{prop} If $R$ and $T$ are operators on a vector space and $R$ is invertible, then $RT$ is similar
to $TR$.
\end{prop}

\begin{exam} If $R$ and $T$ are operators on a vector space, then $RT$ need not be similar to~$TR$.
\end{exam}

\begin{prop} Let $R$ and $T$ be operators on a vector space.  If $R$ is similar to $T$ and $p \in \F[x]$ is a polynomial,
then $p(R)$ is similar to $p(T)$.
\end{prop}

\begin{prop} If $R$ and $T$ are operators on a vector space, $R$ is similar to $T$, and $R$ is invertible, then $T$ is
invertible and $T^{-1}$ is similar to $R^{-1}$.
\end{prop}

\begin{prop} If two matrices $A$ and $B$ are similar, then they have the same spectrum.
\end{prop}

\begin{proof}[\emph{Hint for proof}] You may use familiar facts about determinants that we have not yet proved. \ns
\end{proof}

\begin{defn} An operator on a vector space is
 \index{nilpotent}%
 \index{operator!nilpotent}%
\df{nilpotent} if some power of the operator is~$\vc 0$.
\end{defn}

\begin{prop} An operator $T$ on a finite dimensional complex vector space is nilpotent if and only if $\sigma(T) = \{0\}$.
\end{prop}

\begin{notn} Let $\alpha_1$, \dots, $\alpha_n$ be elements of a field~$\F$.  Then
 \index{diagonal@$\diag(\alpha_1, \dots, \alpha_n)$ (diagonal matrix)}%
$\diag(\alpha_1, \dots, \alpha_n)$ denotes the $n \times n$ matrix whose entries are all
zero except on the main diagonal where they are $\alpha_1$, \dots, $\alpha_n$.  Such a matrix is a
 \index{diagonal!matrix}%
 \index{matrix!diagonal}%
\df{diagonal} matrix.
\end{notn}

\begin{defn} Let $V$ be a vector space of finite dimension~$n$. An operator $T$ on $V$ is
 \index{diagonalizable}%
 \index{operator!diagonalizable}%
\df{diagonalizable} if it has $n$ linearly independent eigenvectors (or, equivalently, if $V$
has a basis of eigenvectors of~$T$).
\end{defn}

\begin{prop} Let $A$ be an $n \times n$ matrix with entries from a field~$\F$. Then $A$,
regarded as an operator on $\F^n$, is diagonalizable if and only if it is similar to a
diagonal matrix.
\end{prop}











\section{The Spectral Theorem - Vector Space Version}

\begin{prop} Let $E_1$, \dots, $E_n$ be the projections associated with a
direct sum decomposition $V = M_1 \oplus \dots \oplus M_n$ of a vector space $V$ and let
$T$ be an operator on~$V$. Then each subspace $M_k$ is invariant under $T$ if and only if
$T$ commutes with each projection~$E_k$.
\end{prop}

\begin{thm}[Spectral Theorem for Vector Spaces]\label{stvs014thm} If $T$ is a
 \index{spectral theorem!for vector spaces}%
 \index{vector!space!spectral theorem for}%
diagonalizable operator on a finite dimensional vector space~$V$, then
 \[ T = \sum_{k=1}^n \lambda_k E_k \]
where $\lambda_1$, \dots, $\lambda_n$ are the (distinct) eigenvalues of $T$ and $\{E_1,
\dots E_n\}$ is the resolution of the identity whose projections are associated with the
corresponding eigenspaces $M_1$, \dots, $M_n$.
\end{thm}

\begin{prop} Let $T$ be an operator on a finite dimensional vector space~$V$. If
$\lambda_1$, \dots, $\lambda_n$ are distinct scalars and $E_1$, \dots, $E_n$ are nonzero
operators on $V$ such that
 \begin{enumerate}
  \item[(i)] $T = \sum_{k=1}^n \lambda_k E_k$,
  \item[(ii)] $I = \sum_{k=1}^n E_k$, and
  \item[(iii)] $E_jE_k = \vc 0$ whenever $j \ne k$,
 \end{enumerate}
then $T$ is diagonalizable, the scalars $\lambda_1$, \dots, $\lambda_n$ are the
eigenvalues of $T$, and the operators $E_1$, \dots, $E_n$ are projections whose ranges
are the eigenspaces of~$T$.
\end{prop}

\begin{prop} If $T$ is a diagonalizable operator on a finite dimensional vector space $V$ over a
field $\F$ and $p \in \F[x]$, then
 \[ p(T) = \sum_{k=1}^n p(\lambda_k)E_k \]
where $\lambda_1$, \dots, $\lambda_n$ are the (distinct) eigenvalues of $T$ and $E_1$,
\dots $E_n$ are the projections associated with the corresponding eigenspaces $M_1$,
\dots, $M_n$.
\end{prop}

\begin{prop}\label{stvs017prop} If $T$ is a diagonalizable operator on a finite dimensional
vector space~$V$, then the projections $E_1$, \dots, $E_n$ associated with the
decomposition of $V$ as a direct sum $\bigoplus M_k$ of its eigenspaces can be expressed
as polynomials in~$T$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Apply the \emph{Lagrange interpolation formula} with
the $t_k$'s being the eigenvalues of~$T$.  \ns
\end{proof}

\begin{exer} Let $T$ be the operator on $\R^3$ whose matrix representation is
\smash[b]{$\begin{bmatrix}   0 &   0 &   2 \\
                             0 &   2 &   0 \\
                             2 &   0 &   0 \end{bmatrix}$}.
Use proposition~\ref{stvs017prop} to write $T$ as a linear combination of projections.
\end{exer}

\begin{exer}\label{stvs019exer} Let $T$ be the operator on $\R^3$ whose matrix
representation is \smash[b]{$\begin{bmatrix} 2  &  -2 &  1  \\
                                            -1  &   1 &  1  \\
                                            -1  &   2 &  0  \end{bmatrix}$}.
Use proposition~\ref{stvs017prop} to write $T$ as a linear combination of projections.
\end{exer}

\begin{exer} Let $T$ be the operator on $\R^3$ whose matrix representation is
  \[ \begin{bmatrix}  \frac13  & -\frac2{\mathstrut3}  & -\frac23  \\
                     -\frac23  &  \frac5{\mathstrut6}  & -\frac76   \\
                     -\frac23  & -\frac7{\mathstrut6}  &  \frac56
     \end{bmatrix}. \]
Write $T$ as a linear combination of projections.
\end{exer}

\begin{prop} An operator $T$ on a finite dimensional vector space is diagonalizable if and only if its minimal polynomial is
of the form $\prod_{k=1}^n (x - \lambda_k)$ for some distinct elements $\lambda_1$, \dots, $\lambda_n$ of the scalar field~$\F$.
\end{prop}

\begin{proof} See \cite{HoffmanK:1971}, page 204, theorem 6.  \ns
\end{proof}















\section{Two Decomposition Theorems}
\begin{thm}[Primary Decomposition Theorem]\label{dnp001thm} Let $T \in \ofml L(V)$
 \index{primary decomposition theorem}%
 \index{decomposition!primary}%
where $V$ is a finite dimensional vector space.  Factor the minimal polynomial
 \[ \sbsb mT = \prod_{k=1}^n \sbsb pk^{\,r_k} \]
into powers of distinct irreducible monic polynomials $\sbsb p1$,
\dots, $\sbsb pn$ and let $W_k = \ker\bigl(\sbsb pk(T)\bigr)^{r_k}$ for
each~$k$. Then
 \begin{enumerate}
  \item[(i)] $V = \bigoplus_{k=1}^n W_k$,
  \item[(ii)] each $W_k$ is invariant under $T$, and
  \item[(iii)] if $T_k = T\bigr|_{W_k}$, then $\sbsb m{T_k} =
\sbsb pk^{\,r_k}$.
 \end{enumerate}
\end{thm}

\begin{proof} See \cite{HoffmanK:1971}, page 220, theorem 12.    \ns
\end{proof}

In the preceding theorem the spaces $W_k$ are the
 \index{generalized eigenspaces}%
 \index{eigenspaces!generalized}%
\df{generalized eigenspaces} of the operator~$T$.

\enlargethispage{\baselineskip}

\begin{thm}[Diagonalizable Plus Nilpotent Decomposition]\label{dnp017thm} Let $T$ be an operator on a finite dimensional vector
 \index{diagonalizable!plus nilpotent decomposition}%
 \index{decomposition!diagonalizable plus nilpotent}%
space~$V$. Suppose that the minimal polynomial for $T$ factors completely into linear factors
  \[ \sbsb mT(x) = (x-\lambda_1)^{d_1} \dots (x-\lambda_r)^{d_r} \]
where $\lambda_1, \dots \lambda_r$ are the (distinct) eigenvalues of~$T$. For each $k$
let $W_k$ be the generalized eigenspace $\ker(T - \lambda_kI)^{d_k}$ and let $E_1$,
\dots, $E_r$ be the projections associated with the direct sum decomposition
  \[ V = W_1 \oplus W_2 \oplus \dots \oplus W_r. \]
Then this family of projections is a resolution of the identity, each $W_k$ is invariant
under~$T$, the operator
  \[ D = \lambda_1E_1 + \dots + \lambda_rE_r \]
is diagonalizable, the operator
  \[ N = T - D \]
is nilpotent, and $N$ commutes with $D$.

Furthermore, if $D_1$ is diagonalizable, $N_1$ is nilpotent, $D_1
+ N_1 = T$, and $D_1N_1 = N_1D_1$, then $D_1 = D$ and $N_1 = N$.
\end{thm}

\begin{proof} See \cite{HoffmanK:1971}, page 222, theorem 13.    \ns \end{proof}

\begin{cor} Every operator on a finite dimensional complex vector space can be written as
the sum of two commuting operators, one diagonalizable and the other nilpotent.
\end{cor}

\begin{exer} Let $T$ be the operator on $\R^2$ whose matrix representation is
$\begin{bmatrix}   2  &  1  \\
                  -1  &  4  \end{bmatrix}$.

\vskip 3 pt

 \begin{enumerate}
  \item[(a)] Explain briefly why $T$ is not diagonalizable.

\vskip 3 pt

  \item[(b)] Find the diagonalizable and nilpotent parts of $T$.

 Answer: $D = \begin{bmatrix}
                     a & b \\
                     b & a
              \end{bmatrix}$ and
        $N = \begin{bmatrix}
                     -c & c \\
                     -c & c
             \end{bmatrix}$ where $a$ = \underbar{\hphantom{OO}}~, $b$ =
\underbar{\hphantom{OO}}~, and $c$ = \underbar{\hphantom{OO}}~.
 \end{enumerate}
\end{exer}

\vskip 5 pt

\begin{exer} Let $T$ be the operator on $\R^3$ whose matrix representation
is $\begin{bmatrix}
          0  &  0  &  -3  \\
         -2  &  1  &  -2  \\
          2  & -1  &   5  \end{bmatrix}$.

\vskip 3 pt
 \begin{enumerate}

  \item[(a)] Find $D$ and $N$, the diagonalizable and nilpotent parts of $T$.
Express these as polynomials in~$T$.

\vskip 3 pt

  \item[(b)] Find a matrix $S$ which diagonalizes~$D$.

\vskip 3 pt

  \item[(c)] Let $[D_1] =  \begin{bmatrix}
                                  2  &  -1  &  -1  \\
                                 -1  &   2  &  -1  \\
                                 -1  &  -1  &   2  \end{bmatrix}$ and
$[N_1] = \begin{bmatrix}
                -2  &  1  &  -2  \\
                -1  & -1  &  -1  \\
                 3  &  0  &   3  \end{bmatrix}$.   Show that $D_1$ is
diagonalizable, that $N_1$ is nilpotent, and that $T = D_1 + N_1$. Why does this not
contradict the uniqueness claim made in theorem~\ref{dnp017thm}?
 \end{enumerate}
\end{exer}

\vskip 5 pt

\begin{exer} Let $T$ be the operator on $\R^4$ whose matrix representation is
$\begin{bmatrix}
            0  &  1  &  0  & -1 \\
           -2  &  3  &  0  & -1 \\
           -2  &  1  &  2  & -1 \\
            2  & -1  &  0  &  3 \end{bmatrix}$.

\vskip 3 pt

 \begin{enumerate}
  \item[(a)] The characteristic polynomial of $T$ is $(\lambda -
  2)^p$ where $p$ = \underbar{\hphantom{OO}}~.


\vskip 3 pt

  \item[(b)] The minimal polynomial of $T$ is $(\lambda -
  2)^r$ where $r$ = \underbar{\hphantom{OO}}~.

\vskip 3 pt

  \item[(c)] The diagonalizable part of $T$ is
 $D = \begin{bmatrix}
                    a & b & b & b \\
                    b & a & b & b \\
                    b & b & a & b \\
                    b & b & b & a
              \end{bmatrix}$ where $a$ = \underbar{\hphantom{OO}} and $b$ =
\underbar{\hphantom{OO}}~.

 \vskip 3 pt

 \item[(d)] The nilpotent part of $T$ is
 \smash[b]{$N = \begin{bmatrix}
              -a &  b &  c & -b \\
              -a &  b &  c & -b \\
              -a &  b &  c & -b \\
               a & -b &  c &  b
           \end{bmatrix}$} where $a$ = \underbar{\hphantom{OO}}~, $b$ =
\underbar{\hphantom{OO}}~, and \\ $c$ = \underbar{\hphantom{OO}}~.
 \end{enumerate}
\end{exer}

\vskip 5 pt

\begin{exer} Let $T$ be the operator on $\R^5$ whose matrix representation is
$\begin{bmatrix}
            1 &   0 &   0 &   1 & -1  \\
            0 &   1 &  -2 &   3 & -3  \\
            0 &   0 &  -1 &   2 & -2  \\
            1 &  -1 &   1 &   0 &  1  \\
            1 &  -1 &   1 &  -1 &  2  \end{bmatrix}$.

\vskip 3 pt

 \begin{enumerate}
  \item[(a)] Find the characteristic polynomial of $T$.

\vskip 3 pt

Answer: $\sbsb cT(\lambda) = (\lambda + 1)^p(\lambda - 1)^q$ where
$p$~=~\underbar{\hphantom{OO}}\, and
$q$~=~\underbar{\hphantom{OO}}~.


\vskip 3 pt

  \item[(b)] Find the minimal polynomial of $T$.

\vskip 3 pt

Answer: $\sbsb mT(\lambda) = (\lambda + 1)^r(\lambda - 1)^s$ where
$r$~=~\underbar{\hphantom{OO}}\, and
$s$~=~\underbar{\hphantom{OO}}~.

\vskip 3 pt

  \item[(c)] Find the eigenspaces $V_1$ and $V_2$ of $T$.

\vskip 3 pt

Answer: $V_1 = \spn\{(a,1,b,a,a)\}$ where
$a$~=~\underbar{\hphantom{OO}}\, and
$b$~=~\underbar{\hphantom{OO}}~; and

\vskip 3 pt

$V_2 = \spn\{(1,a,b,b,b),(b,b,b,1,a)\}$ where
$a$~=~\underbar{\hphantom{OO}}\, and
$b$~=~\underbar{\hphantom{OO}}~.

\vfill\eject

  \item[(d)] Find the diagonalizable part of $T$.

\vskip 3 pt

Answer: $D = \begin{bmatrix}
                 a &  b &  b &  b &  b \\
                 b &  a & -c &  c & -c \\
                 b &  b & -a &  c & -c \\
                 b &  b &  b &  a &  b \\
                 b &  b &  b &  b &  a
             \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OO}}~,
$b$~=~\underbar{\hphantom{OO}}~, and
$c$~=~\underbar{\hphantom{OO}}~.

\vskip 3 pt

  \item[(e)] Find the nilpotent part of $T$.

\vskip 3 pt

Answer: $N = \begin{bmatrix}
                  a &  a &  a &  b & -b \\
                  a &  a &  a &  b & -b \\
                  a &  a &  a &  a &  a \\
                  b & -b &  b & -b &  b \\
                  b & -b &  b & -b &  b \\
              \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OO}}\, and
$b$~=~\underbar{\hphantom{OO}}~.

\vskip 3 pt

  \item[(f)] Find a matrix $S$ which diagonalizes the diagonalizable part $D$
of $T$. What is the diagonal form $\Lambda$ of $D$ associated with
this matrix?

\vskip 3 pt

Answer: $S = \begin{bmatrix}
                 a &  b &  a &  a &  a \\
                 b &  a &  b &  a &  a \\
                 b &  a &  a &  b &  a \\
                 a &  a &  a &  b &  b \\
                 a &  a &  a &  a &  b
 \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OO}}\, and
$b$~=~\underbar{\hphantom{OO}}~.

\vskip 3 pt

and \hphantom{wer:}
     $\Lambda = \begin{bmatrix}
                    -a & 0 & 0 & 0 & 0 \\
                     0 & a & 0 & 0 & 0 \\
                     0 & 0 & a & 0 & 0 \\
                     0 & 0 & 0 & a & 0 \\
                     0 & 0 & 0 & 0 & a
                \end{bmatrix}$ where $a$~=~\underbar{\hphantom{OO}}~.
 \end{enumerate}
\end{exer}




\endinput
\chapter{STOKES' THEOREM}

\section{Integration of Differential Forms}

\begin{defn} Let $\langle s \rangle$ be an oriented $p$\,-simplex in $\R^n$ (where $1 \le p \le n$) and $\mu$ be a $p$\,-form defined on a set $U$ which is open
in the plane of $\langle s \rangle$ and which contains~$[s]$.  If $\langle s \rangle = \langle v_0, \dots, v_p \rangle$ take $(v_1 - v_0, \dots, v_p - v_0)$
to be an ordered basis for the plane of $\langle s \rangle$ and let $x^1$, \dots, $x^p$ be the coordinate projection functions relative to this ordered basis; that
is, if $a = \sum_{k=1}^p a_k(v_k - v_0) \in U$, then $x^j(a) = a_j$ for $1 \le j \le p$.  Then $\phi = (x^1, \dots, x^p) \colon U \sto \R^p$ is a chart on~$U$; so
there exists a smooth function $g$ on $U$ such that $\mu = g\,dx^1 \wedge \dots \wedge dx^p$.
 \index{integral!of a $p$\,-form over a $p$\,-chain}%
Define
  \[ \int\limits_{\langle s \rangle} \mu = \int\limits_{[s]} g\,dx^1 \dots dx^p \]
where the right hand side is an ordinary Riemann integral.  If $\langle v_0 \rangle$ is a $0$\,-simplex, we make a special definition
  \[ \int\limits_{\langle v_0 \rangle} f = f(v_0) \]
for every $0$\,-form~$f$.

Extend the preceding definition to $p$\,-chains by requiring the integral to be linear as a function of simplexes; that is, if $c = \sum a_s \langle s \rangle$ is
a $p$\,-chain (in some simplicial complex) and $\mu$ is a $p$\,-form, define
  \[ \int\limits_c \mu = \sum a(s) \int\limits_{\langle s \rangle} \mu\,. \]
\end{defn}

\begin{defn} For a smoothly triangulated manifold $(M,K,h)$ we define a map
  \[ \int\limits_p \colon \bigwedge\nolimits^{\!p}(M) \sto C^p(K) \]
as follows. If $\omega$ is a $p$\,-form on $M$, then $\int_p\omega$ is to be a linear functional on~$C_p(K)$; that is, a member of $C^p(K) = \bigl(C_p(K)\bigr)^*$.
In order to define a linear functional on $C_p(K)$ it suffices to specify its values on the basis vectors of~$C_p(K)$; that is, on the oriented $p$\,-simplexes
$\langle s \rangle$ which constitute~$C_p(K)$.  Let $h_s \colon U \sto M$ be an extension of $\sbsb{h\bigl|}{[s]}$ to an open set $U$ in the plane
of~$\langle s \rangle$.  Then ${h_s}^*$ pulls back $p$\,-forms on $M$ to $p$\,-forms on~$U$ so that ${h_s}^*(\omega) \in \bigwedge^p(U)$.  Define
  \[ \biggl(\int\limits_p \omega \biggr)\langle s \rangle := \int\limits_{\langle s \rangle} {h_s}^*(\omega)\,. \]
\end{defn}

\begin{exer}\label{intmnf003} Let $V$ be an open subset of $\R^n$, $F\colon V \sto \R^n$, and $c\colon [t_0,t_1] \sto V$ be a smooth curve in~$V$.  Let
$C = \ran c$. It is conventional to define the ``integral of the tangential component of~$F$ over~$C$\,\,'', often denoted by $\int_CF_T$, by the formula
  \begin{equation}\label{intmnf003i}
        \int\limits_CF_T = \int_{t_0}^{t_1} \langle F \circ c, Dc\rangle = \int_{t_0}^{t_1} \langle F(c(t)), c'(t)\rangle\,dt .
  \end{equation}
The ``tangential component of $F$\,'', written $F_T$ may be regarded as the $1$\,-form $\sum_{k=1}^n F^k\, dx^k$.

Make sense of the preceding definition in terms of the definition of the integral of $1$\,-forms over a smoothly triangulated manifold. For simplicity take $n = 2$.
\emph{Hint.} Suppose we have the following:
  \begin{enumerate}
    \item $\langle t_0,t_1 \rangle$ (with $t_0 < t_1$) is an oriented $1$\,-simplex in~$\R$;
    \item $V$ is an open subset of $\R^2$;
    \item $c\colon J \sto V$ is an injective smooth curve in $V$, where $J$ is an open interval containing $[t_0,t_1]$; and
    \item $\omega = a\,dx + b\, dy$ is a smooth $1$\,-form on~$V$.
  \end{enumerate}
First show that
  \[ \bigl(c^*(dx)\bigr)(t) = Dc^1(t) \]
for $t_0 \le t \le t_1$. (We drop the notational distinction between $c$ and its extension $c_s$ to~$J$. Since the tangent space $T_t$ is one-dimensional for
every $t$, we identify $T_t$ with~$\R$.  Choose $v$ (in (3) of proposition~\ref{smplx036prop}) to be the usual basis vector in $\R$, the number~$1$.)

Show in a similar fashion that
  \[ \bigl(c^*(dy)\bigr)(t) = Dc^2(t)\,. \]
Then write an expression for $\bigl(c^*(\omega)\bigr)(t)$. Finally conclude that $\bigl(\int_1 \omega\bigr)(\langle t_0,t_1 \rangle)$ is indeed equal to
$\int_{t_0}^{t_1} \langle (a,b) \circ c, Dc \rangle$ as claimed in~\ref{intmnf003i}.
\end{exer}

\begin{exer} Let $\Sp^1$ be the unit circle in $\R^2$ oriented counterclockwise and let $\vc F$ be the vector field defined by
$\vc F(x,y) = (2x^3 - y^3)\,\vc i + (x^3 + y^3)\,\vc j$. Use your work in exercise~\ref{intmnf003} to calculate $\int_{\Sp^1} F_T$.
\emph{Hint.} You may use without proof two facts: (1) the integral does not depend on the parametrization (triangulation) of the curve, and (2) the results of
exercise~\ref{intmnf003} hold also for simple closed curves in~$\R^2$; that is, for curves $c\colon [t_0,t_1] \sto \R^2$ which are injective on the open interval
$(t_0,t_1)$ but which satisfy $c(t_0) = c(t_1)$.
\end{exer}

\begin{notn} Let $\Ha^n = \{x \in \R^n\colon x_n \ge 0\}$. This is the
 \index{upper!half-space}%
 \index{half-space, upper}%
 \index{hn@$\Ha^n$ (upper half-space)}%
\df{upper half-space} of~$\R^n$.
\end{notn}

\begin{defn} A
 \index{manifold!with boundary}%
\df{$n$\,-manifold with boundary} is defined in the same way as an $n$\,-manifold except that the range of a chart is assumed to be an open subset of~$\Ha^n$.

The
 \index{interior!of a half-space}%
 \index{half-space!interior of a}%
 \index{<topint@$\intrin \Ha^n$ (interior of a half-space)}%
\df{interior} of $\Ha^n$, denoted by $\intrin \Ha^n$, is defined to be $\{x \in \R^n\colon x_n > 0\}$.  (Notice that this is the interior of $\Ha^n$ regarded
as a subset of $\R^n$---\textbf{not} of~$\Ha^n$.) The
 \index{boundary!of a half-space}%
 \index{half-space!boundary of a}%
 \index{<topbnd@$\partial \Ha^n$ (boundary of a half-space)}%
\df{boundary} of $\Ha^n$, denoted by $\partial \Ha^n$, is defined to be $\{x \in \R^n\colon x_n = 0\}$.

If $M$ is an $n$\,-manifold with boundary, a point $m \in M$ belongs to the
 \index{interior!of a manifold with boundary}%
 \index{manifold!with boundary!interior of a}%
 \index{<topint@$\intrin M$ (interior of a manifold with boundary)}%
\df{interior} of $M$ (denoted by $\intrin M$) if $\phi(m) \in \intrin\Ha^n$ for some chart~$\phi$. And it belongs to the
 \index{boundary!of a manifold}%
 \index{manifold!boundary of a}%
 \index{<topbnd@$\partial M$ (boundary of a manifold)}%
\df{boundary} of $M$ (denoted by $\partial M$) if $\phi(m) \in \partial \Ha^n$ for some chart~$\phi$.
\end{defn}

\begin{thm} Let $M$ and $N$ be a smooth $n$\,-manifolds with boundary and $F\colon M\sto N$ be a smooth diffeomorphism.  Then both $\intrin M$ and $\partial M$
are smooth manifolds (without boundary). The interior of $M$ has dimension $n$ and the boundary of $M$ has dimension~$n-1$. The mapping $F$ induces smooth
diffeomorphisms $\intrin F \colon \intrin M \sto \intrin N$ and $\partial F\colon \partial M \sto \partial N$.
\end{thm}

\begin{proof} Consult the marvelous text~\cite{AbrahamMR:1983}, proposition~7.2.6.  \ns
\end{proof}

\begin{exer}\label{intmnf008} Let $V$ be an open subset of $\R^3$, $\vc F \colon V \sto \R^3$ be a smooth vector field, and $(S,K,h)$ be a smoothly triangulated
$2$\,-manifold such that $S \subseteq V$.  It is conventional to define the ``normal component of $\vc F$ over $S$\,\,'', often denoted by $\iint_S \vc F_N$, by
the formula
   \[ \iint\limits_S \vc F_N = \iint\limits_K \langle \vc F \circ h, n \rangle \]
where $n = h_1 \times h_2$. (Notation: $h_k$ is the $k^{\text{th}}$ partial derivative of~$h$.)

Make sense of the preceding definition in terms of the definition of the integral of $2$\,-forms over a smoothly triangulated manifold  (with or without boundary).
In particular, suppose that $\vc F = a\,\vc i + b\,\vc j + c\,\vc k$ (where $a$, $b$, and $c$ are smooth functions) and let
$\omega = a\,dy \wedge dz + b\,dz \wedge dx + c\, dx \wedge dy$.  This $2$\,-form is conventionally called the ``normal component of $\vc F$'' and is denoted
by~$\vc F_N$. Notice that $\vc F_N$ is just $\ast\mu$ where $\mu$ is the $1$\,-form associated with~the vector field~$\vc F$.  \emph{Hint.} Proceed as follows.
 \begin{enumerate}
    \item[(a)] Show that the vector $n(u,v)$ is perpendicular to the surface $S$ at $h(u,v)$ for each $(u,v)$ in $[K]$ by showing that it is perpendicular to
$D(h \circ c)(0)$ whenever $c$ is a smooth curve in $[K]$ such that $c(0) = (u,v)$.
    \item[(b)] Let $u$ and $v$ (in that order) be the coordinates in the plane of $[K]$ and $x$, $y$, and $z$ (in that order) be the coordinates in $\R^3$.
Show that $h^*(dx) = h_1^1\,du + h_2^1\,dv$. Also compute $h^*(dy)$ and $h^*(dz)$.

\noindent\emph{Remark.} If at each point in $[K]$ we identify the tangent plane to $\R^2$ with $\R^2$ itself and if we use conventional notation, the ``$v$''
which appears in (3) of proposition~\ref{smplx036prop} is just not written.  One keeps in mind that the components of $h$ and all the differential forms are
functions on (a neighborhood of)~$[K]$.
    \item[(c)] Now find $h^*(\omega)$. (Recall that $\omega = \vc F_N$ is defined above.)
    \item[(d)] Show for each simplex $(s)$ in $K$ that
  \[ \biggl(\int\limits_2 \omega\biggr)(\langle s \rangle) = \iint\limits_{[s]} \langle \vc F \circ h, n \rangle\,. \]
    \item[(e)] Finally show that if $\langle s_1 \rangle$, \dots, $\langle s_n \rangle$ are the oriented $2$\,-simplexes of $K$ and
$c = \sum_{k=1}^n \langle s_k \rangle$, then
  \[ \biggl(\int\limits_2 \omega\biggr)(c) = \iint\limits_{[K]} \langle \vc F \circ h, n \rangle\,. \]
 \end{enumerate}
\end{exer}

\begin{exer} Let $\vc F(x,y,z) = xz\,\vc i + yz\,\vc j$ and $H$ be the hemisphere of $x^2 + y^2 + z^2 = 4$ for which $z \ge 0$. Use exercise~\ref{intmnf008}
to find $\iint\limits_H\vc F_N$.
\end{exer}





























\section{Generalized Stokes' Theorem}
\begin{thm}[Generalized Stokes' theorem]\label{stokes001thm} Suppose that $(M,K,h)$ is an
 \index{Stokes' theorem}%
oriented smoothly triangulated manifold with boundary. Then the integration operator $\int = \bigl(\int_p\bigr)_{p \in \Z}$ is a cochain map from the
cochain complex $(\bigwedge^{\!*}(M),\,d\,)$ to the cochain complex $(C^*(K),\,\partial^*\,)$.
\end{thm}

\begin{proof} This is an important and standard theorem, which appears in many versions and with many different proofs.  See, for example, \cite{AbrahamMR:1983},
theorem 7.2.6; \cite{Lang:1999}, chapter XVII, theorem 2.1; \cite{MadsenT:1997}, theorem 10.8; or \cite{Warner:1983}, theorems 4.7 and 4.9.   \ns
\end{proof}

Recall that when we say in \emph{Stokes' theorem} that the integration operator is a cochain map, we are saying that the following diagram commutes.
 \[\xy
   \square(-700,0)/{>}``{>}`{>}/<700,700>[`\bigwedge^{\!p}(M)``C^p(K);d``\int`\partial^*]
   \square<1000,700>[\bigwedge^{\!p}(M)`\bigwedge^{\!p+1}(M)`C^p(K)`C^{p+1}(K);d``\int`\partial^*]
   \square(1000,0)/{>}```{>}/<700,700>[\bigwedge^{\!p+1}(M)``C^{p+1}(K)`;d```\partial^*]
 \endxy\]
Thus if $\omega$ is a $p$\,-form on $M$ and $\langle s \rangle$ is an oriented $(p+1)$\,-simplex belonging to $K$, then we must have
 \begin{equation}\label{stokes002thmi}
    \biggl(\int\limits_{p+1}\,d\omega \biggr)(\langle s \rangle) = \biggl(\partial^* \biggl( \int\limits_p\omega\biggr)\biggr)(\langle s \rangle).
 \end{equation}
This last equation~\eqref{stokes002thmi} can be written in terms of integration over oriented simplexes:
  \begin{equation}\label{stokes002thmii}
     \int\limits_{\langle s \rangle}\,d\bigl({h_s}^*\omega\bigr) = \int\limits_{\partial\langle s \rangle}\,{h_s}^*\omega\,.
  \end{equation}
In more conventional notation all mention of the triangulating simplicial complex $K$ and of the map $h$ is suppressed.  This is justified by the fact that
it can be shown that the value of the integral is independent of the particular triangulation used. Then when the equations of the form~\eqref{stokes002thmii}
are added over all the $(p+1)$\,-simplexes comprising $K$ we arrive at a particularly simple formulation of (the conclusion of) \emph{Stokes' theorem}
 \begin{equation}\label{stokes002thmiii}
     \int\limits_M\,d\omega = \int\limits_{\partial M}\,\omega\,.
  \end{equation}
One particularly important topic that has been glossed over in the preceding is a discussion of orientable manifolds (those which possess nowhere vanishing
volume forms), their orientations, and the manner in which an orientation of a manifold with boundary induces an orientation on its boundary. One of many places
where you can find a careful development of this material is in sections 6.5 and 7.2 of~\cite{AbrahamMR:1983}.

\begin{thm} Let $\omega$ be a $1$\,-form on a connected open subset $U$ of~$\R^2$. Then $\omega$ is exact on $U$ if and only if $\int_C \,\omega = 0$ for every
simple closed curve in~$U$.
\end{thm}

\begin{proof} See~\cite{doCarmo:1994}, chapter 2, proposition~1.   \ns
\end{proof}

\begin{exam} Let $\omega = -\dfrac{y\,dx}{x^2 + y^2} + \dfrac{x\,dy}{x^2 + y^2}$. On the region $\R^2 \setminus \{(0,0)\}$ the $1$-form $\omega$ is closed but
not exact.
\end{exam}

\begin{exer} What classical theorem do we get from the version of \emph{Stokes' theorem} given by equation~\eqref{stokes002thmiii} in the special case that $M$ 
is a flat $1$\,-manifold (with boundary) in $\R$ and $\omega$ is a $0$\,-form defined on some open set in $\R$ which contains~$M$? Explain.
\end{exer}

\begin{exer} What classical theorem do we get from the version of \emph{Stokes' theorem} given by equation~\eqref{stokes002thmiii} in the special case that $M$ 
is a (not necessarily flat) $1$\,-manifold (with boundary) in $\R^3$ and $\omega$ is a $0$\,-form defined on some open subset of $\R^3$ which contains~$M$?  Explain.
\end{exer}

\begin{exer}\label{stokes007} What classical theorem do we get from the version of \emph{Stokes' theorem} given by equation~\eqref{stokes002thmiii} in the special 
case that $M$ is a flat $2$\,-manifold (with boundary) in $\R^2$ and $\omega$ is the $1$\,-form associated with a vector field~$\vc F\colon U \sto \R^2$ defined on 
an open subset $U$ of $\R^2$ which contains~$M$?  Explain.
\end{exer}

\begin{exer} Use exercise~\ref{stokes007} to compute $\int_{\Sp^1} (2x^3 - y^3)\,dx + (x^3 + y^3)\,dy$ (where $\Sp^1$ is the unit circle oriented counterclockwise).
\end{exer}

\begin{exer} Let $\vc F(x,y) = (-y,x)$ and let $C_a$ and $C_b$ be the circles centered at the origin with radii $a$ and $b$, respectively, where $a<b$. Suppose
that $C_a$ is oriented clockwise and $C_b$ is oriented counterclockwise.  Find
 \[\int\limits_{C_a} \vc F \cdot d\vc r + \int\limits_{C_b} \vc F \cdot d\vc r~.\]
\end{exer}

\begin{exer} What classical theorem do we get from the version of \emph{Stokes' theorem} given by equation~\eqref{stokes002thmiii} in the special case that $M$ is 
a (not necessarily flat) $2$\,-manifold (with boundary) in $\R^3$ and $\omega$ is the $1$\,-form associated with a vector field~$\vc F\colon U \sto \R^3$ defined on 
an open subset $U$ of $\R^3$ which contains~$M$?  Explain.
\end{exer}

\begin{exer} What classical theorem do we get from the version of \emph{Stokes' theorem} given by equation~\eqref{stokes002thmiii} in the special case that $M$ is 
a (flat) $3$\,-manifold (with boundary) in $\R^3$ and $\omega = \ast\,\mu$ where $\mu$ is the $1$\,-form associated with a vector field~$\vc F\colon U \sto \R^3$ 
defined on an open subset $U$ of $\R^3$ which contains~$M$?  Explain.
\end{exer}

\begin{exer} Your good friend Fred R. Dimm calls you on his cell phone seeking help with a math problem. He says that he wants to evaluate the integral of the
normal component of the vector field on $\R^3$ whose coordinate functions are $x$, $y$, and $z$ (in that order) over the surface of a cube whose edges have
length~$4$.  Fred is concerned that he's not sure of the coordinates of the vertices of the cube.  How would you explain to Fred (over the phone) that it
doesn't matter where the cube is located and that it is entirely obvious that the value of the surface integral he is interested in is~$192$?
\end{exer}









\endinput
\chapter{TENSOR ALGEBRAS}

\section{Grassmann Algebras}
\begin{defn}\label{grassmannalg001def} Let $V$ be an $d$-dimensional vector space over
a field~$\F$. We say that $\bigwedge(V)$ is the
 \index{Grassmann algebra}%
 \index{algebra!Grassmann}%
\df{Grassmann algebra} (or the
 \index{exterior!algebra}%
 \index{algebra!exterior}%
 \index{<@$\bigwedge(V)$ (Grassmann or exterior algebra)}%
 \index{<binop@$\omega \wedge \mu$ (wedge product)}%
\df{exterior algebra}) over $V$ if
 \begin{enumerate}
    \item $\bigwedge(V)$ is a unital algebra over~$\F$ (multiplication is denoted by $\wedge$),
    \item $V$ is ``contained in'' $\bigwedge(V)$,
    \item $v \wedge v = \vc 0$ for every $v \in V$,
    \item $\dim(\bigwedge(V)) = 2^d$, and
    \item $\bigwedge(V)$ is generated by $\sbsb{\vc 1}{\bigwedge(V)}$ and $V$.
 \end{enumerate}
The multiplication $\wedge$ in a Grassmann algebra is called the
 \index{wedge product}%
 \index{product!wedge}%
\df{wedge product} (or the
 \index{exterior!product}%
 \index{product!exterior}%
\df{exterior product}).
\end{defn}

\begin{exer} There are two instances in the preceding definition where I have opted for brevity over precision.  Explain why in
definition~\ref{grassmannalg001def} ``contained in'' appears in quotation marks.  Give a more precise version of condition~(2).  Also,
explain more precisely what is meant, in condition~(5), by saying that $\bigwedge(V)$ is \emph{generated by} $\vc 1$ and~$V$.
\end{exer}

\begin{prop} If $\bigwedge(V)$ is a Grassmann algebra over a vector space $V$, then the zero vector of $V$ is an annihilator in the algebra
$\bigwedge(V)$.  That is, $\sbsb{\vc 0}V \wedge g = \sbsb{\vc 0}V$ for every $g \in \bigwedge(V)$.
\end{prop}








\begin{prop} If $\bigwedge(V)$ is a Grassmann algebra over a vector space $V$, then $\sbsb{\vc 1}{\bigwedge(V)} \notin V$.
\end{prop}

\begin{prop} Let $v$ and $w$ be elements of a finite dimensional vector space~$V$.  In the Grassmann algebra $\bigwedge(V)$ generated by~$V$
   \[ v \wedge w = - w \wedge v\,. \]
\end{prop}

\begin{prop} Let $V$ be a $d$-dimensional vector space with basis $E = \{e^1, \dots, e^d\}$.  For each nonempty subset $S = \{e^{i_1}, e^{i_2}, \dots, e^{i_p}\}$
of $E$ with $i_1 < i_2 < \dots < i_p$ let $e_S = e^{i_1} \wedge e^{i_2} \wedge \dots \wedge e^{i_p}$.  Also let $e_\emptyset = \sbsb{\vc 1}{\bigwedge(V)}$. Then
$\{e_S\colon S \subseteq E\}$ is a basis for the Grassmann algebra $\bigwedge(V)$.
\end{prop}

\begin{defn} An algebra $A$ is a
 \index{algebra!$\Z^+$-graded}%
 \index{Zgraded@$\Z^+$-graded algebra}%
 \index{graded algebra}%
\df{$\Z^+$-graded algebra} if it is a direct sum $A = \bigoplus\limits_{k \ge 0}A_k$ of vector subspaces $A_k$ and its multiplication $\land$ takes elements in
$A_j \times A_k$ to elements in $A_{j+k}$ for all $j$, $k \in \Z^+$.  An element in $A_k$ is said to be a
 \index{homogeneous!elements of a graded algebra}%
 \index{degree!of a homogeneous element}%
\df{homogeneous} element of \df{degree}~$k$ (or of
 \index{grade!of a homogeneous element}%
\df{grade}~$k$).

The definitions of $\Z$-graded algebras, $\N$-graded algebras and $\Z_2$-graded algebras are similar. (In the case of a $Z_2$-graded algebra the indices are $0$
and $1$ and $A_1\wedge A_1 \subseteq A_0$.)  Usually the unmodified expression ``graded algebra'' refers to an $\Z^+$-graded algebra.

Proposition~\ref{grassmannalg005} says that every Grassmann algebra $\bigwedge(V)$ over a vector space $V$ is a graded algebra.  The set of elements homogeneous
of degree~$k$ is denoted by $\bigwedge^k(V)$. An element of $\bigwedge^k(V)$ which can be written in the form $v_1 \wedge v_2 \wedge \dots \wedge v_k$ (where
$v_1$, \dots, $v_k$ all belong to~$V$) is a
 \index{decomposable!element of a Grassmann algebra}%
\df{decomposable} element of
 \index{degree!of a decomposable element}%
\df{degree}~$k$ (or of
 \index{grade!of a decomposable element}%
\df{grade}~$k$).
\end{defn}

\begin{prop}\label{grassmannalg005} Every Grassmann algebra is a $\Z^+$-graded algebra.
\end{prop}

We denote by
 \index{<@$\bigwedge^k(V)$ (homogeneous elements of degree~$k$)}%
$\bigwedge^k(V)$ the subspace of all homogeneous elements of degree $k$ in $\bigwedge(V)$. In particular, $\bigwedge^0(V) = \F$ and $\bigwedge^1(V) = V$.
If the dimension of $V$ is $d$, take $\bigwedge^k(V) = \{\vc 0\}$ for all $k > d$. (And if you wish to regard $\bigwedge(V)$ as a $\Z$-graded algebra also
take $\bigwedge^k(V) = \{\vc 0\}$ whenever $k < 0$.)

\begin{exam} If the dimension of a vector space $V$ is $3$ or less, then every homogeneous element of the corresponding Grassmann algebra is decomposable.
\end{exam}

\begin{exam} If the dimension of a (finite dimensional) vector space $V$ is at least
four, then there exist homogeneous elements in the corresponding Grassmann algebra which
are not decomposable.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Let $e^1$, $e^2$, $e^3$, and $e^4$ be distinct basis elements of~$V$ and consider $(e^1 \wedge e^2) + (e^3 \wedge e^4)$. \ns
\end{proof}

\begin{prop} The elements $v_1$, $v_2$, \dots, $v_p$ in a vector space $V$ are linearly independent if and only if $v_1 \wedge v_2 \wedge \dots \wedge v_p \ne \vc 0$
in the corresponding Grassmann algebra~$\bigwedge(V)$.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be a linear map between finite dimensional vector spaces. Then there exists a unique extension of $T$ to a unital algebra
homomorphism $\bigwedge(T)\colon \bigwedge(V) \sto \bigwedge(W)$. This extension maps $\bigwedge^k(V)$ into $\bigwedge^k(W)$ for each $k \in \N$.
\end{prop}

\begin{exam} The pair of maps $V \mapsto \bigwedge(V)$ and $T \mapsto \bigwedge(T)$ is a covariant functor from the category of vector spaces and linear maps to
the category of unital algebras and unital algebra homomorphisms.
\end{exam}

\begin{prop} If $V$ is a vector space of dimension $d$, then $\dim\bigl(\bigwedge^p(V)\bigr) = \binom dp$ for $0 \le p \le d$.
\end{prop}

\begin{conv}\label{conv_wedge0_is_R} Let $V$ be a $d$-dimensional vector space.  Since the map $\lambda \mapsto \lambda \vc 1_{\bigwedge(V)}$
is an obvious
 \index{conventions!$\bigwedge^0(V) = \F$}%
isomorphism between $\F$ and the one-dimensional space $\bigwedge^0(V)$, we identify these two spaces, and refer to an element of $\bigwedge^0(V)$
 \index{scalar}%
as a \df{scalar}. And since $\bigwedge^d(V)$ is also one-dimensional, its elements are frequently referred to as
 \index{pseudoscalar}%
\df{pseudoscalars}.
\end{conv}

\begin{prop} If $V$ is a finite dimensional vector space, $\omega \in \bigwedge^p(V)$, and $\mu \in \bigwedge^q(V)$, then
   \[ \omega \land \mu = (-1)^{pq}\mu \land \omega. \]
\end{prop}























\section{Existence of Grassmann Algebras}

\begin{defn} Let $V_0$, $V_1$, $V_2$, \dots be vector spaces (over the same field). Then their
 \index{external!direct sum}%
 \index{direct sum!external}%
 \index{sum!direct}%
 \index{<binaryoperationz@$\bigoplus_{k=0}^\infty V_k$ (direct sum)}%
\df{(external) direct sum}, which is denoted by $\bigoplus\limits_{k=0}^\infty V_k$, is defined to be the set of all functions
$v\colon \Z^+ \sto \bigcup_{k=0}^\infty V_k$ with finite support such that $v(k) = v_k \in V_k$ for each $k \in \Z^+$. The usual pointwise
addition and scalar multiplication make this set into a vector space.
\end{defn}

\begin{defn}\label{grassmannalg021def} Let $V$ be a vector space over a field $\F$. Define $\ten^{\,0}(V) = \F$, $\ten^1(V) = V$,
$\ten^{\,2}(V) = V \otimes V$, $\ten^{\,3}(V) = V \otimes V \otimes V$, \dots, $\ten^k(V) = V \otimes \dots \otimes V$ ($k$~factors), \dots\,.  Then
let $\ten(V) = \bigoplus\limits_{k=0}^\infty \ten^k(V)$. Define multiplication on $\ten(V)$ by using the obvious isomorphism
   \[ \ten^k(V) \otimes \ten^m(V) \cong \ten^{k+m}(V) \]
and extending by linearity to all of~$\ten(V)$. The resulting algebra is the
 \index{tensor!algebra}%
 \index{algebra!tensor}%
 \index{tv@$\ten(V)$ (the tensor algebra of~$V$)}%
\df{tensor algebra} of $V$ (or generated by~$V$).
\end{defn}

\begin{prop} The tensor algebra $\ten(V)$ as defined in~\ref{grassmannalg021def} is in fact a unital algebra.
\end{prop}

\begin{prop} Let $V$ be a finite dimensional vector space and $J$ be the ideal in the tensor algebra $\ten(V)$ generated by the set of all elements of the
form $v \otimes v$ where $v \in V$. Then the quotient algebra $\ten(V)/J$ is the Grassmann algebra over $V^*$ (or, equivalently, over~$V$).
\end{prop}

\begin{notn} If $x$ and $y$ are elements of the tensor algebra $\ten(V)$, then in the quotient algebra $\ten(V)/J$ the product of $[x]$ and $[y]$ is written
 \index{<binop@$[x] \wedge [y] = [x \otimes y]$ in $\ten(V)/J$}%
using ``wedge notation''; that is,
   \[ [x] \wedge [y] = [x \otimes y]. \]
\end{notn}

\begin{notn} If $V$ is a vector space over $\F$ and $k \in \N$ we denote
 \index{altkv@$\alt^k(V)$ (set of alternating $k$-linear maps)}%
by $\alt^k(V)$ the set of all alternating $k$-linear maps from $V^k$ into~$\F$. (The
space $\alt^1(V)$ is just~$V^*$.) Additionally, take $\alt^0(V) = \F$.
\end{notn}

\begin{exam} If $V$ is a finite dimensional vector space and $k > \dim V$, then $\alt^k(V) = \{\vc 0\}$.
\end{exam}

\begin{defn} Let $p$, $q \in \N$. We say that a permutation $\sigma \in S_{p+q}$ is a
 \index{pq@$(p,q)$-shuffle}%
 \index{shuffle}%
\df{$(p,q)$-shuffle} if $\sigma(1) < \dots < \sigma(p)$ and $\sigma(p+1) < \dots <
\sigma(p+q)$.  The set of all such permutations is
 \index{spq@$S(p,q)$ (shuffle permutations)}%
denoted by~$S(p,q)$.
\end{defn}

\begin{exam} Give an example of a $(4,5)$-shuffle permutation $\sigma$ of the set $\N_9 = \{1, \dots, 9\}$ such that $\sigma(7) = 4$.
\end{exam}

\begin{defn}\label{mlm032def} Let $V$ be a vector space over a field of characteristic~$0$.  For $p$, $q \in \N$ define
  \[ \land\colon \alt^p(V) \times \alt^q(V) \sto \alt^{p+q}(V) \colon (\omega,\mu) \mapsto \omega \land \mu \]
where
  \[ (\omega \land \mu)(v^1, \dots, v^{p+q}) = \sum_{\sigma \in S(p,q)}(\sgn\sigma)\omega(v^{\sigma(1)},
                            \dots, v^{\sigma(p)}) \mu(v^{\sigma(p+1)}, \dots, v^{\sigma(p+q)}). \]
\end{defn}

\begin{exer} Show that definition~\ref{mlm032def} is not overly optimistic by verifying that if $\omega \in \alt^p(V)$ and $\mu \in \alt^q(V)$, then
$\omega \land \mu \in \alt^{p+q}(V)$.
\end{exer}

\begin{prop} The multiplication defined in~\ref{mlm032def} is associative. That is if $\omega \in \alt^p(V)$, $\mu \in \alt^q(V)$, and $\nu \in \alt^r(V)$,
then
   \[ \omega \land (\mu \land \nu) = (\omega \land \mu) \land \nu. \]
\end{prop}

\begin{exer} Let $V$ be a finite dimensional vector space over a field of characteristic zero.  Explain in detail how to make $\alt^k(V)$ (or, if you prefer,
$\alt^k(V^*$)\,) into a vector space for each $k \in \Z$ and how to make the collection of these into a $\Z$-graded algebra. Show that this algebra is the
Grassmann algebra generated by $V$. \emph{Hint.} Take $\alt^k(V) = \{0\}$ for each $k < 0$ and extend the definition of the wedge product so that if $\alpha \in
\alt^0(V) = \F$ and $\omega \in \alt^p(V)$, then $\alpha \land \omega = \alpha\omega$.
\end{exer}

\begin{prop} Let $\omega_1$, \dots, $\omega_p$ be members of $\alt^1(V)$ (that is, linear functionals on~$V$). Then
  \[ (\omega_1 \land \dots \land \omega_p)(v^1, \dots, v^p) = \det\bigl[\omega_j(v^k)\bigr]_{j,k=1}^p \]
for all $v^1$, \dots, $v^p \in V$.
\end{prop}

\begin{prop}\label{grassmannalg039} If $\{e_1, \dots, e_n\}$ is a basis for an $n$-dimensional vector space~$V$, then
  \[ \{e_{\sigma(1)}^* \land \dots \land e_{\sigma(p)}^* \colon \sigma \in S(p,n-p)\} \]
is a basis for $\alt^p(V)$.
\end{prop}

\begin{prop} For $T \colon V \sto W$ a linear map between vector spaces define
  \[ \alt^p(T)\colon \alt^p(W) \sto \alt^p(V) \colon \omega \mapsto \alt^p(T)(\omega) \]
where $\bigl[\alt^p(T)(\omega)\bigr](v^1, \dots, v^p) = \omega(Tv^1, \dots, Tv^p)$ for
all $v^1$, \dots, $v^p \in V$. Then $\alt^p$ is a contravariant functor from the category
of vector space and linear maps into itself.
\end{prop}

\begin{exer} Let $V$ be an $n$-dimensional vector space and $T \in \ofml L(V)$. If $T$
is diagonalizable, then
  \[ \sbsb cT(\lambda) = \sum_{k=0}^n (-1)^k [\alt^{n-k}(T)]\lambda^k. \]
\end{exer}























\section{The Hodge $*$-operator}
\begin{defn} Let $E$ be a basis for an $n$-dimensional vector space~$V$. Then the $n$-tuple $(e^1, \dots, e^n)$ is an
 \index{basis!ordered}%
 \index{ordered!basis}%
\df{ordered basis} for $V$ if $e^1$, \dots, $e^n$ are the distinct elements of~$E$.
\end{defn}

\begin{defn} Let $E = (e^1, \dots, e^n)$ be an ordered basis for $\R^n$. We say that the basis $E$ is
 \index{right!-handed basis}%
 \index{basis!right-handed}%
\df{right-handed} if $\det[e^1, \dots, e^n] > 0$ and
 \index{left!-handed basis}%
\df{left-handed} otherwise.
\end{defn}

\begin{defn} Let $V$ be a real $n$-dimensional vector space and $T\colon \R^n \sto V$ be
an isomorphism. Then the set of all $n$-tuples of the form $(T(e^1), \dots, T(e^n))$
where $(e^1, \dots, e^n)$ is a right-handed basis in~$\R^n$ is an
 \index{orientation!of a vector space}%
 \index{vector!space!orientation of a}%
\df{orientation} of~$V$. Another orientation consists of the set of $n$-tuples $(T(e^1),
\dots, T(e^n))$ where $(e^1, \dots, e^n)$ is a left-handed basis in~$\R^n$.  Each of
these orientations is the
 \index{opposite orientation}%
\df{opposite} (or
 \index{reverse orientation}%
\df{reverse}) of the other. A vector space together with one of these orientations is an
 \index{oriented!vector space}%
 \index{vector!space!oriented}%
\df{oriented vector space}.
\end{defn}

\begin{exer}\label{hodge_star002} Let $V$ be an $n$-dimensional real inner product space. In exercise~\ref{dual024z} we established an isomorphism
$\Phi\colon v \mapsto v^*$ between $V$ and its dual space~$V^*$.  Show how this isomorphism can be used to induce an inner product on~$V^*$. Then show how
this may be used to create an inner product on $\alt^p(V)$ for $2 \le p \le n$.  \emph{Hint.} For $v$, $w \in V$ let
$\langle v^*,w^* \rangle = \langle v,w \rangle$. Then for $\omega_1, \dots, \omega_p, \mu_1, \dots, \mu_p \in \alt^1(V)$ let
$\langle \omega_1 \land \dots \land \omega_p\,, \mu_1 \land \dots \land \mu_p \rangle = \det[ \langle \omega_j, \mu_k \rangle]$.
\end{exer}

\begin{prop}\label{hodgestar003} Let $V$ be an $d$-dimensional oriented real inner product space.  Fix a unit vector
 \index{vol@$\vol$ (volume element or form)}%
$\vol \in \alt^d(V)$.  This vector is called a
 \index{volume!element}%
\df{volume element}. (In the case where $V = \R^d$, we will always choose $\vol = e_1^* \land \dots \land e_d^*$ where $(e_1, \dots, e_d)$ is the usual
ordered basis for~$\R^d$.)

Let $\omega \in \alt^p(V)$ and $q = d - p$. Then there exists a vector $\ast\,\omega \in \alt^q(V)$ such that
  \[ \langle \ast\,\omega,\mu \rangle \vol = \omega \land \mu \]
for each $\mu \in \alt^q(V)$. Furthermore, the map $\omega \mapsto \ast\,\omega$ from $\alt^p(V)$ into $\alt^q(V)$ is a vector space isomorphism. This
map is the
 \index{Hodge star operator}%
 \index{star operator}%
\df{Hodge star operator}.
\end{prop}

\begin{prop} Let $V$ be a finite dimensional oriented real inner product space of dimension~$n$. Suppose that $p + q = n$.  Then
$\ast\ast\,\omega = (-1)^{pq} \omega$ for every $\omega \in \alt^p(V)$.
\end{prop}




\endinput
\chapter{VECTOR SPACES}

\section{Abelian Groups}\label{abelian_groups}
\begin{conv} In  general, Abelian groups are written additively.  That is,
 \index{conventions!additive notation for Abelian groups}%
the usual notation for the binary operation on an Abelian group is $+$. Of course, there
are special examples where this notation is inappropriate: most notably for the nonzero
real numbers, the strictly positive real numbers, and the nonzero complex numbers under
multiplication.  It is conventional, if not entirely logical to write, ``let $G$ be an
Abelian group,'' when what is meant is, ``let $(G,+)$ be an Abelian group."
\end{conv}

\begin{prop} The identity element in an Abelian group is unique.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $G$ be an Abelian group. Suppose that there are
elements $\vc 0$ and $\wt{\vc 0}$ in $G$ such that $x + \vc 0 = x$ and $x + \wt{\vc 0} =
x$ hold for every $x \in G$.  Prove that $\vc 0 = \wt{\vc 0}$. \ns
\end{proof}

\begin{prop} Each element in an Abelian group has a unique.inverse.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $G$ be an Abelian group and $x \in G$. To prove
uniqueness of the inverse for $x$ suppose that there are elements $y$ and $z$ in $G$ such
that $x + y = \vc 0$ and $x + z = \vc 0$.  Prove that $y = z$. \ns
\end{proof}

\begin{prop}\label{abgp003} If $x$ is an element of an Abelian group such that
$x + x = x$, then $x = \vc 0$.
\end{prop}

\begin{prop} For every element $x$ in an Abelian group $-(-x) = x$.
\end{prop}

\begin{exam}\label{abgp004} Let $S$ be a nonempty set and $\F$ be a field. Denote by
 \index{f(s)@$\fml F(S,\F)$, $\fml F(S)$!$\F$-valued functions on $S$}%
$\fml F(S,\F)$ the family of all $\F$-valued functions on~$S$. For $f$, $g \in \fml
F(S,\F)$ define $f + g$ by
   \[ (f + g)(x) = f(x) + g(x) \]
for all $x \in S$.  Under this operation (called \emph{pointwise addition})
 \index{f(s)@$\fml F(S,\F)$, $\fml F(S)$!as an Abelian group}%
 \index{pointwise!addition}%
 \index{addition!pointwise}%
$\fml F(S,\F)$ is an Abelian group.
\end{exam}

\begin{exam}\label{abgp004a} As a special case of example~\ref{abgp004}, we may regard
Euclidean $n$-space $\R^n$ as an
 \index{Rn@$\R^n$!as an Abelian group}%
Abelian group.
\end{exam}

\begin{exam}\label{abgp004b} As a special case of example~\ref{abgp004}, we may regard
the set $\R^\infty$ of all sequences of real numbers as an
 \index{Rinfty@$\R^\infty$!as an Abelian group}%
Abelian group.
\end{exam}


\begin{exam}\label{abgp006} Let $\E^2$ be the Euclidean plane. It contains points (which
do \emph{not} have coordinates) and lines (which do not have equations).  A
 \index{directed segment}%
 \index{segment}%
\df{directed segment} is an ordered pair of points. Define two directed segments to be
\emph{equivalent} if they are congruent (have the same length), lie on parallel lines,
and have the same direction. This is clearly an equivalence relation on the set
$\ofml{DS}$ of directed segments in the plane.  We denote by $\overrightarrow{PQ}$ the
equivalence class containing the directed segment $(P,Q)$, going from the point $P$ to
the point~$Q$.  Define an operation $+$ on these equivalence classes by
  \[ \overrightarrow{PQ} + \overrightarrow{QR} = \overrightarrow{PR}. \] This operation is
well defined and under it $\ofml{DS}$ is an Abelian group.
\end{exam}

\begin{exer} Suppose that $A$, $B$, $C$, and $D$ are points in the plane such that
$\overrightarrow{AB} = \overrightarrow{CD}$.  Show that $\overrightarrow{AC} =
\overrightarrow{BD}$.
\end{exer}

\begin{defn} Let $G$ and $H$ be Abelian groups.  A map $f\colon G \sto H$ is a
 \index{homomorphism!of Abelian groups}%
\df{homomorphism} if
  \[ f(x + y) = f(x) + f(y) \]
for all $x$, $y \in G$.  We will denote by
 \index{hom@$\Hom(G,H)$, $\Hom(G)$ (group homomorphisms)}%
$\Hom(G,H)$ the set of all homomorphisms from $G$ into~$H$ and will abbreviate
$\Hom(G,G)$ to~$\Hom(G)$.
\end{defn}

\begin{prop} If $f\colon G \sto H$ is a homomorphism of Abelian groups, then
$f(\vc 0) = \vc 0$.
\end{prop}

\begin{prop} If $f\colon G \sto H$ is a homomorphism of Abelian groups, then $f(-x)
= -f(x)$ for each $x \in G$.
\end{prop}

\begin{defn}\label{abgp005def} Let $G$ and $H$ be Abelian groups. For $f$ and $g$ in
$\Hom(G,H)$ we define
 \[ f + g\colon G \sto H\colon x \mapsto f(x) + g(x). \]
\end{defn}

\begin{exam} Let $G$ and $H$ be Abelian groups. With addition as defined
 \index{hom@$\Hom(G,H)$!as an Abelian group}%
in~\ref{abgp005def} $\Hom(G,H)$ is an Abelian group.
\end{exam}

\begin{conv} Let $G$, $H$, and $J$ be Abelian groups and $f\colon G \sto H$ and
 \index{conventions!for homomorphisms write $gf$ for $g \circ f$}%
$g\colon H \sto J$ be homomorphisms. Then the composite of $g$ with $f$ is denoted by
$gf$ (rather than by $g \circ f$).  That is,
   \[ gf\colon G \sto J\colon x \mapsto g(f(x)).\]
\end{conv}

\begin{prop}\label{abgp011} Let $G$, $H$, and $J$ be Abelian groups, $f \in \Hom(G,H)$,
and $g \in \Hom(H,J)$, then the composite $gf$ belongs to $\Hom(G,J)$.
\end{prop}















\section{Functions and Diagrams}
\begin{defn} Let $S$ and $T$ be sets and $f\colon S \sto T$. The set $S$ is the
 \index{domain}%
\df{domain} of~$f$.  The set $T$ is the
 \index{codomain}%
\df{codomain} of~$f$.  And $\{(x,f(x))\colon x \in S\}$ is the
 \index{graph}%
\df{graph} of~$f$. The domain of $f$ is denoted
 \index{domain@$\dom f$ (domain of a function $f$)}%
by~$\dom f$.
\end{defn}

\begin{defn} A function $f$ is
 \index{injective}%
\df{injective} (or
 \index{one-to-one}%
\df{one-to-one}) if $x = y$ whenever $f(x) = f(y)$.  That is, $f$ is injective if no two
distinct elements in its domain have the same image. An injective map is called an
 \index{injection}%
\df{injection}.

A function is
 \index{surjective}%
\df{surjective} (or
 \index{onto}%
\df{onto}) if its range is equal to its codomain. A surjective map is called a
 \index{surjection}%
\df{surjection}.

A function is
 \index{bijective}%
\df{bijective} (or a
 \index{one-to-one correspondence}%
\df{one-to-one correspondence}) if it is both injective and surjective. A bijective map
is called a
 \index{bijection}%
\df{bijection}.
\end{defn}

\begin{defn} It is frequently useful to think of functions as arrows in
diagrams. For example, the situation $h\colon R \sto S$, $j\colon R \sto T$, $k\colon S
\sto U$, $f\colon T \sto U$ may be represented by the following diagram.
 \[\xy
   \square[R`T`S`U;j`h`f`k]
 \endxy\]
The diagram is said to
 \index{commutative!diagram}%
 \index{diagram!commutative}%
\df{commute} if $k \circ h = f \circ j$. Diagrams need not be rectangular. For instance,
 \[\xy
    \btriangle[R`S`U;h`d`k]
 \endxy\]
is a commutative diagram if $d = k \circ h$.
\end{defn}

\begin{exam}  Here is one diagrammatic way of stating the associative
law for composition of functions: If $h\colon R \sto S$, $g\colon S \sto T$, and $f\colon
T \sto U$ and we define $j$ and $k$ so that the triangles in the diagram
 \[\xy
   \square[R`T`S`U;j`h`f`k]
   \morphism(0,0)<500,500>[S`T;g]
 \endxy\]
commute, then the square also commutes.
\end{exam}

\begin{conv}  If $S$, $T$, and $U$ are sets we will often not distinguish between
 \index{conventions!on Cartesian products}%
$(S \times T) \times U$, $S \times (T \times U)$, and $S \times T \times U$.  That is,
the ordered pairs $\bigl((s,t),u\bigr)$ and $\bigl(s,(t,u)\bigr)$ and the ordered triple
$(s,t,u)$ will usually be treated as identical.
\end{conv}

\begin{notn} Let $S$ be a set.  The map
 \[ \id{S}\colon S \sto S \colon x \mapsto x \]
is the \df{identity function} on~$S$.
 \index{iden@$\id{S}$ (identity function on~$S$)}%
 \index{identity!function on a set}%
When no confusion will result we write $\id{}$ for $\id{S}$.
\end{notn}

\begin{defn} Let $S$ and $T$ be sets, $f \colon S \sto T$, and $A\subseteq S$. Then the
 \index{<@$f\mid_{{}_A}$ (restriction of $f$ to $A$)}%
 \index{restriction}%
\df{restriction} of $f$ to $A$, denoted by $\bigl.f\bigr|_A$, is the function
$f\circ\iota_{{}_{A,S}}$, where $\iota_{{}_{A,S}}\colon A \sto S\colon x \mapsto x$ is the
 \index{inclusion map}%
\emph{inclusion map} of $A$ into~$S$. That is, $\bigl.f\bigr|_A$ is the mapping from
$A$ into $T$ whose value at each $x$ in $A$ is $f(x)$.
  \[ \xy
      \btriangle/<-`>`>/[S`A`T;\iota_{{}_{A,S}}`f`\bigl.f\bigr|_A]
     \endxy \]

Suppose that $g \colon A \sto T$ and $A \subseteq S$. A function $f\colon S \sto T$ is an
 \index{extension}%
\df{extension} of $g$ to~$S$ if $f\big|_A = g$, that is, if the diagram
  \[ \xy
     \btriangle/<-`>`>/[S`A`T;\iota_{{}_{A,S}}`f`g]
    \endxy \]
commutes.
\end{defn}

\begin{notn} If $S$, $T$, and $U$ are nonempty sets and if $f\colon S \sto T$ and
$g\colon S \sto U$, then we define the function
 \index{<binaryoperation@$(f,g)$ (function into a product)}%
$(f,g)\colon S \sto T \times U$ by
 \[(f,g)(s) = (f(s),g(s)).\]
Suppose, on the other hand, that we are given a function $h$ mapping $S$ into the
Cartesian product $T \times U$.  Then for each $s \in S$ the image $h(s)$ is an ordered
pair, which we will write as $\bigl(h^1(s), h^2(s)\bigr)$. (The superscripts have nothing
to do with powers.)  Notice that we now have functions $h^1\colon S \sto T$ and
$h^2\colon S \sto U$.  These are the
 \index{components}%
\df{components} of~$h$. In abbreviated notation $h = (h^1,h^2)$.
\end{notn}

\begin{notn} Let $f\colon S \sto U$ and $g\colon T \sto V$ be
 \index{<binop@$f \times g$ (product function)}%
 \index{function!product}%
functions between sets. Then $f \times g$ denotes the map
 \[ f \times g\colon S \times T \sto U \times V\colon
         (s,t) \mapsto \bigl(f(s),g(t)\bigr). \]
\end{notn}

\begin{exer} Let $S$ be a set and $a \colon S \times S \sto S$ be a function such that
the diagram
 \[
  S \times S \times S \two^{a \times\id{}}_{\id{}\times a}
    S \times S \to^a S  \tag{D1}
  \]
commutes. What is $(S,a)$?  \emph{Hint.} Interpret $a$ as, for example, addition (or
multiplication).
\end{exer}

\begin{conv} We will have use for a standard one-element set, which, if we wish, we can
regard as the Cartesian product of an empty family of sets. We will denote it by~$\vc 1$.
 \index{<constant@$\vc 1$ (standard one-element set)}%
For each set $S$ there is exactly one function from $S$ into $\vc 1$.  We will denote it
 \index{epsilon@$\varepsilon_S$, $\varepsilon$ (function from $S$ into~$\vc 1$)}%
by~$\varepsilon_S$.  If no confusion is likely to arise we write $\varepsilon$
for~$\varepsilon_S$.
\end{conv}

\begin{exer} Let $S$ be a set and suppose that $a \colon S \times S \sto S$ and
$\eta \colon \vc 1 \sto S$ are functions such that both diagram (D1) above and the
diagram (D2) which follows commute.
 \[ \xy
    \Atrianglepair/{<-}`{<-}`{<-}`{->}`{<-}/<800,500>%
    [S`{\vc 1\times S}`S \times S`S \times \vc 1;f`a`g`%
       \eta \times \id{}`\id{} \times \eta] \tag{D2}
 \endxy\]
(Here $f$ and $g$ are the obvious bijections.) What is $(S,a,\eta)$?
\end{exer}

\begin{notn} We denote by $\delta$ the \emph{diagonal} mapping of
a set $S$ into $S \times S$.
 \index{diagonal!mapping}%
 \index{function!diagonal}%
 \index{delta@$\delta$ (diagonal mapping)}%
That is,
 \[ \delta\colon S \sto S \times S\colon s \mapsto (s,s).\]
\end{notn}

\begin{exer} Let $S$ be a set and suppose that $a \colon S \times S \sto S$ and
$\eta \colon \vc 1 \sto S$ are functions such that the diagrams (D1) and (D2) above
commute.  Suppose further that there is a function $\iota\colon S \sto S$ for which the
following diagram commutes.
 \[\xy
\xymatrix{&S \ar[r]^-\delta \ar[rrdd]_{\varepsilon}%
          &{S\times S} \ar@<0.7ex>[rr]^{\iota\times\id{}}\ar@<-0.7ex>[rr]_{\id{}\times\iota}%
          &&{S \times S} \ar[r]^-a &S \\  \\
          &&&{\vc 1} \ar[rruu]_{\eta}
  }\tag{D3}
 \endxy \]
What is $(S,a,\eta,\iota)$?
\end{exer}

\begin{notn} Let $S$ be a set. We denote by $\sigma$ the
 \index{interchange operation}%
 \index{switching operation}%
 \index{function!interchange}%
 \index{function!switching}%
 \index{sigma@$\sigma$ (interchange operation)}%
\df{interchange} (or \df{switching}) operation on the $S \times S$.  That is,
 \[ \sigma\colon S \times S \sto S \times S\colon (s,t) \mapsto (t,s).\]
\end{notn}

\begin{exer} Let $S$ be a set and suppose that $a \colon S \times S \sto S$,
$\eta \colon \vc 1 \sto S$, and $\iota\colon S \sto S$ are functions such that the
diagrams (D1), (D2), and (D3) above commute. Suppose further that the following diagram
commutes.
 \[ \xy
    \Vtriangle[{S\times S}`S \times S`S;\sigma`a`a] \tag{D4}
 \endxy\]
What is $(S,a,\eta,\iota,\sigma)$?
\end{exer}

\begin{exer} Let $f\colon G \sto H$ be a function between Abelian groups.
Suppose that the diagram
 \[\xy
    \square<800,500>[G \times G`H \times H`G`H;f \times f`+`+`f]
 \endxy\]
commutes.  What can be said about the function~$f$?
\end{exer}

\begin{notn} If $S$ and $T$ are sets we denote
 \index{F@$\fml F(S,T)$ (functions from $S$ to~$T$)}%
by $\fml F(S,T)$ the family of all functions from $S$ into~$T$. When $\F$ is a field
there are several common notations for the family of $\F$-valued functions on~$S$. We
 \index{l@$l(S,\F)$, $l(S)$ ($\F$-valued functions on $S$)}%
 \index{F@$\fml F(S)$, $\fml F(S,\F)$, $\F^S$ ($\F$-valued functions on $S$)}%
denote by $l(S)$ (or by $l(S,\F)$, or by $\F^S$, or by $\fml F(S,\F)$, or by $\fml F(S)$)
the family of all functions $\alpha\colon S \sto \F$.  For $x \in l(S)$ we frequently
write the value of $x$ at $s \in S$
 \index{<subscript@$x_s$ (alternative notation for $x(s)$)}%
as $x_s$ rather than $x(s)$.  (Sometimes it seems a good idea to reduce the number of
parentheses cluttering a page.)

The
 \index{support}%
 \index{supp@$\supp(f)$ (support of $f$)}%
\df{support} of a function $f\colon S \sto \F$, denoted by $\supp(f)$, is $\{x \in S
\colon f(x) \ne 0\}$.

Furthermore, we will denote
 \index{l@$l_c(S,\F)$, $l_c(S)$ (functions on $S$ with finite support)}%
 \index{finite!support}%
 \index{support!finite}%
by $l_c(S)$ (or by $l_c(S,\F)$, or by $\fml F_c(S)$) the family of all functions
$\alpha\colon S \sto \F$ with finite support; that is, those functions on $S$ which are
nonzero at only finitely many elements of~$S$.
\end{notn}

\begin{exer} Let $S$ be a set with exactly one element.  Discuss the cardinality of
(that is, the number of elements in) the sets $\fml F(\emptyset,\emptyset)$, $\fml
F(\emptyset,S)$,$\fml F(S,\emptyset)$, and $\fml F(S,S)$,
\end{exer}












\section{Rings}
Recall that an ordered triple $(R,+,\cdot)$ is a \emph{ring} if $(R,+)$ is an Abelian
group, $(R,\cdot)$ is a semigroup, and the distributive laws (see \emph{Some Algebraic
Objects}~\ref{distributive_laws}) hold.  The ring is \emph{unital} if, in addition,
$(R\setminus \{\vc 0\},\cdot)$ is a monoid.

\begin{prop} The additive identity of a ring is an annihilator.  That
 \index{annihilator}%
is, for every element $a$ of a ring $\vc 0 a = a\,\vc 0 = \vc 0 $.
\end{prop}

\begin{prop} If $a$ and $b$ are elements of a ring, then $(-a)b = a(-b) = -(ab)$ and
$(-a)(-b) = ab$.
\end{prop}

\begin{prop} Let $a$ and $b$ be elements of a unital ring.  Then $\vc 1 - ab$ is
invertible if and only if $\vc 1 - ba$ is.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Look at the product of $\vc 1 - ba$ and $\vc 1 + bca$
where $c$ is the inverse of $1 - ab$. \ns
\end{proof}

\begin{defn} An element $a$ of a ring is \df{left cancellable}
if $ab = ac$ implies that $b = c$. It is \df{right cancellable} if $ba = ca$ implies that
$b = c$.  A ring has the
 \index{cancellation property}%
\df{cancellation property} if every nonzero element of the ring is both left and right
cancellable.
\end{defn}

\begin{exer} Every division ring has the cancellation property.
\end{exer}

\begin{defn} A nonzero element $a$ of a ring is a
 \index{zero!divisor}%
 \index{divisor!of zero}%
\df{zero divisor} (or \df{divisor of zero}) if there exists a nonzero element $b$ of the
ring such that (i) $ab  = \vc 0$ or (ii) $ba = \vc 0$.
\end{defn}

Most everyone agrees that a nonzero element $a$ of a ring is a \emph{left divisor of
zero} if it satisfies (i) for some nonzero $b$ and a \emph{right divisor of zero} if it
satisfies (ii) for some nonzero~$b$.  There agreement on terminology ceases.  Some
authors (\cite{Cohn:2003}, for example) use the definition above for \emph{divisor of
zero}; others (\cite{Hungerford:1974}, for example) require a \emph{divisor of zero} to
be \emph{both} a left and a right divisor of zero; and yet others (\cite{MacLaneB:1967},
for example) avoid the issue entirely by defining \emph{zero divisors} only for
commutative rings.  Palmer in~\cite{Palmer:1994} makes the most systematic distinctions:
a \emph{zero divisor} is defined as above; an element which is both a left and a right
zero divisor is a \emph{two-sided zero divisor}; and if the same nonzero $b$ makes both
(i) and (ii) hold $a$ is a \emph{joint zero divisor}.

\begin{prop} A division ring has no zero divisors.  That is, if $ab = \vc 0$ in a
division ring, then $a = \vc 0$ or $b = \vc 0$.
\end{prop}

\begin{prop} A ring has the cancellation property if and only if it has no zero divisors.
\end{prop}

\begin{exam} Let $G$ be an Abelian group. Then $\Hom(G)$ is a unital ring (under the
operations of addition and composition).
\end{exam}

\begin{defn} A function $f\colon R \sto S$ between rings is a
 \index{ring!homomorphism}%
 \index{homomorphism!of rings}%
\df{(ring) homomorphism} if
   \begin{equation}\label{rings015i}
        f(x + y) = f(x) + f(y)
   \end{equation}
and
   \begin{equation}\label{rings015ii}
        f(xy) = f(x)f(y)
   \end{equation}
for all $x$ and $y$ in~$R$.  If in addition $R$ and $S$ are unital rings and
   \begin{equation}\label{rings015iii}
        f(\vc 1_R) = \vc 1_S
   \end{equation}
then $f$ is a
 \index{ring!homomorphism!unital}%
 \index{homomorphism!unital ring}%
 \index{unital!ring homomorphism}%
\df{unital (ring) homomorphism}.

Obviously a ring homomorphism $f\colon R \sto S$ is a group homomorphism of $R$ and $S$
regarded as Abelian groups. The
 \index{kernel!of a ring homomorphism}%
\df{kernel} of $f$ as a ring homomorphism is the kernel of $f$ as a homomorphism of
Abelian groups; that is $\ker f = \{x \in R \colon f(x) = \vc 0\}$.

If $f^{-1}$ exists and is also a ring homomorphism, then $f$ is an
 \index{isomorphism!of rings}%
\df{isomorphism} from $R$ to $S$.  If an isomorphism from $R$ to $S$ exists, then $R$ and
$S$ are
 \index{isomorphic}%
\df{isomorphic}.
\end{defn}














\section{Vector Spaces}
\begin{defn}\label{vector_space01} Let $\F$ be a field. An ordered triple $(V,+,M)$ is a
 \index{vector!space}%
\df{vector space over~$\F$} if $(V,+)$ is an Abelian group and $M\colon \F \sto \Hom(V)$
is a unital ring homomorphism.  An element of $V$ is a
 \index{vector}%
\df{vector} and an element of $\F$ is a
 \index{scalar}%
\df{scalar}.  A vector space whose scalars are real numbers is a
 \index{vector!space!real}%
 \index{real!vector space}%
 \index{space!vector}%
\df{real vector space} and one with complex numbers as scalars is a
 \index{vector!space!complex}%
 \index{complex!vector space}%
\df{complex vector space}. The vector space $\{\vc 0\}$ containing a single element is
the
 \index{vector!space!trivial}%
 \index{trivial!vector space}%
\df{trivial vector space}.
\end{defn}

\begin{exer} The definition of \emph{vector space} found in many elementary texts is
something like the following: a \emph{vector space} is a set $V$ together with operations
of addition and scalar multiplication which satisfy the following axioms:
 \begin{enumerate}
  \item[(1)] if $\vc x$, $\vc y \in V$, then $\vc x + \vc y \in V$;
  \item[(2)] $(\vc x + \vc y) + \vc z = \vc x + (\vc y + \vc z)$ for every
 \index{associative}%
$\vc x$, $\vc y$, $\vc z \in V$ (associativity);
  \item[(3)] there exists $\vc 0 \in V$ such that $\vc x + \vc 0 = \vc x$ for every
 \index{identity!additive}%
$\vc x \in V$ (existence of additive identity);
  \item[(4)] for every $\vc x\in V$ there exists $-\vc x \in V$ such that
 \index{additive!inverses}%
 \index{inverse!additive}%
$\vc x + (-\vc x) = \vc 0$ (existence of additive inverses);
  \item[(5)] $\vc x + \vc y = \vc y + \vc x$ for every $\vc x$, $\vc y \in V$
 \index{commutative}%
(commutativity);
  \item[(6)] if $\alpha \in \F$ and $\vc x \in V$, then $\alpha \vc x \in V$;
  \item[(7)] $\alpha (\vc x + \vc y) = \alpha\vc x + \alpha\vc y$ for every $\alpha \in \F$
and every $\vc x$, $\vc y \in V$;
  \item[(8)] $(\alpha + \beta)\vc x = \alpha\vc x + \beta\vc x$ for every $\alpha$,
$\beta \in \F$ and every $\vc x \in V$;
  \item[(9)] $(\alpha\beta)\vc x = \alpha(\beta\vc x)$ for every $\alpha$,
$\beta \in \F$ and every $\vc x \in V$; and
  \item[(10)] $1\,\vc x = \vc x$ for every $\vc x \in V$.
 \end{enumerate}
Verify that this definition is equivalent to the one given above in~\ref{vector_space01}.
\end{exer}

\begin{prop} If $x$ is an element of a vector space, then $(-1)x$ is the additive inverse
of~$x$. That is, $(-1)x = -x$.  (Here, of course, $1$ is the multiplicative identity of
the field~$\F$.)
\end{prop}

\begin{exam} Let $\F$ be a field. Then $\F$ can be regarded as a vector space over
itself.
\end{exam}

\begin{exam}\label{vector_space02} Let $S$ be a nonempty set and $\F$ be a field. In
example~\ref{abgp004} we saw that the family $\fml F(S,\F)$ of $\F$-valued functions on
$S$ is an Abelian group under pointwise addition.  For $f \in \fml F(S,\F)$ and $\alpha
\in \F$ define $\alpha f$ by
   \[ (\alpha f)(x) = \alpha\cdot f(x) \]
for all $x \in S$.  Under this operation (called \emph{pointwise scalar multiplication})
the Abelian group $\fml F(S,\F)$ becomes a
 \index{f(s)@$\fml F(S,\F)$, $\fml F(S)$!as a vector space}%
 \index{pointwise!scalar multiplication}%
 \index{scalar!multiplication!pointwise}%
vector space.  When $\F = \R$ we write $\fml F(S)$ for $\fml F(S,\R)$.
\end{exam}

\begin{exam}\label{vector_space02a} As a special case of example~\ref{vector_space02},
we may regard Euclidean $n$-space $\R^n$ as a
 \index{Rn@$\R^n$!as a vector space}%
vector space.
\end{exam}

\begin{exam}\label{vector_space02b} As another special case of
example~\ref{vector_space02}, we may regard the set $\R^\infty$ of all sequences of real
numbers as a
 \index{Rinfty@$\R^\infty$!as a vector space}%
vector space.
\end{exam}

\begin{exam}\label{vector_space02c} Yet another special case of
example~\ref{vector_space02}, is the vector space $\mathbf M_{m \times n}(\F)$ of $m
\times n$ matrices of
 \index{M@$\mathbf M_{m \times n}(\F)$!as a vector space}%
 \index{M@$\mathbf M_n = \mathbf M_n(\F) = M_{m \times n}(\F)$}%
members of $\F$ (where $m$, $n \in \N$).  We will use $\mathbf M_n(\F)$ as shorthand
for~$\mathbf M_{n\times n}(\F)$ and $\mathbf M_n$ for $\mathbf M_n(\R)$.
\end{exam}

\begin{exer} Let $V$ be the set of all real numbers.  Define an operation of ``addition''
by
 \[x \boxplus y = \text{ the maximum of $x$ and $y$}\]
for all $x$, $y \in V$.  Define an operation of ``scalar multiplication'' by
 \[\alpha \boxdot x = \alpha x\]
for all $\alpha \in \R$ and $x \in V$. Prove or disprove: under the operations $\boxplus$
and $\boxdot$ the set $V$ is a vector space.
\end{exer}

\begin{exer} Let $V$ be the set of all real numbers $x$ such that $x > 0$.
Define an operation of ``addition'' by
 \[x \boxplus y = xy\]
for all $x$, $y \in V$.  Define an operation of ``scalar multiplication'' by
 \[\alpha \boxdot x = x^\alpha\]
for all $\alpha \in \R$ and $x \in V$. Prove or disprove: under the operations $\boxplus$
and $\boxdot$ the set $V$ is a vector space.
\end{exer}

\begin{exer} Let $V$ be $\R^2$, the set of all ordered pairs $(x,y)$ of real
numbers. Define an operation of ``addition'' by
  \[(u,v) \boxplus (x,y) = (u + x + 1, v + y + 1)\]
for all $(u,v)$ and $(x,y)$ in $V$.  Define an operation of ``scalar multiplication'' by
 \[\alpha \boxdot (x,y) = (\alpha x, \alpha y)\]
for all $\alpha \in \R$ and $(x,y) \in V$.  Prove or disprove: under the operations
$\boxplus$ and $\boxdot$ the set $V$ is a vector space.
\end{exer}

\begin{exer} Let $V$ be the set of all $n \times n$ matrices of real numbers. Define
an operation of ``addition'' by
 \[A \boxplus B = \tfrac12(AB + BA)\]
for all $A$, $B \in V$.  Define an operation of ``scalar multiplication'' by
 \[\alpha \boxdot A  = \vc 0\]
for all $\alpha \in \R$ and $A \in V$.  Prove or disprove: under the operations
$\boxplus$ and $\boxdot$ the set $V$ is a vector space. (If you have forgotten how to
multiply matrices, look in any beginning linear algebra text.)
\end{exer}

\begin{prop} If $x$ is a vector and $\alpha$ is a scalar, then $\alpha x = \vc 0$ if and
only if $\alpha = 0$ or $x = \vc 0$.
\end{prop}

In example~\ref{abgp006} we saw how to make the family of equivalence classes of directed
segments in the plane into an Abelian group. We may also define scalar multiplication on
these equivalence classes by declaring that
  \begin{enumerate}
    \item if $\alpha > 0$, then $\alpha \overrightarrow{PQ} = \overrightarrow{PR}$ where
$P$, $Q$, and $R$ are collinear, $P$ \emph{does not} lie between $Q$ and $R$, and the
length of the directed segment $(P,R)$ is $\alpha$ times the length of $(P,Q)$;
    \item if $\alpha = 0$, then  $\alpha \overrightarrow{PQ} = \overrightarrow{PP}$; and
    \item if $\alpha < 0$, then $\alpha \overrightarrow{PQ} = \overrightarrow{PR}$ where
$P$, $Q$, and $R$ are collinear, $P$ \emph{does} lie between $Q$ and $R$, and the length
of the directed segment $(R,P)$ is $\alpha$ times the length of $(P,Q)$.
  \end{enumerate}

\begin{exer} Show that the scalar multiplication presented above is well-defined and that
it makes the Abelian group of equivalence classes of directed segments in the plane into
a vector space.
\end{exer}

\begin{rem}\label{vector_spaces_rem_vgeom} Among the methods for proving elementary facts about Euclidean geometry of the plane three of the most
common are \emph{synthetic geometry}, \emph{analytic geometry}, and \emph{vector geometry}.  In \emph{synthetic geometry} points do not have coordinates,
lines do not have equations, and vectors are not mentioned; but standard theorems from Euclid's \emph{Elements} are used. \emph{Analytic geometry} makes
use of a coordinate system in terms of which points are assigned coordinates and lines are described by equations; little or no use is made of vectors or
major theorems of Euclidean geometry.  \emph{Vector geometry} uses vectors as defined in the preceding exercise, but does not rely on Euclidean theorems
or coordinate systems.  Although there is nothing illogical about mixing these methods in establishing a result, it is interesting to try to
construct separate proofs of some elementary results using each method in turn.  That is what the next four exercises are about.
\end{rem}

\begin{exer} Use each of the three geometric methods described above to show that the diagonals of a parallelogram bisect each other.
\end{exer}

\begin{exer} Use each of the three geometric methods described above to show that if the diagonals of a quadrilateral bisect each other then the
quadrilateral is a parallelogram.
\end{exer}

\begin{exer} Use each of the three geometric methods described above to show that the line joining the midpoints of the non-parallel sides of a trapezoid
is parallel to the bases and its length is half the sum of the lengths of the bases.
\end{exer}

\begin{exer} Use each of the three geometric methods described above to show that the line segments joining the midpoints of adjacent sides of an arbitrary
quadrilateral form a parallelogram.
\end{exer}

\begin{exer} Three vertices of a parallelogram $PQRS$ in $3$-space are $P = (1,3,2)$, $Q = (4,5,3)$, and $R = (2,-1,0)$. What are the coordinates of the
point $S$, opposite~$Q$?
\end{exer}


















\section{Subspaces}
\begin{defn} A subset $M$ of a vector space $V$ is a
 \index{subspace}%
\df{subspace} of $V$ if it is a vector space under the operations it inherits from~$V$.
\end{defn}

\begin{notn} For a vector space $V$ we will write
 \index{<binrel@$M \preceq V$ ($M$ is a subspace of~$V$)}%
$M \preceq V$ to indicate that $M$ is a subspace of~$V$.  To distinguish this concept
from other uses of the word ``subspace''
 \index{subspace}%
(\emph{topological subspace}, for example) writers frequently use the expressions
\emph{linear subspace}, \emph{vector subspace}, or \emph{linear manifold}.
\end{notn}

\begin{prop} A nonempty subset of $M$ of a vector space $V$ is a subspace of $V$ if and
only if it is closed under addition and scalar multiplication. (That is: if $\vc x$ and
$\vc y$ belong to $M$, so does $\vc x + \vc y$; and if $\vc x$ belongs to $M$ and $\alpha
\in \F$, then $\alpha \vc x$ belongs to~$M$.
\end{prop}

\begin{exam} In each of the following cases prove or disprove that the set of points
$(x,y,z)$ in $\R^3$ satisfying the indicated condition is a subspace of $\R^3$.
   \begin{enumerate}
     \item[(a)] $x + 2y -3z = 4$.
     \item[(b)] $\dfrac{x-1}2 = \dfrac{y+2}3 = \dfrac z4$.
     \item[(c)] $x + y + z = 0$ and $x - y + z = 1$.
     \item[(d)] $x = - z$ and $x = z$.
     \item[(e)] $x^2 + y^2 = z$.
     \item[(f)] $\dfrac x2 = \dfrac{y-3}5$.
   \end{enumerate}
\end{exam}

\begin{prop} Let $\fml M$ be a family of subspaces of a vector space~$V$. Then the
intersection $\bigcap \fml M$ of this family is itself a subspace of~$V$.
\end{prop}

\begin{exer}\label{subspace002} Let $A$ be a nonempty set of vectors in a vector space~$V$.
Explain carefully why it makes sense to say that the intersection of the family of all
subspaces containing $A$ is
 \index{smallest!subspace containing a set of vectors}%
``the smallest subspace of $V$ which contains~$A$''.
\end{exer}

\begin{exer} Find and describe geometrically the smallest subspace of $\R^3$ containing
the vectors $(0,-3,6)$ and $(0,1,-2)$.
\end{exer}

\begin{exer} Find and describe geometrically the smallest subspace of $\R^3$ containing
the vectors $(2,-3,-3)$ and $(0,3,2)$.
\end{exer}

\begin{exam} Let $\R^\infty$ denote the vector space of all sequences of real numbers.
(See example~\ref{vector_space02}.) In each of the following a subset of $\R^\infty$ is
described.  Prove or disprove that it is a subspace of~$\R^\infty$.
   \begin{enumerate}
     \item[(a)] Sequences that have infinitely many zeros
(for example, $(1,1,0,1,1,0,1,1,0,\dots)$).
     \item[(b)] Sequences which are eventually zero.  (A sequence $(x_k)$ is
 \index{eventually!zero}%
\emph{eventually zero} if there is an  index $n_0$ such that $x_n = 0$ whenever $n \ge
n_0$.)
     \item[(c)] Sequences that are absolutely summable.  (A sequence $(x_k)$ is
 \index{absolutely!summable!sequence}%
 \index{summable!absolutely}%
\emph{absolutely summable} if  $\sum_{k=1}^\infty \abs{x_k} < \infty$.)
     \item[(d)] Bounded sequences.  (A sequence $(x_k)$ is
  \index{bounded!sequence}%
  \index{sequence!bounded}%
\emph{bounded} if there is a positive number $M$ such that $\abs{x_k} \le M$ for
every~$k$.)
     \item[(e)] Decreasing sequences.  (A sequence $(x_k)$ is
  \index{decreasing!sequence}%
  \index{sequence!decreasing}%
\emph{decreasing} if $x_{n+1} \le x_n$ for each~$n$.)
     \item[(f)] Convergent sequences. (A sequence $(x_k)$ is
  \index{convergent!sequence}%
  \index{sequence!convergent}%
\emph{convergent} if there is a number $\ell$ such that the sequence is eventually in
every neighborhood of~$\ell$; that is, if there is a number $\ell$ such that for every
$\epsilon >0$ there exists $n_0 \in \N$ such that $\abs{x_n - ell} < \epsilon$ whenever
$n \ge n_0$.)
     \item[(g)] Arithmetic progressions. (A sequence $(x_k)$ is
  \index{arithmetic!progression}%
  \index{progression!arithmetic}%
\emph{arithmetic} if it is of the form $(a, a+k, \\a+2k, a+3k, \dots)$ for some
constant~$k$.)
     \item[(h)] Geometric progressions.  (A sequence $(x_k)$ is
  \index{geometic!progression}%
  \index{progression!geometic}%
\emph{geometric} if it is of the form $(a,ka,k^2a,k^3a, \dots)$ for some constant~$k$.)
   \end{enumerate}
\end{exam}

\begin{notn}\label{subspace001} Here are some frequently encountered families of
functions:
 \begin{align}
   \fml F &= \fml F[a,b]
      = \{f\colon f \text{ is a real valued function on the interval $[a,b]$}\} \\
   \fml P &= \fml P[a,b]
      = \{p\colon  p \text{ is a polynomial function on $[a,b]$}\} \\
   \fml P_4 &= \fml P_4[a,b]
     = \{p \in \fml P\colon  \text{ the degree  of $p$ is less than~4}\} \\
   \fml Q_4 &= \fml Q_4[a,b]
     = \{p \in \fml P\colon  \text{ the degree of $p$ is equal to~4}\} \\
   \fml C &= \fml C[a,b]
     = \{f \in \fml F\colon  f \text{ is continuous}\} \\
   \fml D &= \fml D[a,b]
     = \{f \in \fml F\colon  f \text{ is  differentiable}\}\\
   \fml K &= \fml K[a,b]
     = \{f \in \fml F\colon  f \text{ is a constant function}\} \\
   \fml B &= \fml B[a,b]
     = \{f \in \fml F\colon  f \text{ is bounded}\}   \\
   \fml J &= \fml J[a,b]
     = \{f \in \fml F\colon  f \text{ is integrable}\}
 \end{align}
(A function $f \in \fml F$ is
 \index{bounded!function}%
 \index{function!bounded}%
\df{bounded} if there exists a number $M \ge 0$ such that $\abs{f(x)} \le M$ for all $x$
in $[a,b]$. It is
 \index{integrable!function}%
 \index{function!integrable}%
\df{(Riemann) integrable} if it is bounded and $\int_a^bf(x)\,dx$ exists.)
\end{notn}

\begin{exer} For a fixed interval $[a,b]$, which sets of functions in the
list~\ref{subspace001} are vector subspaces of which?
\end{exer}

\begin{notn} If $A$ and $B$ are subsets of a vector space then the
\df{sum} of $A$ and $B$, denoted by $A + B$, is defined by
   \[ A + B := \{a + b\colon  a \in A \text{ and } b \in B\}. \]
The set $A - B$ is defined similarly.  For a set $\{a\}$ containing a single element we
write $a + B$ instead of $\{a\} + B$.
\end{notn}

\begin{exer} Let $M$ and $N$ be subspaces of a vector space $V$.  Consider the
following subsets of~$V$.
   \begin{enumerate}
     \item $M \cup N$.  (A vector $v$ belongs to $M \cup N$ if it belongs to
either $M$ or~$N$.)
     \item $M + N$.
     \item $M \setminus N$  (A vector $v$ belongs to $M \setminus N$ if it belongs to
$M$ but not to~$N$.)
     \item $M - N$.
   \end{enumerate}
For each of the sets (a)--(d) above, either prove that it \emph{is} a subspace of~$V$ or
give a counterexample to show that it \emph{need not} be a subspace of~$V$.
\end{exer}

\begin{defn}\label{vsp_int_dir_sum} Let $M$ and $N$ be subspaces of a vector space $V$. If
$M \cap N = \{\vc 0\}$ and $M + N = V$, then $V$ is the
 \index{direct sum!internal!for vector spaces}%
 \index{sum!internal direct}%
 \index{<binop@$M \oplus N$ (vector space direct sum of $M$ and~$N$)}%
\df{(internal) direct sum} of $M$ and $N$. In this case we write
   \[ V = M \oplus N. \]
In this case the subspaces $M$ and $N$ are \df{complementary} and each is the
 \index{complement}%
 \index{complementary subspace}%
 \index{subspace!complementary}%
\df{complement} of the other.
\end{defn}

\begin{exam} In $\R^3$ let $M$ be the line $x = y = z$, $N$ be the line $x =
\frac12y = \frac13z$, and $L = M + N$.  Then $L = M \oplus N$.
\end{exam}

\begin{exam} Let $M$ be the plane $x + y + z = 0$ and $N$ be the line $x = y =
z$ in $\R^3$. Then $\R^3 = M \oplus N$.
\end{exam}

\begin{exam} Let $\fml C = \fml C[-1,1]$ be the vector space of all continuous
real valued functions on the interval $[-1,1]$. A function $f$ in $\fml C$ is
 \index{even function}%
\df{even} if $f(-x) = f(x)$ for all $x \in [-1,1]$; it is
 \index{odd function}%
\df{odd} if $f(-x) = -f(x)$ for all $x \in [-1,1]$. Let $\fml C_o = \{f \in \fml C\colon
f \text{ is odd }\}$ and $\fml C_e = \{f \in \fml C\colon  f \text{ is even }\}$. Then
$\fml C = \fml C_o \oplus \fml C_e$.
\end{exam}

\begin{exam} Let $\fml C = \fml C[0,1]$ be the family of continuous real valued
functions on the interval~$[0,1]$.  Define
 \[f_1(t) = t \qquad \text{ and } \qquad f_2(t) = t^4\]
for $0 \le t \le 1$. Let $M$ be the set of all functions of the form $\alpha f_1 + \beta
f_2$ where $\alpha,\beta \in \R$.  And let $N$ be the set of all functions $g$ in $\fml
C$ which satisfy
 \[\int_0^1tg(t)\,dt = 0 \qquad \text{ and }
               \qquad \int_0^1t^4g(t)\,dt =  0.\]
Then $\fml C = M \oplus N$.
\end{exam}

\begin{exer} In the preceding example let $g(t) = t^2$ for $0 \le t \le 1$. Find
polynomials $f \in M$ and $h \in N$ such that $f = g + h$.
\end{exer}

\begin{thm}[Vector Decomposition Theorem]\label{subspace008} Let $V$ be a vector space
 \index{vector!decomposition theorem}%
such that $V~=~M~\oplus~N$. Then for every vector $v \in V$ there exist unique vectors $m
\in M$ and $n \in N$ such that $v = m + n$.
\end{thm}

\begin{exer} Define what it means for a vector space $V$ to be the direct sum of
subspaces $M_1$, \dots, $M_n$. Show (using your definition) that if $V$ is the direct sum
of these subspaces, then for every $v \in V$ there exist unique vectors $m_k \in M_k$
(for $k = 1$, \dots, $n$) such that $v = m_1 + \dots + m_n$.
\end{exer}




























\section{Linear Combinations and Linear Independence}

Some authors of linear algebra texts make it appear as if the terms \emph{linear
dependence} and \emph{linear independence}, \emph{span}, and \emph{basis} pertain only to
\emph{finite} sets of vectors.  This is misleading.  The terms should make sense for
\emph{arbitrary} sets.  In particular, do not be misled into believing that a basis for a
vector space must be a finite set of vectors (or a sequence of vectors).

\begin{defn} A vector $y$ is a
 \index{linear!combination}%
 \index{combination!linear}%
\df{linear combination} of vectors $x_1$, \dots, $x_n$ if there exist scalars $\alpha_1$,
\dots $\alpha_n$ such that $y = \sum_{k=1}^n \alpha_k x_k$. \emph{Note:} linear
combinations \emph{are} finite sums.  The linear combination $\sum_{k=1}^n \alpha_k\vc
x_k$ is
 \index{linear!combination!trivial}%
 \index{trivial!linear combination}%
\df{trivial} if all the coefficients $\alpha_1$, \dots $\alpha_n$ are zero.  If at least
one $\alpha_k$ is different from zero, the linear combination is \df{nontrivial}.
\end{defn}

\begin{exam} In $\R^2$ the vector $(8,2)$ is a linear combination of the vectors $(1,1)$
and $(1,-1)$.
\end{exam}

\begin{exam} In $\R^3$ the vector $(1,2,3)$ is \emph{not} a linear combination of the
vectors $(1,1,0)$ and $(1,-1,0)$.
\end{exam}

\begin{defn}\label{lin_comb001} Let $A$ be a subset of a vector space~$V$.  The
 \index{span}%
\df{span} of $A$ is the intersection of the family of all subspaces of $V$ which
contain~$A$. It is denoted
 \index{span@$\spn(A)$ (the span of $A$)}%
by $\spn(A)$ (or by $\spn_\F(A)$ if we wish to emphasize the role of the scalar
field~$\F$). The subset $A$ \df{spans} the space $V$ if $V = \spn(A)$.  In this case we
also say that $A$ is a \df{spanning set} for~$V$.
\end{defn}

\begin{prop}\label{lin_comb002} If $A$ is a nonempty subset of a vector space~$V$,
then $\spn A$ is the set of all linear combinations of elements of~$A$.
\end{prop}

\begin{rem}\label{lin_comb003} Occasionally one must consider the not-too-interesting
question of what is meant by the \emph{span} of the empty set.  According to the
``abstract'' definition~\ref{lin_comb001} above it is the intersection of all the
subspaces which contain the empty set. That is,
 \index{span!of the empty set}%
$\spn \emptyset = \{\vc 0\}$.  (Had we preferred proposition~\ref{lin_comb002} as a more
``constructive'' definition of \emph{span}---the set of all linear combinations of
elements in $\emptyset$---then the span of the empty set would have been just $\emptyset$
itself.)
\end{rem}

\begin{exam} For each $n = 0$, $1$, $2$, \dots define a function $p_n$ on $\R$ by $p_n(x)
= x^n$.  Let $\fml P$ be the set of polynomial functions on~$\R$. It is a subspace of the
vector space of continuous functions on~$\R$. Then $\fml P = \spn\{p_0, p_1, p_2,
\dots\}$. The exponential function $\exp$, whose value at $x$ is $e^x$, is not in the
span of the set $\{p_0, p_1, p_2 \dots\}$.
\end{exam}

\begin{defn} A subset $A$ (finite or not) of a vector space is
 \index{linearly!dependent}%
 \index{dependent}
\df{linearly dependent} if the zero vector $\vc 0$ can be written as a nontrivial linear
combination of elements of~$A$; that is, if there exist distinct vectors $x_1, \dots, x_n \in A$
and scalars $\alpha_1, \dots, \alpha_n$, \textbf{not all zero,} such that $\sum_{k=1}^n
\alpha_kx_k = \vc 0$. A subset of a vector space is
 \index{linearly!independent}%
 \index{independent}
\df{linearly independent} if it is not linearly dependent.
\end{defn}

Technically, it is a \emph{set} of vectors that is linearly dependent or independent.
Nevertheless, these terms are frequently used as if they were properties of the vectors
themselves.  For instance, if $S = \{x_1, \dots, x_n\}$ is a finite set of vectors in a
vector space, you may see the assertions ``the set $S$ is linearly independent'' and
``the vectors $x_1$, \dots $x_n$ are linearly independent'' used interchangeably.

Supersets of linearly dependent sets are linearly dependent and subsets of linearly
independent sets linearly independent.

\begin{prop} Suppose that $V$ is a vector space and $A \subseteq B \subseteq V$. If $A$
is linearly dependent, then so is~$B$.  Equivalently, if $B$ is linearly independent,
then so is~$A$.
\end{prop}

\begin{exer} Let $w = (1,1,0,0)$, $x = (1,0,1,0)$, $y = (0,0,1,1)$, and $z = (0,1,0,1)$.

\vskip 3 pt
 \begin{enumerate}
  \item[(a)] Show that $\{w,x,y,z\}$ does not span $\R^4$ by finding a vector $u$ in
$\R^4$ such that $u \notin \spn (w,x,y,z)$.

\vskip 3 pt

  \item[(b)] Show that $\{w,x,y,z\}$ is a linearly dependent set of
vectors by finding scalars $\alpha$, $\beta$, $\gamma$, and $\delta$---not all
zero---such that $\alpha w + \beta x +  \gamma y + \delta z = 0$.

\vskip 3 pt

  \item[(c)] Show that $\{w,x,y,z\}$ is a linearly dependent set by writing $z$
as a linear combination of $w$, $x$, and~$y$.
 \end{enumerate}
\end{exer}

\begin{exam} The (vectors going from the origin to) points on the
unit circle in $\R^2$ are linearly dependent.
\end{exam}

\begin{exam} For each $n = 0$, $1$, $2$, \dots define a function
$p_n$ on $\R$ by $p_n(x) = x^n$.  Then the set $\{p_0, p_1, p_2, \dots \}$ is a linearly
independent subset of the vector space of continuous functions on~$\R$.
\end{exam}

\begin{exam} In the vector space $\fml C[0,\pi]$ of continuous functions on the
interval $[0,\pi]$ define the vectors $f$, $g$, and $h$ by
 \begin{align*}
      f(x) &= x      \\
      g(x) &= \sin x \\
      h(x) &= \cos x
 \end{align*}
for $0 \le x \le \pi$. Then $f$, $g$, and $h$ are linearly independent.
\end{exam}

\begin{exam} In the vector space $\fml C[0,\pi]$ of continuous functions on
$[0,\pi]$ let $f$, $g$, $h$, and $j$ be the vectors defined by
 \begin{align*}
      f(x) &= 1         \\
      g(x) &= x         \\
      h(x) &= \cos x    \\
      j(x) &= \cos^2\frac x2
 \end{align*}
for $0 \le x \le \pi$. Then $f$, $g$, $h$, and $j$ are linearly dependent.
\end{exam}

\begin{exer} Let $a$, $b$, and $c$ be distinct real numbers.  Show that the vectors
$(1,1,1)$, $(a,b,c)$, and $(a^2,b^2,c^2)$ form a linearly independent subset of~$\R^3$.
\end{exer}

\begin{exer} In the vector space $\fml C[0,1]$ define the vectors $f$, $g$,
and $h$ by
 \begin{align*}
      f(x) &= x     \\
      g(x) &= e^x   \\
      h(x) &= e^{-x}
 \end{align*}
for $0 \le x \le 1$. Are $f$, $g$, and $h$ linearly independent?
\end{exer}

\begin{exer} Let $u = (\lambda,1,0)$, $v = (1,\lambda,1)$, and $w = (0,1,\lambda)$.
Find \textbf{all} values of $\lambda$ which make $\{u,v,w\}$ a linearly dependent subset
of~$\R^3$.
\end{exer}

\begin{exer} Suppose that $\{u,v,w\}$ is a linearly independent set in a vector space~$V$.
Show that the set $\{u + v, u + w, v + w\}$ is linearly independent in~$V$.
\end{exer}


















\section{Bases for Vector Spaces}
\begin{defn} A set $B$ (finite or not) of vectors in a vector space $V$ is a
 \index{basis}%
\df{basis} for $V$ if it is linearly independent and spans~$B$.
\end{defn}

\begin{exam}The vectors $e^1 = (1,0,0)$, $e^2 = (0,1,0)$, and $e^3 = (0,0,1)$ constitute
an ordered basis for the vector space~$\R^3$. This is the
 \index{standard basis!for $\R^3$}%
 \index{basis!standard!for $\R^3$}%
\df{standard basis} for~$\R^3$.  In elementary calculus texts these vectors are usually
called $\vc i$, $\vc j$, and $\vc k$, respectively.

More generally, in $\R^n$ for $1 \le k \le n$ let $e^k$ be the $n$-tuple which is zero in
every coordinate except the $k^{\text{th}}$ coordinate where is value is~$1$.  Then
$\{e^1,e^2, \dots,e^n\}$ is the
 \index{standard basis!for $\R^n$}%
 \index{basis!standard!for $\R^n$}%
\df{standard basis} for~$\R^n$.
\end{exam}

\begin{exam} The space
 \index{p@$\fml P_n(J)$ (polynomial functions of degree strictly less than~$n$)}%
$\fml P_n(J)$ of polynomial functions of degree strictly less than $n \in \N$ on some
interval $J \subseteq \R$ with nonempty interior is a vector space of dimension~$n$. For
each positive integer $n = 0$, $1$, $2$, \dots let $p_n(t) = t^n$ for all $t \in J$.
 \index{standard basis!for $\fml P_n(J)$}%
Then $\{p_0, p_1, p_2, \dots, p_{n-1}\}$ is a basis for~$\fml P_n(J)$.  We take this to
be the \df{standard basis} for~$\fml P_n(J)$.
\end{exam}

\begin{exam}  The space
 \index{p@$\fml P(J)$ (polynomial functions on $J$)}%
$\fml P(J)$ of polynomial functions on some interval $J \subseteq \R$ with nonempty
interior is an infinite dimensional vector space. For each $n = 0$, $1$, $2$, \dots
define a function $p_n$ on $J$ by $p_n(x) = x^n$. Then the set $\{p_0, p_1, p_2, \dots
\}$ is a basis for the vector space $\fml P(J)$ of polynomial functions on~$J$.
 \index{standard basis!for $\fml P_n(J)$}%
We take this to be the \df{standard basis} for~$\fml P(J)$.
\end{exam}

\begin{exam} Let $\ofml M_{m \times n}$ be the vector space of all $m \times n$ matrices
of real numbers. For $1 \le i \le m$ and $1 \le j \le n$ let $E^{ij}$ be the $m \times n$
matrix whose entry in the $i^{\text{th}}$ row and $j^{\text{th}}$ column is~$1$ and all
of whose other entries are~$0$.  Then $\bigl\{E^{ij}\colon 1 \le i \le m \text{ and } 1
\le j \le n \bigr\}$ is a basis for~$\ofml M_{m \times n}$.
 \index{standard basis!for $\ofml M_{m \times n}$}%
\end{exam}

\begin{exer} A $2 \times 2$ matrix $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ has
\emph{zero trace} if $a + d = 0$. Show that the set of all such matrices is a subspace of $\ofml M_{2 \times 2}$ and find a basis for
it.
\end{exer}

\begin{exer} Let $\fml U$ be the set of all matrices of real numbers of
the form $\begin{bmatrix}
          u & -u-x \\
          0 &   x \end{bmatrix}$ and $\fml V$ be the set of all real
matrices of the form
  $\begin{bmatrix}
          v &  0 \\
          w & -v \end{bmatrix}$. Find bases for $\fml U$, $\fml V$, $\fml U + \fml
V$, and $\fml U \cap \fml V$.
\end{exer}

To show that every nontrivial vector space has a basis.we need to invoke \emph{Zorn's
lemma}, a set theoretic assumption which is equivalent to the \emph{axiom of choice}. To
this end we need to know about such things as partial orderings, chains, and maximal
elements.

\begin{defn} A
 \index{relation}%
\df{relation} on a set $S$ is a subset of the Cartesian product $S \times S$.  If the
relation is denoted by $\le$, then it is conventional to write $x \le y$ (or
equivalently, $y \ge x$) rather than $(x,y) \in \; \le$\,.
\end{defn}

\begin{defn} A relation $\le$ on a set $S$ is
 \index{reflexive}%
\df{reflexive} if $x \le x$ for all $x \in S$.  It is
 \index{transitive!propertty of a partial ordering}%
\df{transitive} if $x \le z$ whenever $x \le y$ and $y \le z$.  It is
 \index{antisymmetric}%
\df{antisymmetric} if $x = y$ whenever $x \le y$ and $y \le x$.  A relation which is
reflexive, transitive, and antisymmetric is a
 \index{partial!ordering}%
 \index{ordered!partially}%
\df{partial ordering}.  A \df{partially ordered set} is a set on which a partial ordering
has been defined.
\end{defn}

\begin{exam} The set $\R$ of real numbers is a partially ordered set under the
usual relation~$\le$.
\end{exam}

\begin{exam} A family $\sfml A$ of subsets of a set $S$ is a partially ordered set
under the relation~$\subseteq$.  When $\sfml A$ is ordered in this fashion it is said to
be
 \index{ordered!by inclusion}%
\df{ordered by inclusion}.
\end{exam}

\begin{exam} Let $\fml F(S)$ be the family of real valued functions defined on a
set~$S$.  For $f$, $g\in \fml F(S)$ write $f \le g$ if $f(x) \le g(x)$ for every $x \in
S$.  This is a partial ordering on~$\fml F(S)$.  It is known as
 \index{ordered!pointwise}%
 \index{pointwise!partial ordering}%
\df{pointwise ordering}.
\end{exam}

\begin{defn} Let $A$ be a subset of a partially ordered set~$S$.  An element $u \in S$
is an
 \index{upper!bound}%
 \index{bound!upper}%
\df{upper bound} for~$A$ if $a \le u$ for every $a~\in~A$.  An element $m$ in the
partially ordered set $S$ is
 \index{maximal}%
\df{maximal} if there is no element of the set which is strictly greater than~$m$; that
is, $m$ is maximal if $c = m$ whenever $c \in S$ and $c \ge m$.  An element $m$ in $S$ is
the
 \index{largest}%
\df{largest} element of $S$ if $m \ge s$ for every $s \in S$.

  Similarly an element $l \in S$
is a
 \index{lower!bound}%
 \index{bound!lower}%
\df{lower bound} for~$A$ if $l \le a$ for every $a~\in~A$.  An element $m$ in the
partially ordered set $S$ is
 \index{minimal}%
\df{minimal} if there is no element of the set which is strictly less than~$m$; that is,
$m$ is minimal if $c = m$ whenever $c \in S$ and $c \le m$.  An element $m$ in $S$ is the
 \index{smallest}%
\df{smallest} element of $S$ if $m \le s$ for every $s \in S$.
\end{defn}

\begin{exam} Let $S = \{a,b,c\}$ be a three-element set.  The family $\sfml P(S)$ of all
subsets of $S$ is partially ordered by inclusion. Then $S$ is the largest element of
$\sfml P(S)$---and, of course, it is also a maximal element of~$\sfml P(S)$.  The family
$\sfml Q(S)$ of all proper subsets of $S$ has no largest element; but it has three
maximal elements $\{b,c\}$, $\{a,c\}$, and $\{a,b\}$.
\end{exam}

\begin{prop} A linearly independent subset of a vector space~$V$ is a basis for~$V$
if and only if it is a maximal linearly independent subset.
\end{prop}

\begin{prop} A spanning subset for a nontrivial vector space~$V$ is a basis for~$V$ if
and only if it is a minimal spanning set for~$V$.
\end{prop}

\begin{defn} Let $S$ be a partially ordered set with partial
ordering~$\le$.
      \begin{enumerate}
       \item Elements $x$ and $y$ in $S$ are
 \index{comparable}%
\df{comparable} if either $x \le y$ or $y \le x$.
       \item If $\le$ is a partial ordering with respect to which any two elements of
$S$ are comparable, it is a
 \index{linear!ordering}%
 \index{ordering!linear}%
\df{linear ordering} (or a
 \index{total!ordering}%
 \index{ordering!total}%
\df{total ordering}) and $S$ is a
 \index{linearly ordered set}%
\df{linearly ordered set}.
       \item A linearly ordered subset of $S$ is a
 \index{chain}%
\df{chain} in~$S$.
      \end{enumerate}
\end{defn}

\begin{ax}[Zorn's lemma]\label{zorn_lemma} A partially ordered set in which
 \index{Zorn's lemma}%
every chain has an upper bound has a maximal element.
\end{ax}

\begin{thm}\label{bases025} Let $A$ be a linearly independent subset of a vector
space~$V$. Then there exists a basis for~$V$ which contains~$A$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Order the set of linearly independent subsets of
$V$ which contain~$A$ by inclusion. Apply \emph{Zorn's lemma}. \ns
\end{proof}

\begin{cor} Every vector space has a basis.
\end{cor}

Note that the empty set is a basis for the trivial vector space. (See
remark~\ref{lin_comb003}.)

\begin{prop}\label{bases028} Let $B$ be a basis for a vector space~$V$.  Every element
of $V$ can be written in a unique way as a linear combination of elements of~$B$.
\end{prop}

\begin{notn}\label{bases111} Let $B$ be a basis for a vector space~$V$ over a field~$\F$
and $x \in V$. By the preceding proposition there exists a unique finite set $S$ of
vectors in $B$ and for each element $e$ in $S$ there exists a unique scalar $x_e$ such
that $x = \sum_{e \in S} x_e e$.  If we agree to let $x_e = 0$ whenever $e \in B
\setminus S$, we can just as well write $x = \sum_{e \in B} x_e e$.  Although this
notation may make it appear as if we are summing over an arbitrary, perhaps uncountable,
set, the fact of the matter is that all but finitely many of the terms are zero.  The
function $x\colon B \sto \F\colon e \mapsto x_e$ has finite support, so no
``convergence'' problems arise. Treat $\sum_{e \in B} x_e e$ as a finite sum.
Associativity and commutativity of addition in $V$ make the expression unambiguous.
\end{notn}

Notice in the preceding that the symbol ``x'' ends up denoting two different things: a
vector in $V$ and a function in $l_c(B,\F)$.  We show in
proposition~\ref{inv_lin_maps005} that this identification is harmless.  It is a good
idea to teach yourself to feel comfortable with identifying these two objects whenever
you are dealing with a vector space \emph{with a basis}.

\begin{notn}\label{bases115} In finite dimensional vector spaces it is usual to
adopt some special notational conventions. Let $V$ be an $n$-dimensional vector space
with an ordered basis $\{e^1, e^2, \dots, e^n\}$.  If $x \in V$, then by
proposition~\ref{bases028} we know that there are unique scalars $x_{e^1}$, $x_{e^2}$,
\dots $x_{e^n}$ such that
  \[ x = \sum_{k=1}^n x_{e^k}\,e^k. \]
The notation can be unambiguously simplified by writing
  \[ x = \sum_{k=1}^n x_k\,e^k. \]
Since the scalars $x_1$, $x_2$, \dots $x_n$ uniquely determine the vector $x$ it has
 \index{conventions!notations for a vector in a finite dimensional space}%
become standard to write
  \[ x = (x_1, x_2, \dots, x_n) \qquad \text{or}
            \qquad x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}. \]
That is, a vector $x$ in a finite dimensional space with an ordered basis may be
represented as an $n$-tuple or as an $n \times 1$-matrix.
 \index{row vector}%
 \index{vector!row}%
 \index{column vector}%
 \index{vector!column}%
The first of these is frequently referred to as a \emph{row vector} and the second as a
\emph{column vector}.
\end{notn}

Next we verify that every subspace has a complementary subspace.

\begin{prop}\label{bases029} Let $M$ be a subspace of a vector space~$V$.  Then there
exists a subspace $N$ of~$V$ such that $V = M \oplus N$.
\end{prop}

\begin{lem}\label{bases030} Let $V$ be a vector space with a finite basis  $\{e^1,
\dots,e^n\}$ and let $v = \sum_{k=1}^n \alpha_k e^k$ be a vector in~$V$.  If $p \in \N_n$
and $\alpha_p \ne 0$, then $\{e^1, \dots,e^{p-1},v,e^{p+1}, \dots, e^n\}$ is a basis
for~$V$.
\end{lem}

\begin{prop}\label{bases031} If some basis for a vector space $V$ contains $n$ elements,
then every linearly independent subset of $V$ with $n$ elements is also a basis.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Suppose $\{e^1,\dots,e^n\}$ is a basis for~$V$ and
$\{v^1, \dots,v^n\}$ is linearly independent in~$V$. Start by using lemma~\ref{bases030}
to show that (after perhaps renumbering the $e^k$'s) the set $\{v^1, e^2, \dots, e^n\}$
is a basis for~$V$.  \ns
\end{proof}

\begin{cor}\label{bases032} If a vector space $V$ has a finite basis $B$, then every basis
for $V$ is finite and contains the same number of elements as~$B$.
\end{cor}

\begin{defn} A vector space is
 \index{finite dimensional}%
\df{finite dimensional} if it has a finite basis and the
 \index{dimension!of a vector space}%
 \index{vector!space!dimension of a}%
 \index{dimV@$\dim V$ (dimension of a vector space $V$)}%
\df{dimension} of the space is the number of elements in this (hence any) basis for the
space. The dimension of a finite dimensional vector space $V$ is denoted by $\dim V$. If
the space does not have a finite basis, it is
 \index{infinite dimensional}%
\df{infinite dimensional}.
\end{defn}

Corollary~\ref{bases032} can be generalized to arbitrary vector spaces.

\begin{thm} If $B$ and $C$ are bases for a vector space $V$, then $B$ and $C$ are
cardinally equivalent; that is, there exists a bijection from $B$ onto~$C$.
\end{thm}

\begin{proof} See \cite{Roman:2005}, page 45, Theorem 1.12. \ns \end{proof}

\begin{prop} Let $V$ be a vector space and suppose that $V = U \oplus W$.  Prove that
if $B$ is a basis for $U$ and $C$ is a basis for $W$, then
$B\mspace{1mu}\cup\mspace{1mu}C$ is a basis for~$V$.  From this conclude that $\dim V =
\dim U + \dim W$.
\end{prop}

\begin{defn}\label{transpose_defn} The
 \index{transpose!of a matrix}%
 \index{matrix!transpose of a}%
\df{transpose} of an $n \times n$ matrix $A = \bigl[a_{ij}\bigr]$ is the matrix
 \index{<at@$A^t$ (transpose of $A$)}%
$A^t = \bigl[a_{ji}\bigr]$ obtained by interchanging the rows and columns of~$A$. The
matrix $A$ is
 \index{symmetric!matrix}%
 \index{matrix!symmetric}%
\df{symmetric} if $A^t = A$.
\end{defn}

\begin{exer} Let $\fml S_3$ be the vector space of all symmetric $3 \times 3$
matrices of real numbers.
 \begin{enumerate}
  \item[(a)] What is the dimension of $\fml S_3$?
  \item[(b)] Find a basis for $\fml S_3$.
 \end{enumerate}
\end{exer}


\endinput
.

\vskip 2 in


\begin{quote}\noindent  It is not essential for the value of an education that every idea be understood
at the time of its accession.  Any person with a genuine intellectual interest and a wealth of
intellectual content acquires much that he only gradually comes to understand fully in the
light of its correlation with other related ideas. \dots Scholarship is a progressive process,
and it is the art of so connecting and recombining individual items of learning by the force
of one's whole character and experience that nothing is left in isolation, and each idea
becomes a commentary on many others.
\end{quote}

\vskip .2 true in

\hskip 2.5 true in     - NORBERT WIENER

% May: Elements of Modern Mathematics, p. xiv

\endinput
