\chapter{BANACH SPACES}

\section{Natural Transformations}
 Recall that if $V$ is a normed linear space its
 \index{dual!space}%
 \index{space!dual}%
 \index{<unopurstar@$V^*$ (dual space of a normed linear space)}%
\emph{dual space} $V^*$ is the set of all continuous linear functionals on~$V$.  On $V^*$ addition and
scalar multiplication are defined pointwise. The norm is the usual operator norm: if $f \in V^*$, then
 \[ \norm{f} := \inf\{M > 0\colon \abs{f(x)} \le M\norm{x}\quad \text{for all $x \in V$}\} = \sup\{\abs{f(x)}\colon \norm{x} \le 1\}. \]
Under the metric induced by this norm the dual space $V^*$ is complete and thus is a Banach space.

\begin{defn}\label{defn_nls_adjoint} If $T\colon V \sto W$ is a continuous linear map between normed linear spaces,
 \index{<unopurstar@$T^*$ (normed linear space adjoint)}%
$T^*\colon W^* \sto V^*$ is defined as for vector spaces: $T^*(g) = g\,T$ for every
$g \in W^*$.  The map $T^*$, is called the
 \index{adjoint!of a linear map!between normed linear spaces}%
 \index{linear!map!adjoint of a}%
\df{adjoint} of~$T$.
\end{defn}

\begin{exam}\label{exam_dual_functor} The adjoint $T^*$ of a bounded linear map between normed linear spaces $V$ and 
$W$ is itself a bounded linear map between $W^*$ and~$V^*$. In the category of normed linear spaces and continuous linear
 \index{functor!duality}%
 \index{duality functor}%
maps the pair of maps $V \mapsto V^*$ (object map) and $T \mapsto T^*$ (morphism map) is a contravariant
functor from $\cat{NLS_\infty}$ to itself.
\end{exam}

Notice that the preceding definition~\ref{defn_nls_adjoint} differs from the definition of the adjoint of a mapping
between Hilbert spaces (see~\ref{def000926}).  Since every Hilbert space is a normed linear space, this could conceivably
generate confusion.  It is worthwhile convincing oneself of a simple fact: because of the anti-isomorphism (and consequent
isomorphism) between a Hilbert space and its dual provided by the \emph{Riesz-Fr\'echet theorem} (see~\ref{0022057}
and~\ref{cor_Hsp_iso_dual}) the normed linear space adjoint of a bounded linear map is \emph{very} similar (although
certainly not identical) to its Hilbert space adjoint.

\begin{exam}\label{C057717}
The dual $V^{**}$ of the dual $V^*$ of a normed linear space $V$ is the \df{second dual} of~$V$
 \index{dual!second}%
and $T^{**} := \bigl(T^*\bigr)^*$ is a bounded linear map of $V^{**}$ into~$W^{**}$.  On the category
$\cat{BAN_{\boldsymbol\infty}}$ of Banach spaces and bounded linear transformations the
 \index{second!dual!functor for normed linear spaces}%
 \index{functor!second dual!for normed linear spaces}%
\df{second dual functor} comprises the object map $B \mapsto B^{**}$ and the morphism map $T \mapsto T^{**}$.
It is a covariant functor.
\end{exam}

\begin{defn}\label{defn_nat_transf} Let $\cat A$ and $\cat B$ be categories and $F$,$G \colon \cat A \sto \cat B$ be
covariant functors.  A
 \index{natural!transformation}%
 \index{transformation!natural}%
\df{natural transformation} from $F$ to $G$ is a map $\tau$ which assigns to each object
$A$ in $\cat A$ a morphism $\tau_A \colon F(A) \sto G(A)$ in $\cat B$ in such a way that
for every morphism $\alpha \colon A \sto A'$ in $\cat A$ the following diagram commutes.
   \[ \CD
      F(A) @>  F(\alpha)>>  F(A')  \\
         @V\tau_AVV                @VV\tau_{A'}V  \\
      G(A) @>> G(\alpha)>  G(A')
   \endCD \]
We denote such a transformation by $\tau \colon F \sto G$. (The definition of a natural
transformation between two contravariant functors should be obvious: just reverse the
horizontal arrows in the preceding diagram.

A natural transformation $\tau \colon  F \sto G$ is a
 \index{natural!equivalence}%
 \index{equivalence!natural}%
\df{natural equivalence} if each morphism $\tau_A$ is an isomorphism in~$\cat B$.
\end{defn}

\begin{defn}\label{defn_nat_embed} If $V$ is a normed linear space, then the map $\sbsb{\tau}V \colon V \sto V^{**} \colon x
\mapsto x^{**}$ where $x^{**}(f) = f(x)$ for all $f \in V^*$ is called the
 \index{tauv@$\sbsb{\tau}V$ (natural embedding of normed linear spaces)}%
 \index{natural!embedding!for normed linear spaces}%
 \index{embedding!natural}%
\df{natural embedding} of $V$ into~$V^{**}$. (It is not altogether obvious that this mapping is
injective. We will show that it is in proposition~\ref{C067137}.)
\end{defn}

\begin{exam}\label{exam_2dual_nat} In the category $\cat{BAN_{\boldsymbol\infty}}$ of Banach spaces
and bounded linear transformations let $I$ be the identity functor and $(\,\cdot\,)^{**}$ be the
second dual functor.  Then the
 \index{natural!embedding!as a functor}%
 \index{functor!the natural embedding as a}%
 \index{taub@$\tau_B$ (natural embedding of $B$ into $B^{**}$)}%
\emph{natural embedding} $\tau$ is a natural transformation from $I$ to~$(\,\cdot\,)^{**}$.
\end{exam}

\begin{exam} Explain exactly what is meant when people say that the isomorphism
    \[ \fml C(X \uplus Y) \cong \fml C(X) \times \fml C(Y) \]
is a natural equivalence.  The categories involved are $\cat{CpH}$ (compact Hausdorff
spaces and continuous maps) and $\cat{BAN_\infty}$ (Banach spaces and bounded linear
transformations).
\end{exam}

\begin{exam}\label{C057724} On the category $\cat{CpH}$ of compact Hausdorff spaces and
continuous maps let $I$ be the identity functor and let $\fml C^*$ be the functor which
takes a compact Hausdorff space $X$ to the Banach space $\fml (C(X))^*$ and takes a
continuous function $\phi\colon X \sto Y$ between compact Hausdorff spaces to the
contractive linear transformation $\fml C^*\phi = (\fml C\phi)^*$. Define the
 \index{evaluation!map}%
\emph{evaluation map} $E$ by
  \[ E_X\colon X \sto \fml C^*(X)\colon x \mapsto E_X(x) \]
where $(E_X(x))(f) = f(x)$ for all $f \in \fml C(X)$.  (This notation can be cumbersome.  In practice, one usually writes
just $E_x$ for $E_X(x)$.  Thus we have $E_x(f) = f(x)$, which certainly looks better than $(E_X(x))(f) = f(x)$.  In most
contexts it seems unlikely that confusion will result from this simplification of notation.)
 \begin{enumerate}
  \item[(a)]  Notice that this definition does make sense. If $x \in X$, then $E_X(x)$ really
does belong to~$\fml C^*(X)$.
  \item[(b)] Show that the map $E$ makes the following diagram commute
     \[ \xymatrix@+30pt{{X}\ar[r]^*+{\phi} \ar[d]_*+{E_X}
            & {Y} \ar[d]^*+{E_Y} \\
          {\fml C^*(X)} \ar[r]_*+{\fml C^*(\phi)} & {\fml C^*(Y)}   }
     \]
 \end{enumerate}
\end{exam}

The preceding example may seem a bit disappointing.  It seems very much as if we are
attempting to establish a natural transformation between the identity functor (on
$\cat{CpH}$) and the $\fml C^*$ functor.  The problem, of course, is that the two
functors have different target categories.  This is a question we will deal with later.
The evaluation map $E_X\colon X \sto \fml C^*(X)$ is certainly not surjective.
Notice, for example, that every functional of the form $E_X(x)$
preserves multiplication (as well as addition and scalar multiplication). A consequence
of this turns out to be that such functionals all live on the unit sphere of $\fml C^*(X)$.
With a new topology (the so-called \emph{weak-star topology}---see~\ref{C067434}) on
$\fml C^*(X)$ the range of $E_X$ turns out to be a compact Hausdorff space which is naturally
equivalent to $X$ itself.  Even more remarkably, we will eventually show that the range of $E_X$
can be regarded as the maximal ideal space of the algebra~$\fml C(X)$. (See~\ref{000731}.)

\begin{prop}\label{C067137} If $V$ is a normed linear space the natural embedding
$\sbsb{\tau}V \colon V \sto V^{**}\colon x \mapsto x^{**}$ (defined in~\ref{defn_nat_embed})
is an isometric linear map.
\end{prop}

\begin{defn} A Banach space $B$ is
 \index{reflexive!Banach space}%
\df{reflexive} if the natural embedding $\sbsb{\tau}B\colon B \sto B^{**}$ (defined in the preceding proposition) is onto.
\end{defn}

Example~\ref{exam_2dual_nat} shows that in the category of Banach spaces and bounded linear
maps, the mapping $\tau\colon B \mapsto \sbsb{\tau}B$ is a natural transformation from the
identity functor to the second dual functor.  Invoking the \emph{Hahn-Banach theorem} we can
say more.

\begin{prop} In the category of reflexive Banach spaces and bounded linear maps the mapping
$\tau\colon B \mapsto \sbsb{\tau}B$ is a natural equivalence between the identity and second
dual functors.
\end{prop}

\begin{exam} Every Hilbert space is reflexive.
\end{exam}

\begin{exer} Let $V$ and $W$ be normed linear spaces, $U$ be a nonempty convex subset of $V$,
and $f \colon U \sto W$.  Without using any form of the \emph{mean value theorem} show
that if $df = \vc 0$ on $U$, then $f$ is constant on~$U$.  (For the definition of $df$, the
\emph{differential} of~$f$, see sections 13.1--2 in my notes~\cite{Erdman:2007}.)
\end{exer}

\begin{exam}\label{C067181} Let $l_\infty$ be the Banach space of all bounded sequences of
real numbers and $c$ be the subspace of $l_\infty$ comprising all sequences $x = (x_n)$
such that $\lim_{n \sto \infty} x_n$ exists. Define
  \[ f\colon c \sto \R\colon x \mapsto \lim_{n\sto \infty} x_n. \]
Then $f$ is a continuous linear functional on $c$ and $\norm f = 1$.  By the
\emph{Hahn-Banach theorem} $f$ has an extension to $\wh f \in {l_\infty}^*$ with
$\norm{\wh f} = 1$. For every
subset $A$ of $\N$ define a sequence $\bigl(x^A_n\bigr)$ by $x^A_n = \left\{%
\begin{array}{ll}
    1, & \hbox{if $n \in A$;} \\
    0, & \hbox{if $n \notin A$.} \\
\end{array}%
\right.$ Next define $ \mu\colon \sfml P(\N) \sto \R \colon A \mapsto \wh
f\bigl(x^A_n\bigr)$. Then $\mu$ is a finitely additive set function but is not countably
additive.
\end{exam}












\section{Alaoglu's Theorem}
\begin{notn} Let $V$ be a normed linear space, $M \subseteq V$, and $F \subseteq V^\ast$.
Then we define
 \begin{align*}
         M^\perp &:= \{f \in V^*\colon f(x)=0 \text{ for all $x \in M$}\} \\
         F_\perp &:= \{x \in V\colon f(x)=0 \text{ for all $f \in F$}\}
  \end{align*}
One may read $M^\perp$ as ``M perp'' or ``M upper perp''; and $F_\perp$ is ``F perp'' or ``F lower perp''.
The set $M^\perp$ is the
 \index{annihilator}%
 \index{<unopur@$M^\perp$ (annihilator of a set)}%
\df{annihilator} of~$M$ and $F_\perp$ is the
 \index{pre-annihilator}%
 \index{<unopvr@$F_\perp$ (pre-annihilator of a set)}%
\df{pre-annihilator} of~$F$.
\end{notn}

\begin{prop}[Annihilators]  Let $B$ be a Banach space, $M \subseteq B$, and $F \subseteq B^*$. Then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item If $M \subseteq N \subseteq B$, then $N^\perp \subseteq M^\perp$.
  \item If $F \subseteq G \subseteq B^*$, then $G_\perp \subseteq F_\perp$.
  \item $M^\perp$ is a closed linear subspace of $B^*$.
  \item $F_\perp$ is a closed linear subspace of $B$.
  \item $M \subseteq M^\perp{}_\perp$.
  \item $F \subseteq F_\perp{}^\perp$.
  \item $M^\perp{}_\perp = \bigvee M$.
  \item $M^\perp = B^*$ if and only if $M = \{0\}$.
  \item $F_\perp = B$ if and only if $F = \{0\}$.
  \item If $M$ is a linear subspace of $B$, then $M^\perp = \{0\}$ if and only if $M$ is
dense in $B$.
  \item\label{Fpp_reflexive_case} If $B$ is reflexive, then $F_\perp{}^\perp = \bigvee F$.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] For (k) show that $\tau(F_\perp) = F^\perp$ (where $\tau$ is the
natural embedding) and that therefore $F_\perp{}^\perp = F^\perp{}_\perp$.  \ns
\end{proof}

\begin{exam} Let $B$ be a Banach space that is \emph{not} reflexive.  Let $F = \ran \tau$ (where $\tau$ is the
natural embedding).  Then $F_\perp{}^\perp = B^{**} \ne \bigvee F$, so it is clear that~\ref{Fpp_reflexive_case}
in the preceding proposition need not hold in the nonreflexive case.
\end{exam}

The next result is the Banach space analog of the \emph{fundamental theorem of linear algebra}~\ref{00016025} and
proposition~\ref{prop_ker_adj_perp_ran_op}.

\begin{prop}\label{prop_FTLA_Bsp} Let $T \in \ofml B(B,C)$ where $B$ and $C$ are Banach spaces. Then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $\ker T^\ast = (\ran T)^\perp$.
  \item $\ker T = (\ran T^*)_\perp$.
\vskip 3 pt
  \item $\clo{\ran T} = (\ker T^*)_\perp$.
\vskip 3 pt
  \item $\clo{\ran T^*} \subseteq (\ker T)^\perp$.
 \end{enumerate}
\end{prop}

\begin{exam} An example of a Banach space operator $T$ for which $\clo{\ran T^*}$ and $(\ker T)^\perp$ are different
is given in exercise 7, page 243 of~\cite{BrownP:1970}.
\end{exam}

The following example is of course a trivial consequence of proposition~\ref{00015032}, but try to give a simple
direct proof.

\begin{exam}\label{X_l2ball_not_cpt} The closed unit ball in $l_2$ is not compact.
\end{exam}

The usual norm topology on normed linear spaces turns out to be much too large for some applications.  As we have seen
it has so many open sets that the closed unit ball is not compact (except in the finite dimensional case). On the
other hand, too few open sets (the indiscrete topology, as an extreme example) would destroy duality theory by failing
to provide a plentiful supply of continuous linear functionals.  The so-called \emph{weak-star topology} (defined
below) on the dual of a normed linear space turns out to be just the right size.  It has enough open sets to guarantee
a vigorous duality theory and few enough open sets to make the closed unit ball compact
(see \emph{Alaoglu's theorem}~\ref{C067441}).

\begin{defn}\label{C067434} Let $V$ be a normed linear space. The
 \index{weak!topology!on normed spaces}%
 \index{topology!weak!on normed linear spaces}%
\df{weak topology} on $V$ is the weak topology induced by the elements of~$V^*$. (So, of
course, the weak topology on the dual space $V^*$ is the weak topology induced by the
elements of~$V^{**}$.) When a net $(x_\lambda)$ converges weakly to a vector $a$ in $V$
we write
 \index{<arrowright@$x_\lambda \to^w a$ (weak convergence)}%
$x_\lambda \to^w a$. The
 \index{w*@$w^*$-topology (weak star topology)}%
 \index{topology!weak star}%
\df{$w^*$-topology} (pronounced \emph{weak star topology}) on the dual space $V^*$ is the
weak topology induced by the elements of $\ran \tau$ where $\tau$ is the natural
injection of $V$ into~$V^{**}$.  When a net $(f_\lambda)$ converges weakly to a vector $g$
in $V^*$ we write
 \index{<arrowright@$f_\lambda \to^{w^*} g$ (weak star convergence)}%
$f_\lambda \to^{w^*} g$. Notice that when a Banach space $B$ is reflexive (in particular, for Hilbert
spaces) the weak and weak star topologies on $B^*$ are identical.
\end{defn}

\begin{prop} Let $f \in V^*$ where $V$ is a Banach space.  For every $\epsilon > 0$ and every
finite subset $F \subseteq V$ define
  \[ U(f;F;\epsilon)
           = \{g \in V^*\colon \abs{f(x) - g(x)} < \epsilon \text{ for all $x \in F$}\}. \]
The family of all such sets is a base for the $w^*$-topology on~$V^*$.
\end{prop}

The next result, \emph{Alaoglu's theorem}, asserting the $w^*$-compactness of the closed unit ball in the
dual of a normed linear space $V$ is one of the most useful theorems in functional analysis.  Thus
at first it may seem somewhat surprising that it is so hard to find a complete and carefully explained
proof of this result.  The standard proof is in a sense very simple; it is basically just recognizing
that the members of a family $\fml F$ of scalar valued functions can be regarded either
  \begin{enumerate}
     \item[(i)] as functions defined on an uncountable product of compact subsets of the scalar field, or alternatively
     \item[(ii)] as members of the closed unit ball of~$V^*$.
  \end{enumerate}
The crux of the proof is convincing yourself that the convergence of a net of such functions in the product space
is exactly ``the same thing'' as its $w^*$-convergence in the closed unit ball of~$V^*$.  Trying simultaneously to
identify and distinguish between two sets of functions is notationally challenging.  So my best advice is to work through
it yourself until you see how ingenious, but basically simple, the argument is.

\begin{thm}[Alaoglu's theorem]\label{C067441} The closed unit ball of a normed linear space is
 \index{Alaoglu's theorem}%
compact in the $w^*$-topology.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Let $V$ be a normed linear space and denote by $[V^*]_1$ the closed unit ball of its
dual space.  For each $x \in V$ define $D_x = \{\lambda \in \K \colon \abs\lambda \le \norm x\}$.  The product
\smash[b]{$\mathbf P = \prod\limits_{x \in V} D_x$} is compact by \emph{Tychonoff's theorem}.  Consider the function
   \[ \Phi\colon [V^*]_1 \sto \mathbf P \colon f \mapsto \bigl(f(x)\bigr)_{x \in V} \]
where $[V^*]_1$ has the $w^*$-topology and $\mathbf P$ has the product topology (see~\ref{C021437}).  Here we have written
$\bigl(f(x)\bigr)_{x \in V}$ for the indexed family of numbers (or generalized sequence) rather than the more usual
$\bigl(f_x\bigr)_{x \in V}$.  (For more on indexed families see~\cite{Erdman:2007}, section~7.2.)  It is easy to see that
$\Phi$ is injective.  To show that it is continuous let $\bigl(f_\lambda\bigr)_{\lambda \in \Lambda}$ be a net in $[V^*]_1$
and $g$ be a function in $[V^*]_1$ such that $f_\lambda \to^{w^*} g$.  Notice that this happens if and only if
$\pi_x(f_\lambda) \sto \pi_x(g)$ for every $x \in V$ (where $\pi_x$ is a coordinate projection on~$\mathbf P$---see~\ref{C015531}).
Finally, let $\bigl(f_\lambda\bigr)_{\lambda \in \Lambda}$ be a net in $[V^*]_1$ such that $\bigl(\Phi(f_\lambda)\bigr)_{\lambda \in \Lambda}$
converges in~$\mathbf P$.  Show that the net $\bigl(f_\lambda(x)\bigr)_{\lambda \in \Lambda}$ converges in $\K$ for every $x \in V$.
Let $g(x) = \lim\limits_\lambda f_\lambda(x)$ for all $x \in V$ and prove that $g$ is linear, that $\norm g \le 1$, that
$f_\lambda \to^{w^*} g$ in $[V^*]_1$, and that $\Phi(f_\lambda) \sto \Phi(g)$ in~$\mathbf P$.  \ns
\end{proof}

To the best of my knowledge the most reader-friendly version of this standard proof can be found in~\cite{BrownP:1977}, Theorem 15.11.
A much shorter, much easier notationally, much slicker, and much less edifying proof is available in~\cite{Pedersen:1995}, theorem 2.5.2.
I find it less edifying because of its reliance on universal nets.  A net in a set $S$ is
 \index{universal!net}%
 \index{net!universal}%
\df{universal} if for every subset $T$ of $S$ the net is either eventually in $T$ or eventually in its complement. Despite the fact that
every net can be shown to have a universal subnet, even to find a singe nontrivial example of a universal net (that is, one that is not
eventually constant) requires the \emph{axiom of choice}.



















\section{The Open Mapping Theorem}\label{C0694}
A question one can (and should) ask of any concrete category is whether every bijective morphism is necessarily an
isomorphism.  In some categories the answer is trivially \emph{yes} (for example, in $\cat{VEC}$).  In other
categories the answer is trivially \emph{no} (in $\cat{TOP}$, for example, consider the identity map from the
reals with the discrete topology to the reals with their usual topology).  Frequently in categories which arise in
studying analysis, algebra is tugging enthusiastically in one direction while topology is vigorously resisting, pulling
in the other.  Here the question can become quite fascinating---and the answer definitely nontrivial.  In the category
$\cat{BAN_\infty}$ of Banach spaces and continuous linear maps, the question turns out to be intriguingly deep.  In this
case algebra prevails; the answer is affirmative.  It turns out to be a direct consequence of the celebrated \emph{open
mapping theorem}.  Notice as you work through the following proof of this result that for the map in question
completeness of domain and completeness of codomain are both essential---and for quite different reasons.

\begin{defn} A mapping $f \colon X \sto Y$ between topological spaces is
 \index{open!mapping}%
 \index{mapping!open}%
\df{open} if it takes open sets to open sets; that is, $f$ is an open mapping provided that $f^{\sto}(U)$ is open in $Y$
whenever $U$ is open in~$X$.
\end{defn}

The \emph{open mapping theorem} says that continuous linear surjections between Banach spaces are always open mappings.
There is one technical detail that makes the proof of this result a bit tricky.  Suppose that $T$ is a continuous linear
map from a Banach space $B$ into a normed linear space~$V$ and $R$ is the image under $T$ of an open ball around the
origin in~$B$.  We will need to know that if the closure of $R$ contains some open ball about the origin in $V$, then
$R$ itself contains the same ball. It will be convenient to isolate this crucial bit of information in the next proposition.

\begin{notn} In a normed linear space $V$ we denote by
 \index{<unop@$(V)_r$ (open ball of radius $r$ about the origin)}%
$(V)_r$ the open ball about the origin in $V$ of radius~$r$.
\end{notn}

\begin{prop}\label{prop_for_OMT} Let $T\colon B \sto V$ be a continuous linear map from a Banach space into a normed linear space.
If $r$, $s > 0$ and $(V)_s \subseteq \clo{T\bigl(\,(B)_r\,\bigr)}$, then $(V)_s \subseteq T\bigl(\,(B)_r\,\bigr)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] First verify that without loss of generality we may assume that $r = s = 1$.  Thus
our hypothesis is that $(V)_1 \subseteq \clo{T\bigl(\,(B)_1\,\bigr)}$.

Let $z \in (V)_1$.  The proof is complete if we can show that $z \in T\bigl(\,(B)_1\,\bigr)$.  Choose $\delta > 0$ such
that $\norm z < 1 - \delta$ and let $y = (1 - \delta)^{-1}z$.  Construct a sequence $(v_k)_{k=0}^\infty$ of vectors in~$V$
which satisfy
   \begin{enumerate}
     \item[(i)] $\norm{y - v_k} < \delta^k$ for $k = 0,1,2,\dots$ and
     \item[(ii)] $v_k - v_{k-1} \in T\bigl(\,\delta^{k-1}(B)_1\,\bigr)$ for $k = 1,2,3,\dots$.
   \end{enumerate}
To this end, start with $v_0 = \vc 0$. Use the hypothesis to conclude that $y - v_0 \in \clo{T\bigl(\,(B)_1\,\bigr)}$ and
that therefore there is a vector $w_0 \in T\bigl(\,(B)_1\,\bigr)$ such that $\norm{(y - v_0) - w_o} < \delta$.  Let
$v_1 = v_0 + w_0$ and check that (i) and (ii) hold for $k = 1$.

Then find $w_1 \in T\bigl(\,\delta (B)_1\,\bigr)$ so that $\norm{(y - v_1) - w_1} < \delta^2$ and let $v_2 = v_1 + w_1$.
Check that (i) and (ii) hold for $k= 2$.  Proceed inductively.

For each $n \in \N$ choose a vector $u_n \in \delta^{n-1}(B)_1$ such that $T(u_n) = v_n - v_{n-1}$. Show that the sequence
$(u_n)$ is summable (use\ref{prop_abs_sum_implies_sum}) and let $x = \sum_{n=1}^\infty u_n$.  Conclude the proof by showing
that $Tx = y$  and that $z \in T\bigl(\,(B)_1\,\bigr)$.   \ns
\end{proof}

\begin{thm}[Open mapping theorem]\label{C069414} Every bounded linear surjection between
 \index{open!mapping!theorem}%
 \index{bijective morphisms!are invertible!in $\cat{BAN_1}$}%
Banach spaces is an open mapping.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Suppose that $T\colon B \sto C$ is a bounded linear surjection between Banach spaces.
Start by verifying the following claim: All we need to show is that $T\bigl((B)_1\bigr)$ contains an open ball about the
origin in~$C$.  Suppose for the moment we know that such a ball exists.  Call it~$W$.  Given an open subset $U$ of $B$, we
wish to show that its image under $T$ is open in~$C$.  To this end take an arbitrary point $y$ in $T(U)$ and choose an $x$
in $U$ such that $y = Tx$.  Since $U - x$ is a neighborhood of the origin in $B$, we can find a number $r > 0$ such that
$r\,(B)_1 \subseteq U - x$.  Verify that $T(U)$ is open by proving that $y + rW \subseteq T(U)$.

To establish the claim above, let $V = T\bigl((B)_1\bigr)$ and notice that since $T$ is surjective, $C = T(B) =
\bigcup_{k=1}^\infty k\clo V$.  Use the version of the \emph{Baire category theorem} which says that if a nonempty
complete metric space is the union of a sequence of closed sets, then at least one of these sets has nonempty interior.
(See, for example, Theorem 29.3.21 in\cite{Erdman:2007}.)  Choose $k \in \N$ so that $\bigl(k\clo V\bigr)^o \neq \emptyset$.
Thus there exists $y \in \clo V$ and $r > 0$ such that $y + (C)_r \subseteq k\clo V$.  It is not hard to see from the
convexity of $k\clo V$ that $(C)_r \subseteq k\clo V$.  (Write an arbitrary point $z$ in $(C)_r$ as
$\frac12(y + z) + \frac12(-y + z)$.) Now you have $(C)_{r/k} \subseteq \clo V$.  Use proposition~\ref{prop_for_OMT}.
\ns \end{proof}

\begin{cor}\label{C069417} Every bounded linear bijection between Banach spaces is an
isomorphism.
\end{cor}

\begin{cor} Every bounded linear surjection between Banach spaces
 \index{BAN@$\cat{BAN_\infty}$!quotients in}%
 \index{quotient!map!in $\cat{BAN_\infty}$}%
is a quotient map in~$\cat{BAN_\infty}$.
\end{cor}

\begin{exam} Let $\fml C_u$ be the set of continuous real valued functions on $[0,1]$ under
the uniform norm and $\fml C_1$ be the same set of functions under the $\mathrm{L_1}$ norm
(\,$\norm{f}_1 = \int_0^1\abs{f}\,d\lambda$\,). Then the identity map $I\colon \fml C_u
\sto \fml C_1$ is a continuous linear bijection but is not open. (Why does this not
contradict the \emph{open mapping theorem}?)
\end{exam}

\begin{exam} Let $l$ be the set of sequences of real numbers that are eventually zero,
$V$ be $l$ equipped with the $l_1$ norm, and $W$ be $l$ with the uniform norm.  The
identity map $I\colon V \sto W$ is a bounded linear bijection but is not an isomorphism.
\end{exam}

\begin{exam} The mapping $T\colon \R^2 \sto \R\colon (x,y) \mapsto x$ shows that although
bounded linear surjections between Banach spaces map open sets to open sets they need not
map closed sets to closed sets.
\end{exam}

\begin{prop} Let $V$ be a vector space and suppose that $\norm{\cdot}_1$ and
$\norm{\cdot}_2$ are norms on~$V$ whose corresponding topologies are $\ofml T_1$
and~$\ofml T_2$.  If $V$ is complete with respect to both norms and if $\ofml T_1
\supseteq \ofml T_2$, then $\ofml T_1 = \ofml T_2$.
\end{prop}

\begin{exer} Does there exist a sequence $(a_n)$ of complex numbers satisfying the
following condition: a sequence $(x_n)$ in $\C$ is absolutely summable if and only if
$(a_nx_n)$ is bounded? \emph{Hint.} Consider $ T\colon l_\infty \sto l_1\colon (x_n)
\mapsto (x_n/a_n)$.
\end{exer}

\begin{exam}\label{C069431} Let $\fml C^2([a,b])$ be the family of all twice continuously
differentiable real valued functions on the interval $[a,b]$. Regard it as a vector space
in the usual fashion. For each $f \in \fml C^2([a,b])$ define
    \[ \norm{f} = \norm{f}_u + \norm{f'}_u + \norm{f''}_u\,. \]
This is in fact a norm and under this norm $\fml C^2([a,b])$ is a Banach space.
\end{exam}

\begin{exam}\label{C069432} Let $\fml C^2([a,b])$ be the Banach space given in
exercise~\ref{C069431}, and $p_0$ and $p_1$ be members of $\fml C([a,b])$.   Define
  \[T\colon C^2([a,b]) \sto C([a,b])\colon
                      f \mapsto f'' + p_1f' + p_0f.\]
Then $T$ is a bounded linear map.
\end{exam}

\begin{exer} Consider a differential equation of the form
   \[ y'' + p_1\,y' + p_0\,y = q  \tag{$\ast$} \]
where $p_0$, $p_1$, and $q$ are (fixed) continuous real valued functions on the interval
$[a,b]$.  Make precise, and prove, the assertion that \emph{the solutions to $(\ast)$
depend continuously on the initial values}.  You may use without proof a standard
theorem: For every point $c$ in $[a,b]$ and every pair of real numbers $a_0$ and $a_1$,
there exists a unique solution of $(\ast)$ such that $y(c) = a_0$ and $y'(c) = a_1$.
\emph{Hint.} For a fixed $c \in [a,b]$ consider the map
  \[ S\colon \fml C^2([a,b]) \sto \fml C([a,b]) \times \R^2 \colon
                                        f \mapsto \bigl(Tf,f(c),f'(c)\bigr) \]
where $\fml C^2([a,b])$ is the Banach space of example~\ref{C069431} and $T$ is the
bounded linear map of example~\ref{C069432}.
\end{exer}

\begin{defn} An bounded linear map  $T\colon V \sto W$ between normed linear spaces  is
 \index{bounded!away from zero}%
\df{bounded away from zero} (or
 \index{bounded!below}%
\df{bounded below}) if there exists a number $\delta > 0$ such that $\norm{Tx} \ge
\delta\norm{x}$ for all $x \in V$. (Recall that the word ``bounded'' means one thing when
modifying a linear map and and something quite different when applied to a general
function; so does the expression ``bounded away from zero''. See
definition~\ref{def_bdd_away_zero}.)
\end{defn}

\begin{prop} A bounded linear map between Banach spaces is bounded away from zero if and
only if it has closed range and zero kernel.
\end{prop}




















\section{The Closed Graph Theorem}
A simple necessary and sufficient condition for a linear map $T\colon B \sto C$ between Banach spaces
to be continuous is that its graph be closed.  In one direction this is almost obvious.  The converse is
called the \emph{closed graph theorem}.  When we ask for the graph of $T$ to be
 \index{graph!closed}%
``closed'' we mean closed as a subset of $B \times C$ with the product topology.  Recall (from~\ref{exam_prod_top_norm})
that the topology on the direct sum $B \oplus C$ induced by the product norm is the same as the product topology on $B \times C$.

\begin{exer} If $T\colon B \sto C$ is a continuous linear map between Banach spaces, then its graph is closed.
\end{exer}

\begin{thm}[Closed graph theorem]\label{thm_CGT} Let $T\colon B \sto C$ be a linear map between Banach
 \index{closed!graph theorem}%
spaces.  If the graph of $T$ is closed, then $T$ is continuous.
\end{thm}

\begin{proof}[Hint for proof] Let $G = \{(x,Tx)\colon x \in B\}$ be the graph of~$T$. Apply
(corollary~\ref{C069417} of) the \emph{open mapping theorem} to the map
  \[ \pi\colon G \sto B\colon (x,Tx) \mapsto x\,. \]
\ns \end{proof}

The next proposition gives a very simple necessary and sufficient condition for the graph of a linear map between Banach
spaces to be closed.

\begin{prop}\label{prop_nasc_closed_graph} Let $T\colon B \sto C$ be a linear mapping between Banach spaces. Then $T$ is
continuous if and only if the following condition is satisfied:
   \[ \text{if $x_n \sto 0$ in $B$ and $Tx_n \sto c$ in $C$, then $c = 0$.} \]
\end{prop}

\begin{prop} Suppose $T \colon B \sto C$ is a linear map between Banach spaces such that if
$x_n \sto 0$ in $B$ then $(g \circ T)(x_n) \sto 0$ for every $g \in C^*$.  Then $T$ is
bounded.
\end{prop}

\begin{prop} Let $T \colon B \sto C$ be a linear function between Banach spaces. Define
$T^*f = f \circ T$ for every $f \in C^*$.  If $T^*$ maps $C^*$ into $B^*$, then $T$ is
continuous.
\end{prop}

\begin{defn}\label{C069624} A linear map $T$ from a vector space into itself is
 \index{idempotent}%
\df{idempotent} if $T^2 = T$.
\end{defn}

\begin{prop}\label{prop_cont_idem_op} An idempotent linear map from a Banach space into itself is
continuous if and only if it has closed kernel and range.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For the ``if'' part, use proposition~\ref{prop_nasc_closed_graph}.  \ns
\end{proof}

The next proposition is a useful application of the \emph{closed graph theorem} to Hilbert space operators.

\begin{prop} Let $H$ be a Hilbert space and $S$ and $T$ be functions from $H$ into itself such that
    \[ \langle Sx,y \rangle = \langle x,Ty \rangle \]
for all $x$, $y \in H$.  Then $S$ and $T$ are bounded linear operators.
\end{prop}

\begin{proof}[\emph{Hint for proof}] To verify the linearity of the map $S$ consider the inner product of the vectors
$S(\alpha x + \beta y) - \alpha Sx - \beta Sy$ and $z$, where $x$, $y$, $z \in H$ and $\alpha$, $\beta \in \K$.
To establish the continuity of $S$, show that its graph is closed.  \ns
\end{proof}

Recall that in the preceding proposition the operator $T$ is the \emph{(Hilbert space) adjoint} of $S$;
that is, $T = S^*$.

\begin{defn} Let $a < b$ in~$\R$. A function $f\colon [a,b] \sto \R$ is
 \index{continuously differentiable}%
 \index{differentiable!continuously}%
continuously differentiable if it is differentiable on (an open set containing) $[a,b]$
and its derivative $f\,'$ is continuous on~$[a,b]$.  The set of all continuously
differentiable real valued functions on $[a,b]$ is denoted by $\fml C^1(\,[a,b]\,)$.
\end{defn}

\begin{exam} Let $D\colon \fml C^1(\,[0,1]\,) \sto C(\,[0,1]\,)\colon f \mapsto f\,'$ where
both $C^1(\,[0,1]\,)$ and $C(\,[0,1]\,)$ are equipped with the uniform norm. The mapping
$D$ is linear and has closed graph but is not continuous.  (Why does this not contradict
the \emph{closed graph theorem}?  What can we conclude about~$\fml C^1(\,[0,1]\,)$?)
\end{exam}

\begin{prop} Suppose that $S\colon A \sto B$ and $T\colon B \sto C$ are linear maps between
Banach spaces and that $T$ is bounded and injective.  Then $S$ is bounded if and only if $TS$ is.
\end{prop}

















\section{Banach Space Duality}

\begin{conv}\label{conv_subsp_Bsp} In the context of Banach spaces we adopt the same convention for the use of the word
``subspace'' we used for Hilbert spaces.  The word ``subspace'' will always mean
 \index{subspace!of a Banach space}%
 \index{conventions!subspaces of Banach spaces are closed}%
\emph{closed vector subspace}.  To indicate that $M$ is a subspace of a Banach space $B$ we write
 \index{<binrelle@$\preccurlyeq$ (subspace of a Banach or Hilbert space)}%
$M \preccurlyeq B$.
\end{conv}

\begin{prop} If $M$ is a subspace of a Banach space $B$,
 \index{quotient!Banach space}%
 \index{Banach!space!quotient}%
 \index{space!quotient!Banach}%
then the quotient normed linear space $B/M$ is also a Banach space.
\end{prop}

\begin{proof}[\emph{Hint for proof}] To show that $B/M$ is complete it suffices to show that from an
arbitrary Cauchy sequence $(u_n)$ in $B/M$ we can extract a convergent subsequence.  To this end
notice that if $(u_n)$ is Cauchy, then for every $k \in \N$ there exists $N_k \in \N$ such that
$\norm{u_n - u_m} < 2^{-k}$ whenever $m$, $n \ge N_k$.  Now use induction. Choose $n_1 = N_1$.
Having chosen $n_1$, $n_2$, \dots , $n_k$ so that $n_1 < n_2 < \dots < n_k$ and $N_j \le n_j$ for
$1 \le j \le k$, choose $n_{k+1} = \max\{N_{k+1}, n_k + 1\}$.  This results in a subsequence
$\bigl(u_{n_k}\bigr)$ satisfying $\norm{u_{n_k} - u_{n_{k+1}}} < 2^{-k}$ for all $k \in \N$.
From each equivalence class $u_{n_k}$ choose a vector $x_k$ in $B$ so that $\norm{x_k - x_{k+1}} < 2^{-k+1}$.
Then proposition~\ref{prop_abs_sum_implies_sum} shows that the sequence $(x_k - x_{k+1})$ is
summable.  From this conclude that the sequences $(x_k)$ and $\bigl(u_{n_k}\bigr)$ converge.
(The details of this argument are nicely written out in the proof of theorem 3.4.13 in~\cite{BrownP:1970}.) \ns
\end{proof}

\begin{prop} If $J$ is a closed ideal in a Banach algebra,
 \index{quotient!Banach algebra}%
 \index{Banach!algebra!quotient}%
 \index{algebra!quotient!Banach}%
then $A/J$ is a Banach algebra.
\end{prop}

\begin{thm}[Fundamental quotient theorem for $\cat{BALG}$] Let $A$ and $B$ be Banach algebras
 \index{BALG@$\cat{BALG}$!quotients in}%
 \index{quotient!theorem!for $\cat{BALG}$}%
and $J$ be a proper closed ideal in~$A$. If $\phi$ is a homomorphism from $A$ to $B$ and
$\ker\phi \supseteq J$, then there exists a unique homomorphism $\wt\phi\colon A/J \sto
B$ which makes the following diagram commute.
   \[ \xymatrix@+30pt{A \ar[d]_*+{\pi} \ar[dr]^*+{\phi} & \\
                           A/J \ar[r]_*+{\widetilde\phi} & B  }\]
Furthermore, $\wt\phi$ is injective if and only if $\ker\phi = J$; and $\wt\phi$ is
surjective if and only if $\phi$ is.
\end{thm}

\begin{prop}\label{C037821} Let $J$ be a proper closed ideal in a unital commutative Banach
algebra~$A$.  Then $J$ is maximal if and only if $A/J$ is a field.
\end{prop}

The next proposition specifies quite simple conditions under which a epimorphism between Banach
spaces turns out to be a factor of other morphisms.  At first glance the usefulness of such a
result may seem a bit doubtful.  When you prove proposition~\ref{00151231} notice, however, that
this is \emph{exactly} what you need.

\begin{prop}\label{prop_surj_as_factor} Consider the category of Banach spaces
and continuous linear maps.  If $T\colon A \sto C$, $R\colon A \sto B$, $R$ is
surjective, and $\ker R \subseteq \ker T$, then there exists a unique
$S\colon B \sto  C$ such that $SR = T$. Furthermore, $S$ is injective if and
only if $\ker R = \ker T$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider the following commutative diagram.
Use the \emph{open mapping theorem}.
 \[\xy
    \scalefactor{1.4}%
    \Atrianglepair/>`>`>`<-`>/[A`B`A/\ker R`C;R`\pi`T`\wt R`\wt T]
 \endxy\]   \ns
\end{proof}

It is interesting to note that it is possible to state the preceding result in such a way that
the uniqueness claim is strengthened slightly.  As stated, the conclusion is that there is a
unique bounded linear map $S$ such that $SR = T$.  With no extra work we could have rephrased
the proposition as follows.

\begin{prop}\label{prop_surj_as_factor2} Consider the category of Banach spaces
and continuous linear maps.  If $T\colon A \sto C$, $R\colon A \sto B$, $R$ is
surjective, and $\ker R \subseteq \ker T$, then there exists a unique function
$S$ mapping $B$ into $C$ such that $S\circ R = T$.  The function $S$ must be
bounded and linear.  Furthermore, it is injective if and only if $\ker R = \ker T$.
\end{prop}

Is it worthwhile stating the fussier, second version of the result? It depends, I think, on
where one is headed.  It doesn't seem attractive to opt for the longer version solely on the
grounds that it is more general.  On the other hand, suppose that one later needs the following result.

\begin{prop}\label{prop_factor_blm} If a bounded linear map $T\colon A \sto C$ between Banach spaces can be factored
into the composite of two functions $T = f \circ R$ where $R \in \ofml B(A,B)$ is surjective,
then $f \colon B \sto C$ is bounded and linear.
\end{prop}

\begin{proof}[\emph{Hint for proof}] All that is needed here in order to apply~\ref{prop_surj_as_factor2}
is to notice that $f(0) = 0$ and therefore $\ker R \subseteq \ker T$.   \ns
\end{proof}

Since the preceding is an almost obvious corollary of~\ref{prop_surj_as_factor2}, it is nice to have
this fussier version available.  I suppose that one might argue that while~\ref{prop_factor_blm} may not be
entirely obvious from the statement of~\ref{prop_surj_as_factor}, it is certainly obvious from its
proof, so why bother with the more complicated version?  Such things are surely a matter of taste.

\begin{defn}\label{defn_Bsp_exact_seq}  A sequence of Banach spaces and continuous linear maps
   \[\xymatrix{{\cdots}\ar[r] & B_{n-1}\ar[r]^{T_n}
          & B_n\ar[r]^{T_{n+1}} & B_{n+1}\ar[r] & {\cdots}
   }\]
is said to be
 \index{sequence!exact}%
 \index{exact!sequence}%
\df{exact at} $B_n$ if $\ran T_n = \ker T_{n+1}$. A sequence is \df{exact} if it is
exact at each of its constituent Banach spaces.  A sequence of Banach spaces and
continuous linear maps of the form
   \begin{equation}\label{001500202i}\xymatrix{
      0 \ar[r] & A \ar[r]^S & B\ar[r]^T & C\ar[r] & 0
   }\end{equation}
 \index{sequence!short exact}%
 \index{short exact sequence}%
which is exact at $A$, $B$, and $C$ is a \df{short exact sequence}.  (Here $\vc 0$ denotes the trivial zero dimensional
Banach space, and the unlabeled arrows are the obvious maps.)
\end{defn}

The preceding definitions were for the
 \index{BAN@$\cat{BAN_\infty}$!the category}%
 \index{category!$\cat{BAN_\infty}$ as a}%
category $\cat{BAN_\infty}$ of Banach spaces and continuous linear maps.  Of course the
notion of an \emph{exact sequence} makes sense in many situations---for example, in the
categories of Banach algebras, $C^*$-algebras, Hilbert spaces, vector spaces, Abelian
groups, and modules, among others.

\begin{prop}\label{00151231} Consider the following diagram in the category of Banach spaces
and continuous linear maps
 \[\xymatrix{
      0\ar[r] & A\ar[d]^f\ar[r]^j & B\ar[d]^g\ar[r]^k
              & C\ar@{-->}[d]^h \ar[r] & 0 \\
      0\ar[r] & A'\ar[r]_{j'} & B'\ar[r]_{k'}
      & C'\ar[r] & 0
 }\]
If the rows are exact and the left square commutes, then there exists a unique
continuous linear map $h\colon C \sto C'$ which makes the right square commute.
\end{prop}

\begin{prop}[The Five Lemma]\label{00151232}
 \index{five lemma}%
Suppose that in the following diagram of Banach spaces and continuous linear maps
 \[\xymatrix{
      0\ar[r] & A\ar[d]^f\ar[r]^j & B\ar[d]^g\ar[r]^k
              & C\ar[d]^h \ar[r] & 0 \\
      0\ar[r] & A'\ar[r]_{j'} & B'\ar[r]_{k'}
      & C'\ar[r] & 0
 }\]
the rows are exact and the squares commute.  Then the following hold.
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item If $g$ is surjective, so is $h$.
  \item If $f$ is surjective and $\ran j \supseteq \ker g$, then $h$ is injective.
  \item If $f$ and $h$ are surjective, so is $g$.
  \item If $f$ and $h$ are injective, so is $g$.
 \end{enumerate}
\end{prop}

There are many advantages to working with Hilbert or Banach space operators having closed range.
One immediate advantage is that we recapture the simplicity of the \emph{fundamental theorem of linear algebra}
(\ref{00016025}) from the somewhat more complex versions given in proposition~\ref{prop_ker_adj_perp_ran_op}
and theorem~\ref{prop_FTLA_Bsp}.  On the other hand there are disadvantages.  They do not form a category; the composite
of two closed range operators may fail to have closed range.

\begin{prop} If a bounded linear map $T\colon B \sto C$ between two Banach spaces has closed range, the so does its
adjoint~$T^*$ and $\ran T^* = (\ker T)^\perp$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Conclude from proposition~\ref{prop_FTLA_Bsp}(d) that one need only show that
$(\ker T)^\perp \subseteq \ran T^*$.  Let $f$ be an arbitrary functional in $(\ker T)^\perp$. What must be found is a
functional $g$ in $C^*$ so that $f = T^*g$.  To this end use the \emph{open mapping theorem} to show that the map $\wt T_\ast$
in the following diagram is invertible. (The maps $T_\ast\colon B \sto \ran T$ and $T$ are exactly the same, except that their
codomains differ.)

 \[\xy
    \scalefactor{1.4}%
    \Atrianglepair/>`>`>`<-`>/[B`\ran T`B/\ker T`\K;T_\ast`\pi`f`\wt{T_\ast}`\wt f]
 \endxy\]

Let $g_0 := \wt f\,{\wt T_\ast}^{-1}$. Use the \emph{Hahn-Banach theorem} to produce the desired~$g$.   \ns
\end{proof}

\begin{prop} In the category $\cat{BAN_\infty}$ if the sequence $\xymatrix{A \ar[r]^S & B \ar[r]^T & C}$ is exact at $B$
and $T$ has closed range, then $\xymatrix{C^* \ar[r]^{T^*} & B^* \ar[r]^{S^*} & A^*}$ is exact at~$B^*$.
\end{prop}

\begin{defn} A functor from the category of Banach spaces and continuous linear maps into itself is
 \index{exact!functor}%
 \index{functor!exact}%
\df{exact} if it takes short exact sequences to short exact sequences.
\end{defn}

\begin{exam}\label{exam_dualfunctor_exact} The functor $B \mapsto B^*$, $T \mapsto T^*$ (see~\ref{exam_dual_functor})
from the category of Banach spaces and continuous linear maps into itself is exact.
\end{exam}

\begin{exam} Let $B$ be a Banach space and $\sbsb{\tau}B\colon x \mapsto x^{**}$ be the natural embedding from $B$ into~$B^{**}$
(see definition~\ref{defn_nat_embed}). Then the quotient space $B^{**}/\ran\sbsb{\tau}B$ is a Banach space.
\end{exam}

\begin{notn} We will denote by $\clo B$ the quotient
space $B^{**}/\ran \sbsb{\tau}B$ in the preceding example.
\end{notn}

\begin{prop} If $T\colon B \sto C$ is a continuous linear map between Banach spaces, then there exists a unique continuous linear map
$\clo T\colon \clo B \sto \clo C$ such that $\clo T(\phi + \ran\sbsb{\tau}B) = (T^{**}\phi) + \ran\sbsb{\tau}C$ for all $\phi \in B^{**}$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Apply~\ref{00151231} to the following diagram:
 \begin{equation}\label{eqn_exactCD_Bbar}\xymatrix{
      0\ar[r] & B\ar[d]^T\ar[r]^{\sbsb{\tau}B} & B^{**}\ar[d]^{T^{**}}\ar[r]^{\sbsb{\pi}B}
              & \clo B\ar@{-->}[d]^{\clo T} \ar[r] & 0 \\
      0\ar[r] & C\ar[r]_{\sbsb{\tau}C} & C^{**}\ar[r]_{\sbsb{\pi}C} & \clo C\ar[r] & 0
 }\end{equation}  \ns
\end{proof}

\begin{exam}\label{exam_bar_functor} The pair of maps $B \mapsto \clo B$, $T \mapsto \clo T$ from the 
category $\cat{BAN_\infty}$ into itself is an
 \index{<unopu@$B \mapsto \clo B$, $T \mapsto \clo T$ (the functor)}%
 \index{functor!from a Banach space $B$ to $\clo B$}%
exact covariant functor.
\end{exam}

\begin{prop} In the category of Banach spaces and continuous linear maps, if
the sequence
 \[\begin{CD}
  0 \longrightarrow A @> S >> B @> T >> C \longrightarrow 0
  \end{CD}\]
is exact, then the following diagram commutes and has exact rows
and columns.
 \[\bfig
    \iiixiii{'7777}[A`A^{**}`\overline A`B`B^{**}`\overline B`C`C^{**}`\overline C;%
``````S`{S^{**}}`{\overline S}`T`{T^{**}}`{\overline T}]
  \efig\]
\end{prop}

\begin{cor} Every subspace of a reflexive Banach space is reflexive.
\end{cor}

\begin{cor} If $M$ is a subspace of a reflexive Banach space~$B$, then $B/M$ is reflexive.
\end{cor}

\begin{cor} Let $M$ be a subspace of a Banach space~$B$.  If both $M$ and $B/M$ are reflexive,
then so is~$B$.
\end{cor}




















\begin{defn} Let $T \in \ofml L(B,C)$, where $B$ and $C$ are Banach spaces.  Define
$\coker T$, the
 \index{cokernel}%
 \index{cokernel@$\coker T$ (the cokernel of $T$)}%
\df{cokernel} of~$T$, to be $C/\ran T$.
\end{defn}



Unmasking the identity of the dual of a Banach space is frequently a bit challenging.  Following are a few
important examples.  First, however, a word of caution.  Notation for objects living in normed linear spaces
of sequences can be treacherous.  In dealing with completeness of such spaces,one deals with sequences of
sequences of complex numbers.  It is a kindness towards the reader (and likely to yourself a month later
when you try to read something you wrote!) to distinguish notationally between sequences of complex numbers
and sequences of sequences. There are may ways of accomplishing this: double subscripts, subscripts and
superscripts, functional notation for complex sequences, \emph{etc.}

Another problematic notation that arises in sequence spaces is $\sum_{k=1}^\infty a_k \vc e^k$, where $\vc e^k$
is the sequence which is $1$ in the $k^{\text{th}}$ coordinate and $0$ everywhere else. (Recall that in
example~\ref{C063149} we used these vectors for the usual (or standard) orthonormal basis for the Hilbert
space~$l_2$.)  There appears to be little confusion that results from regarding $\sum_{k=1}^\infty a_k \vc e^k$
simply as shorthand for the sequence $(a_k) = (a_1,a_2,a_3,\dots)$ of complex numbers.  If, however, one wishes
to treat it literally as an infinite series, then care is needed.  In the spaces $\sbsb c0$ and $l_1$, there is
no problem. It is indeed the limit as $n$ gets large of $\sum_{k=1}^n a_k \vc e^k$.  (Why?)  But in the spaces
$c$ and $l_\infty$, for example, things go horribly wrong.  To see what can happen, consider the constant sequence
$(1,1,1,\dots)$ (which belongs to both $c$ and $l_\infty$). Then
$\lim_{n \sto\infty}\norm{(1,1,1,\dots) - \sum_{k=1}^n \vc e^k}$ fails to approach zero in either case.
In $c$ the limit is $1$ and in $l_1$ it is~$\infty$.

\begin{exam}\label{exam_dual_C0} The normed linear space $\sbsb c0$ (see~\ref{C0231303})
 \index{c@$c_0$!as a Banach space}%
 \index{Banach!space!$c_0$ as a}%
 \index{l@$l_1$!as the dual of $c_0$}%
 \index{dual!of $c_0$}%
is a Banach space and its dual is $l_1$ (see~\ref{C023191}.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Consider the map
$T\colon {\sbsb c0}^* \sto l_1\colon f \mapsto \sum_{k=1}^\infty f(\vc e^k)\vc e^k$.
(Don't forget to show that $T$ really maps into~$l_1$.) Verify that $T$ is an isometric isomorphism.
To prove that $T$ is surjective, start with a vector $b \in l_1$ and consider the function
$f\colon \sbsb c0 \sto \C\colon a \mapsto \sum_{k=1}^\infty a_kb_k$.    \ns
\end{proof}

\begin{cor} The space $l_1$ is a
 \index{l@$l_1$!as a Banach space}%
 \index{Banach!space!$l_1$ as a}%
Banach space.
\end{cor}

It is interesting that, as it turns out, the spaces $\sbsb c0$ and $c$ are not isometrically isomorphic,
even though their dual spaces are.  The computation of the dual of $c$ is technically slightly fussier than
the one for~${\sbsb c0}^{\!*}$.  It may be helpful to consider the \emph{Schauder bases} for these two spaces.

\begin{defn} A sequence $(\vc e^k)$ of vectors in a Banach space is a
 \index{Schauder basis}%
 \index{basis!Schauder}%
\df{Schauder basis} for the space if for every $x \in B$ there exists a unique sequence $(\alpha_k)$ of
scalars such that $x = \sum_{k=1}^\infty \alpha_k \vc e^k$.
\end{defn}

Two comments about the preceding definition: The word ``unique'' is very important; and it is not true that
every Banach space has a Schauder basis.

\begin{exam} An orthonormal basis in a separable Hilbert space is a Schauder basis for the space.  (See
propositions~\ref{prop_unique_onbasis} and~\ref{prop_Schauder_basis_Hsp}.)
\end{exam}

\begin{exam}\label{exam_dual_c0} We choose the
 \index{usual!Schauder basis for~$\sbsb c0$}%
 \index{Schauder!basis!usual (or standard) for~$\sbsb c0$}%
 \index{basis!standard (or usual)}%
\df{standard} (or
 \index{standard!Schauder basis for $\sbsb c0$}%
\df{usual}) \df{basis vectors} for $\sbsb c0$ in the same way as for~$l_2$ (see~\ref{C063149}).
For each $n \in \N$ let $\vc e^n$ be the sequence in $l_2$ whose $n^{\text{th}}$ coordinate is $1$
and all the other coordinates are~$0$. Then $\{\vc e^n\colon n \in \N\}$ is a Schauder basis
for~$\sbsb c0$.
\end{exam}

At first glance the space $c$ of all convergent sequences may seem enormously larger than $\sbsb c0$,
which contains only those sequences which converge to zero.  After all, corresponding to each vector
$x$ in $\sbsb c0$ there are uncountably many distinct vectors in $c$ (just add any nonzero complex
number to each term of the sequence~$x$).  Is it possible for $c$, containing, as it does, uncountably
many copies of the uncountable collection of members of~$\sbsb c0$, to have a (necessarily countable)
Schauder basis?  The answer, upon reflection, is \emph{yes}.  All we need to do is add a single element
to the basis for~$\sbsb c0$ to deal with the translations.

\begin{exam} Let $\vc e^1$, $\vc e^2$, \dots be as in the preceding example~\ref{exam_dual_c0}.  Also
denote by $\vc 1$ the constant sequence $(1,1,1,\dots)$ in~$c$. Then $(\vc 1, \vc e^1, \vc e^2, \dots)$
is a Schauder basis for~$c$.
\end{exam}

\begin{exam}\label{exam_dual_c} The normed linear space $c$ (see~\ref{C0231302})
 \index{c@$c$!as a Banach space}%
 \index{Banach!space!$c$ as a}%
 \index{l@$l_1$!as the dual of $c$}%
 \index{dual!of $c$}%
is a Banach space and its dual is $l_1$ (see~\ref{C023191}).
\end{exam}

\begin{proof}[\emph{Hint for proof}] Observe initially that for every $f \in c^*$ the sequence
$\bigl(f(\vc e^k)\bigr)$ is absolutely summable and that as a consequence $\sum_{k=1}^\infty a_k f(\vc e^k)$
converges for every sequence $a = (a_1,a_2,\dots)$ in~$c$.  Also note that each $a \in c$ can be written as
$a = a_\infty \vc 1 + \sum_{k=1}^\infty (a_k -a_\infty)\vc e^k$ where $a_\infty := \lim_{k \sto \infty}a_k$
and let $\vc 1 := (1,1,1,\dots)$.  Use this to prove that for every $f \in c^*$
   \[ \abs{f(a)} \le \biggl[ \abs{\beta_f} + \sum_{k=1}^\infty \abs{f(\vc e^k)} \biggr] {\norm a}_\infty \]
where $\beta_f := f(\vc 1) - \sum_{k=1}^\infty f(\vc e^k)$.  For $t = (t_0,t_1,t_2,\dots) \in l_1$ and
$a = (a_1,a_2,a_3,\dots) \in c$ define
   \[ S_t(a) := t_0 a_\infty + \sum_{k=1}^\infty t_ka_k \]
and verify that $S\colon t \mapsto S_t$ is a continuous linear map from $l_1$ into~$c^*$.
Next, for $f \in c^*$ define
   \[ Tf := (\beta_f, f(\vc e^1), f(\vc e^2), \dots) \]
and show that $T\colon f \mapsto Tf$ is a linear isometry from $c^*$ into~$l_1$. Now in order to see that $c^*$ and
$l_1$ are isometrically isomorphic all that is needed is to show that $ST = \id{c^*}$ and $TS = \id{l_1}$.  \ns
\end{proof}











\begin{defn} A topological space is
 \index{locally!compact}%
 \index{compact!locally}%
\df{locally compact} if every point of the space has a compact neighborhood.
\end{defn}

\begin{notn} Let $X$ be a locally compact space.  A scalar valued function $f$ on $X$ belongs to $\fml C_0(X)$ if
 \index{C@$\fml C_0(X)$!space of continuous functions which vanish at infinity}%
it is continuous and vanishes at infinity (that is, if for every number $\epsilon > 0$ there exists a compact subset
of $X$ outside of which $\abs{f(x)} < \epsilon$).
\end{notn}

\begin{exam}\label{exam_C0X} If $X$ is a locally compact Hausdorff space, then, under pointwise algebraic operations and the uniform norm,
 \index{Banach!space!$\fml C_0(X)$ as a}%
 \index{C@$\fml C_0(X)$!as a Banach space}%
$\fml C_0(X)$ is a Banach space.
\end{exam}

We recall one of the most important theorems from real analysis.

\begin{thm}[Riesz representation]\label{C057951} Let $X$ be a locally compact Hausdorff
 \index{Riesz!representation theorem}%
space.  If $\phi$ is a bounded linear functional on $\fml C_0(X)$, then there exists a
unique complex regular Borel measure $\mu$ on $X$ such that
  \[ \phi(f) = \int_X f\,d\mu \]
for all $f \in \fml C_0(X)$ and $\norm\mu = \norm\phi$.
\end{thm}

\begin{proof} This theorem and the following corollary are nicely presented in Appendix C of Conway's
functional analysis text~\cite{Conway:1990}. See also~\cite{Folland:1984}, theorem 7.17; \cite{HewittS:1965},
theorems 20.47 and 20.48; \cite{McDonaldW:1999}, theorem 9.16;and \cite{Rudin:1987}, theorem 6.19.  \ns
\end{proof}

\begin{cor}\label{C057952} If $X$ is a locally compact Hausdorff space, then $\bigl(\fml C_0(X)\,\bigr)^*$
 \index{dual!of $\fml C_0(X)$}%
 \index{C@$\fml C_0(X)$!dual of}%
 \index{M@$\textbf{M}(X)$!as the dual of $\fml C_0(X)$}%
is isometrically isomorphic to~$\textbf{M}(X)$,
 \index{M@$\textbf{M}(X)$!regular complex Borel measures on~$X$}%
the Banach space of all regular complex Borel measures on~$X$.
\end{cor}

\begin{cor} If $X$ is a locally compact Hausdorff space, then $\textbf{M}(X)$
 \index{M@$\textbf{M}(X)$!as a Banach space}%
 \index{Banach!space!$\textbf{M}(X)$ as a}%
is a Banach space.
\end{cor}










In~\ref{defn_nls_adjoint} we defined the adjoint of a linear operator between Banach
spaces and in~\ref{def000926} we defined the adjoint of a Hilbert space
operator. In the case of an operator on a Hilbert space (which is also a Banach space)
it is natural to ask what relationship there is, if any, between these two ``adjoints''.
They certainly can not be equal since the former acts between the dual spaces and the latter
between the original spaces.  In the next proposition we make use of the anti-isomorphism
$\psi$ defined in~\ref{000341} (see~\ref{0022057}) to demonstrate that the two adjoints are
``essentially'' the same.

\begin{prop} Let $T$ be an operator on a Hilbert space $H$ and $\psi$ be the
anti-isomorphism defined in~\ref{000341}.  If we denote (temporarily) the Banach space
dual of $T$ by $T'\colon H^* \sto H^*$, then $T' = \psi\, T^* \psi^{-1}$.  That is, the
following diagram commutes.
   \[ \xy
          \Square/{->}`{<-}`{<-}`{->}/[H^*`H^*`H`H;T'`\psi`\psi`T^*]
      \endxy \]
\end{prop}

\begin{defn}\label{0022091} Let $A$ be a subset of a Banach space $B$.  Then the
 \index{annihilator}%
\df{annihilator} of $A$, denoted
 \index{<perp@$A^\perp$ (annihilator of a set)}%
by~$A^\perp$, is $\{f \in B^*\colon f(a) = 0 \text{ for every $a \in A$}\}$.
\end{defn}

It is possible for the conflict in notations between \ref{0001511} and \ref{0022091} to
cause confusion. To see that in a Hilbert space the annihilator of a subspace and the
orthogonal complement of the subspace are ``essentially'' the same thing, use the
isometric anti-isomorphism $\psi$ between $H$ and $H^*$ discussed in~\ref{0022057} to identify them.

\begin{prop} Let $M$ be a subspace of a Hilbert space $H$. If we (temporarily) denote the
annihilator of $M$ by~$M^a$, then $M^a = \psi^{\sto}\bigl(M^\perp\bigr)$.
\end{prop}
















\section{Projections and Complemented Subspaces}
\begin{defn}
 A bounded linear map from a Banach space into itself is a
 \index{projection!in a Banach space}%
 \index{operator!projection}%
\df{projection} if it is idempotent.  If $E$ is such a map, we say that it is a
projection \df{along $\ker E$ onto $\ran E$}.
\end{defn}

Notice that this is most definitely \emph{not} the same thing as a projection in a
Hilbert space.  Hilbert space projections are \emph{orthogonal projections}; that is,
they are self-adjoint as well as idempotent.  The range of a Hilbert space projection
is perpendicular to its kernel.  Since the terms ``orthogonal'', ``self-adjoint'', and
``perpendicular'' make no sense in Banach spaces, we would not expect the two notions
of ``projection'' to be identical.  In both cases, however, the projections \emph{are}
bounded, linear, and idempotent.

\begin{defn}  Let $M$ be a closed linear subspace of a Banach space~$B$.  If there exists a
closed linear subspace $N$ of $B$ such that $B = M \oplus N$, then we say that $M$ is a
 \index{complemented subspace}%
 \index{subspace!complemented}%
\df{complemented} subspace, that $N$ is its
 \index{complement!of a Banach subspace}%
\df{(Banach space) complement}, and that the subspaces $M$ and $N$ are
\df{complementary}.
\end{defn}

\begin{prop}\label{C069814} Let $M$ and $N$ be complementary subspaces of a Banach space~$B$. Clearly, each
element in $B$ can be written uniquely in the form $m + n$ for some $m \in M$ and $n \in
N$. Define a mapping $E \colon B \sto B$ by $E(m + n) = m$ for $m \in M$ and $n \in N$.
Then $E$ is the projection along $N$ onto~$M$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use~\ref{prop_cont_idem_op}.  \ns
\end{proof}

\begin{prop}\label{C069817} If $E$ is a projection on a Banach space $B$, then its kernel and range are
complementary subspaces of~$B$.
\end{prop}

The following is an obvious corollary of the two preceding propositions

\begin{cor}\label{cor_compl_ran_proj} A subspace of a Banach space is complemented if and only if it is
the range of a projection.
\end{cor}

\begin{prop} Let $B$ be a Banach space.  If $E \colon B \sto B$ is a projection onto a subspace
$M$ along a subspace $N$, then $I - E$ is a projection onto $N$ along~$M$.
\end{prop}

\begin{prop}\label{C069824} If $M$ and $N$ are complementary subspaces of a Banach space $B$, then $B/M$ and
$N$ are isomorphic.
\end{prop}

\begin{prop}\label{prop_linv_Bsp} Let $T \colon B \sto C$ be a bounded linear map between Banach spaces.
 \index{left!inverse!of a Banach space operator}%
If $T$ is injective and $\ran T$ is complemented, then $T$ has a left inverse.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For this hint we introduce a (somewhat fussy) notation.
If $f\colon X \sto Y$ is a function between two sets, we define
$f_\bullet \colon X \sto \ran f\colon x \mapsto f(x)$.  That is, $f_\bullet$ is exactly the same
function as $f$, except that its codomain is the range of~$f$.

Since $\ran T$ is complemented there exists a projection $E$ from $C$ onto~$\ran T$
(by~\ref{cor_compl_ran_proj}).  Show that $T_\bullet$ has a continuous inverse.
Let $S = {T_\bullet}^{-1}E_\bullet$.    \ns
\end{proof}

\begin{prop}\label{prop_rinv_Bsp} Let $S \colon C \sto B$ be a bounded linear map between Banach spaces.
 \index{right!inverse!of a Banach space operator}%
If $S$ is surjective and $\ker S$ is complemented, then $S$ has a right inverse.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Since $\ker S$ is complemented there exists $N \preceq C$ such that
$C = N \oplus \ker S$. Show that the restriction of $S$ to $N$ has a continuous inverse and compose this
inverse with the inclusion mapping of $N$ into~$C$.   \ns
\end{proof}

The next result serves as a converse to both propositions~\ref{prop_linv_Bsp} and~\ref{prop_rinv_Bsp}.

\begin{prop}\label{C069831} Let $T \colon B \sto C$ and $S\colon C \sto B$ be bounded linear maps between Banach
spaces.  If $ST = \id B$, then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item[(a)] $S$ is surjective;
  \item[(b)] $T$ is injective;
  \item[(c)] $TS$ is a projection along $\ker S$ onto~$\ran T$; and
  \item[(d)] $C = \ran T \oplus \ker S$.
 \end{enumerate}
\end{prop}

\begin{prop} If in the category $\cat{BAN_\infty}$ the sequences
 \[ \begin{CD} 0 @>>> A @>j>> B @>f>> C @>>> 0
    \end{CD}\]
 and
 \[ \begin{CD} 0 @>>> C @>g>> B @>p>> D @>>> 0
    \end{CD}\]
are exact and if $fg = \id{C}$, then $pj$ is an isomorphism from $A$ onto~$D$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use \ref{C069831} to see that $B = \ker p \oplus \ran j$. Consider the map
$\bigl.p\bigr|_{\ran j}\,j_\bullet$ (notation for $j_\bullet$ as in the hint for~\ref{prop_linv_Bsp}). \ns
\end{proof}

\begin{prop}\label{prop_merge_exactCD} If in the category $\cat{BAN_\infty}$ the diagrams
 \begin{equation} \begin{CD} 0 @>>> A @>j>> B @>f>> C @>>> 0  \\
      @.   @VSVV   @VVUV   @.    @.    \\
 0 @>>> A' @>>j'> B' @>>f'> C' @>>> 0
    \end{CD}\end{equation}
and
 \[ \begin{CD} 0 @>>> C @>g>> B @>p>> D @>>> 0   \\
      @.     @.   @VUVV   @VVTV     @.  \\
  0 @>>> C' @>>g'> B' @>>p'> D' @>>> 0
    \end{CD}\]
commute, if the rows are exact, if $fg = \id{C}$ and if $f'g' = \id{C'}$, then the diagram
 \[ \begin{CD}  A  @>pj>>  D   \\
       @VSVV   @VVTV  \\
     A'  @>>p'j'> D'
    \end{CD}\]
commutes and the maps $pj$ and $p'j'$ are isomorphisms.
\end{prop}

In~\ref{exam_dual_functor} and~\ref{exam_dualfunctor_exact} we saw that in the category of Banach spaces
and continuous maps the duality functor $B \mapsto B*$, $T \mapsto T^*$ is an exact contravariant functor.
And in~\ref{exam_bar_functor} we saw that in the same category the pair of maps $B \mapsto \clo B$,
$T \mapsto \clo T$ is an exact covariant functor.  Composing these two functors creates two new exact 
contravariant functors, which we will denote by $F$ and $G$:
  \[ \xymatrix{B\ar[r]^F &{\clo B}^{\,*}}\,,\,\xymatrix{T\ar[r]^F &{\clo T}^{\,*}} \qquad \text{and} \qquad
           \xymatrix{B\ar[r]^G &\clo{B^*}}\,,\,\xymatrix{T\ar[r]^G &\clo{T^*}}  \]
It is natural to wonder about the relationship between ${\clo B}^{\,*}$ and $\clo{B^*}$.  The next proposition
tells us not only that these two spaces are always isomorphic, but they are naturally isomorphic.  That is, the
 \index{natural!equivalence!of ${\clo B}^{\,*}$ and $\clo{B^*}$}%
functors $F$ and $G$ are naturally equivalent.

\begin{prop} If $T \in \ofml B(B,C)$, where $B$ and $C$ are Banach spaces, then there exist isomorphisms 
$\sbsb{\phi}B \colon {\clo B}^{\,*} \sto \clo{B^*}$ and $\sbsb{\phi}C \colon {\clo C}^{\,*} \sto \clo{C^*}$ such that
   \[ {\clo T}^{\,*} = {\sbsb{\phi}B}^{\!\!\!-1}\,\, \clo{T^*}\,\,\sbsb{\phi}C\,. \]  
\end{prop}

\begin{proof}[\emph{Hint for proof}] Since the duality functor is exact (see~\ref{exam_dualfunctor_exact}) we may 
take the dual of the commutative diagram~\eqref{eqn_exactCD_Bbar} to obtain a second commutative diagram with exact
rows.  Now write down a third commutative diagram by replacing each occurrence of $B$ in the diagram~\eqref{eqn_exactCD_Bbar}
by~$B^*$.  Check that ${\sbsb{\tau}B}^{\!\!*}\,\sbsb{\tau}{B^*} = \id{B^*}$ so that you can use proposition~\ref{prop_merge_exactCD}
on the last two diagrams thus obtaining the commutative diagram
     \[ \xy
          \Square/{<-}`{->}`{->}`{<-}/[{\clo B}^{\,*}`{\clo C}^{\,*}`\clo{B^*}`\clo{C^*};{\clo T}^{\,*}`\sbsb{\phi}B`\sbsb{\phi}C`\clo{T^*}]
      \endxy \]
where $\sbsb{\phi}B := \sbsb{\pi}{B^*}\,{\sbsb{\pi}B}^{\!\!*}$ is an isomorphism.   \ns 
\end{proof}

\begin{cor} A Banach space is reflexive if and only if its dual is.  
\end{cor}

\begin{prop}\label{prop_fd_subsp_complm} Every finite dimensional subspace of a Banach space is complemented.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $F$ be a finite dimensional subspace of a Banach space $B$ and let
$\{\vc e^1, \dots, \vc e^n\}$ be a Hamel basis for~$F$.  Then for each $x \in F$ there exist unique scalars
$\alpha_1(x)$, \dots , $\alpha_n(x)$ such that $x = \sum_{k=1}^n \alpha_k(x)\vc e^k$. For each $1 \le k \le n$
verify that $\alpha_k$ is a bounded linear functional, which can be extended to all of~$B$.  Let $N$ be the
intersection of the kernels of these extensions and show that $B = F \oplus N$.  \ns
\end{proof}

\begin{defn} Recall that in~\ref{000082} we defined the \emph{codimension} of a subspace of a vector space
as the dimension of its vector space complement (which always exists by proposition~\ref{0000824}).  We adopt
the same language for Banach spaces: the
 \index{codimension}%
\df{codimension} of any \emph{vector subspace} of a Banach space is the dimension of its \emph{vector space
complement}.
\end{defn}

For the following corollary (of proposition~\ref{prop_fd_subsp_complm}) keep in mind the
convention~\ref{conv_subsp_Bsp} on the use of the word ``subspace'' in the context of Banach spaces.

\begin{cor} In a Banach space every subspace of finite codimension is complemented.
\end{cor}




















Let $M$ be a nonzero subspace of a Banach space~$M$.  Consider the following two properties of the pair $B$ and~~$M$.

\textbf{Property P:} \emph{There exists a projection of norm $1$ from $B$ onto $M$.}

\textbf{Property E:} \emph{For every Banach space $C$, every bounded linear map from $M$ to $C$ can be extended to a bounded
linear map from $B$ to $C$ without increasing its norm.}

\begin{prop} For a nonzero subspace $M$ of a Banach space $B$, properties \textbf{P} and \textbf{E} are equivalent.
\end{prop}

\begin{proof}[\emph{Hint for proof}] To show that \textbf{E} implies \textbf{P} start by extending the identity map
on $M$ to all of~$B$.   \ns
\end{proof}

















\section{The Principle of Uniform Boundedness}
\begin{defn} Let $S$ be a set and $V$ be a normed linear space. A family $\fml F$ of
functions from $S$ into $V$ is
 \index{pointwise!bounded}%
 \index{bounded!pointwise}%
\df{pointwise bounded} if for every $x \in S$ there exists a constant $M_x > 0$ such that
$\norm{f(x)} \le M_x$ for every $f \in \fml F$.
\end{defn}

\begin{defn} Let $V$ and $W$ be normed linear spaces. A family $\fml T$ of bounded linear maps
from $V$ into $W$ is
 \index{uniformly!bounded}%
 \index{bounded!uniformly}%
\df{uniformly bounded} if there exists $M > 0$ such that $\norm T \le M$ for every $T \in
\fml T$.
\end{defn}

\begin{exer}\label{C070114} Let $V$ and $W$ be normed linear spaces. If a family $\fml T$ of
bounded linear maps from $V$ into $W$ is uniformly bounded, then it is pointwise bounded.
\end{exer}

The \emph{principle of uniform boundedness} is the assertion that the converse
of~\ref{C070114} holds whenever $V$ is complete.  To prove this we will make use of a
``local'' analog of this for continuous functions on a complete metric space.  It says, roughly, that if
a family of real valued functions on a complete metric space is pointwise bounded then it is uniformly
bounded on some nonempty open subset of the space.

\begin{prop}\label{C070117} Let $M$ be a complete metric space and $\fml F \subseteq \fml C(M,\R)$.
If for every $x \in M$ there exists a constant $N_x > 0$ such that $\abs{f(x)} \le N_x$ for
every $f \in \fml F$, then there exists a nonempty open set $U$ in $M$ and  a number $N > 0$ such
that $\abs{f(u)} \le N$ for every $f \in \fml F$ and every $u \in U$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For each natural number $k$ let $A_k =
\bigcap\{f^{\gets}\bigl(\,[-k,k]\,\bigr)\colon f \in \fml F\}$.  Use the same version of the
\emph{Baire category theorem} we used in proving the \emph{open mapping theorem} (\ref{C069414})
to conclude that for at least one $n \in \N$ the set $A_n$ has nonempty
interior. Let $U = \intr{A_n}$.   \ns
\end{proof}

\begin{thm}[Principle of uniform boundedness]\label{thm_PUB} Let $B$ be a Banach space and $W$ be a normed linear space.
 \index{principle!of uniform boundedness}%
 \index{boundedness!principle of uniform}%
 \index{uniform!boundedness!principle of}%
If a family $\ofml T$ of bounded linear maps from $B$ into $W$ is pointwise
bounded, then it is uniformly bounded.
\end{thm}

\begin{proof}[\emph{Hint for proof}] For every $T \in \ofml T$ define
  \[ \sbsb fT\colon B \sto \R\colon x \mapsto \norm{Tx}. \]
Use the preceding proposition to show that there exist a nonempty open subset $U$ of $B$
and a constant $M > 0$ such that $\sbsb fT(x) \le M$ for every $T \in \ofml T$ and every
$x \in U$. Choose a point $a \in B$ and a number $r > 0$ so that $\clo{B_r(a)} \subseteq
U$. Then verify that
  \[ \norm{Ty} \le r^{-1}\bigl(\, \sbsb fT(a + ry) + \sbsb fT(a)\,\bigr) \le 2Mr^{-1} \]
for every $T \in \ofml T$ and every $y \in B$ such that $\norm y \le 1$. \ns
\end{proof}

\begin{thm}[Banach-Steinhaus]  Let $B$ be a Banach space, $W$ be a normed linear space,
 \index{Banach-Steinhaus theorem}%
and $(T_n)$ be a sequence of bounded linear maps from $B$ into~$W$. If the pointwise limit
$\lim_{n \sto \infty} T_nx$ exists for every $x$ in~$B$, then the map $S\colon B \sto
W\colon x \mapsto \lim_{n \sto \infty} T_nx$ is a bounded linear transformation.
\end{thm}

\begin{proof}[\emph{Hint for proof}] For every $x \in B$ the sequence $\bigl(\norm{T_nx}\bigr)_{n=1}^\infty$
is convergent, therefore bounded. Use~\ref{thm_PUB}.   \ns
\end{proof}

\begin{defn} A subset $A$ of a normed linear space $V$ is
 \index{bounded!weakly!set in a normed linear space}%
 \index{weak!boundedness!in a normed linear space}%
 \index{w@$w$-bounded}%
\df{weakly bounded} ($w$-bounded) if $f^{\sto}(A)$ is a bounded subset of the scalar
field of $V$ for every $f \in V^*$.
\end{defn}

\begin{prop} A subset $A$ of a Hilbert space $H$ is weakly bounded if and only if for every $y \in H$
 \index{weak!boundedness!in a Hilbert space}%
the set $\{\langle a,y \rangle \colon a \in A\}$ is bounded.
\end{prop}

The preceding proposition is more or less obvious.  The next one is an important consequence of the
\emph{principle of uniform boundedness}.

\begin{prop}\label{C070131} A subset of a normed linear space is weakly bounded if and only if it
is bounded.
\end{prop}

\begin{proof}[\emph{Hint for proof}] If $A$ is a weakly bounded subset of a normed linear space $V$
and $f \in V^*$, what can you say about $\sup\{a^{**}(f)\colon a \in A\}$?  \ns
\end{proof}

\begin{exer} Your good friend Fred R. Dimm needs help again.  This time he is worried about
the assertion (see~\ref{C070131}) that a subset of a normed linear space is bounded if
and only if it is weakly bounded.  He is considering the sequence $(a^n)$ in the Hilbert
space $l^2$ defined by $a^n = \sqrt{n}\,\vc e^n$ for each $n \in \N$, where $\{\vc
e^n\colon n \in \N\}$ is the usual orthonormal basis for~$l_2$ (see
example~\ref{C063149}).  He sees that it is obviously not bounded (given the usual
topology on~$l^2$).  But it looks to him as if it is weakly bounded.  If not, he argues, then
according to the \emph{Riesz-Fr\'echet theorem} \ref{000342} there would exist a sequence $x$ in
$l^2$ such that the set $\{\abs{\langle a^n,x \rangle}\colon n \in\N\}$ is unbounded. It looks
to him as if this cannot happen because such a sequence would have to decrease more slowly than
the sequence $\bigl(n^{-1/2}\bigr)$, which already decreases too slowly to belong to~$l^2$.
Put Fred's mind to rest by finding him a suitable sequence.  \emph{Hint:} Use a sequence with
lots of zeros.
\end{exer}

\begin{defn} A sequence $(x_n)$ in a normed linear space $V$ is
 \index{weak!Cauchy sequence}%
 \index{Cauchy!sequence!weak}%
 \index{sequence!weak Cauchy}%
\df{weakly Cauchy} if the sequence $(f(x_n))$ is Cauchy for every $f$ in~$V^*$.  The
sequence
 \index{weak!convergence!of sequences in a normed linear space}%
 \index{convergence!weak}%
 \index{sequence!weak convergence of a}%
\df{converges weakly} to a point $a \in V$ if $f(x_n) \sto f(a)$ for every $f \in V^*$
(that is, if it converges in the weak topology induced by the members of~$V^*$). In this
case we write
 \index{<arrowtow@$x_n \to^w a$ (weak sequential convergence)}%
$x_n \to^w a$.  The space $V$ is
 \index{complete!weakly}%
 \index{weak!completeness}%
\df{weakly sequentially complete} if every weakly Cauchy sequence in $V$ converges weakly.
\end{defn}

\begin{prop} In a normed linear space every weakly Cauchy sequence is bounded.
\end{prop}

\begin{prop} Every reflexive Banach space is weakly sequentially complete.
\end{prop}

\begin{exam} Let $f_n(t) = \left\{
                             \begin{array}{ll}
                               1 - nt, & \hbox{if $0 \le t \le \frac1n$;} \\
                               0, & \hbox{if $\frac1n < t \le 1$.}
                             \end{array}
                           \right\}$ for every $n \in \N$. The sequence $(f_n)$ of functions shows that the Banach
space $\fml C([0,1])$ is not w-sequentially complete.
\end{exam}

\begin{cor} The Banach space $\fml C([0,1])$ is not reflexive.
\end{cor}

\begin{prop} Let $\fml E$ be an orthonormal basis for a Hilbert space~$H$ and $(x_n)$ be a
sequence in~$H$. Then $x_n \to^w \vc 0$ if and only if $(x_n)$ is bounded and $\langle
x_n,e \rangle \sto 0$ for every $e \in \fml E$.
\end{prop}

\begin{defn} Let $H$ be a Hilbert space.  A net $(T_\lambda)$ in $\ofml B(H)$
 \index{weak!operator topology!convergence in the}%
 \index{operator!topology!weak}%
 \index{topology!weak operator}%
 \index{weak!convergence!of Hilbert space operators}%
\df{converges weakly} (or \df{converges in the weak operator topology}) to $S \in \ofml B(H)$ if
  \[ \langle T_\lambda x,y \rangle \sto \langle Sx,y \rangle \]
for every $x$, $y \in H$.  In this case we write
$T_\lambda~\to^{\textbf{WOT}}~S$.
 \index{<arrowtowot@$T_\lambda~\to^{\textbf{WOT}}~S$}%

A family $\ofml T \subseteq \ofml B(H)$ is
 \index{weak!boundedness!of a family of Hilbert space operators}%
 \index{bounded!weakly}
 \index{bounded!in the weak operator topology}%
 \index{weak!operator topology!boundedness in}%
 \index{WOT@\textbf{WOT} (weak operator topology)}%
\df{weakly bounded} (or \df{bounded in the weak operator topology}) if for every $x$, $y \in H$
there exists a positive constant $\alpha_{x,y}$ such that
  \[\abs{\langle Tx,y \rangle} \le \alpha_{x,y}\]
for every $T \in \ofml T$.
\end{defn}

It is unfortunate that the preceding definition can be the source of confusion.  Since $\ofml B(H)$ is a
normed linear space it already has a ``weak'' topology (see~\ref{C067434}).  So how does one deal with two
different topologies on $\ofml B(H)$ having the same name?  Some authors call the one just defined the
\emph{weak operator topology} thereby forestalling any possible confusion.  Other writers feel that the
weak topology in the normed linear space sense is of so little use in operator theory on Hilbert spaces
that unless specifically modified the phrase \emph{weak topology} in the context of Hilbert space operators
should be understood to refer to the one just defined. (See, for example, Halmos's comments in the two
paragraphs preceding Problem 108 in~\cite{Halmos:1982}.)  Similarly, often one sees the phrase \emph{a
bounded set of Hilbert space operators.}  Unless otherwise qualified, this is intended to say that the set
in question is \emph{uniformly bounded}.  In any case, be careful of these phrases when reading.

Here is an important consequence of the \emph{principle of uniform boundedness}~\ref{thm_PUB}.

\begin{prop} Every family of Hilbert space operators which is bounded in the weak operator
topology is uniformly bounded.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $H$ be a Hilbert space and $\ofml T$ a family of operators on~$H$
which is bounded with respect to the weak operator topology. Use proposition~\ref{C070131} to show, for a
fixed $y \in H$, that $\{T^*y\colon T \in \ofml T\}$ is a bounded subset of~$H$.  Use this (and~\ref{C070131}
again) to show that $\{Tx\colon \text{$T \in \ofml T$, $x \in H$, and $\norm x \le 1$}\}$ is bounded in~$H$. \ns
\end{proof}

As noted above, many writers would shorten the statement of the preceding proposition to:
\emph{Every weakly bounded family of Hilbert space operators is bounded.}

\begin{defn}
A net $(T_\lambda)$ in $\ofml B(H)$
 \index{strong!operator topology!convergence in the}%
 \index{operator!topology!strong}%
 \index{topology!strong operator}%
 \index{strong!convergence!of Hilbert space operators}%
 \index{SOT@\textbf{SOT} (strong operator topology)}%
\df{converges strongly} (or \df{converges in the strong operator topology}) to $S \in \ofml B(H)$ if
  \[ T_\lambda x \sto Sx \]
for every $x \in H$.  In this case we write $T_\lambda~\to^{\textbf{SOT}}~S$.
 \index{<arrowtowot@$T_\lambda~\to^{\textbf{SOT}}~S$}%

The usual norm convergence of operators in $\ofml B(H)$ is often referred to as
 \index{uniform!operator topology!convergence in the}%
 \index{operator!topology!uniform}%
 \index{topology!uniform operator}%
 \index{uniform!convergence!of Hilbert space operators}%
\df{uniform convergence} (or \df{convergence in the uniform operator topology}).  That is, we write
 \index{<arrowtowot@$T_\lambda~\sto~S$}%
$T_\lambda \sto S$ if $\norm{S - T_\lambda}\sto 0$.
\end{defn}

\begin{prop} Let $H$ be a Hilbert space, $(T_n)$ be a sequence in $\ofml B(H)$,
and $B \in \ofml B(H)$. Then the following implications hold:
   \[ T_n \sto B \quad \implies \quad T_n~\to^{\textbf{SOT}}~B
                                \quad \implies \quad T_n~\to^{\textbf{WOT}}~B. \]
\end{prop}

\begin{prop} Let $(S_n)$ and $(T_n)$ be sequences of operators on a Hilbert space~$H$. \\
If $S_n \to^{\textbf{WOT}} A$ and $T_n \to^{\textbf{SOT}} B$, then $S_nT_n \to^{\textbf{WOT}} AB$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Write $S_nT_n - AB$ as $S_nT_n - S_nB + S_nB - AB$.  \ns
\end{proof}

\begin{exam} In the preceding proposition the hypothesis $T_n \to^{\textbf{SOT}} B$ cannot
be replaced by $T_n~\to^{\textbf{WOT}}~B$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Consider powers of the unilateral shift operator.  \ns
\end{proof}







\endinput

\chapter{A VERY BRIEF DIGRESSION ON THE LANGUAGE OF CATEGORIES}\label{categories}

In mathematics we study things (objects) and certain mappings between them (morphisms).
To mention just a few, sets and functions, groups and homomorphisms, topological spaces
and continuous maps, vector spaces and linear transformations, and Hilbert spaces and
bounded linear maps.  These examples come from different branches of mathematics---set
theory, group theory, topology, linear algebra, and functional analysis, respectively.
But these different areas have many things in common: in many field terms like product,
coproduct, subobject, quotient, pullback, isomorphism, and projective limit  appear.
Category theory is an attempt to unify and formalize some of these common concepts.
In a sense, category theory is the study of what different branches of mathematics
have in common.  Perhaps a better description is: categorical language tells us ``how things work'',
not ``what they are''.

In these notes, indeed any text at this level, ubiquitously uses the language of sets without
assuming a detailed prior study of axiomatic set theory. Similarly, we will cheerfully use the
\emph{language} of categories without first embarking on a foundationally satisfactory study of
category theory (which itself is a large and important area of research).  Sometimes textbooks make
learning even the language of categories challenging by leaning heavily on one's algebraic background.
Just as most people are comfortable using the language of sets (whether or not they have made a serious
study of set \emph{theory}), nearly everyone should find the use of categorical language both convenient
and enlightening without elaborate prerequisites.

For those who wish to delve more deeply into the subject I can recommend a very gentle entr\'ee to the world
of categories which appears as Chapter 3 of Semadeni's beautiful book~\cite{Semadeni:1971}.  A classic
text written by one of the founders of the subject is~\cite{MacLane:1971}.  A more recent text is~\cite{LawvereS:1997}.

By pointing to unifying principles the language of categories often provides striking insight into
``the way things work'' in mathematics. Equally importantly, one gains in efficiency by not having to go
through essentially the same arguments over and over again in just slightly different contexts.

For the moment we do little more than define ``object'', ``morphism'', and ``functor'', and give a few examples.









\section{Objects and Morphisms}\label{C0151}
\begin{defn} Let $\sfml A$ be a class, whose members we call
 \index{object (of a category)}%
\df{objects}. For every pair $(S,T)$ of objects we associate a class $\mor ST$, whose
members we call
 \index{morphism!of a category}%
\df{morphisms} (or
 \index{arrow}%
\df{arrows}) from $S$ to $T$. We assume that $\mor ST$ and $\mor UV$ are disjoint
unless $S = U$ and $T = V$.

We suppose further that there is an operation $\circ$ (called \df{composition}) that
associates with every $\alpha \in \mor ST$ and every $\beta \in \mor TU$ a morphism
$\beta \circ \alpha \in \mor SU$ in such a way that:
 \begin{enumerate}
  \item $\gamma \circ (\beta \circ \alpha) = (\gamma \circ
\beta) \circ \alpha$ whenever $\alpha \in \mor ST$, $\beta \in \mor TU$, and $\gamma \in
\mor UV$;
  \item for every object $S$ there is a morphism $I_S \in
\mor SS$ satisfying $\alpha \circ I_S = \alpha$ whenever $\alpha \in \mor ST$ and $I_S
\circ \beta = \beta$ whenever $\beta \in \mor RS$.
 \end{enumerate}
Under these circumstances the class $\sfml A$, together with the associated families of
morphisms, is a
 \index{category}%
\df{category}.

We will reserve the notation
 \index{<arrowto@$S \to^\alpha T$ (morphism in a category)}%
$S \to^\alpha T$ for a situation in which $S$ and $T$ are objects in some category and
$\alpha$ is a morphism belonging to $\mor ST$. As is the case with groups and vector
spaces we usually omit the composition symbol $\circ$ and write
  \index{<binopconcat@$\beta\alpha$ (notation for composition of morphisms)}%
  \index{conventions!notation for composition!of morphisms}%
$\beta\alpha$ for $\beta \circ \alpha$.

A category is
 \index{small}%
 \index{category!small}%
\df{locally small} if $\mor ST$ is a set for every pair of objects $S$ and $T$ in the category.  It is
 \index{locally!small}%
 \index{category!locally small}%
\df{small} if, in addition, the class of all its objects is a set.  In these notes the categories in which we are interested
will be locally small.
\end{defn}

\begin{exam} The category
 \index{category!$\cat{SET}$ as a}%
 \index{SET@$\cat{SET}$!the category}%
$\cat{SET}$ has sets for objects and functions (maps) as morphisms.
\end{exam}

\begin{exam} The category
 \index{category!$\cat{AbGp}$ as a}%
 \index{AbGp@$\cat{AbGp}$!the category}%
$\cat{AbGp}$ has Abelian groups for objects and group homomorphisms as morphisms.
\end{exam}

\begin{exam} The category
 \index{category!$\cat{VEC}$ as a}%
 \index{VEC@$\cat{VEC}$!the category}%
$\cat{VEC}$ has vector spaces for objects and linear transformations as morphisms.
\end{exam}







\begin{defn}\label{C008111} Let $\le$ be a relation on a nonempty set $S$.
 \begin{enumerate}
  \item If the relation
 \index{<le@$\le$ (notation for a partial ordering)}%
 $\le$ is reflexive (that is, $x \le x$ for all $x \in S$) and transitive (that is, if $x \le y$ and $y \le z$, then $x  \le z$), it is a
 \index{reflexive}%
 \index{transitive}%
 \index{preordering}%
 \index{ordering!pre-}%
\df{preordering}.
  \item If $\le$ is a preordering and is also antisymmetric (that is, if $x \le y$ and $y \le x$, then $x = y$), it is a
 \index{antisymmetric}%
 \index{partial!ordering}%
 \index{ordering!partial}%
\df{partial ordering}.
  \item Elements $x$ and $y$ in a set on which a preordering has been defined are
 \index{comparable}%
\df{comparable} if either $x \le y$ or $y \le x$.
  \item If $\le$ is a partial ordering with respect to which any two elements are comparable, it is a
 \index{linear!ordering}%
 \index{ordering!linear}%
\df{linear ordering} (or a
 \index{total!ordering}%
 \index{ordering!total}%
\df{total ordering}).
  \item If the relation $\le$ is a preordering (respectively, partial ordering,
linear ordering) on $S$, then the pair $(S,\le)$ is a
 \index{preordered set}%
\df{preordered set} (respectively,
 \index{partially ordered set}%
\df{partially ordered set},
 \index{linearly ordered set}%
\df{linearly ordered set}).
  \item A linearly ordered subset of a partially ordered set $(S,\le)$ is a
 \index{chain}%
\df{chain} in~$S$.
 \end{enumerate}
We may write $b \ge a$ as a substitute for $a \le b$.  The notation
 \index{<l@$<$ (notation for a strict ordering)}%
$a < b$ ( or equivalently, $b > a$) means that $a \le b$ and $a \ne b$.
\end{defn}

\begin{defn} A map $f\colon S \sto T$ between preordered sets is
 \index{order!preserving map}%
\df{order preserving} if $x \le y$ in $S$ implies $f(x) \le f(y)$ in~$T$.
\end{defn}

\begin{exam} The category
 \index{category!$\cat{POSET}$ as a}%
 \index{POSET@$\cat{POSET}$!the category}%
$\cat{POSET}$ has partially ordered sets for objects and order preserving maps as morphisms.
\end{exam}

\begin{notn} Let $f \colon S \sto T$ be a function between sets. Then for $A \subseteq S$ we define
$f^{\sto}(A) = \{f(x)\colon x \in A\}$ and for $B \subseteq T$ we define
 \index{<superscript@$f^{\sto}(A)$ or $f(A)$ (image of $A$ under $f$)}%
 \index{<superscript@$f^{\gets}(B)$ (preimage of $B$ under $f$)}%
$f^{\gets}(B) = \{ x \in S\colon f(x) \in B\}$. We say that $f^{\sto}(A)$ is the
 \index{image}%
\df{image of $A$ under $f$} and that $f^{\gets}(B)$ is the
 \index{preimage}%
\df{preimage} (or the
 \index{inverse!image}%
 \index{image!inverse}%
\df{inverse image}) \df{of $B$ under~$f$}.  Sometimes, to avoid clutter, I will succumb to the dubious practice of
writing $f(A)$ for~$f^{\sto}(A)$.
\end{notn}

\begin{defn} Recall that a family $\sfml T$ of subsets of a set $X$ is a
 \index{topology}%
\df{topology} on $X$ if it contains both the empty set $\emptyset$ and $X$ itself and is closed under taking of unions
($\bigcup\sfml S \in \sfml T$ whenever $\sfml S \subseteq \sfml T$) and of finite intersections ($\bigcap\sfml F \in \sfml T$
whenever $\sfml F \subseteq \sfml T$ and $\sfml F$ is finite).  A nonempty set $X$ together with a topology on $X$ is a
 \index{topological!space}%
 \index{space!topological}%
\df{topological space}. If $(X,\sfml T)$ is a topological space, a member of $\sfml T$ is an
 \index{open}%
 \index{<binrel@$\open UX$ ($U$ is an open subset of~$X$)}%
\df{open set}.  To indicate that a set $U$ is an open subset of $X$ we often write $\open UX$.  The complement of an open set is a
 \index{closed}%
\df{closed set}.  A function $f\colon X \sto Y$ between topological spaces is
 \index{continuous!function}%
 \index{function!continuous}%
\df{continuous} if the inverse image of every open set is open; that is, if $\open{f^{\gets}(V)}X$ whenever $\open VY$.
(If you have not previously encountered topological spaces, look at the first two items in both the first section of
chapter 10 and the first section of chapter 11 of my online notes~\cite{Erdman:2007}.)
\end{defn}

\begin{exam} Every nonempty set $X$ admits a smallest (coarsest) topology; it is the one consisting of exactly two sets,
$\emptyset$ and~$X$.  This is the
 \index{indiscrete}%
 \index{topology!indiscrete}%
\df{indiscrete} topology on~$X$. (Note the spelling: there is another word in the English language, ``indiscreet'',
which means something quite different.)  The set $X$ also admits a largest (finest) topology, which comprises the
family $\sfml P(X)$ of all subsets of~$X$.  This is the
 \index{discrete}%
 \index{topology!discrete}%
\df{discrete} topology on~$X$.
  \end{exam}

\begin{exam} The category
 \index{category!$\cat{TOP}$ as a}%
 \index{TOP@$\cat{TOP}$!the category}%
$\cat{TOP}$ has topological spaces for objects and continuous functions as morphisms.
\end{exam}

\begin{defn}\label{defn_alg} Let $(A,+,M)$ be a vector space over $\K$ which is equipped with
another binary operation $A \times A \sto A$ where $(x,y) \mapsto x \cdot y$ in such
a way that $(A,+,\cdot\,)$ is a ring. (The notation $x \cdot y$ is usually shortened to $xy$.) If additionally the equations
  \begin{equation}\label{eq_def_algebra}
     \alpha (xy) = (\alpha x)y = x(\alpha y)
  \end{equation}
hold for all $x$, $y \in A$ and $\alpha \in \K$, then $(A,+,M,\,\cdot\,)$ is an
 \index{algebra}%
\df{algebra} over the field~$\K$ (sometimes referred to as a \df{linear associative
algebra}).  We abuse terminology in the usual way by writing such things as, ``Let $A$ be an
algebra.'' We say that an algebra $A$ is
 \index{unital!algebra}%
 \index{algebra!unital}%
\df{unital} if its underlying ring $(A,+,\cdot\,)$ has a multiplicative identity; that is, if there exists
 \index{multiplicative!identity}%
 \index{identity!multiplicative}%
an element $\vc 1_A \ne \vc 0$ in $A$ such that $\vc 1_A \cdot x = x \cdot \vc 1_A = x$ for all $x \in A$.
  And it is
 \index{commutative!algebra}%
 \index{algebra!commutative}%
\df{commutative} if its ring is; that is, if $xy = yx$ for all $x$, $y \in A$.

A subset $B$ of an algebra $A$ is a
 \index{subalgebra}%
\df{subalgebra} of $A$ if it is an algebra under the operations it inherits from~$A$.  A
subalgebra $B$ of a unital algebra $A$ is a
 \index{unital!subalgebra}%
 \index{subalgebra!unital}%
\df{unital subalgebra} if it contains the multiplicative identity of~$A$.

\textbf{CAUTION.} To be a unital subalgebra it is \emph{not} enough for $B$ to have a
multiplicative identity of its own; it must contain the identity of~$A$.  Thus, an
algebra can be both unital and a subalgebra of $A$ without being a unital subalgebra
of~$A$.

A map $f\colon A \sto B$ between algebras is an
 \index{homomorphism!of algebras}%
 \index{algebra!homomorphism}%
\df{(algebra) homomorphism} if it is a linear map between $A$ and $B$ as vector spaces which preserves multiplication
(that is, $f(xy) = f(x)f(y)$ for all $x$, $y \in A$).  In other words, an algebra homomorphism is a linear ring homomorphism.
It is a
 \index{unital!algebra homomorphism}%
 \index{algebra!homomorphism!unital}%
 \index{homomorphism!of algebras!unital}%
\df{unital (algebra) homomorphism} if it preserves identities; that is, if both $A$ and $B$ are unital algebras and $f(\vc 1_A) = 1_B$.
The
 \index{kernel!of an algebra homomorphism}%
\df{kernel} of an algebra homomorphism $f\colon A \sto B$ is, of course, $\{a \in A \colon f(a) = \vc 0\}$.

If $f^{-1}$ exists and is also an algebra homomorphism, then $f$ is an
 \index{isomorphism!of algebras}%
\df{isomorphism} from $A$ to $B$.  If an isomorphism from $A$ to $B$ exists, then $A$ and
$B$ are
 \index{isomorphic}%
\df{isomorphic}.
\end{defn}

\begin{exam} The category
 \index{category!$\cat{ALG}$ as a}%
 \index{ALG@$\cat{ALG}$!the category}%
$\cat{ALG}$ has algebras for objects and algebra homomorphisms for morphisms.
\end{exam}

The preceding examples are examples of \emph{concrete categories}---that is, categories
in which the objects are sets (together, usually, with additional structure) and the
morphism are functions (usually preserving this extra structure).  In these notes the
categories of interest to us are both concrete and locally small.  Here (for those who are curious) is a
more formal definition of \emph{concrete category}.

\begin{defn}\label{C015124} A category $\sfml A$ together with a function $\abs{\hphantom{O}}$ which assigns
to each object $A$ in $\sfml A$ a set $\abs A$ is a
 \index{concrete category}%
 \index{category!concrete}%
\df{concrete} category if the following conditions are satisfied:
 \begin{enumerate}
    \item every morphism $A \to^f B$ is a function from $\abs A$ to~$\abs B$;
    \item each identity morphism $I_A$ in $\sfml A$ is the identity function on~$\abs A$; and
    \item composition of morphisms $A \to^f B$ and $B \to^g C$ agrees with composition
of the functions $f\colon \abs A \sto \abs B$ and $g\colon \abs B \sto \abs C$.
 \end{enumerate}
If $A$ is an object in a concrete category $\sfml A$, then $\abs A$ is the
 \index{underlying set}%
 \index{set!underlying}%
 \index{<unop@$\abs A$ (underlying set of the object~$A$)}%
\df{underlying set} of~$A$.
\end{defn}

Although it is true that the categories of interest in these notes are concrete
categories, it may nevertheless be interesting to see an example of a category that is
\emph{not} concrete.

\begin{exam}\label{C015127} Let $G$ be a monoid (that is, a semigroup with identity).
 \index{monoid}%
Consider a category $\cat C$ having exactly one object, which we call~$\star$. Since there
is only one object there is only one family of morphisms $\mor{\star}{\star}$, which we
take to be~$G$. Composition of morphisms is defined to be the monoid multiplication.
That is, $a \circ b := ab$ for all $a$, $b \in G$. Clearly composition is associative and
the identity element of $G$ is the identity morphism. So $\cat C$ is a category.
\end{exam}

\begin{defn} In any concrete category we will call an injective morphism a
 \index{monomorphism}%
\df{monomorphism} and a surjective morphism an
 \index{epimorphism}%
\df{epimorphism}.
\end{defn}

\begin{cau} The definitions above reflect the original Bourbaki use of the term and are the
ones most commonly adopted by mathematicians outside of category theory itself where
``monomorphism'' means ``left cancellable'' and ``epimorphism'' means ``right
cancellable''.  (Notice that the terms \emph{injective} and \emph{surjective} may not
make sense when applied to morphisms in a category that is not concrete.)

A morphism $B \to^g C$ is
 \index{left!cancellable}%
 \index{cancellable!left}%
\df{left cancellable} if whenever morphisms $A \to^{f_1} B$ and $A \to^{f_2} B$ satisfy
$gf_1 = gf_2$, then $f_1 = f_2$.  Mac Lane suggested calling left cancellable morphisms
 \index{monic}%
\df{monic} morphisms.  The distinction between monic morphisms and monomorphisms turns
out to be slight.  In these notes almost all of the morphisms we encounter are monic if
and only if they are monomorphisms. As an easy exercise prove that any injective morphism
in a (concrete) category is monic.  The converse sometimes fails.

In the same vein Mac Lane suggested calling a
 \index{right!cancellable}%
 \index{cancellable!right}%
\emph{right cancellable} morphism (that is, a morphism $A \to^f B$ such that whenever
morphisms $B \to^{g_1} C$ and $B \to^{g_2} C$ satisfy $g_1f = g_2f$, then $g_1 = g_2$) an
 \index{epic}%
\df{epic} morphism.  Again it is an easy exercise to show that in a (concrete) category
any epimorphism is epic.  The converse, however, fails in some rather common categories.
\end{cau}

\begin{defn} The terminology for inverses of morphisms in categories is essentially the same
as for functions.  Let $A \to^\alpha B$ and $B \to^\beta A$ be
morphisms in a category. If $\beta \circ \alpha = I_A$, then $\beta$ is a
 \index{left!inverse!of a morphism}%
 \index{inverse!left}%
\df{left inverse} of $\alpha$ and, equivalently, $\alpha$ is a
 \index{right!inverse!of a morphism}%
 \index{inverse!right}%
\df{right inverse} of $\beta$. We say that the morphism $\alpha$ is an
 \index{isomorphism!in a category}%
\df{isomorphism} (or is
 \index{invertible!morphism}%
\df{invertible}) if there exists a morphism $B \to^\beta A$ which is both a left and a
right inverse for~$\alpha$.  Such a function is denoted by $\alpha^{-1}$ and is called the
 \index{<unopur@$\alpha^{-1}$ (inverse of a morphism $\alpha$)}%
 \index{inverse!of a morphism}%
\df{inverse} of~$\alpha$.
\end{defn}

\begin{prop} Let $A \to^\alpha B$ be a morphism in a category.  If $\alpha$ has both a left inverse $\lambda$ and a
right inverse $\rho$, then $\lambda = \rho$ and consequently $\alpha$ is an isomorphism.
\end{prop}

In any concrete category one can inquire whether every bijective morphism (that is, every
map which is both a monomorphism and an epimorphism) is an isomorphism.  The answer is
often a trivial \emph{yes} (as in $\cat{SET}$, $\cat{AbGp}$, and $\cat{VEC}$) or a trivial
\emph{no} (for example, in the category $\cat{POSET}$
 \index{bijective morphisms!need not be invertible!in $\cat{POSET}$}%
 \index{POSET@$\cat{POSET}$!bijective morphism in}%
 \index{POSET@$\cat{POSET}$!the category}%
of partially ordered sets and order preserving maps.  But on
occasion the answer turns out to be a fascinating and deep result (see for example the \emph{open mapping
theorem}~\ref{C069414}).

\begin{exam} In the category $\cat{SET}$
 \index{bijective morphisms!are invertible!in $\cat{SET}$}%
 \index{SET@$\cat{SET}$!bijective morphism in}%
every bijective morphism is an isomorphism.
\end{exam}

\begin{exam}\label{C015216} The category
 \index{LAT@$\cat{LAT}$!the category}%
 \index{category!$\cat{LAT}$ as a}%
$\cat{LAT}$ has lattices as objects and lattice
homomorphisms as morphisms. (A
 \index{lattice}%
\df{lattice} is a partially ordered set in which every pair of elements has an infimum and a supremum and a
 \index{lattice!homomorphism}%
 \index{homomorphism!lattice}%
\df{lattice homomorphism} is a map between lattices which preserves infima and suprema.)
In this category bijective morphisms
 \index{bijective morphisms!are invertible!in $\cat{LAT}$}%
 \index{LAT@$\cat{LAT}$!bijective morphism in}%
are isomorphisms.
\end{exam}

\begin{exam} A
 \index{homeomorphism}%
\df{homeomorphism} is a continuous bijection $f$ between topological spaces whose inverse $f^{-1}$ is also continuous.  Homeomorphisms
are the isomorphisms in the category $\cat{TOP}$.  In this category not every continuous bijection is a homeomorphism.
\end{exam}

\begin{exam} If in the category $\cat C$ of example~\ref{C015127} the monoid $G$ is a group
then every morphism in $\cat C$ is an isomorphism.
\end{exam}






\section{Functors}
\begin{defn} If $\cat{A}$ and $\cat{B}$ are categories a
 \index{covariant functor}%
 \index{functor!covariant}%
\df{covariant functor} $F$ from $\cat A$ to $\cat B$ (written ${\cat A}\to^F{\cat B}$) is
a pair of maps: an
 \index{object!map}%
 \index{map!object}%
\df{object map} $F$ which associates with each object $S$ in $\cat A$ an object $F(S)$ in
$\cat B$ and a
 \index{morphism!map}%
 \index{map!morphism}%
\df{morphism map} (also denoted by $F$) which associates with each morphism $f \in \mor
ST$ in $\cat A$ a morphism $F(f) \in \mor {F(S)}{F(T)}$ in $\cat B$, in such a way that
 \begin{enumerate}
  \item $F(g \circ f) = F(g) \circ F(f)$ whenever $g \circ f$ is defined in $\cat A$; and
  \item $F(\id S) = \id{F(S)}$ for every object $S$ in $\cat A$.
 \end{enumerate}

The definition of a
 \index{contravariant functor}%
 \index{functor!contravariant}%
\df{contravariant functor} ${\cat A}\to^F{\cat B}$ differs from the preceding definition
only in that, first, the morphism map associates with each morphism $f \in \mor ST$ in
$\cat A$ a morphism $F(f) \in \mor {F(T)}{F(S)}$ in $\cat B$ and, second, condition (a)
above is replaced by
 \begin{enumerate}
  \item[(\rm{a'})] $F(g \circ f) = F(f) \circ F(g)$ whenever $g \circ f$
is defined in~$\cat A$.
 \end{enumerate}
\end{defn}

\begin{exam}\label{functors005exam} A
 \index{forgetful functor}%
 \index{functor!forgetful}%
\df{forgetful functor} is a functor that maps objects and morphisms from a category $\cat
C$ to a category $\cat C'$ with less structure or fewer properties.  For example, if $V$
is a vector space, the functor $F$ which ``forgets'' about the operation of scalar
multiplication on vector spaces would map $V$ into the category of Abelian groups. (The
Abelian group $F(V)$ would have the same set of elements as the vector space $V$ and the
same operation of addition, but it would have no scalar multiplication.)  A linear map
$T\colon V \sto W$ between vector spaces would be taken by the functor $F$ to a group
homomorphism $F(T)$ between the Abelian groups $F(V)$ and~$F(W)$.

Forgetful functor can ``forget'' about properties as well.  If $G$ is an object in the
category of Abelian groups, the functor which ``forgets'' about commutativity in Abelian
groups would take $G$ into the category of groups.

It was mentioned in the preceding section that all the categories that are of interest in
these notes are concrete categories (ones in which the objects are sets with additional
structure and the morphisms are maps which preserve, in some sense, this additional
structure). We will have several occasions to use a special type of forgetful
functor---one which forgets about all the structure of the objects except the underlying
set and which forgets any structure preserving properties of the morphisms. If $A$ is an
object in some concrete category $\cat C$, we denote by
 \index{<unop@$\abs A$ (the forgetful functor acting on~$A$)}%
$\abs{A}$ its underlying set. And if \mbox{$A~\to^f~B$} is a morphism in $\cat C$ we
denote by $\abs f$ the map from $\abs A$ to $\abs B$ regarded simply as a function
between sets.  It is easy to see that $\abs{\hphantom{M}}$\,, which takes objects in
$\cat C$ to objects in $\cat{SET}$ (the category of sets and maps) and morphisms in $\cat
C$ to morphisms in $\cat{SET}$, is a covariant functor.

In the category $\cat{VEC}$ of vector spaces and linear maps, for example,
$\abs{\hphantom{M}}$ causes a vector space $V$ to ``forget'' about both its addition and
scalar multiplication ($\abs V$ is just a set). And if $T\colon V \sto W$ is a linear
transformation, then $\abs T\colon \abs V \sto \abs W$ is just a map between sets---it
has ``forgotten'' about preserving the operations.
\end{exam}


\begin{defn} A partially ordered set is
 \index{order!complete}%
 \index{complete!order}%
\df{order complete} if every nonempty subset has a supremum (that is, a least upper
bound) and an infimum (a greatest lower bound).
\end{defn}

\begin{defn} Let $S$ be a set.  Then the
 \index{power set}%
 \index{P@$\sfml P(S)$ (power set of $S$)}%
\df{power set} of $S$, denoted by $\sfml P(S)$, is the family of all subsets of~$S$.
\end{defn}

\begin{exam}[The power set functors] Let $S$ be a nonempty set.
 \begin{enumerate}
  \item The power set $\sfml P(S)$ of $S$ partially ordered by $\subseteq$ is
order complete.
  \item The class of order complete partially ordered sets and order
preserving maps is a category.
 \index{functor!power set}%
 \index{power set!functor}%
  \item For each function $f$ between sets let $\sfml P(f) = f^{\sto}$. Then
$\sfml P$ is a covariant functor from the category of sets and functions to the category
of order complete partially ordered sets and order preserving maps.
  \item For each function $f$ between sets let $\sfml P(f) = f^{\gets}$. Then
$\sfml P$ is a contravariant functor from the category of sets and functions to the
category of order complete partially ordered sets and order preserving maps.
 \end{enumerate}
\end{exam}

\begin{defn}\label{defn_vsp_adjoint} Let $T\colon V \sto W$ be a linear map between vector spaces. For every
$g \in W^{\#}$ let $T^{\#}(g) = g\,T$.  Notice that $T^{\#}(g) \in V^{\#}$.  The map $T^{\#}$ from the
vector space $W^{\#}$ into the vector space~$V^{\#}$ is the (vector space)
 \index{adjoint!vector space}%
 \index{vector!space!adjoint map}%
 \index{<unopur@$T^\#$ (vector space adjoint)}%
\df{adjoint} map of~$T$.
\end{defn}

\begin{exam} The pair of maps $V \mapsto V^{\#}$ (taking a vector space to its algebraic dual) and
$T \mapsto T^{\#}$ (taking a linear map to its dual) is a contravariant functor from the category $\cat{VEC}$
of vector spaces and linear maps to itself.
\end{exam}

\begin{exam}\label{0007606} If $\cat C$ is a category let
 \index{category@$\cat{C^2}$ (pairs of objects and morphisms)}%
$\cat{C^2}$ be the category whose objects are ordered pairs of objects in $\cat C$ and
whose morphisms are ordered pairs of morphisms in~$\cat C$.  Thus if $A \to^f C$ and $B
\to^g D$ are morphisms in $\cat C$, then $(A,B) \to^{(f,g)} (C,D)$ (where $(f,g)(a,b) =
\bigl(f(a),g(b)\bigr)$ for all $a \in A$ and $b \in B$) is a morphism in~$\cat{C^2}$.
Composition of morphism is defined in the obvious way: $(f,g) \circ (h,j) = (f \circ h, g
\circ j)$. We define the
 \index{diagonal!functor}%
 \index{functor!diagonal}%
\df{diagonal functor} $\cat C \to^{\ftr D} \cat{C^2}$ by $\ftr D(A) := (A,A)$. This is a
covariant functor.
\end{exam}

\begin{exam}\label{C029717} Let $X$ and $Y$ be topological spaces and $\phi\colon X \sto Y$
be continuous. Define $\fml C \phi$ on $\fml C(Y)$ by
    \[ \fml C \phi(g) = g \circ \phi \]
for all $g \in \fml C(Y)$.  Then the pair of maps $X \mapsto \fml C(X)$ and
 \index{C@$\fml C$ (the functor)}%
$\phi \mapsto \fml C \phi$ is a contravariant functor from the category $\cat{TOP}$ of topological
spaces and continuous maps to the category of unital algebras and unital algebra
homomorphisms.
\end{exam}













\endinput

\chapter{COMPACT OPERATORS}\label{chap_cpt_ops}

\section{Definition and Elementary Properties}

We will pick up where we left off at the end of chapter 5, studying classes of operators.
Our context will be slightly more general; in the case of compact operators we will be
looking at operators on Banach spaces.  First, let us recall a few definitions and
elementary facts from advanced calculus (or a first course in analysis).

\begin{defn} A subset $A$ of a metric space $M$ is
 \index{total!boundedness}%
 \index{bounded!totally}%
\df{totally bounded} if for every $\epsilon > 0$ there exists a finite subset $F$ of $A$
such that for every $a \in A$ there is a point $x \in F$ such that $d(x,a) < \epsilon$.
For normed linear spaces this definition may be rephrased: A subset $A$ of a normed linear
space $V$ is \df{totally bounded} if for every open ball $B$ about zero in $V$ there exists
a finite subset $F$ of $A$ such that $A \subseteq F + B$.  This has a more or less standard
paraphrase: A space is totally bounded if it can be kept under surveillance by a finite number
of arbitrarily near-sighted policemen.  (Some authors call this property \emph{precompact}.)
\end{defn}

\begin{prop} A metric space is compact if and only if its is complete and totally bounded.
\end{prop}

\begin{prop} A metric space is compact if and only if it is complete and totally bounded.
\end{prop}

\begin{defn} A subset $A$ of a topological space $X$ is
 \index{compact!relatively}%
 \index{relatively compact}%
\df{relatively compact} if its closure is compact.
\end{defn}

\begin{prop} In a metric space every relatively compact set is totally bounded.
\end{prop}

In a complete metric space the converse is true.

\begin{prop} In a complete metric space every totally bounded set is relatively compact.
\end{prop}

\begin{prop} In a metric space every totally bounded set is separable.
\end{prop}

\begin{thm}[Arzel\`a-Ascoli Theorem]\label{AAThm} Let $X$ be a compact Hausdorff space. A subset $\fml F$
of $\fml C(X)$ is totally bounded whenever it is (uniformly) bounded and equicontinuous.
\end{thm}

In the following we will be contrasting the \emph{weak} and \emph{strong} topologies on normed linear spaces.
The term
 \index{weak!\emph{vs.} strong topologies on normed spaces}%
 \index{topology!weak!\emph{vs.} strong on normed space}%
 \index{strong!\emph{vs.} weak topologies on normed spaces}%
 \index{conventions!on normed spaces \emph{strong topology} refers to the norm topology}%
\emph{weak topology} will refer to the usual weak topology determined by the bounded linear functionals
on the space as defined in~\ref{C067434}, while \emph{strong topology} will refer to the norm topology on the space.
Keep in mind, however, that the modifier ``strong'' in the context of normed linear spaces is, strictly speaking, 
redundant.  It is used solely for emphasis.  For example, many writers would state the next proposition as: 
\emph{Every weakly convergent sequence in a normed linear space is bounded.}

\begin{prop} Every weakly convergent sequence in a normed linear space is strongly bounded.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use the \emph{principle of uniform boundedness}~\ref{thm_PUB}. \ns
\end{proof}

\begin{defn} We say that a linear map $T\colon V \sto W$ is
 \index{weak!continuity}%
 \index{continuous!weakly}%
 \index{linear!map!weak continuity of a}%
\df{weakly continuous} if it takes weakly convergent nets to weakly convergent nets.
\end{defn}

\begin{prop} Every (strongly) continuous linear map between normed linear spaces is weakly continuous.
\end{prop}

\begin{defn} A linear map $T\colon V \sto W$ between normed linear spaces is
 \index{compact!linear map}%
 \index{linear!map!compact}%
 \index{operator!compact}%
\df{compact} if it takes bounded sets to relatively compact sets. Equivalently, $T$ is compact if it
maps the closed unit ball in $V$ onto a relatively compact set in~$W$.   The family of all compact linear maps
from $V$ to $W$ is denoted
 \index{K@$\ofml K(V,W)$!family of compact linear maps}%
 \index{K@$\ofml K(V)$!family of compact operators}%
by~$\ofml K(V,W)$; and we write $\ofml K(V)$ for~$\ofml K(V,V)$.
\end{defn}

\begin{prop} Every compact linear map is bounded.
\end{prop}

\begin{prop} A linear map $T\colon V \sto W$ between normed linear spaces is compact if and only if it maps
every bounded sequence in $V$ to a sequence in $W$ which has a convergent subsequence.
\end{prop}

\begin{exam} Every finite rank operator on a Banach space is compact.
\end{exam}

\begin{exam} Every bounded linear functional on a Banach space $B$ is a compact linear map from $B$ to~$\K$.
\end{exam}

\begin{exam} Let $C$ be the Banach space $\fml C([0,1])$ and $k \in \fml C([0,1] \times [0,1])$.  For each $f \in C$
 \index{integral!operator}%
 \index{operator!integral}%
define a function $Kf$ on $[0,1]$ by
   \[ Kf(x) = \int_0^1 k(x,y)f(y)\,dy\,. \]
Then the integral operator $K$ is a compact operator on~$C$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Use the \emph{Arzel\`a-Ascoli theorem}~\ref{AAThm}.  \ns
\end{proof}

\begin{exam}\label{X_square_int_kernel} Let $H$ be the Hilbert space $\lfs2{[0,1]}$ of (equivalence classes of) functions which
are square-integrable on $[0,1]$ with respect to Lebesgue measure and
 \index{integral!operator}%
 \index{operator!integral}%
$k$ be square-integrable function
on $[0,1] \times [0,1]$.  For each $f \in H$ define a function $Kf$ on $[0,1]$ by
   \[ Kf(x) = \int_0^1 k(x,y)f(y)\,dy\,. \]
Then the integral operator $K$ is a compact operator on~$H$.
\end{exam}

\begin{prop} An operator on a Hilbert space is compact if and only if it maps the closed unit ball in the space
onto a compact set.
\end{prop}

\begin{cor}\label{cor_id_op_not_cpt} The identity operator on an infinite dimensional Hilbert space is not compact.
(See example~\ref{X_l2ball_not_cpt}
\end{cor}

\begin{exam} If $B$ is a Banach space the family $\ofml B(B)$ of operators on $B$ is a
 \index{Banach!algebra!$\ofml B(B)$ as a}%
 \index{B@$\ofml B(V)$!as a unital Banach algebra}%
unital Banach algebra and the
 \index{K@$\ofml K(V)$!as a proper closed ideal}%
family $\ofml K(B)$ of compact operators on $B$ is a closed ideal in~$\ofml B(B)$.
If $B$ is infinite dimensional the ideal $\ofml K(H)$ is proper.
\end{exam}

\begin{prop} Let $T\colon B \sto C$ be a bounded linear map between Banach spaces. Then $T$ is compact
if and only if $T^*$ is.
\end{prop}

\begin{defn}\label{0013} A
 \index{C*@$C^*$-algebra}%
 \index{algebra!$C^*$-}%
\df{$C^\ast$-algebra} is a Banach algebra $A$ with involution which satisfies
   \[ \norm{a^*a} = \norm{a}^2 \]
for every $a \in A$.  This property of the norm is usually referred to as
 \index{C*@$C^*$-condition}%
\emph{the $C^*$-condition}. An algebra norm satisfying this condition is a
 \index{C*@$C^*$-norm}%
 \index{norm!$C^*$-}%
\df{$C^*$-norm}.  A
 \index{C*@$C^*$-subalgebra}%
 \index{subalgebra!$C^*$-}%
\df{$C^*$-subalgebra} of a $C^*$-algebra $A$ is a closed $*\,$-subalgebra of~$A$.

We denote by $\cat{CSA}$ the category of $C^*$-algebras and $*\,$-homomorphisms.
 \index{category!$\cat{CSA}$ as a}%
 \index{CSA@$\cat{CSA}$!the category}%
It may seem odd at first that in this category, whose objects have both algebraic and topological properties,
that the morphisms should be required to preserve only algebraic structure.  Remarkably enough, it turns out
that every morphism in $\cat{CSA}$ is automatically continuous---in fact, contractive (see
proposition~\ref{00152171})---and that
 \index{bijective morphisms!are invertible!in $\cat{CSA}$}%
every injective morphism is an isometry (see proposition~\ref{00152181}).  It is clear that
(as in definition~\ref{00109}) every bijective morphism in this category is an isomorphism.  And thus every bijective $*\,$-homomorphism
is an isometric $*\,$-isomorphism.
\end{defn}

\begin{exam}  The vector space $\C$ of complex numbers with the usual multiplication
 \index{C*@$C^*$-algebra!$\C$ as a}%
 \index{C@$\C$!as a $C^*$-algebra}%
of complex numbers and complex conjugation $z \mapsto \conj z$ as involution is a unital
commutative $C^*$-algebra.
\end{exam}

\begin{exam} If $X$ is a compact Hausdorff space, the algebra $\fml C(X)$ of continuous
 \index{C*@$C^*$-algebra!$\fml C(X)$ as a}%
 \index{C@$\fml C(X)$!as a $C^*$-algebra}%
complex valued functions on $X$ is a unital commutative $C^*$-algebra when involution is
taken to be complex conjugation.
\end{exam}

\begin{exam} If $X$ is a locally compact Hausdorff space, the Banach algebra
 \index{C*@$C^*$-algebra!$\fml C_0(X)$ as a}%
 \index{C@$\fml C_0(X)$!as a $C^*$-algebra}%
$\fml C_0(X) = \fml C_0(X,\C)$ of continuous complex valued functions on $X$ which vanish
at infinity is a (not necessarily unital) commutative $C^*$-algebra when involution is
taken to be complex conjugation.
\end{exam}

\begin{exam} If $(X,\mu)$ is a measure space, the algebra $\lfs\infty(X,\mu)$ of
 \index{C*@$C^*$-algebra!$L_\infty(S)$ as a}%
 \index{L@$\lfs\infty(S)$!as a $C^*$-algebra}%
essentially bounded measurable complex valued functions on $X$ (again with complex
conjugation as involution) is a $C^*$-algebra.  (Technically, of course, the members of
$\lfs\infty(X,\mu)$ are equivalence classes of functions which differ on sets of measure zero.)
\end{exam}

\begin{exam}\label{001305fa} The algebra $\ofml B(H)$ of bounded linear operators on a Hilbert space
 \index{C*@$C^*$-algebra!$\ofml B(H)$ as a}%
 \index{B@$\ofml B(V)$!as a $C^*$-algebra}%
$H$ is a unital $C^*$-algebra.  The family $\ofml K(H)$ of compact operators is a closed $*\,$-ideal
in the algebra.
\end{exam}

\begin{exam}\label{001306} As a special case of the preceding example note that the set
 \index{C*@$C^*$-algebra!M@$\mathbf M_n$ as a}%
 \index{Ma@$\mathbf M_n = \M n\C$!as a $C^*$-algebra}%
$\mathbf M_n$ of $n \times n$ matrices of complex numbers is a unital $C^*$-algebra.
\end{exam}

\begin{prop}\label{prop_K_is_clo_FR} In a Hilbert space $H$ the ideal $\ofml K(H)$ of compact operators is the closure of
the ideal $\ofml{FR}(H)$ of finite rank operators.
\end{prop}

In the decade or so before the Second World War, some leading Polish mathematicians gathered regularly in coffee
houses in Lwow to discuss results and propose problems.  First they met in the \emph{Caf\'e Roma} and later in the
\emph{Scottish Caf\'e}.  In 1935 Stefan Banach produced a bound notebook in which they could write down the problems.
This was the origin of the now famous \emph{Scottish Book}~\cite{Mauldin:1981}.  On November 6, 1936 Stanislaw Mazur
wrote down Problem 153, known as the \emph{approximation problem}, which asked, essentially, if the preceding proposition
holds for Banach spaces.  Can every compact Banach space operator be approximated in norm by finite rank operators?
Mazur offered as the prize for solving this problem a live goose (one of the more generous prizes promised in the
\emph{Scottish Book}).  The problem remained open for decades,  until 1972 when it was solved negatively by Per Enflo,
who succeeded in finding a separable reflexive Banach space in which such an approximation fails.  In a ceremony held
that year in Warsaw, Enflo received his live goose from Mazur himself.  The result was published in Acta
Mathematica~\cite{Enflo:1973} in~1973.

\begin{exam} The diagonal operator $\diag(1,\frac12,\frac13,\dots)$ is a compact operator on the Hilbert space~$l_2$.
\end{exam}











\section{Partial Isometries}
\begin{defn}  An element $v$ of a $C^*$-algebra $A$ is a
 \index{partial!isometry}%
\df{partial isometry} if $v^*v$ is a projection in~$A$.  (Since $v^*v$ is always
self-adjoint, it is enough to require that $v^*v$ be idempotent.)  The element $v^*v$ is
the
 \index{initial!projection}%
 \index{projection!initial}%
 \index{partial!isometry!initial projection of a}%
\df{initial} (or
 \index{support!projection}%
 \index{projection!support}%
 \index{partial!isometry!support projection of a}%
\df{support}) \df{projection} of~$v$ and $vv^*$ is the
 \index{final!projection}%
 \index{projection!final}%
 \index{partial!isometry!final projection of a}%
\df{final} (or
 \index{range!projection}%
 \index{projection!range}%
 \index{partial!isometry!range projection of a}%
\df{range}) \df{projection} of~$v$.  (It is an obvious consequence of the next
proposition that if $v$ is a partial isometry, then $vv^*$ is in fact a projection.)
\end{defn}

\begin{prop}\label{pi0111} If $v$ is a partial isometry in a $C^*$-algebra, then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $vv^*v = v$\,; and
  \item $v^*$ is a partial isometry.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $z = v - vv^*v$ and consider $z^*z$.  \ns
\end{proof}

\begin{prop}\label{pi0114} Let $v$ be a partial isometry in a $C^*$-algebra~$A$.  Then its initial
projection $p$ is the smallest projection (with respect to the partial ordering~$\preceq$
on $\fml P(A)$\,) such that $vp = v$ and its final projection $q$ is the smallest
projection such that $qv = v$.
\end{prop}

\begin{prop}\label{pi0117} If $V$ is a partial isometry on a Hilbert space $H$ (that is, if $V$ is a partial
isometry in the $C^*$-algebra~$\ofml B(H)$\,), then the initial projection $V^*V$ is the
projection of $H$ onto $(\ker V)^\perp$ and the final projection $VV^*$ is the projection
of $H$ onto~$\ran V$.
\end{prop}

Because of the preceding result $(\ker V)^\perp$ is called the
 \index{initial!space}%
 \index{space!initial}%
 \index{space!support}%
 \index{support!space}%
\df{initial} (or \df{support}) \df{space} of~$V$ and $\ran V$ is sometimes called the
 \index{final!space}%
 \index{space!final}%
\df{final space} of~$V$.)

\begin{prop}\label{pi0121} An operator on a Hilbert space is a partial isometry if and only if it is an
isometry on the orthogonal complement of its kernel.
\end{prop}





\begin{thm}[Polar Decomposition]\label{0022287} If $T$ is an operator on a Hilbert
 \index{polar decomposition}%
 \index{decomposition!polar}%
space $H$, then there exists a partial isometry $V$ on $H$ such that
 \begin{enumerate}
    \item[(i)] the initial space of $V$ is $(\ker T)^\perp$,
    \item[(ii)] the final space of $V$ is $\clo{\ran T}$, and
    \item[(iii)] $T = V\abs T$.
 \end{enumerate}
This decomposition of $T$ is unique in the sense that if $T = V_0P$ where $V_0$ is a
partial isometry and $P$ is a positive operator such that $\ker V_0 = \ker P$, then $P =
\abs T$ and $V_0 = V$.
\end{thm}

\begin{proof} See \cite{Conway:1990}, VIII.3.11; \cite{Davidson:1996}, theorem I.8.1;
\cite{KadisonR:1983}, theorem 6.1.2; \cite{Murphy:1990}, theorem 2.3.4;
or~\cite{Pedersen:1995}, theorem 3.2.17. \ns
\end{proof}

\begin{cor} If $T$ is an
  \index{polar decomposition!of an invertible operator}%
 \index{operator!invertible!polar decomposition of}%
 \index{invertible!operator!polar decomposition of}%
invertible operator on a Hilbert space, then the partial isometry in the polar
decomposition (see~\ref{0022287}) is unitary.
\end{cor}

\begin{prop} If $T$ is a
 \index{polar decomposition!of a normal operator}%
 \index{operator!normal!polar decomposition of}%
 \index{normal!operator!polar decomposition of}%
normal operator on a Hilbert space $H$, then there exists a unitary operator $U$ on $H$
which commutes with $T$ and satisfies $T = U\abs T$.
\end{prop}

\begin{proof} See \cite{Pedersen:1995}, proposition 3.2.20.\ns \end{proof}

\begin{exer} Let $(S, \fml A,\mu)$ be a $\sigma$-finite measure space and $\phi \in
L_\infty(\mu)$. Find the polar decomposition of the
 \index{multiplication operator!polar decomposition of}%
 \index{operator!multiplication!polar decomposition of}%
 \index{polar decomposition!of a multiplication operator}%
multiplication operator $M_\phi$ (see example~\ref{C063527}).
\end{exer}









\section{Trace Class Operators}

Let's start by reviewing some of the standard properties of the \emph{trace} of an $n \times n$ matrix.

\begin{defn}\label{tr0113} Let $a = [a_{ij}] \in \mathbf M_n$. Define $\tr a$, the
 \index{trace!of a matrix}%
 \index{matrix!trace of a}%
 \index{tr@$\tr$ (trace)}%
\df{trace} of $a$ by
  \[ \tr a = \sum_{i = 1}^n a_{ii}. \]
\end{defn}

\begin{prop}\label{tr0117} The function $\tr\colon \mathbf M_n \sto \C$ is linear.
\end{prop}

\begin{prop}\label{tr0121} If $a$, $b \in \mathbf M_n$, then $\tr (ab) = \tr (ba)$.
\end{prop}

The trace is invariant under similarity.  Two $n \times n$ matrices $a$ and $b$ are
 \index{similar!matrices}%
 \index{matrices!similar}%
\df{similar} if there exists an invertible matrix $s$ such that $b = sas^{-1}$.

\begin{prop}\label{tr0124} If matrices in $\mathbf M_n$ are similar, then they have the same trace.
\end{prop}

\begin{defn}\label{tr0211} Let $H$ be a separable Hilbert space, $(e_n)$ be an orthonormal basis for~$H$, and $T$ be
a positive operator on~$H$.  Define the
 \index{trace}%
 \index{tr@$\tr$ (trace)}%
\df{trace} of~$T$~by
    \[ \tr T:=\sum_{k=1}^\infty \langle Te_k,e_k \rangle. \]
(The trace is not necessarily finite.)
\end{defn}

\begin{prop} If $S$ and $T$ are positive operators on a separable Hilbert space and $\alpha \in  \K$, then
   \[  \tr(S + T) = \tr S + \tr T \]
and
   \[ \tr(\alpha T) = \alpha \tr T. \]
\end{prop}

\begin{prop} If $T$ is an operator on a separable Hilbert space then $\tr(T^*T) = \tr(TT^*)$.
\end{prop}

To show that the definition given above of the trace of a positive operator does not depend on the orthonormal basis chosen for the
Hilbert space we will need a result (given next) which depends on a fact which appears later in these notes: every positive Hilbert
space operator has a (positive) square root (see example~\ref{X_sqroot_op}).

\begin{prop}\label{tr0221} On the family of positive operators on a separable Hilbert space $H$ the trace is invariant under unitary equivalence;
that is, if $T$ is positive and $U$ is unitary, then $\tr T = \tr(UTU^*)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] In the preceding proposition let $S = UT^{\frac12}$.    \ns
\end{proof}

\begin{prop}\label{tr0224} The definition of the \emph{trace}~\ref{tr0211} of a positive operator on a separable Hilbert space is independent
of the choice of orthonormal basis for the space.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $(e^k)$ and $(f^k)$ be orthonormal bases for the space and $T$ be a positive operator
on the space.  Take $U$ to be the unique unitary operator with the property that $e^k = Tf^k$ for each $k \in \N$.  \ns
\end{proof}

\begin{defn} Let $V$ be a vector space.  A subset $C$ of $V$ is a
 \index{cone}%
\df{cone} in $V$ is $\alpha C \subseteq C$ for every $\alpha \ge 0$.  A cone $C$ in $V$ is
 \index{cone!proper}%
 \index{proper!cone}%
\df{proper} is $C \cap (-C) = \{\vc 0\}$.
\end{defn}

 \begin{prop} A cone $C$ in a vector space is convex if and only if $C + C \subseteq C$.
\end{prop}

\begin{exam} On a separable Hilbert space the family of all positive operators with finite trace forms a proper convex cone in the vector
space~$\ofml B(H)$.
\end{exam}

\begin{defn} In a separable Hilbert space $H$ the linear subspace of $\ofml B(H)$ spanned by the positive operators with finite trace
is the family $\ofml T(H)$ of
 \index{trace!class}%
 \index{operator!trace class}%
 \index{T@$\ofml T(H)$ (trace class operators on~$H$)}%
\df{trace class} operators on~$H$.
\end{defn}

\begin{prop} Let $H$ be a separable Hilbert space with orthonormal basis~$(e_n)$.  If $T$ is a trace class operator on $H$, then
   \[ \tr T = \sum_{k=1}^\infty \langle Te_k,e_k \rangle \]
and the sum on the right is absolutely convergent.
\end{prop}
















\section{Hilbert-Schmidt Operators}

\begin{defn} An operator $A$ on a separable Hilbert space $H$ is a
 \index{Hilbert!-Schmidt operator}%
 \index{operator!Hilbert-Schmidt}%
\df{Hilbert-Schmidt} operator if $A^*A$ is of trace class; that is, if $\sum_{k=1}^\infty \norm{Ae_k}^2 < \infty$,
where $(e_k)$ is an orthonormal basis for~$H$.  The family of all Hilbert-Schmidt operators on $H$ is
 \index{HS@$\ofml{HS}(H)$ (Hilbert-Schmidt operators on~$H$)}%
denoted by~$\ofml{HS}(H)$.
\end{defn}

\begin{prop} If $H$ is a separable Hilbert space, then the family of Hilbert-Schmidt operators on $H$ is a two-sided
ideal in~$\ofml B(H)$.
\end{prop}

\begin{exam}  The family of Hilbert-Schmidt operators on a Hilbert space $H$ can be made into an inner product space.
\end{exam}

\begin{proof}[\emph{Hint for proof}]  Polarization.  If $A$, $B \in \ofml{HS}(H)$,
consider $\sum_{k=0}^3 i^k \bigl(A + i^kB\bigr)^*(A + i^kB)$.  \ns
\end{proof}

\begin{exam} The inner product you defined in the preceding example makes $\ofml{HS}(H)$ into a Hilbert space.
\end{exam}

\begin{prop} On a separable Hilbert space every Hilbert-Schmidt operator is compact.
\end{prop}

\begin{proof}[\emph{Hint for proof}] If $(e_k)$ is an orthonormal basis for the Hilbert space, consider the
operators $F_n := AP_n$ where $P_n$ is the projection onto the span of $\{e_1 \dots, e_n\}$. \ns
\end{proof}

\begin{exam} The integral operator defined in example~\ref{X_square_int_kernel} is a Hilbert-Schmidt operator.
\end{exam}

\begin{exam}
    The Volterra operator defined in example~\ref{000319} is a Hilbert-Schmidt operator.
\end{exam}











\endinput
\chapter{DISTRIBUTIONS}

\section{Inductive Limits}\label{sec_ind_limits}
\begin{defn}\label{defn_dir_sys} Let $D$ be a directed set under a partial ordering~$\le$ and let $\{A_i\colon i \in D\}$ be a family
of objects in some category~$\cat C$.  Suppose that for each $i$, $j \in D$ with $i \le j$ there exists a morphism
$\phi_{j,i}\colon A_i \sto A_j$ satisfying $\phi_{ki} = \phi_{kj}\phi_{ji}$ whenever $i \le j \le k$ in~$D$.  Suppose also that
$\phi_{ii}$ is the identity mapping on $A_i$ for each $i \in D$. Then the indexed family $\vc A = \bigl(A_i\bigr)_{i \in D}$ of
objects together with the indexed family $\bs\phi = \bigl(\phi_{ji}\bigr)$ of \emph{connecting morphisms} is called a
 \index{connecting morphisms}%
 \index{morphism!connecting}%
 \index{directed!system}%
 \index{system!directed}%
\df{directed system} in~$\cat C$.
\end{defn}

\begin{defn}\label{defn_ind_lim} Let the pair $(\vc A, \bs\phi)$ be a directed system (as above) in a category~$\cat C$ whose
underlying directed set is~$D$.  An
 \index{inductive!limit}%
 \index{limit!inductive}%
\df{inductive limit} (or
 \index{direct!limit}%
 \index{limit!direct}%
\df{direct limit}) of the system $(\vc A,\bs\phi)$ is a pair $(L,\bs\mu)$ where $L$ is an object in $\cat C$
and $\bs\mu = \bigl(\mu_i\bigr)_{i \in D}$ is a family of morphisms $\mu_j\colon A_j \sto L$ in $\cat C$ which satisfy
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $\mu_i = \mu_j \phi_{ji}$ whenever $i \le j$ in $D$, and
  \item if $(M,\bs\lambda)$ is a pair where $M$ is an object in $\cat C$ and $\bs\lambda = \bigl(\lambda_i\bigr)_{i \in D}$
is an indexed family of morphisms $\lambda_j\colon A_j \sto M$ in $\cat C$ satisfying $\lambda_i = \lambda_j\phi_{ji}$ whenever $i \le j$ in~$D$,
then there exists a unique morphism $\psi\colon L \sto M$ such that $\lambda_i = \psi\mu_i$ for each $i \in \N$.
 \end{enumerate}
Abusing language in a standard fashion we usually say that $L$ is the inductive limit of the system $\vc A = (A_i)$ and
 \index{<arrowto@$\underrightarrow{\lim} A_i$ (inductive limit)}%
write $L = \underrightarrow{\lim} A_i$.

\vskip 15 pt

   \[\xymatrix@+15pt@C+25pt{&&&& L\ar@{-->}[dddd]^\psi \\
                   &&&&   \\
                  {\cdots}\ar[r] & A_i \ar[uurrr]^{\mu_i}\ar[ddrrr]^{\lambda_i}\ar[r]^(.6){\phi_{ji}} &
                       A_j\ar[uurr]^{\mu_j}\ar[ddrr]^{\lambda_j}\ar[r] & {\cdots} &  \\
                   &&&&   \\
                   &&&& M
    }\]
\end{defn}

In an arbitrary (concrete) category inductive limits need not exist.  However, if they do they are unique.

\begin{prop}\label{prop_ind_lim_unique} Inductive limits (if they exist in a category) are unique (up to isomorphism).
\end{prop}

\begin{exam} Let $S$ be a nonempty object in the category $\cat{SET}$. Consider the family $\sfml P(S)$ of all subsets of $S$ directed
by the partial ordering~$\subseteq$.  Whenever $A \subseteq B \subseteq S$, take the connecting morphism $\sbsb\iota{BA}$ to be the
inclusion mapping from $A$ into~$B$. Then $\sfml P(S)$, together with the family of connecting morphisms, form a directed system.  The
inductive limit if this system is~$S$.
\end{exam}

\begin{exam} Direct limits exist in the category $\cat{VEC}$ of vector spaces and linear maps.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Let $(\vc V, \bs\phi)$ be a directed system in $\cat{VEC}$ (as in definition~\ref{defn_dir_sys})
and $D$ be its underlying directed set.  For the inductive limit of this system try
\smash[b]{$\displaystyle{\prod\limits_{i \in D}V_i \bs{\biggl/} \bigoplus\limits_{i \in D}V_i}$} (see examples~\ref{C015544}
and~\ref{X_dir_sum_VEC}).

It is helpful to keep in mind that two elements $\vc x$ and $\vc y$ in the quotient space lie in the same equivalence
class if their difference has finite support, that is, if $x_i$ and $y_i$ are eventually equal. You may find it helpful to make use of the functions
\smash[b]{$\nu_i\colon V_i \sto \prod\limits_{k \in D}V_k$} (where $i \in D$) defined by  $\nu_i(u) =\left\{
                                                                                      \begin{array}{ll}
                                                                                        \phi_{ji}(u), & \hbox{if $i \le j$\,;} \\
                                                                                        0, & \hbox{otherwise.}
                                                                                      \end{array}
                                                                                    \right. $  \ns
\end{proof}

\begin{exam} The sequence $\Z \to^{\vc 1} \Z \to^{\vc 2} \Z \to^{\vc 3} \Z \to^{\vc 4} \dots$
(where $\vc n \colon \Z \sto \Z$ satisfies $\vc n(1) = n$) is an inductive sequence of Abelian
groups whose inductive limit is the set $\Q$ of rational numbers.
\end{exam}

\begin{exam} The sequence $\Z \to^{\vc 2} \Z \to^{\vc 2} \Z \to^{\vc 2} \Z \to^{\vc 2} \dots$
is an inductive sequence of Abelian groups whose inductive limit is the set of dyadic rational
numbers.
\end{exam}











\section{$LF$-spaces}\label{sec_LFsps}
Since our principal goal in this chapter is an introduction to the theory of distributions, we will from now on restrict our attention
to an especially simple type of inductive limit---the \emph{strict inductive limit of an inductive sequence}.  In particular, we will
interested in strict inductive limits of increasing sequences of Fr\'echet spaces.

Recall that a weak topology on a set $S$ is induced by a family of functions $f_\alpha \colon S \sto
X_\alpha$ mapping into topological spaces.  It is the weakest topology on $S$ which makes
all these functions continuous (see exercise~\ref{C021421}). A dual notion is that of a
 \index{topology!strong}%
 \index{strong!topology}%
\df{strong topology}, a topology induced by a family of functions mapping \emph{from}
topological spaces $X_\alpha$ \emph{into}~$S$.  It is the strongest topology under which
all these functions are continuous.  One strong topology which we have already encountered is
the \emph{quotient topology} (see proposition~\ref{prop_quotient_top_strong}).  Another important
example, which we introduce next, is the \emph{inductive limit topology}.


\begin{defn} A
 \index{inductive!sequence!strict}%
 \index{sequence!strict inductive}%
 \index{strict inductive!sequence}%
\df{strict inductive sequence} is a sequence $(X_i)$ of locally convex spaces such that for each $i \in \N$
   \begin{enumerate}
     \item[(i)] $X_i$ is a closed subspace of $X_{i+1}$ and
     \item[(ii)] the topology $\sfml T_i$ on the space $X_i$ is the restriction to $X_i$ of the sets belonging to $\sfml T_{i+1}$.
   \end{enumerate}
This is, of course, a directed system in which the connecting morphisms $\phi{ji}$ are just the inclusion maps of $X_i$ into $X_j$ for $i < j$.

The
 \index{inductive!limit!strict}%
 \index{limit!strict inductive}%
 \index{strict inductive!limit}%
\df{strict inductive limit} of such a sequence of spaces is their union $L = \bigcup_{i=1}^\infty X_i$.  We give $L$ the largest
locally convex topology under which all the inclusion maps $\psi_i\colon X_i \sto L$ are continuous. This is the
 \index{inductive!limit!topology}%
 \index{topology!inductive limit}%
 \index{limit!inductive!topology}%
\df{inductive limit topology} on~$L$.
\end{defn}

\begin{prop} The inductive limit topology defined above exists.
\end{prop}

\begin{defn} The strict inductive limit of a sequence of Fr\'echet spaces is an
 \index{LF@$LF$-space}%
 \index{space!$LF$-}%
\df{$LF$-space}.
\end{defn}

\begin{exam} The space $\fml D(\Omega) = \fml C^\infty_c(\Omega)$ of test functions on an open subset $\Omega$ of a Euclidean space
is an $LF$-space (see notation~\ref{mi_notn}).
\end{exam}

\begin{prop} Every $LF$-space is complete.
\end{prop}

\begin{proof} See~\cite{Treves:1967}, theorem 13.1.  \ns
\end{proof}

\begin{prop} Let $T\colon L \sto Y$ be a linear map from an $LF$-space to a locally convex space.  Suppose that $L = \underrightarrow{\lim}X_k$
where $(X_k)$ is a strict inductive sequence of Fr\'echet spaces.  Then $T$ is continuous if and only if its restriction $T\bigr|_{X_k}$
to each $X_k$ is continuous.
\end{prop}

\begin{proof} See~\cite{Treves:1967}, proposition~13.1, or~\cite{Conway:1990}, proposition IV.5.7, or~\cite{Grubb:2009}, theorem B.18(c).  \ns
\end{proof}

\begin{cau} Fran\c{c}ois Treves, in his beautifully written book on \emph{Topological Vector Spaces, Distributions, and Kernels}, warns
his readers against a particularly insidious error: it is very easy to assume that if $M$ is a closed vector subspace of an $LF$-space
$L = \underrightarrow{\lim}X_k$, then the topology which $M$ inherits from $L$ must be the same as the inductive limit topology it gets
from the strict inductive sequence of Fr\'echet spaces~$M \cap X_k$.  This turns out \emph{not} to be true in general.
\end{cau}

\begin{prop} Let $T\colon L \sto Y$ be a linear map from an $LF$-space to a locally convex space.  Then $T$ is continuous if and only if
it is sequentially continuous.
\end{prop}




















\section{Distributions}
\begin{defn} Let $\Omega$ be a nonempty open subset of~$\R^n$.  A function $f\colon \Omega \sto \R$ is
 \index{integrable!locally}%
 \index{locally!integrable}%
\df{locally integrable} if $\int_K \abs{f}\,d\mu < \infty$ for every compact subset $K$ of~$\Omega$.  (Here $\mu$ is the usual $n$-dimensional
Lebesgue measure.)  The family of all such functions on $\Omega$ is denoted
 \index{L@$\locint\Omega$!locally integrable functions on~$\Omega$}%
by~$\locint\Omega$.
\end{defn}

\begin{exam} The constant function $\vc 1$ on the real line is locally integrable, even though it is certainly not integrable.
\end{exam}

A locally integrable function need not even be continuous. On the  real line, for example, the characteristic function of any Lebesgue measurable
set is locally integrable.  Imagine how convenient it would be if we could speak meaningfully of the \emph{derivative} of any such function and,
even better, be permitted to differentiate it as often as we please. ---  Welcome to the world of distributions. Here
   \begin{enumerate}
       \item every locally integrable function may be regarded as a distribution, and
       \item every distribution is infinitely differentiable, and furthermore,
       \item all the familiar differentiation rules of beginning calculus hold.
   \end{enumerate}

\begin{defn} Let $\Omega$ be a nonempty open subset of~$\R^n$.  A
 \index{distribution}%
\df{distribution} is a continuous linear functional on the space $\fml D(\Omega) = \fml C^\infty_c(\Omega)$ of test functions on~$\Omega$.
Although such a functional is, technically, defined on a space of functions on $\Omega$, we will follow the usual sloppy practice
and refer to it as a \emph{distribution on~$\Omega$}.  The set of all distributions on $\Omega$, the dual space of $\fml D(\Omega)$
 \index{D@$\fml D^*(\Omega)$ (space of distributions on~$\Omega$)}%
will be denoted by~$\fml D^*(\Omega)$.
\end{defn}

\begin{prop} Let $\Omega$ be an open subset of~$\R^n$.  A linear functional $L$ on the space $\fml D(\Omega)$ of test functions on $\Omega$ is
a distribution if and only if $L(\phi_k) \sto 0$ whenever $(\phi_k)$ is a sequence of test functions such that $\phi_k^{(\bs\alpha)} \sto 0$
uniformly for every multi-index $\bs\alpha$ and there exists a compact set $K \subseteq \Omega$ which contains the support of each~$\phi_k$.
\end{prop}

\begin{proof} See~\cite{Conway:1990}, proposition IV.5.21 or~\cite{Treves:1967}, proposition 21.1(b).  \ns
\end{proof}

\begin{exam}  If $\Omega$ is a nonempty open subset of $\R^n$ and $f$ is a locally integrable function on $\Omega$, then the map
   \[ L_f\colon \fml D(\Omega) \sto \K \colon \phi \mapsto \int f\phi\,d\lambda \]
is a distribution.  Such a distribution is called a
 \index{regular!distribution}%
 \index{distribution!regular}%
 \index{L@$L_f$ (regular distribution associated with~$f$)}%
\df{regular} distribution. A distribution which is not regular is called a
 \index{singular!distribution}%
 \index{distribution!singular}%
\df{singular} distribution.
\end{exam}

\emph{Note.}  Let $u$ be a distribution and $\phi$ be a test function on $\Omega$. Standard notations for $u(\phi)$ are
$\langle u,\phi \rangle$ and $\langle \phi,u \rangle$.  Also we use the
 \index{<unoptop@$\tilde f$ (regular distribution associated with~$f$)}%
notation $\tilde f$ for $L_f$. Thus in the preceding
example
    \[ \langle \tilde f,\phi \rangle = \langle \phi,\tilde f \rangle
            = \tilde f(\phi) = \langle L_f,\phi \rangle
            = \langle \phi,L_f \rangle = L_f(\phi) \]
for $f \in \locint\Omega$.

\begin{exam} If $\mu$ is a regular Borel measure on a nonempty open subset $\Omega$ of~$\R^n$ and $f$ is a locally integrable
function on $\Omega$, then the map
    \[ L_\mu\colon \fml D(\Omega) \sto \R\colon \phi \mapsto \int_\Omega f\phi \,d\mu \]
is a distribution on $\Omega$.
\end{exam}

\begin{notn} Let $a \in \R$. Define $\mu_a$,
 \index{Dirac!measure}%
 \index{measure!Dirac}%
 \index{mua@$\mu_a$ (Dirac measure concentrated at~$a$)}%
\df{Dirac measure} concentrated at $a$, on the Borel sets of $\R$ by
   \[ \mu_a(B) = \begin{cases}  1, &\text{if $a \in B$} \\
                                0, &\text{if $a \notin B$}.
                 \end{cases} \]
We denote the corresponding distribution by $\delta_a$ and call it the
 \index{Dirac!distribution}%
 \index{distribution!Dirac}%
 \index{delta@$\delta_a$, $\delta$ (Dirac distribution)}%
\df{Dirac delta distribution at $a$}.  Thus $\delta_a = L_{\mu_a}$.  If $a = 0$, we write $\delta$ for $\delta_0$.
Historically the distribution $\delta$ has been afflicted with a notoriously misleading (although not technically
incorrect) name: the \emph{Dirac delta function}.
\end{notn}

\begin{exam} If $\delta_a$ is the \emph{Dirac delta distribution} at a point $a \in \R$ and $\phi$ is a test function on~$\R$,
then $\delta_a(\phi) = \phi(a)$.
\end{exam}

\begin{notn} Let $H$ denote the characteristic function of the interval $[0,\infty)$.  This is the
 \index{Heaviside!function}%
 \index{function!Heaviside}%
\df{Heaviside function}.  Its corresponding distribution $\widetilde H = L_H$ is the
 \index{Heaviside!distribution}%
 \index{distribution!Heaviside}%
 \index{H@$\wt H$ (Heaviside distribution)}%
\df{Heaviside distribution}.
\end{notn}

\begin{exam} If $\wt H$ is the \emph{Heaviside distribution} and $\phi \in \fml D(\R)$, then $\wt H(\phi) = \int_0^\infty \phi$.
\end{exam}

\begin{exam} Let $f$ be a differentiable function on $\R$ whose derivative is locally integrable.  Then
    \begin{equation}\label{deriv_reg_distr}
        L_{f\,'}(\phi) = -L_f(\phi\,')
    \end{equation}
for every $\phi \in \fml D(\R)$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Integration by parts. \ns
\end{proof}

Now an important question arises: How can we define the ``derivative'' of a distribution?  It is certainly natural to want a regular
distribution to behave in much the same way as the function from which it arises.  That is, we would like the derivative $(L_f)'$ of
a regular distribution $L_f$ to correspond with the regular distribution arising from the derivative of the function~$f$.  That is to
say, what we want is $(L_f)' = L_{f\,'}$.  This suggests, according to the preceding example, that for differentiable functions $f$
with locally integrable derivatives we should define
   \begin{equation}\label{deriv_distr1}
        (L_f)'(\phi) = -L_f(\phi\,')
   \end{equation}
for every $\phi \in \fml D(\R)$. The observation that the right side of the equation~\eqref{deriv_distr1} makes sense for
\emph{any distribution whatever} motivates the following definition.

\begin{defn}\label{def_deriv_distr} Let $u$ be a distribution on a nonempty open subset $\Omega$ of~$\R$. We define $u\,'$, the
 \index{derivative!of a distribution}%
 \index{distribution!derivative of a}%
\df{derivative} of $u$,
 \index{D@$Du$ (derivative of a distribution~$u$)}%
 \index{<unopur@$u'$ (derivative of a distribution~$u$)}%
by
    \[ u\,'(\phi) := -u(\phi\,') \]
for every test function $\phi$ on~$\Omega$.  We also use the notation $Du$ for the derivative of~$u$.
\end{defn}

\begin{exam} The \emph{Dirac distribution} $\delta$ is the derivative of the \emph{Heaviside distribution}~$\widetilde H$.
\end{exam}

\begin{exer} Let $S$ be the signum (or sign) function on $\R$; that is, let $S = 2H - 1$, where $H$ is the Heaviside function.
Also let $g$ be the function on $\R$ such that $g\colon x \mapsto -x - 3$ for $x < 0$ and $g\colon x \mapsto x + 5$ for $x \ge 0$.
   \begin{enumerate}[label=\rm{(\alph*)}]
     \item Find constants $\alpha$ and $\beta$ such that ${\tilde g}' = \alpha \widetilde S + \beta \delta$.
     \item Find a function $a$ on $\R$ such that ${\tilde a}' = \wt S$.
   \end{enumerate}
\end{exer}

\begin{exer} Let $f(x) = \left\{
                           \begin{array}{ll}
                             1 - \abs x, & \hbox{if $\abs x \le 1$;} \\
                                 0,      & \hbox{otherwise.}
                           \end{array}
                         \right.$   By calculating both sides of equation~\eqref{deriv_reg_distr} separately, show that it is
classically correct for every test functions whose support is contained in the interval~$[-1,1]$.
\end{exer}

\begin{exer} Let $f(x) = \sbsb\chi{(0,1)}$ for $x \in~\R$ and $\phi \in \fml D\bigl((-1,1)\bigr)$. Compare the values of $L_{f\,'}(\phi)$ and
$\bigl(L_f\bigr)'(\phi)$.
\end{exer}

\begin{exer} Let $f(x) = \left\{
                           \begin{array}{ll}
                             x, & \hbox{if $0 \le x \le 1$;} \\
                             x + 2, & \hbox{if $1 < x \le 2$;} \\
                             0, & \hbox{otherwise.}
                           \end{array}
                         \right.$  Compare the values of $L_{f\,'}(\phi)$ and $\bigl(L_f\bigr)'(\phi)$ for a test function~$\phi$.
\end{exer}

\begin{exer} Let $h(x) = \left\{
                           \begin{array}{ll}
                             2x, & \hbox{if $x < -1$;} \\
                             1, & \hbox{if $-1 \le x < 1$;} \\
                             0, & \hbox{if $x \ge 1$.}
                           \end{array}
                         \right.$ Find a locally integrable function $f$ such that ${\tilde f}\,' = \tilde h + 3\delta_1 - \delta_{-1}$.
\end{exer}

No difficulty is encountered in extending definition~\ref{def_deriv_distr} to partial derivatives of distributions.

\begin{defn} Let $u$ be a distribution on some nonempty open subset $\Omega$ of $\R^n$ for some $n \ge 2$ and $\bs\alpha$
be a multi-index. We define the
 \index{differential operator!acting on a distribution}%
 \index{D@$D^{\bs\alpha}$ (multi-index notation for differentiation)}%
\df{differential operator} $D^{\bs\alpha} = \bigl(\frac{\partial}{\partial x_1}\bigr)^{\alpha_1} \dots
\bigl(\frac{\partial}{\partial x_n}\bigr)^{\alpha_n}$ of order $\abs{\bs\alpha}$ acting on $u$ by
   \[ D^{\bs\alpha}u(\phi) := (-1)^{\abs{\bs\alpha}}u\bigl(\phi^{(\bs\alpha)}) \]
for every test function $\phi$ on~$\Omega$.
 \index{<unopura@$u^{\bs\alpha}$ (multi-index notation for differentiation)}%
An alternative notation for $D^{\bs\alpha}u$ is $u^{(\bs\alpha)}$.
\end{defn}

\begin{exer} A \df{dipole} (of electric moment 1 at the origin in $\R$) may be thought of as the ``limit'' as $\epsilon \sto 0^+$
 \index{dipole}%
of a system $T_\epsilon$ of two charges $-\frac1\epsilon$ and $\frac1\epsilon$ placed at $0$ and $\epsilon$, respectively.  Show
how this might lead one to define a dipole mathematically as $-\delta\,'$.  \emph{Hint.}  Think of $T_\epsilon$ as the
distribution $\frac1\epsilon \delta_\epsilon - \frac1\epsilon \delta$.
\end{exer}

\begin{defn} Let $X$ be a Hausdorff locally convex space and $X^*$ be its dual space, that is, the space of all continuous linear
functionals on~$X$.  For every $f \in X^*$ define
  \[ p_f\colon X \sto \K\colon x \mapsto \abs{f(x)}. \]
Clearly each $p_f$ is a seminorm on~$X$.  Just as in the case of normed linear spaces we define the
 \index{weak!topology!on locally convex spaces}%
 \index{sigma@$\sigma(X,X^*)$ (weak topology on~$X$)}%
\df{weak topology} on $X$, denoted by $\sigma(X,X^*)$, to be the weak topology generated the family $\{p_f\colon f \in X^*\}$
of seminorms on~$X$. Similarly, for each $x \in X$ define
   \[ p_x\colon X^* \sto \K \colon f \mapsto \abs{f(x)}. \]
Again each $p_x$ is a seminorm on~$X^*$ and we define the
 \index{w*@$w^*$-topology (weak star topology)}%
 \index{sigma@$\sigma(X^*,X)$ ($w^*$-topology on~$X^*$)}%
\df{$w^*$-topology} on $X^*$, denoted by $\sigma(X^*,X)$, to be the weak topology generated by the family $\{p_x\colon x \in X\}$
of seminorms on~$X^*$.
\end{defn}

\begin{prop} If the space of distributions $\fml D^*(\Omega)$ on an open subset $\Omega$ of $\R^n$ is given the $w^*$-topology,
then a net $(u_\lambda)$ of distributions on $\Omega$ converges to a distribution $v$ on $\Omega$ if and only if
$\left<u_\lambda, \phi\right> \sto \left<v, \phi\right>$ for every test function $\phi$ on $\Omega$.
\end{prop}

\begin{prop} Suppose that a net $(u_\lambda)$ of distributions converges to a distribution~$v$.  Then ${u_\lambda}^{(\alpha)} \sto v^{(\alpha)}$
for every multi-index $\alpha$.
\end{prop}

\begin{proof} See \cite{Rudin:1991}, Theorem 6.17; or \cite{Yosida:1965}, Chapter II, Section 3, Theorem; or \cite{Grubb:2009}, Theorem 3.9;
or~\cite{HirschL:1999}, Chapter 8, Proposition 2.4.
\end{proof}

\begin{prop} If $g$, $f_k \in \locint\Omega$ for every $k$ (where $\open{\Omega}{\R^n}$) and $f_k \sto g$ uniformly on
every compact subset of $\Omega$, then $\tilde f_k \sto \tilde g$.
\end{prop}

\begin{prop} If $f \in \fml C^\infty(\R^n)$, then $\left(L_f\right)^{(\bs\alpha)} = L_{f^{(\bs\alpha)}}$ for any multi-index $\bs\alpha$.
\end{prop}

\begin{exer} If $\delta$ isn't a function on $\R$, then what do people think they mean when they write
   \[ \lim_{k \sto \infty} \frac{k}{\pi(1+k^2x^2)} = \delta(x)\quad? \]
Show that, properly interpreted (as an assertion about distributions), it is even correct.  \emph{Hint.}
If $s_k(x) = k\,\pi^{-1}(1+k^2x^2)^{-1}$ and $\phi$ is a test function on $\R$, then
$\int_{-\infty}^\infty s_k\phi = \int_{-\infty}^\infty s_k\phi(0) + \int_{-\infty}^\infty s_k\eta$ where $\eta(x) = \phi(x) - \phi(0)$.
Show that $\int_{-\infty}^\infty s_k\eta \sto 0$ as $k \sto \infty$.
\end{exer}

\begin{exer} Let $f_n(x) = \sin nx$ for $n \in \N$ and $x \in \R$.  The sequence $(f_n)$ does not converge classically;
but it does converge distributionally (to $0$).  Make this assertion precise, and prove it.  \emph{Hint.}  Antidifferentiate.
\end{exer}

\begin{prop} If $\phi$ is an infinitely differentiable function on $\R$ with compact support, then
    \[ \lim_{n \sto \infty}\frac2\pi \int_{-\infty}^\infty \frac{n^3x}{\bigl(1 + n^2x^2\bigr)^2}\,\, \phi(x)\,dx = \phi'(0)\,.\]
\end{prop}

\begin{exer} Make sense of the expression
   \[ 2\pi\sum_{n = -\infty}^\infty \delta(x-2\pi n) =  1 +  2\sum_{n = 1}^\infty \cos nx\,, \]
and show that, properly interpreted, it is correct.  \emph{Hint.}  Anyone depraved enough to believe that $\delta(x)$ \emph{means}
something would be likely to interpret $\delta(x-a)$ as being the same thing as $\delta_a(x)$. You might start the process of
rationalization by calculating the Fourier series of the function $g$ defined by $g(x) = \frac1{4\pi}x^2 - \frac12 x$ on $[0,2\pi]$
and then extended periodically to $\R$.  You may need to look up theorems on pointwise and uniform convergence of Fourier series
and on term-by-term differentiation of such series.
\end{exer}

\begin{defn} Let $\Omega$ be an open subset of $\R^n$, $u \in \fml D^*(\Omega)$, and $f \in \fml C^\infty(\Omega)$.
 \index{multiplication!of a distribution by a smooth function}%
 \index{<binopconcat@$fu$ (product of a smooth function with a distribution)}%
Define
   \[ (fu)(\phi) := u(f\phi) \]
for all test functions $\phi$ on~$\Omega$.   Equivalently, we may write $\langle fu,\phi \rangle = \langle u,f\phi \rangle$.
\end{defn}

\begin{prop} The function $fu$ in the preceding definition is a distribution on~$\Omega$.
\end{prop}

\begin{proof} See~\cite{Rudin:1991}, page 159, section 6.15.    \ns
\end{proof}





















\section{Convolution}

This section and section~\ref{section_Fourier_transform} provide a \emph{very} brief introduction to the convolution
and Fourier transforms of distributions.  For many of the proofs, and a more detailed exposition, the reader is referred
to Walter Rudin's elegant \emph{Functional Analysis} text~\cite{Rudin:1991}.

To avoid the appearance in formulas of an excessive number of factors of the form $\sqrt{2\pi}$ and its reciprocal, we will
for the remainder of this chapter integrate with respect to
 \index{normalized Lebesgue measure}%
 \index{m@$m$ (normalized Lebesgue measure)}%
\df{normalized Lebesgue measure} $m$ on~$\R$. It is defined by
   \[ m(A) := \frac1{\sqrt{2\pi}}\lambda(A) \]
for every Lebesgue measurable subset $A$ of $\R$ (where $\lambda$ is ordinary Lebesgue measure on~$\R$).

We review a few elementary facts from real analysis concerning convolution of scalar valued functions.  Recall that a complex or extended
real valued function on $\R$ is said to be
 \index{Lebesgue integrable}%
 \index{integrable}%
\emph{Lebesgue integrable} if $\int_\R\abs f\, dm < \infty$
(where $m$ is normalized Lebesgue measure on~$\R$).
 \index{L@$\lfs 1(\R)$!space of Lebesgue integrable functions}%
 \index{L@$\lfs 1(\R)$!as a Banach space}%
 \index{Banach!space!$\lfs 1(\R)$ as a}%
We denote by $\lfs 1(\R)$ the Banach space of all equivalence classes of Lebesgue integrable functions on~$\R$, two functions being \emph{equivalent}
if the Lebesgue measure of the set on which they differ is zero.  The norm on this Banach space
   \index{<unopn@$\norm{\hphantom{x}}_1$ ($1$-norm)}%
is given by
   \[ {\norm f}_1 := \int_R \abs f\,dm. \]

\begin{prop} Let $f$ and $g$ be Lebesgue integrable functions on~$\R$. Then the function
   \[ y \mapsto f(x - y)g(y) \]
belongs to $\lfs 1(\R)$ for almost all~$x$.  Define a
 \index{<binopast@$f \ast g$ (convolution of two functions)}%
function $f \ast g$ by
   \[ (f \ast g)(x) := \int_\R f(x - y)g(y)\,dm(y) \]
wherever the right hand side makes sense.  Then the function $f \ast g$ belongs to $\lfs 1(\R)$.
\end{prop}

\begin{proof} See~\cite{HewittS:1965}, Theorem 21.31.  \ns
\end{proof}

\begin{defn} The function $f \ast g$ defined above is the
 \index{convolution!of functions in $\lfs 1(\R)$}%
\df{convolution} of $f$ and~$g$.
\end{defn}

\begin{prop} The Banach space $\lfs 1(\R)$ under convolution is a commutative Banach algebra.  It is, however, \emph{not} unital.
\end{prop}

\begin{proof} See~\cite{HewittS:1965}, Theorems 21.34 and 21.35.  \ns
\end{proof}

\begin{defn}\label{defn_Fourier_transform} For each $f$ in $\lfs 1(\R)$ define a function $\hat f$ by
    \[ \hat f(x) = \int_{-\infty}^\infty f(t) e^{-itx}\,dm(t). \]
The function $\hat f$ is the
 \index{Fourier!transform}%
 \index{transform!Fourier}%
 \index{F@$\mathfrak F(f)$ (Fourier transform of~$f$)}%
 \index{<unoptop@$\hat f$ (Fourier transform of~$f$)}%
\df{Fourier transform} of~$f$.  Sometimes we write $\mathfrak Ff$ for~$\hat f$.
\end{defn}

\begin{thm}[Riemann-Lebesgue Lemma] If $f \in \lfs 1(\R)$,
 \index{Riemann-Lebesgue lemma}%
then $\hat f \in \fml C_0(\R)$.
\end{thm}

\begin{proof} See~\cite{HewittS:1965}, 21.39.   \ns
\end{proof}

\begin{prop} The Fourier transform
   \[ \mathfrak F\colon \lfs 1(\R) \sto \fml C_0(\R)\colon f \mapsto \hat f \]
is a homomorphism of Banach algebras.  Neither algebra is unital.
\end{prop}

For applications the most useful aspect of the preceding proposition is that the Fourier
transform converts convolution in $\lfs 1(\R)$ to
pointwise multiplication in~$\fml C_0(\R)$; that is, $\wh{fg} =\hat f\hat g$.

We next define what it means to take the convolution of a distribution and a test function.

\begin{notn} For a point $x \in \R$ and a scalar valued function $\phi$ on $\R$
 \index{phi@$\phi_x$}%
define
   \[ \phi_x\colon \R \sto \R\colon y \mapsto \phi(x - y). \]
\end{notn}

\begin{prop} If $\phi$ and $\psi$ are scalar valued functions on $\R$ and $x \in \R$, then
   \[ (\phi + \psi)_x = \phi_x + \psi_x\,. \]
\end{prop}

\begin{prop}\label{prop_convol_distr1} For $u \in \fml D^*(\R)$ and $\phi \in \fml D(\R)$,
 \index{<binopast@$u \ast \phi$ (convolution of a distribution and a test function)}%
let
   \[ (u \ast \phi)(x) := \langle u,\phi_x \rangle \]
for all $x \in \R$.  Then $u \ast \phi \in \fml C^\infty(\R)$.
\end{prop}

\begin{proof} See~\cite{Rudin:1991}, Theorem 6.30(b).  \ns
\end{proof}

\begin{defn} When $u$ and $\phi$ are as in the preceding proposition, the function $u \ast \phi$ is the
 \index{convolution!of a distribution and a test function}%
\df{convolution} of $u$ and~$\phi$.
\end{defn}

\begin{prop} If $u$ and $v$ are distributions on $\R$ and $\phi$ is a test function on $\R$, then
   \[ (u + v) \ast \phi = (u \ast \phi) + (v \ast \phi)\,. \]
\end{prop}

\begin{prop} If $u$ is a distribution on $\R$ and $\phi$ and $\psi$ are test functions on $\R$, then
   \[ u \ast (\phi + \psi) = (u \ast \phi) + (u \ast \psi)\,. \]
\end{prop}

\begin{exam} If $H$ is the Heaviside function and $\phi$ is a test function on~$\R$, then
   \[ (\wt H \ast \phi)(x) = \int_{-\infty}^x \phi(t)\,dt. \]
\end{exam}

\begin{prop}\label{prop_convol_distr2} If $u \in \fml D^*(\R)$ and $\phi \in \fml D(\R)$, then
   \[ D^{\bs\alpha}(u \ast \phi) = (D^{\bs\alpha}u) \ast \phi = u \ast \bigl(D^{\bs\alpha}(\phi)\bigr). \]
\end{prop}

\begin{proof} See~\cite{Rudin:1991}, Theorem 6.30(b).  \ns
\end{proof}

\begin{prop}\label{prop_convol_distr3} If $u \in \fml D^*(\R)$ and $\phi$, $\psi \in \fml D(\R)$, then
   \[ u \ast (\phi \ast \psi) = (u \ast \phi) \ast \psi. \]
\end{prop}

\begin{proof} See~\cite{Rudin:1991}, Theorem 6.30(c).  \ns
\end{proof}

\begin{prop} Let $u$, $v \in \fml D^*(\R)$.  If
   \[ u \ast \phi = v \ast \phi \]
for all $\phi \in \fml D(\R)$, then $ u = v$.
\end{prop}

\begin{defn} Let $\Omega$ be an open subset of $\R^n$ and $u \in \fml D^*(\R^n)$.  We say that ``$u = 0$ on $\Omega$'' if $u(\phi) = 0$
for every $\phi \in \fml D(\Omega)$.  In the same vein we say that ``two distributions $u$ and $v$ are equal on $\Omega$'' if $u - v = 0$
on~$\Omega$.  A point $x \in \R^n$ belongs to the
 \index{support!of a distribution}%
 \index{supp@$\supp u$ (support of a distribution~$u$)}%
\df{support} of $u$ if there is \emph{no} open neighborhood of $x$ on which $u = 0$.  Denote the support of $u$ by~$\supp u$.
\end{defn}

\begin{exam} The support of the Dirac delta distribution $\delta$ is~$\{0\}$.
\end{exam}

\begin{exam} The support of the Heaviside distribution $\wt H$ is~$[0,\infty)$.
\end{exam}

\begin{rem} Suppose that a distribution $u$ on $\R^n$ has compact support.  Then it can be shown that $u$ has a unique extension to a
continuous linear functional on~$\fml C^\infty(\R^n)$, the family of all smooth functions on~$\R^n$.  Under these conditions the conclusion
of proposition~\ref{prop_convol_distr1} remains true and the following definition applies.  If $u \in \fml D^*(\R^n)$ has compact support
and $\phi \in \fml C^\infty(\R^n)$, then the conclusion of proposition~\ref{prop_convol_distr2} remains true.  And if, additionally, $\psi$
is a test function on $\R^n$, then $u \ast \psi$ is a test function on $\R^n$ and the conclusion of~\ref{prop_convol_distr3} continues to hold.
For a thorough discussion and verification of these claims see~\cite{Rudin:1991}, Theorems 6.24 and~6.35.
\end{rem}

\begin{prop} If $u$ and $v$ are distributions on $\R^n$ and at least one of them has compact support, then there exists a unique distribution
$u \ast v$ such that
   \[ (u \ast v) \ast \phi = u \ast (v \ast \phi) \]
for all test functions $\phi$ on~$\R^n$.
\end{prop}

\begin{proof} See \cite{Rudin:1991}, Definition 6.36\emph{ff.}   \ns
\end{proof}

\begin{defn} In the preceding proposition the distribution $u \ast v$ is the
 \index{convolution!of two distributions}%
 \index{<binopast@$u \ast v$ (convolution of two distributions)}%
\df{convolution} of $u$ and~$v$.
\end{defn}

\begin{exam} If $\delta$ is the Dirac delta distribution, then
   \[ \tilde 1 \ast \delta\,' = \tilde 0\,. \]
\end{exam}

\begin{exam} If $\delta$ is the Dirac delta distribution and $H$ is the Heaviside function, then
   \[ \delta\,' \ast \wt H = \delta\,. \]
\end{exam}

\begin{prop} If $u$ and $v$ are distributions on $\R^n$ and at least one of them has compact support, then $u \ast v = v \ast u$.
\end{prop}

\begin{proof} See \cite{Rudin:1991}, Theorem 6.37(a).  \ns
\end{proof}

\begin{exer} Prove the preceding proposition under the assumption that \emph{both} $u$ and $v$ have compact support.  \emph{Hint.}
Work with $(u \ast v) \ast (\phi \ast \psi)$ where $\phi$ and $\psi$ are test functions, keeping in mind that convolution of functions
is commutative.
\end{exer}

\begin{prop} If $u$ and $v$ are distributions on $\R^n$ and at least one of them has compact support, then
  \[ \supp(u \ast v) \subseteq \supp u + \supp v. \]
\end{prop}

\begin{proof} See \cite{Rudin:1991}, Theorem 6.37(b).  \ns
\end{proof}

\begin{prop}\label{prop_assoc_convol_distr} If $u$, $v$, and $w$ are distributions on $\R^n$ and at least two of them have
compact support, then
    \[ u \ast (v \ast w) = (u \ast v) \ast w. \]
\end{prop}

\begin{exam} The expression $\tilde 1 \ast \delta\,' \ast \wt H$ is ambiguous.  Thus the hypothesis concerning the support of the
distributions in the preceding proposition cannot be omitted.
\end{exam}

\begin{prop} Under addition, scalar multiplication, and convolution the family of all distributions on $\R$ with compact support
is a unital commutative algebra.
\end{prop}

















\section{Distributional Solutions to Ordinary Differential Equations}

\begin{notn} Let $\Omega$ be a nonempty open subset of $\R$, $n \in \N$, and $a_0$, $a_1$, \dots $a_n \in \fml C^\infty(\Omega)$.
We consider the (ordinary)
 \index{differential!operator}%
 \index{operator!differential}%
differential operator
   \[ L = \sum_{k=0}^n a_k(x)\frac{d^k}{dx^k} \]
and its associated
 \index{differential!equation}%
 \index{equation!differential}%
(ordinary) differential equation
   \begin{equation}\label{eqn_ODE1}
        Lu = v
   \end{equation}
where $u$ and $v$ are distributions on~$\Omega$.  In connection with equation~\eqref{eqn_ODE1} one can also consider two variants:
the equation
   \begin{equation}\label{eqn_ODE2}
        \langle Lu, \phi\rangle = \langle v, \phi\rangle
   \end{equation}
where $\phi$ is a test function, and the equation
   \begin{equation}\label{eqn_ODE3}
        \langle u, L^*\phi\rangle = \langle v, \phi\rangle
   \end{equation}
where $\phi$ is a test function and $L^* := \displaystyle\sum_{k=0}^n (-1)^k \frac{d^k(a_k\phi)}{dx^k}$ is the
 \index{adjoint!formal}%
 \index{formal adjoint}%
\df{formal adjoint} of~$L$.
\end{notn}

\begin{defn} Suppose that in the equations above the distribution $v$ is given. If there is a regular distribution $u$ which
satisfies~\eqref{eqn_ODE1}, we say that $u$ is a
 \index{classical solution}%
 \index{solution!classical}%
\df{classical} solution to the differential equation.  If $u$ is a regular distribution corresponding to a function which does
\emph{not} satisfy~\eqref{eqn_ODE1} in the classical sense but which \emph{does} satisfy~\eqref{eqn_ODE2} for every test function~$\phi$,
then we say that $u$ is a
 \index{weak!solution}%
 \index{solution!weak}%
\df{weak} solution to the differential equation.  And if $u$ is a singular distribution which satisfies~\eqref{eqn_ODE3} for every test
function~$\phi$, we say that $u$ is a
 \index{distributional solution}%
 \index{solution!distributional}%
\df{distributional} solution to the differential equation. The family of
 \index{generalized solutions}%
 \index{solution!generalized}%
\df{generalized} solutions comprises all classical, weak, and distributional solutions.
\end{defn}

\begin{exam} The only generalized solutions to the equation $\dfrac{du}{dx} = 0$ on~$\R$ are the constant distributions (that is, the
regular distributions arising from constant functions). Thus every generalized solution is classical.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Suppose that $u$ is a generalized solution to the equation.  Choose a test function $\sbsb\phi 0$ such
that $\int_\R \sbsb\phi 0 = 1$.  Let $\phi$ be an arbitrary test function on~$\R$. Define
   \[ \psi(x) = \phi(x) - \sbsb\phi 0(x)\int_R \phi \]
for all $x \in \R$.  Observe that $\langle u,\psi \rangle = 0$ and compute $\langle u,\phi \rangle$.  \ns
\end{proof}

\begin{exam} The Heaviside distribution is a weak solution to the differential equation $x\dfrac{du}{dx}~=~0$ on~$\R$.  Of course, the
constant distributions are classical solutions.
\end{exam}

\begin{exam} The Dirac $\delta$ distribution is a distributional solution to the differential equation $x^2\dfrac{du}{dx} = 0$ on~$\R$.
(The Heaviside distribution is a weak solution; and constant distributions are classical solutions.)
\end{exam}

\begin{exam} For $p = 0,1,2,\dots$ the $p^{\textrm{th}}$ derivative $\dfrac{d^P\delta}{dx^p}$ of the Dirac delta distribution is a solution
to the differential equation $x^{p+2}\dfrac{du}{dx} = 0$ on~$\R$.
\end{exam}

\begin{exer} Find a distributional solution to the differential equation $x \dfrac{du}{dx} + u = 0$ on~$\R$.
\end{exer}













\section{The Fourier Transform}\label{section_Fourier_transform}

In~\ref{defn_Fourier_transform} we defined the Fourier transform of an $\lfs 1\R$ function.  An equivalent way of expressing
this definition is in terms of convolution:
  \[ (\mathfrak F f)(x) = (f \ast \varepsilon^x)(0) \]
for $x \in \R$, where $\varepsilon^x$ is the function $t \mapsto e^{itx}$.

\begin{defn} We wish to extend this helpful transform to distributions.  However the space $\fml D^*(\R)$ turns out to be much too large
to support a reasonable definition for it.  By starting with a larger set of ``test functions'', those in the Schwartz space
$\fml S(\R)$ (in which $\fml D(\R)$ is dense), we get a smaller space of continuous linear functionals, on which, happily, we
can define in a natural way a \emph{Fourier transform}.
\end{defn}

Before defining the Fourier transform on distributions, let us recall two important facts from real analysis concerning the
Fourier transform acting on smooth integrable functions.

\begin{prop}[Fourier Inversion Formula] If $f \in \fml S(\R)$,
 \index{Fourier!inversion formula}%
then
   \[ f(x) = \int_\R \hat f \varepsilon^x\,dm\,. \]
\end{prop}

\begin{prop}\label{prop_FT_isomor1} The Fourier transform
   \[ \mathfrak F\colon \fml S(\R) \sto \fml S(\R)\colon f \mapsto \hat f \]
is a Fr\'echet space isomorphism (that is, it is a bijective continuous linear map with a continuous inverse) and for all
$f \in \fml S(\R)$ and $x \in \R$ we have $\mathfrak F^2f(x) = f(-x)$ and therefore $\mathfrak F^4f = f$.
\end{prop}

\begin{proof} Proofs of the two preceding results, together with much other information about this fascinating transform
and its many applications to both ordinary and partial differential equations, can be found in a great variety of sources.
A few that come readily to mind are~\cite{Cerda:2010}, Chapter~7; \cite{Horvath:1966}, Chapter~4, Section~11;
\cite{McDonaldW:1999}, Section~11.3; \cite{Rudin:1991}, Chapter~7; \cite{Treves:1967}, Chapter~25; and \cite{Yosida:1965},
Chapter~VI.   \ns
\end{proof}

\begin{defn} The inclusion function $\iota\colon \fml D(\R) \sto \fml S(\R)$ is continuous.  Then the mapping between their dual spaces
   \[ u\colon \fml S^*(\R) \sto \fml D^*(\R)\colon \tau \mapsto u_\tau = \tau \circ \iota \]
is a vector space isomorphism from $\fml S^*(\R)$ onto a space of distributions on~$\R$, which we call the space of
 \index{tempered distribution}%
\df{tempered distributions} (some authors prefer the phrase
 \index{distribution!tempered}%
\df{temperate distributions}).  It is conventional to identify linear functionals $\tau$ and $u_\tau$ above.  Thus $\fml S^*(\R)$
itself is regarded as the space of tempered distributions.
\end{defn}

If you have trouble verifying any of the assertions in the preceding definition, look at Theorem 7.10\emph{ff} in~\cite{Rudin:1991}.

\begin{exam} Every distribution with compact support is tempered.
\end{exam}

\begin{defn} Define the
 \index{Fourier!transform}%
 \index{transform!Fourier}%
 \index{F@$\mathfrak F(u)$ (Fourier transform of~$u$)}%
 \index{<unoptop@$\hat u$ (Fourier transform of~$u$)}%
\df{Fourier transform} $\hat u$ of a tempered distribution $u \in \fml S^*(\R)$ by
   \[ \hat u(\phi) = u(\hat\phi) \]
for all $\phi \in \fml S(\R)$.  The notation $\mathfrak Fu$ is an alternative to~$\hat u$.
\end{defn}

\begin{prop} If $f \in \lfs1\R$, then we may treat $\tilde f$ as a tempered distribution and in that case
   \[ \hat{\tilde f} = \tilde{\hat f}\,. \]
\end{prop}

\begin{exam} The Fourier transform of the Dirac delta function is given by
   \[ \hat\delta = \tilde{\vc 1}\,. \]
\end{exam}

\begin{exam} The Fourier transform of the regular distribution $\tilde{\vc 1}$ is given by
   \[ \hat{\tilde{\vc 1}} = \delta\,. \]
\end{exam}

Proposition~\ref{prop_FT_isomor1} has a gratifying generalization to tempered distributions.

\begin{prop} The Fourier transform $\mathfrak F\colon \fml S^*(\R) \sto \fml S^*(\R)$ is a bijective continuous
linear map whose inverse is also continuous. Furthermore, $\mathfrak F^4$ is the identity map on~$\fml S^*(\R)$.
\end{prop}

We will return briefly to our discussion of the Fourier transform in the next chapter.


















\endinput
\chapter{EXTENSIONS}

\section{Essentially Normal Operators}
\begin{defn} Let $T$ be an operator on a Hilbert space~$H$.  The
 \index{spectrum!essential}%
 \index{essential!spectrum}%
 \index{sigmae@$\sigma_e(T)$ (essential spectrum of $T$)}%
\df{essential spectrum} of $T$, denoted by $\sigma_e(T)$, is the spectrum of the image of
$T$ in the Calkin algebra $\ofml Q(H)$; that is,
   \[ \sigma_e(T) = \sigma_{\ofml Q(H)}(\pi(T))\,. \]
\end{defn}

\begin{prop}If $T$ is an operator on a Hilbert space $H$, then
  \[ \sigma_e(T) = \{\lambda \in \C\colon T - \lambda I \notin \ofml F(H)\}\,. \]
\end{prop}

\begin{prop} The essential spectrum of a self-adjoint Hilbert space operator $T$ is the union
of the accumulation points of the spectrum of $T$ with the eigenvalues of $T$ having
infinite multiplicity.  The members of $\sigma(T) \setminus \sigma_e(T)$ are the isolated
eigenvalues of finite multiplicity.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, proposition 2.2.2. \ns \end{proof}

Here are two theorems from classical functional analysis.

\begin{thm}[Weyl]\label{005121} If $S$ and $T$ are operators on a Hilbert space
 \index{Weyl's theorem}%
whose difference is compact, then their spectra agree except perhaps for eigenvalues.
\end{thm}

\begin{thm}[Weyl-von Neumann]\label{005124} Let $T$ be a self-adjoint operator on
 \index{Weyl-von Neumann theorem}%
a separable Hilbert space~$H$.  For every $\epsilon > 0$ there exists a diagonalizable
operator $D$ such that $T - D$ is compact and $\norm{T - D} < \epsilon$.
\end{thm}

\begin{proof} See \cite{Conway:2000}, 38.1; \cite{Davidson:1996}, corollary II.4.2; or
\cite{HigsonR:2000}, 2.2.5. \ns
\end{proof}

\begin{defn} Let $H$ and $K$ be Hilbert spaces.  Operators $S \in \ofml B(H)$ and $T \in \ofml
B(K)$ are
 \index{essentially!unitarily equivalent}%
 \index{unitary!equivalence!essential}%
 \index{equivalence!essential unitary}%
\df{essentially unitarily equivalent} (or
 \index{compalent}%
\df{compalent}) if there exists a unitary map $U\colon H \sto K$ such that $S - UTU^*$ is
a compact operator on~$H$.  (We extend definitions~\ref{000161} and~\ref{0001612} in the
obvious way: $U \in \ofml B(H,K)$ is
 \index{unitary!bounded linear map}%
 \index{bounded!linear map!unitary}%
\df{unitary} if $U^*U = I_H$ and $UU^* = I_K$; and $S$ and $T$ are
 \index{unitary!equivalence!of operators}%
 \index{equivalence!unitary}%
\df{unitarily equivalent} if there exists a unitary map $U\colon H \sto K$ such that $S =
UTU^*$.)
\end{defn}

\begin{prop}\label{005134} Self-adjoint operators $S$ and $T$ on separable Hilbert spaces are
essentially unitarily equivalent if and only if they have the same essential spectrum.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, proposition 2.2.4.  \ns \end{proof}

\begin{defn} A Hilbert space operator $T$ is
 \index{essentially!normal}%
 \index{normal!essentially}
 \index{operator!essentially normal}%
\df{essentially normal} if its
 \index{commutator}%
 \index{<brackets@$[T,T^*]$ (commutator of $T$)}%
\emph{commutator} $[T,T^*] := TT^* - T^*T$ is compact. That is to say: $T$ is essentially
normal if its image $\pi(T)$ is a normal element of the Calkin algebra. The operator $T$
is
 \index{essentially!self-adjoint}%
 \index{self-adjoint!essentially}
 \index{operator!essentially self-adjoint}%
\df{essentially self-adjoint} if $T - T^*$ is compact; that is, if $\pi(T)$ is
self-adjoint in the Calkin algebra.
\end{defn}

\begin{exam} The unilateral shift operator $S$ (see example~\ref{C063526})
 \index{unilateral shift!essential normality of}%
is essentially normal (but not normal).
\end{exam}





















\section{Toeplitz Operators}
\begin{defn} Let
 \index{zeta@$\zeta$ (identity function on the unit circle)}%
$\zeta\colon \T \sto \T\colon z \mapsto z$ be the identity function on the unit circle.
Then $\{\zeta^n\colon n \in \Z\}$ is an orthonormal basis for the Hilbert space $L_2(\T)$ of
(equivalence classes of) functions square integrable on $\T$ with respect to (suitably
normalized) arc length measure. We denote by $H^2$ the subspace of $L_2(\T)$ which is the
closed linear span of $\{\zeta^n\colon n \ge 0\}$ and by $P_+$ the (orthogonal)
projection on $L_2(\T)$ whose range (and codomain) is~$H^2$. The space $H^2$ is an
example of a
 \index{Hardy spaces!$H^2$}%
 \index{space!Hardy}%
 \index{H@$H^2$}%
\emph{Hardy space}. For every $\phi \in L_\infty(\T)$ we define a mapping $T_\phi$ from
the Hilbert space $H^2$ into itself by $T_\phi = P_+M_\phi$ (where $M_\phi$ is the
multiplication operator defined in example~\ref{C063527}). Such an operator is a
 \index{Toeplitz!operator}%
 \index{operator!Toeplitz}%
 \index{T@$T_\phi$ (Toeplitz operator with symbol~$\phi$)}%
\df{Toeplitz operator} and $\phi$ is its
 \index{symbol!of a Toeplitz operator}%
 \index{Toeplitz!operator!symbol of}%
\df{symbol}.  Clearly $T_\phi$ is an operator on $H^2$ and $\norm{T_\phi} \le \norm
\phi_\infty$.
\end{defn}

\begin{exam} The Toeplitz operator $T_\zeta$
 \index{unilateral shift!is a Toeplitz operator}%
 \index{Toeplitz!operator!unilateral shift as a}%
acts on the basis vectors $\zeta^n$ of $H^2$ by $T_\zeta(\zeta^n) = \zeta^{n+1}$, so it
is unitarily equivalent to $S$ the unilateral shift.
\end{exam}

\begin{prop}\label{005216} The map $T\colon L_\infty(\T) \sto \ofml B(H^2)\colon \phi
\mapsto T_\phi$ is positive, linear, and involution preserving.
\end{prop}

\begin{exam}\label{005217} The Toeplitz operators $T_\zeta$ and $T_{\conj\zeta}$ show that
the map $T$ in the preceding proposition is \emph{not} a representation of $L_\infty(\T)$
on~$\ofml B(H^2)$. (Compare this with example~\ref{002802}.)
\end{exam}

\begin{notn} Let $H^\infty := H^2 \cap L_\infty(\T)$.
 \index{H@$H^\infty$}%
 \index{Hardy spaces!$H^\infty$}%
 \index{space!Hardy}%
This is another example of a \emph{Hardy space}.
\end{notn}

\begin{prop} An essentially bounded function $\phi$ on the unit circle belongs to
$H^\infty$ if and only if the multiplication operator $M_\phi$ maps $H^2$ into~$H^2$.
\end{prop}

Although (as we saw in example~\ref{005217}) the map $T$ defined in
proposition~\ref{005216} is not in general multiplicative, it is multiplicative for a
large class of functions.

\begin{prop} If $\phi \in L_\infty(\T)$ and $\psi \in H^\infty$, then $T_{\phi\psi} =
T_\phi T_\psi$ and $T_{\conj\psi \phi} = T_{\conj\psi} T_\phi$.
\end{prop}

\begin{prop} If the Toeplitz operator $T_\phi$ with symbol $\phi \in L_\infty(\T)$ is
invertible then the function $\phi$ is invertible.
\end{prop}

\begin{proof} See \cite{Douglas:1972}, proposition 7.6. \ns \end{proof}

\begin{thm}[Hartman-Wintner Spectral Inclusion Theorem] If $\phi \in L_\infty(\T)$,
 \index{Hartman-Wintner spectral inclusion theorem}%
then $\sigma(\phi) \subseteq \sigma(T_\phi)$ and $\rho(T_\phi) = \norm{T_\phi} =
\norm{\phi}_\infty$.
\end{thm}

\begin{proof} See \cite{Douglas:1972}, corollary 7.7 or \cite{Murphy:1990}, theorem 3.5.7. \ns
\end{proof}

\begin{cor} The mapping $T\colon L_\infty(\T) \sto \ofml B(H^2)$ defined in
proposition~\ref{005216} is an isometry.
\end{cor}

\begin{prop} A Toeplitz operator $T_\phi$ with symbol $\phi \in L_\infty(\T)$ is compact if
and only if $\phi = \vc 0$.
\end{prop}

\begin{proof} See \cite{Murphy:1990}, theorem 3.5.8.  \ns \end{proof}

\begin{prop} If $\phi \in \fml C(\T)$ and $\psi \in L_\infty(\T)$, then the
semi-commutators $T_\phi T_\psi - T_{\phi\psi}$ and $T_\psi T_\phi - T_{\psi\phi}$ are
compact.
\end{prop}

\begin{proof} See \cite{Arveson:2002}, proposition 4.3.1; \cite{Davidson:1996}, corollary
V.1.4; or \cite{Murphy:1990}, lemma 3.5.9. \ns
\end{proof}

\begin{cor} Every Toeplitz operator with continuous symbol is essentially normal.
\end{cor}

\begin{defn} Suppose that $H$ is a separable Hilbert space with basis $\{e_0, e_1, e_2,
\dots\}$ and that $T$ is an operator on $H$ whose (infinite) matrix representation is
$[t_{ij}]$. If the entries in this matrix depend only on the difference of the indices $i
- j$ (that is, if each diagonal parallel to the main diagonal is a constant sequence),
then the matrix is a
 \index{Toeplitz!matrix}%
 \index{matrix!Toeplitz}%
\df{Toeplitz matrix}.
\end{defn}

\begin{prop} Let $T$ be an operator on the Hilbert space $H^2$.  The matrix representation of
$T$ with respect to the usual basis $\{\zeta^n\colon n \ge 0\}$ is a Toeplitz matrix if
and only if $S^*TS = T$ (where $S$ is the unilateral shift).
\end{prop}

\begin{proof} See \cite{Arveson:2002}, proposition 4.2.3.  \ns \end{proof}

\begin{prop} If $T_\phi$ is a Toeplitz operator with symbol $\phi \in L_\infty(\T)$, then
$S^*T_\phi S = T_\phi$.  Conversely, if $R$ is an operator on the Hilbert space $H^2$
such that $S^*RS = R$, then there exists a unique $\phi \in L_\infty(\T)$ such that $R =
T_\phi$.
\end{prop}

\begin{proof} See \cite{Arveson:2002}, theorem 4.2.4.  \ns \end{proof}

\begin{defn} The
 \index{Toeplitz!algebra}%
 \index{algebra!Toeplitz}%
 \index{T@$\ofml T$ (Toeplitz algebra)}%
\df{Toeplitz algebra} $\ofml T$ is the $C^*$-subalgebra of $\ofml B(H^2)$ generated by
the unilateral shift operator; that is, $\ofml T = C^*(S)$.
\end{defn}

\begin{prop} The set $\ofml K(H^2)$ of compact operators on $H^2$ is an ideal in the Toeplitz
algebra~$\ofml T$.
\end{prop}

\begin{prop} The Toeplitz algebra comprises all compact perturbations of Toeplitz operators
with continuous symbol.  That is,
   \[ \ofml T = \{ T_\phi + K\colon \phi \in \fml C(\T) \text{ and } K \in \ofml K(H^2)\} \,. \]
Furthermore, if $T_\phi + K = T_\psi + L$ where $\phi$, $\psi \in \fml C(\T)$ and $K$, $L
\in \ofml K(H^2)$, then $\phi = \psi$ and $K = L$.
\end{prop}

\begin{proof} See \cite{Arveson:2002}, theorem 4.3.2 or \cite{Davidson:1996}, theorem V.1.5.
\ns \end{proof}

\begin{prop}\label{005251} The map $\pi \circ T\colon \fml C(\T) \sto Q(H^2)\colon \phi
\mapsto \pi(T_\phi)$ is a unital $*\,$-monomorphism.  So the map $\alpha\colon \fml C(\T)
\sto \ofml T/\ofml K(H^2)\colon \phi \mapsto \pi(T_\phi)$ establishes and isomorphism
between the $C^*$-algebras $\fml C(\T)$ and $\ofml T/\ofml K(H^2)$.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, proposition 2.3.3 or \cite{Murphy:1990},
theorem 3.5.11.  \ns
\end{proof}

\begin{cor} If $\phi \in \fml C(\T)$, then $\sigma_e(T_\phi) = \ran\phi$.
\end{cor}

\begin{prop}\label{005253} The sequence
   \[ \vc 0 \to \ofml K(H^2) \to \ofml T \to^\beta \fml C(\T) \to \vc 0 \]
is exact. It does not split.
\end{prop}

\begin{proof} The map $\beta$ is defined by $\beta(R) := \alpha^{-1}\bigl(\pi(R)\bigr)$ for
every $R \in \ofml T$ (where $\alpha$ is the isomorphism defined in the preceding
proposition~\ref{005251}).  See \cite{Arveson:2002}, remark 4.3.3; \cite{Davidson:1996},
theorem V.1.5; or \cite{HigsonR:2000}, page 35. \ns
\end{proof}

\begin{defn}\label{0052531} The short exact sequence in the preceding proposition~\ref{005253}
is the
 \index{Toeplitz!extension}%
 \index{extension!Toeplitz}%
\df{Toeplitz extension} of $\fml C(\T)$ by~$\ofml K(H^2)$.
\end{defn}

\begin{rem} A version of the diagram for the Toeplitz extension which appears frequently looks
something like
     \[ \vc 0 \to \ofml K(H^2) \to \ofml T \two/->`<-/^\beta_T \fml C(\T) \to \vc 0 \tag{1} \]
where $\beta$ is as in~\ref{005253} and $T$ is the mapping $\phi \mapsto T_\phi$.  It is
possible to misinterpret this diagram. It may suggest to the unwary that this is a split
extension especially in as much as it is certainly true that $\beta \circ T = I_{\fml
C(\T)}$. The trouble, of course, is that this is not a diagram in the category
$\cat{CSA}$ of $C^*$-algebras and $*\,$-homomorphisms.  We have already seen in
example~\ref{005217} that the mapping $T$ is not a $*\,$-homomorphism since it does not
always preserve multiplication. Invertible elements in $\fml C(\T)$ need not lift to
invertible elements in the Toeplitz algebra~$\ofml T$; so the $*\,$-epimorphism $\beta$
does not have a right inverse in the category~$\cat{CSA}$.

Some authors deal with the problem by saying that the sequence (1) is semisplit (see, for
example, Arveson~\cite{Arveson:2002}, page 112). Others borrow a term from category
theory where the word ``section'' means ``right invertible''.
Davidson~\cite{Davidson:1996}, for example, on page 134, refers to the mapping $\beta$ as
a ``continuous section'' and Douglas~\cite{Douglas:1972}, on page 179, says it is an
``isometrical cross section''.  While it is surely true that $\beta$ has $T$ as a right
inverse in the category $\cat{SET}$ of sets and maps and even in the category of Banach
spaces and bounded linear maps, it has no right inverse in $\cat{CSA}$.
\end{rem}

\begin{prop} If $\phi$ is a function in $\fml C(\T)$, then $T_\phi$ is Fredholm if and only if
it is never zero.
\end{prop}

\begin{proof} See \cite{Arveson:2002}, page 112, corollary 1; \cite{Davidson:1996}, theorem
V.1.6; \cite{Douglas:1972}, theorem 7.2 6; or \cite{Murphy:1990}, corollary 3.5.12. \ns
\end{proof}

\begin{prop}\label{005271} If $\phi$ is an invertible element of $\fml C(\T)$, then there
exists a unique integer $n$ such that $\phi = \zeta^n \exp \psi$ for some $\psi \in \fml
C(\T)$.
\end{prop}

\begin{proof} See \cite{Arveson:2002}, propositions 4.4.1 and 4.4.2 or \cite{Murphy:1990},
lemma 3.5.14. \ns
\end{proof}

\begin{defn} The integer whose existence is asserted in the preceding proposition~\ref{005271}
is the
 \index{winding number}%
 \index{w@$w(\phi)$ (winding number of~$\phi$)}%
\df{winding number} of the invertible function $\phi \in \fml C(\T)$.  It is denoted
by~$w(\phi)$.
\end{defn}

\begin{thm}[Toeplitz index theorem]\label{005274}
 \index{Toeplitz!index theorem}%
If $T_\phi$ is a Toeplitz operator with a nowhere vanishing continuous symbol, then it is
a Fredholm operator and
   \[ \ind(T_\phi) = -w(\phi)\,. \]
\end{thm}

\begin{proof} Elementary proofs can be found in \cite{Arveson:2002}, theorem 4.4.3  and
\cite{Murphy:1990}. For those with a little background in homotopy of curves
proposition~\ref{005271}, which leads to the definition above of \emph{winding number},
can be bypassed. It is an elementary fact in homotopy theory that the fundamental group
$\pi^1(\C \setminus 0)$ of the punctured plane is infinite cyclic.  There is an
isomorphism $\tau$ from $\pi^1(\C \setminus 0)$ to $\Z$ that associates the integer $+1$
with (the equivalence class containing) the function $\zeta$.  This allows us to
associate with each invertible member $\phi$ of $\fml C(\T)$ the integer corresponding
under $\tau$ to its equivalence class. We call this integer the
 \index{winding number}%
 \index{fundamental!group}%
 \index{group!fundamental}%
\emph{winding number} of~$\phi$.  Such a definition makes it possible to give a shorter
more elegant proof of the \emph{Toeplitz index theorem}.  For a treatment in this vein
see \cite{Douglas:1972}, theorem 7.26 or \cite{HigsonR:2000}, theorem 2.3.2.  For
background concerning the fundamental group see \cite{Massey:1967}, chapter two;
\cite{Wallace:1970}, appendix A; or \cite{Willard:1968}, sections 32 and 33. \ns
\end{proof}

\begin{thm}[Wold decomposition]
 \index{Wold decomposition}%
 \index{decomposition!Wold}%
Every
 \index{proper!isometry}%
 \index{isometry!proper}%
proper (that is, non-unitary) isometry on a Hilbert space is a direct sum of copies of
the unilateral shift or else a direct sum of a unitary operator together with copies of
the shift.
\end{thm}

\begin{proof} See \cite{Davidson:1996}, theorem V.2.1; \cite{Halmos:1982}, problem (and
solution) 149; or \cite{Murphy:1990}, theorem 3.5.17. \ns
\end{proof}

\begin{thm}[Coburn]
 \index{Coburn's theorem}%
Let $v$ be an isometry in a unital $C^*$-algebra~$A$.  Then there exists a unique unital
$*\,$-homomorphism $\tau$ from the Toeplitz algebra $\ofml T$ to $A$ such that
$\tau(T_\zeta) = v$. Furthermore, if $vv^* \ne \vc 1$, then $\tau$ is an isometry.
\end{thm}

\begin{proof} See \cite{Murphy:1990}, theorem 3.5.18 or \cite{Davidson:1996}, theorem V.2.2. \ns
\end{proof}






















\section{Addition of Extensions}

 \index{conventions!Hilbert spaces are separable and infinite dimensional (after section 9.2)}%
\begin{center}
 \fbox{\parbox{5.5in}{\textbf{From now on all Hilbert spaces will be separable and infinite
 dimensional.}}}
\end{center}

\begin{prop} A Hilbert space operator $T$ is essentially self-adjoint if and only if it is a
compact perturbation of a self-adjoint operator.
\end{prop}

\begin{prop}\label{005414} Two essentially self-adjoint Hilbert space operators $T_1$ and $T_2$
are essentially unitarily equivalent if and only if they have the same essential
spectrum.
\end{prop}

The analogous result does not hold for essentially normal operators.

\begin{exam} The Toeplitz operators $T_\zeta$ and $T_{\zeta^2}$ are essentially normal with
the same essential spectrum, but they are not essentially unitarily equivalent.
\end{exam}

\begin{defn} We now restrict our attention to a special class of extensions. If $H$ is a
Hilbert space and $A$ is a $C^*$-algebra, we say that $(\ofml E,\phi)$ is an
 \index{extension!of $\ofml K(H)$ by $A$}%
\df{extension of $\ofml K = \ofml K(H)$ by $A$} if $\ofml E$ is a unital $C^*$-subalgebra
of $\ofml B(H)$ containing $\ofml K$ and $\phi$ is a unital $*\,$-homomorphism such that
the sequence
 \begin{equation}\label{005431i}
    \vc 0 \to \ofml K \to^\iota \ofml E \to^\phi A \to \vc 0
 \end{equation}
(where $\iota$ is the inclusion mapping) is exact. In fact, we will be concerned almost
exclusively with the case where $A = \fml C(X)$ for some compact metric space~$X$.  We
will say that two extensions $(\ofml E,\phi)$ and $(\ofml E\,',\phi\,')$ are
 \index{equivalence!of extensions}%
 \index{extension!equivalence of}%
\df{equivalent} if there exists an isomorphism $\psi\colon \ofml E \sto \ofml E\,'$ which
makes the following diagram commute.
 \begin{equation}\label{005431ii}
   \xymatrix{
     \vc 0\ar[r] & \ofml K\ar[r]^\iota\ar[d]_{\psi|_{\ofml K)}} & \ofml E\ar[r]^\phi\ar[d]^\psi
                 & A\ar[r]\ar@{=}[d] & \vc 0 \\
     \vc 0\ar[r] & \ofml K\ar[r]^\iota & \ofml E\,'\ar[r]^{\phi\,'} & A\ar[r] & \vc 0
 }\end{equation}
Notice that this differs slightly from the definition of \emph{strong equivalence} given
in~\ref{0015151}. We denote the family of all equivalence classes of such extensions
 \index{ext@$\ext A$, $\ext X$!equivalence classes of extensions}%
by~$\ext A$.  When $A = \fml C(X)$ we write $\ext X$ rather than~$\ext \fml C(X)$.
\end{defn}

\begin{defn} If $U\colon H_1 \sto H_2$ is a unitary mapping between Hilbert spaces, then the mapping
   \[ \ad_U\colon \ofml B(H_1) \sto \ofml B(H_2)\colon T \mapsto UTU^* \]
is called
 \index{conjugation}%
 \index{adU@$\ad_U$ (conjugation by $U$)}%
\df{conjugation} by~$U$.
\end{defn}

It is clear that $\ad_U$ is an isomorphism between the $C^*$-algebras $\ofml B(H_1)$ and
$\ofml B(H_2)$.  In particular, if $U$ is a unitary operator on $H_1$, then $\ad_U$ is an
automorphism of both $\ofml K(H_1)$ and $\ofml B(H_1)$. Furthermore, conjugations are the
only automorphisms of the $C^*$-algebra~$\ofml K(H)$.

\begin{prop}\label{005436} If $H$ is a Hilbert space and $\phi\colon \ofml K(H) \sto \ofml K(H)$
is an automorphism, then $\phi = \ad_U$ for some unitary operator $U$ on~$H$
\end{prop}

\begin{proof} See \cite{Davidson:1996}, lemma V.6.1.  \ns \end{proof}

\begin{prop} If $H$ is a Hilbert space and $A$ is a $C^*$-algebra, then extensions
$(\ofml E,\phi)$ and $(\ofml E\,',\phi\,')$ in $\ext A$ are equivalent if and only if
there exists a unitary operator $U$ in $\ofml B(H)$ such that $\ofml E\,' = U \ofml E
U^*$ and $\phi = \phi\,' \ad_U$.
\end{prop}

\begin{exam} Suppose that $T$ is an essentially normal operator on a Hilbert space~$H$. Let
 \index{E@$\ofml E_T$ ($C^*$-algebra generated by $T$ and $\ofml K(H)$)}%
$\ofml E_T$ be the unital $C^*$-algebra generated by $T$ and~$\ofml K(H)$.  Since
$\pi(T)$ is a normal element of the Calkin algebra, the unital $C^*$-algebra $\ofml
E_T/\ofml K(H)$ that it generates is commutative.  Thus the \emph{abstract spectral
theorem}~\ref{00144} gives us a $C^*$-algebra isomorphism $\Psi\colon \fml C(\sigma_e(T))
\sto \ofml E_T/\ofml K(H)$.  Let $\sbsb{\phi}T = \Psi^{-1} \circ\pi\bigr|_{\ofml E_T}$.
Then the sequence
  \[ \vc 0 \to \ofml K(H) \to^\iota \ofml E_T \to^{\sbsb{\phi}T} \fml C(\sigma_e(T)) \to \vc 0 \]
is exact.  This is the
 \index{extension!determined by an essentially normal operator}%
\df{extension of $\ofml K(H)$ determined by~$T$}.
\end{exam}

\begin{prop} Let $T$ and $T\,'$ be essentially normal operators on a Hilbert space~$H$.  These
operators are essentially unitarily equivalent if and only if the extensions they
determine are equivalent.
\end{prop}

\begin{prop} If $\ofml E$ is a $C^*$-algebra such that $\ofml K(H) \subseteq \ofml E \subseteq
\ofml B(H)$ for some Hilbert space~$H$, $X$ is a nonempty compact subset of $\C$, and
$(\ofml E,\phi)$ is an extension of $\ofml K(H)$ by $\fml C(X)$, then every element of
$\ofml E$ is essentially normal.
\end{prop}

\begin{defn} If $\phi_1\colon A_1 \sto B$ and $\phi_2\colon A_2 \sto B$ are unital
$*\,$-homomorphisms between $C^*$-algebras, then a
 \index{pullback}%
\df{pullback of $A_1$ and $A_2$ along $\phi_1$ and $\phi_2$}, denoted by
$(P,\pi_1,\pi_2)$, is a $C^*$-algebra $P$ together with a pair of unital
$*\,$-homomorphisms $\pi_1\colon P \sto A_1$ and $\pi_2\colon P \sto A_2$ which satisfy
the following two conditions:
 \begin{enumerate}
    \item[(i)] the diagram
        \[ \xy
          \Square[P`A_2`A_1`B;\pi_2`\pi_1`\phi_2`\phi_1]
      \endxy \]
commutes and
    \item[(ii)] if $\rho_1\colon Q \sto A_1$ and $\rho_2\colon Q \sto A_2$ are unital
$*\,$-homomorphisms of $C^*$-algebras such that the diagram
        \[ \xy
          \Square[Q`A_2`A_1`B;\rho_2`\rho_1`\phi_2`\phi_1]
      \endxy \]
commutes, then there exists a unique unital $*\,$-homomorphism $\Psi\colon Q \sto P$
which makes the diagram
       \[  \xy
         \pullback|brrb|<800,800>[P`A_2`A_1`B;\pi_2`\pi_1`\phi_2`\phi_1]%
         |amb|/{>}`{-->}`{>}/<500,500>[Q;\rho_2`\Psi`\rho_1]
       \endxy \]
commute.
 \end{enumerate}
\end{defn}

\begin{prop} Let $H$ be a Hilbert space, $A$ be a unital $C^*$-algebra, and $\tau\colon
A \sto \ofml Q(H)$ be a unital $*\,$-monomorphism. Then there exists (uniquely up to
isomorphism) a pullback $(\ofml E,\pi_1,\pi_2)$ of $A$ and $\ofml B(H)$ along $\tau$ and
$\pi$ such that $(\ofml E,\pi_2)$ is an extension of $\ofml K(H)$ by $A$ which makes the
following diagram commute.
 \begin{equation}\label{005464i}
   \xymatrix{
     \vc 0\ar[r] & \ofml K(H)\ar[r]^\iota\ar@{=}[d] & \ofml E\ar[r]^{\pi_2}\ar[d]^{\pi_1}
                 & A\ar[r]\ar[d]^\tau & \vc 0 \\
     \vc 0\ar[r] & \ofml K(H)\ar[r] & \ofml B(H)\ar[r]^{\pi} & \ofml Q(H)\ar[r] & \vc 0
 }\end{equation}
\end{prop}

\begin{proof}(Sketch.) Let $\ofml E = \{T \oplus a \in \ofml B(H) \oplus A\colon \tau(a) =
\pi(T)\}$, $\pi_1\colon \ofml E \sto \ofml B(H)\colon T \oplus a \mapsto T$, and
$\pi_2\colon \ofml E \sto \ofml A\colon T \oplus a \mapsto a$. The uniqueness (of any
pullback) is proved using the usual ``abstract nonsense''.
\end{proof}

\begin{prop}\label{005466}  Let $H$ be a Hilbert space, $A$ be a unital $C^*$-algebra, and
$(\ofml E,\phi)$ be an extension of $\ofml K(H)$ by~$A$. Then there exists a unique
$*\,$-monomorphism $\tau \colon A \sto \ofml Q(H)$ which makes the
diagram~\eqref{005464i} commute.
\end{prop}

\begin{defn} Let $H$ be a Hilbert space and $A$ be a unital $C^*$-algebra. Two unital
$*\,$-monomorphisms $\tau_1$ and $\tau_2$ from $A$ into $\ofml Q(H)$ are
 \index{unitary!equivalence!of $*\,$-monomorphisms}%
 \index{equivalence!unitary!of $*\,$-monomorphisms}%
\df{unitarily equivalent} if there exists a unitary operator on $H$ such that $\tau_2 =
\ad_U \tau_1$. Unitary equivalence of unital $*\,$-monomorphisms is of course an
equivalence relation.  The equivalence class containing $\tau_1$ is denoted
by~$[\tau_1]$.
\end{defn}

\begin{prop}  Let $H$ be a Hilbert space and $A$ be a unital $C^*$-algebra.  Two extensions of
$\ofml K(H)$ by $A$ are equivalent if and only if their corresponding unital
$*\,$-monomorphisms (see~\ref{005466}) are unitarily equivalent.
\end{prop}

\begin{cor} If  $H$ is a Hilbert space and $A$ a unital $C^*$-algebra there is a one-to-one
correspondence between equivalence classes of extensions of $\ofml K(H)$ by $A$ and
unitary equivalence classes of unital $*\,$-monomorphisms from $A$ into the Calkin
algebra~$\ofml Q(H)$.
\end{cor}

\begin{conv} In light of the preceding corollary we will regard members of $\ext A$ (or
$\ext X$) as either
 \index{conventions!E@$\ext A$ consists of extensions or morphisms}%
equivalence classes of extensions or unitary equivalence classes of $*\,$-monomorphisms,
whichever seems the most convenient at the moment.
\end{conv}

\begin{prop}\label{0054702} Every (separable infinite dimensional) Hilbert space $H$ is
isometrically isomorphic to $H \oplus H$. Thus the $C^*$-algebras $\ofml B(H)$ and $\ofml
B(H \oplus H)$ are isomorphic.
\end{prop}

\begin{defn}\label{0054715} Let $\tau_1$, $\tau_2 \colon A \sto \ofml Q(H)$ be unital
$*\,$-monomorphisms (where $A$ is a $C^*$-algebra and $H$ is a Hilbert space). We define
a unital $*\,$-monomorphism $\tau_1 \oplus \tau_2 \colon A \sto \ofml Q(H)$ by
  \[ (\tau_1 \oplus \tau_2)(a) := \rho\bigl(\tau_1(a) \oplus \tau_2(a)\bigr) \]
for all $a \in A$ where (as in Douglas\cite{Douglas:1980}) $\nu$ is the isomorphism
established in~\ref{0054702} and $\rho$ is the map which makes the following diagram
commute.
 \begin{equation}\label{0054715i}
  \xymatrix{
    \ofml B(H) \oplus \ofml B(H)\ar[r]\ar[dd]_{\pi \oplus \pi} & \ofml B(H \oplus H)\ar[r]^-\nu
                  & \ofml B(H)\ar[dd]^\pi   \\ & & & \\
    \ofml Q(H) \oplus \ofml Q(H)\ar[rr]_-\rho && \ofml Q(H)
 }\end{equation}
 We then define the obvious operation of addition on $\ext A$:
   \[ [\tau_1] + [\tau_2] := [\tau_1 \oplus \tau_2] \]
for $[\tau_1]$,  $[\tau_2] \in \ext A$.
\end{defn}

\begin{prop} The operation of addition (given in~\ref{0054715}) on $\ext A$ is well defined
and under this operation $\ext A$ becomes a commutative semigroup.
\end{prop}

\begin{defn}\label{0054811} Let $A$ be a unital $C^*$-algebra and $\mathbf r\colon A \sto
\ofml B(H)$ be a nondegenerate representation of $A$ on some Hilbert space~$H$. Let $P$
be a Hilbert space projection and $M$ be the range of~$P$.  Suppose that $P\mathbf r(a) -
\mathbf r(a)P \in \ofml K(H)$ for every $a~\in~A$. Denote by $P_M$ the projection $P$
with its codomain set equal to its range; that is, $P_M\colon H \sto M \colon x \mapsto
Px$.  Then for each $a \in A$ we define the
 \index{operator!Toeplitz!abstract}%
 \index{Toeplitz!operator!abstract}%
 \index{abstract!Topelitz!operator}%
\df{abstract Toeplitz operator} $T_a \in \ofml B(M)$
 \index{symbol!of an abstract Toeplitz operator}%
 \index{T@$T_a$ (abstract Toeplitz operator)}%
\df{with symbol $a$ associated with} the pair~$(\mathbf r,P)$ by $T_a = P_M\mathbf
r(a)\bigr|_M$.
\end{defn}

\begin{defn} Let notation be as in the preceding definition~\ref{0054811}. Then we define the
 \index{extension!Toeplitz!abstract}%
 \index{Toeplitz!extension!abstract}%
 \index{abstract!Topelitz!extension}%
\df{abstract Toeplitz extension $\tau_P$ associated with the pair $(\mathbf r,P)$} by
   \[ \tau_P\colon A \sto \ofml Q(M)\colon a \mapsto \pi(T_a)\,. \]
\end{defn}

Notice that the (unital $*\,$-monomorphism associated with the concrete) Toeplitz
extension defined in~\ref{0052531} is an example of an abstract Toeplitz extension, and
also that, in general, abstract Toeplitz extensions need not be injective.

\begin{defn} Let $A$ be a unital $C^*$-algebra and $H$ a Hilbert space.   In the spirit of
\cite{HigsonR:2000}, definition 2.7.6, we will say that a unital $*\,$-monomorphism
$\tau\colon A \sto \ofml Q(H)$ is
 \index{semisplit}%
\df{semisplit} if there exists a unital $*\,$-monomorphism $\tau\,'\colon A \sto \ofml
Q(H)$ such that $\tau \oplus \tau'$ splits.
\end{defn}

\begin{prop} Suppose that $A$ is a unital $C^*$-algebra and $H$ is a Hilbert space.  Then a unital
$*\,$-monomorphism $\tau\colon A \sto \ofml Q(H)$ is semisplit if and only if it is
unitarily equivalent to an abstract Toeplitz extension.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, proposition 2.7.10.  \ns \end{proof}

















\vskip .2 in


%\section{Tensor Products of $C^*$-algebras}


\vskip .2 in


















\section{Completely Positive Maps}
In example~\ref{001306} it is shown how the set $\mathbf M_n$ of $n \times n$ matrices of
complex numbers can be made into a $C^*$-algebra.  We now generalize that example to the
algebra $\M nA$ of $n \times n$ matrices of elements of a $C^*$-algebra~$A$.

\begin{exam} In example~\ref{000533a} it is asserted that if $A$ is a $C^*$-algebra, then under
the usual algebraic operations the set $\M nA$ of $n \times n$ matrices of elements of
$A$ is an algebra.  This algebra can be made into a $*\,$-algebra by taking conjugate
transposition as involution.  That is, define
   \[ [ a_{ij}]^* := \bigl[ {a_{j\,i}}^*\bigr] \]
where $a_{ij} \in A$ for $1 \le i,j \le n$.

Let $H$ be a Hilbert space. For the moment it need not be infinite dimensional or
separable. Denote by $H^n$ its $n$-fold direct sum. That is,
   \[ H^n := \bigoplus_{k=1}^n H_k \]
where $H_k = H$ for $k = 1, \dots, n$.  Let $[T_{jk}] \in \mathbf M_n(\ofml B(H))$. For
$\vc x = x_1 \oplus \dots \oplus x_n \in H^n$ define
   \[ \vc T \colon H^n \sto H^n \colon \vc x \mapsto [T_{jk}] \vc x =
            \sum_{k=1}^n T_{1k}x_k \oplus \dots \oplus \sum_{k=1}^n T_{nk}x_k \,. \]
Then $\vc T \in \ofml B(H^n)$. Furthermore, the map
   \[ \Psi \colon \mathbf M_n\bigl(\ofml B(H)\bigr) \sto \ofml B(H^n) \colon
             [T_{jk}] \mapsto \vc T \]
is an isomorphism of $*\,$-algebras.  Use $\Psi$ to transfer the operator norm on $\ofml
B(H^n)$ to~$\mathbf M_n\bigl(\ofml B(H)\bigr)$.  That is, define
   \[ \bignorm{\,[T_{jk}]\,} := \norm{\vc T}\,. \]
This makes $\mathbf M_n\bigl(\ofml B(H)\bigr)$ into a $C^*$-algebra isomorphic to~$\ofml
B(H^n)$.

Now suppose that $A$ is an arbitrary $C^*$-algebra.  Version III of the
\emph{Gelfand-Naimark theorem} (see~\ref{002973}) allows us to identify $A$ with a
$C^*$-subalgebra of $\ofml B(H)$ for some Hilbert space $H$ and, consequently,
 \index{M@$\M nA$!as a $C^*$-algebra}%
 \index{C*@$C^*$-algebra!M@$\M nA$ as a}%
$\M nA$ with a $C^*$-subalgebra of $\mathbf M_n\bigl(\ofml B(H)\bigr)$.  With this
identification $\M nA$ becomes a $C^*$-algebra. (Notice that by corollary~\ref{00131104}
norms on $C^*$-algebras are unique; so it is clear that the norm on $\mathbf M_n(A)$ is
independent of the particular way in which $A$ is represented as a $C^*$-subalgebra of
operators on some Hilbert space.)
\end{exam}

\begin{exam} Let $k \in \N$ and $A = \mathbf M_k$. Then for every $n \in \N$
   \[ \mathbf M_n(A) = \mathbf M_n(\mathbf M_k) \cong \mathbf M_{nk}\,. \]
The isomorphism is the obvious one: just delete the inner brackets.  For example, the
isomorphism from $\mathbf M_2(\mathbf M_2)$ to $\mathbf M_4$ is given by
  \[ \begin{bmatrix} {} & {} \\
          \begin{bmatrix}
                          a_{11} & a_{12} \\
                          a_{21} & a_{22}
          \end{bmatrix}       &
          \begin{bmatrix} b_{11} & b_{12} \\
                          b_{21} & b_{22}
          \end{bmatrix}       \\*[25pt]
          \begin{bmatrix} c_{11} & c_{12} \\
                          c_{21} & c_{22}
          \end{bmatrix}       &
          \begin{bmatrix} d_{11} & d_{12} \\
                          d_{21} & d_{22}
          \end{bmatrix}       \\
                    {}  & {}
     \end{bmatrix}
    \quad \mapsto \quad
          \begin{bmatrix} a_{11} & a_{12} & b_{11} & b_{12} \\
                          a_{21} & a_{22} & b_{21} & b_{22} \\
                          c_{11} & c_{12} & d_{11} & d_{12} \\
                          c_{21} & c_{22} & d_{21} & d_{22}
          \end{bmatrix}
  \]
\end{exam}

\begin{defn} A mapping $\phi\colon A \sto B$ between $C^*$-algebras is \df{positive} if
it takes positive elements to positive elements; that is, if $\phi(a) \in B^+$ whenever
$a \in A^+$.
\end{defn}

\begin{exam} Every $*\,$-homomorphism between $C^*$-algebras is positive. (See
proposition~\ref{0018201}.)
\end{exam}

\begin{prop} Every positive linear map between $C^*$-algebras is bounded.
\end{prop}

\begin{notn} Let $A$ and $B$ be $C^*$-algebras and $n \in \N$. A linear map $\phi\colon A \sto
B$ induces a linear map $\phi^{(\,n)} := \phi \otimes \id{\mathbf M_n}\colon A \otimes
\mathbf M_n \sto B \otimes \mathbf M_n$. As we have seen $A \otimes \mathbf M_n$ can be
identified with $\mathbf M_n(A)$. Thus we may think of $\phi^{(\,n)}$ as the map from
$\mathbf M_n(A)$ to $\mathbf M_n(B)$ defined by $\phi^{(\,n)}\big(\,[a_{jk}]\,\bigr) =
\bigl[ \phi(a_{jk})\bigr]$.
\end{notn}

It is easy to see that if $\phi$ preserves multiplication, then so does each
$\phi^{(\,n)}$ and if $\phi$ preserves involution, then so does each $\phi^{(\,n)}$.
Positivity, however, is not always preserved as the next example~\ref{005824} shows.

\begin{defn} In $\mathbf M_n$ the
 \index{standard!matrix units}%
 \index{matrix!units}%
 \index{units!matrix}%
\df{standard matrix units} are the matrices $e^{jk}$ ($1 \le j,k \le n$) whose entry in
the $j^{\text{th}}$ row and $k^{\text{th}}$ column is $1$ and whose other entries are
all~$0$.
\end{defn}

\begin{exam}\label{005824} Let $A = \mathbf M_2$.  Then $\phi\colon A \sto A\colon a \mapsto a^t$
(where $a^t$ is the transpose of~$a$) is a positive mapping. The map $\phi^{(2)}\colon
\mathbf M_2(A) \sto \mathbf M_2(A)$ is not positive.  To see this let $e^{11}$, $e^{12}$,
$e^{21}$, and $e^{22}$ be the standard matrix units for~$A$.  Then
  \[ \phi^{(2)}\left(\begin{bmatrix} e^{11} & e^{12} \\ e^{21} & e^{22} \end{bmatrix}\right)
    = \begin{bmatrix} \phi(e^{11}) & \phi(e^{12}) \\ \phi(e^{21}) & \phi(e^{22}) \end{bmatrix}
    = \begin{bmatrix} e^{11} & e^{21} \\ e^{12} & e^{22} \end{bmatrix}
    = \begin{bmatrix} 1 & 0 & 0 & 0 \\
                      0 & 0 & 1 & 0 \\
                      0 & 1 & 0 & 0 \\
                      0 & 0 & 0 & 1
      \end{bmatrix} \]
which is not positive.
\end{exam}

\begin{defn} A linear mapping $\phi\colon A \sto B$ between unital $C^*$-algebras is
 \index{npositive@$n$-positive}%
 \index{positive!$n$-}%
\df{$n$-positive} (for $n \in \N$) if $\phi^{(\,n)}$ is positive.  It is
 \index{positive!completely}%
 \index{completely!positive}%
\df{completely positive} if it is $n$-positive for every $n \in \N$.
\end{defn}

\begin{exam} Every $*\,$-homomorphism between $C^*$-algebras is completely positive.
\end{exam}

\begin{prop}\label{00582811} Let $A$ be a unital $C^*$-algebra and $a \in A$.  Then $\norm a \le 1$ if and only if
$\begin{bmatrix} \vc 1 & a \\ a^* & \vc 1 \end{bmatrix}$ is positive in~$\mathbf M_2(A)$.
\end{prop}

\begin{prop} Let $A$ be a unital $C^*$-algebra and $a$, $b \in A$.  Then $a^*a \le b$ if
and only if $\begin{bmatrix} \vc 1 & a \\ a^* & b \end{bmatrix}$ is positive in~$\mathbf
M_2(A)$.
\end{prop}

\begin{prop} Every unital $2$-positive map between unital $C^*$-algebras is contractive.
\end{prop}

\begin{prop}[Kadison's inequality]\label{0058283} If $\phi\colon A \sto B$ is a unital
$2$-positive map
 \index{Kadison's inequality}%
 \index{inequality!Kadison}%
between unital $C^*$-algebras, then
   \[ \bigl(\phi(a)\bigr)^*\phi(a) \le \phi(a^*a) \]
for every $a \in A$.
\end{prop}

\begin{defn} It is easy to see that if $\phi\colon A \sto B$ is a bounded linear map
between $C^*$-algebras, then $\phi^{(\,n)}$ is bounded for each~$n \in \N$.  If
$\norm{\phi}_{\mathrm{cb}} := \sup\{\norm{\phi^{(\,n)}} \colon n \in \N\} < \infty$, then
$\phi$ is
 \index{completely!bounded}%
 \index{bounded!completely}%
 \index{<unopn@$\norm{\hphantom{\phi}}_{\mathrm{cb}}$ (norm of completely bounded map)}%
\df{completely bounded}.
\end{defn}

\begin{prop}\label{00582921} If $\phi\colon A \sto B$ is a completely positive unital
linear map between unital $C^*$-algebras, then $\phi$ is completely bounded and
   \[ \norm{\phi(\vc 1)} = \norm\phi = \norm\phi_{\mathrm{cb}}\,. \]
\end{prop}

\begin{proof} See \cite{Paulsen:2002}. \ns \end{proof}

\begin{thm}[Stinespring's dilation theorem]\label{005831} Let $A$ be a unital
 \index{Stinespring!dilation theorem}%
 \index{dilation!theorem of Stinespring}%
$C^*$-algebra, $H$ be a Hilbert space, and $\phi\colon A \sto \ofml B(H)$ be a unital
linear map.  Then $\phi$ is completely positive if and only if there exists a Hilbert
space $H_0$, an isometry $V\colon H \sto H_0$, and a nondegenerate representation $\vc
r\colon A \sto \ofml B(H_0)$ such that $\phi(a) = V^*\vc r(a)V$ for every $a \in A$.
\end{thm}

\begin{proof} See \cite{HigsonR:2000}, theorem 3.1.3 or \cite{Paulsen:2002}, theorem 4.1. \ns
\end{proof}

\begin{notn} Let $A$ be a $C^*$-algebra, $f\colon A \sto \ofml B(H_0)$, and
$g\colon A \sto \ofml B(H)$, where $H_0$ and $H$ are Hilbert spaces. We write
 \index{<relation@$g \lesssim f$ (relation between operator valued maps)}%
$g \lesssim f$ if there exists an isometry $V\colon H \sto H_0$ such that $g(a) -
V^*f(a)V \in \ofml K(H)$ for every $a \in A$. The relation $\lesssim$ is a preordering
but not a partial ordering; that is, it is reflexive and transitive but not
antisymmetric.
\end{notn}

\begin{thm}[Voiculescu]\label{0058335} Let $A$ be a separable unital
 \index{Voiculescu's theorem}%
$C^*$-algebra, $\vc r\colon A \sto \ofml B(H_0)$ be a nondegenerate representation, and
$\phi\colon A \sto \ofml B(H)$ (where $H$ and $H_0$ are Hilbert spaces).  If $\phi(a) =
\vc 0$ whenever $\vc r(a) \in \ofml K(H_0)$, then $\phi \lesssim \vc r$.
\end{thm}

\begin{proof} The proof is long and complicated but ``elementary''.  See \cite{HigsonR:2000},
sections 3.5--3.6. \ns
\end{proof}

\begin{prop}\label{0058338} Let $A$ be a separable unital $C^*$-algebra, $H$ be a Hilbert
space, and $\tau_1$, $\tau_2 \colon A \sto \ofml Q(H)$ be unital $*\,$-monomorphisms. If
$\tau_2$ splits, then $\tau_1 + \tau_2$ is unitarily equivalent to~$\tau_1$.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, theorem 3.4.7. \ns \end{proof}

\begin{cor}\label{0058339} Suppose that $A$ is a separable unital $C^*$-algebra, $H$ is a
Hilbert space, and $\tau \colon A \sto \ofml Q(H)$ is a split unital $*\,$-monomorphism.
Then $[\tau]$ is an additive identity in~$\ext A$.
\end{cor}

Note that a unital $*\,$-monomorphism $\tau\colon A \sto \ofml Q(H)$ from a separable
unital $C^*$-algebra to the Calkin algebra of a Hilbert space $H$ is semisplit if and
only if $[\tau]$ has an additive inverse in $\ext A$.  The next proposition says that
this happens if and only if $\tau$ has a completely positive lifting to~$\ofml B(H)$.

\begin{prop}\label{0058411} Let $A$ be a separable unital $C^*$-algebra and $H$ be a Hilbert
space. A $*\,$-monomorphism $\tau\colon A \sto \ofml Q(H)$ is semisplit if and only if
there exists a unital $*\,$-homomorphism $\wt\tau\colon A \sto \ofml B(H)$ such that the
diagram
  \[
    \xy
      \dtriangle/<-`->`->/[\ofml B(H)`A`\ofml Q(H);\wt\tau`\pi`\tau]
    \endxy
   \]
commutes.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, theorem 3.1.5.  \ns \end{proof}

\begin{defn} A $C^*$-algebra $A$ is
 \index{nuclear}%
 \index{C*@$C^*$-algebra!nuclear}%
\df{nuclear} if for every $C^*$-algebra $B$ there is a unique $C^*$-norm on the algebraic
tensor product $A \odot B$.
\end{defn}

\begin{exam}\label{0058513} Every finite dimensional $C^*$-algebra is nuclear.
 \index{nuclear!finite dimensional algebras are}%
\end{exam}

\begin{proof} See \cite{Murphy:1990}, theorem 6.3.9.  \ns \end{proof}

\begin{exam}\label{0058514} The $C^*$-algebra $\mathbf M_n$
 \index{M@$\mathbf M_n$!as a nuclear $C^*$-algebra}%
 \index{nuclear!$\mathbf M_n$ is}%
is nuclear.
\end{exam}
\begin{proof} See \cite{Blackadar:2006}, II.9.4.2.  \ns \end{proof}

\begin{exam}\label{0058515} If $H$ is a Hilbert space,
 \index{K@$\ofml K(H)$!as a nuclear $C^*$-algebra}%
 \index{nuclear!$\ofml K(H)$ is}%
the $C^*$-algebra $\ofml K(H)$ of compact operators is nuclear.
\end{exam}
\begin{proof} See \cite{Murphy:1990}, example 6.3.2.  \ns \end{proof}

\begin{exam}\label{0058517} If $X$ is a compact Hausdorff space, then the $C^*$-algebra
 \index{continuous@$\fml C(X)$!as a nuclear $C^*$-algebra}%
 \index{nuclear!$\fml C(X)$ is}%
$\fml C(X)$ is nuclear.
\end{exam}

\begin{proof} See \cite{Paulsen:2002}, proposition 12.9.  \ns \end{proof}

\begin{exam}\label{0058519} Every commutative $C^*$-algebra is nuclear.
 \index{nuclear!commutative $C^*$-algebras are}%
\end{exam}

\begin{proof} See \cite{Fillmore:1996}, theorem 7.4.1 or \cite{Wegge-Olsen:1993}, theorem
T.6.20. \ns
\end{proof}

The conclusion of the next result is known as the
 \index{completely!positive!lifting property}%
\emph{completely positive lifting property}.

\begin{prop}\label{0058541} Let $A$ be a nuclear separable unital $C^*$-algebra and $J$ be a
separable ideal in a unital $C^*$-algebra~$B$. Then for every completely positive map
$\phi\colon A \sto B/J$ there exists a completely positive map $\wt\phi\colon A \sto B$
which makes the diagram
  \[
    \xy
      \dtriangle/<-`->`->/[B`A`B/J;\wt\phi``\phi]
    \endxy
   \]
commute.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, theorem 3.3.6.  \ns \end{proof}

\begin{cor}\label{0058543} If $A$ is a nuclear separable unital $C^*$-algebra,
 \index{ext@$\ext A$, $\ext X$!is an Abelian group}%
then $\ext A$ is an Abelian group.
\end{cor}







\endinput
\chapter{FREDHOLM THEORY}

\section{The Fredholm Alternative}

In 1903 Erik Ivar Fredholm published a seminal paper on integral equations in the journal
\emph{Acta Mathematica}.  Among many important results was the theorem we know today as
the \emph{Fredholm alternative}. We state a version of this result in the language
available to Fredholm at the time.

\begin{prop}[Fredholm Alternative I]\label{004011} Let $k$ be a continuous complex valued
function on the unit square $[0,1] \times [0,1]$. \textbf{Either} the nonhomogeneous
 \index{Fredholm alternative!version I}%
equations
 \begin{align}
   \lambda f(s) &- \int_0^1 k(s,t)f(t)\,dt = g(s)\label{004011i} \text{ and}    \tag{1}\\
   \conj{\lambda} h(s) &- \int_0^1 \conj{k(t,s)}h(t)\,dt = j(s)\label{004011ii} \tag{2}
 \end{align}
have solutions $f$ and $h$ for every given $g$ and $j$, respectively, the solutions being
unique, in which case the corresponding homogeneous equations
 \begin{align} \lambda f(s) &- \int_0^1 k(s,t)f(t)\,dt = 0\label{004011iii} \text{ and} \tag{3}\\
        \conj{\lambda} h(s) &- \int_0^1 \conj{k(t,s)}h(t)\,dt = 0\label{004011iv} \tag{4}
 \end{align}
have only the trivial solution; \textbf{---or else---}\\
the homogeneous equations \eqref{004011iii} and \eqref{004011iv} have the same (nonzero)
finite number of linearly independent solutions $f_1,\dots,f_n$ and $h_1,\dots,h_n$,
respectively, in which case the nonhomogeneous equations \eqref{004011i} and
\eqref{004011ii} have a solution if and only if $g$ and $j$ satisfy
 \begin{align} \int_0^1 h_k(t)\conj{g(t)}\,dt &= 0\label{004011v} \text{ and} \tag{5}\\
              \int_0^1 j(t) \conj{f_k(t)}\,dt &= 0\label{004011vi} \tag{6}
 \end{align}
for $k = 1, \dots, n$.
\end{prop}

By 1906 David Hilbert had noticed that integration as such had very little to do with the
correctness of this result.  What was important, he discovered, was the compactness of
the resulting integral operator.  (In the early $20^{\text{th}}$ century
\emph{compactness} was called \emph{complete continuity}.  The term \emph{Hilbert space}
was used as early as 1911 in connections with the sequence space~$l_2$.  It was not until
1929 that John von Neumann introduced the notion of---and axioms
defining---\emph{abstract Hilbert spaces}.)  So here is a somewhat updated version of
\emph{Fredholm alternative}.

\begin{prop}[Fredholm Alternative II]\label{004031} If $K$ is a compact Hilbert
 \index{Fredholm alternative!version II}%
space operator, $\lambda \in \C$, and $T = \lambda I - K$, then \textbf{either} the
nonhomogeneous equations
 \begin{align}
            Tf &= g\label{004031i} \text{ and} \tag{1'}\\
            T^* h &= j\label{004031ii} \tag{2'}
 \end{align}
have solutions $f$ and $h$ for every given $g$ and $j$, respectively, the solutions being
unique, in which case the corresponding homogeneous equations
 \begin{align}
            Tf &= 0\label{004031iii} \text{ and} \tag{3'}\\
          T^*h &= 0\label{004031iv} \tag{4'}
 \end{align}
have only the trivial solution; \textbf{---or else---}\\
the homogeneous equations \eqref{004031iii} and \eqref{004031iv} have the same (nonzero)
finite number of linearly independent solutions $f_1,\dots,f_n$ and $h_1,\dots,h_n$,
respectively, in which case the nonhomogeneous equations \eqref{004031i} and
\eqref{004031ii} have a solution if and only if $g$ and $j$ satisfy
 \begin{align} h_k &\perp g\label{004031v} \text{ and} \tag{5'}\\
                 j &\perp f_k\label{004031vi} \tag{6'}
 \end{align}
for $k = 1, \dots, n$.
\end{prop}

Notice that by making use of a few elementary facts concerning kernels and ranges of
operators and orthogonality in Hilbert spaces, we can compress the statement
of~\ref{004031} quite a bit.

\begin{prop}[Fredholm Alternative IIIa]\label{004033} If $T = \lambda I - K$ where
 \index{Fredholm alternative!version IIIa}%
$K$ is a compact Hilbert space operator and $\lambda \in \C$, then
 \begin{enumerate}
  \item $T$ is injective if and only if it is surjective,
  \item $\ran T^* = (\ker T)^\perp$, and
  \item $\dim\ker T = \dim \ker T^*$.
 \end{enumerate}
Also, conditions (1) and (2) hold for $T^*$ as well as~$T$.
\end{prop}


















\section{The Fredholm Alternative -- continued}
\begin{defn}\label{004034} An operator $T$ on a Banach space is a
 \index{Riesz-Schauder operator}%
 \index{operator!Riesz-Schauder}%
\df{Riesz-Schauder} operator if it can be written in the form $T = S + K$ where $S$ is
invertible, $K$ is compact, and $SK = KS$.
\end{defn}

The material in section~\ref{sec_onbases} and the preceding definition make it possible to
generalize the version of the \emph{Fredholm alternative} given in~\ref{004033} to Banach
spaces.

\begin{prop}[Fredholm Alternative IIIb]\label{0040343} If $T$ is a Riesz-Schauder
 \index{Fredholm alternative!version IIIb}%
operator on a Banach space, then
 \begin{enumerate}
  \item $T$ is injective if and only if it is surjective,
  \item $\ran T^* = (\ker T)^\perp$, and
  \item $\dim\ker T = \dim \ker T^* < \infty$.
 \end{enumerate}
Also, conditions (1) and (2) hold for $T^*$ as well as~$T$.
\end{prop}

\begin{prop} If $M$ is a closed subspace of a Banach space, then $M^\perp \cong (B/M)^*$.
\end{prop}

\begin{proof} See \cite{Conway:1990}, page 89. \ns \end{proof}

\begin{defn} If $T\colon V \sto W$ is a linear map between vector spaces, then its
 \index{cokernel}%
\df{cokernel} is defined by
  \[ \coker T = W/ \ran T\,. \]
Recall that the
 \index{codimension}%
\df{codimension} of a subspace $U$ of a vector space $V$ is the dimension of~$V/U$.  Thus
when $T$ is a linear map $\dim\coker T = \codim\ran T$.
\end{defn}

In the category $\cat{HIL}$ of Hilbert spaces and continuous linear maps the range of a
morphism need not be an object of the category. Specifically the range of an operator
need not be closed.

\begin{exam}\label{004066} The Hilbert space operator
   \[ T\colon l_2 \sto l_2\colon (x_1,x_2,x_3,\dots) \mapsto
                                   (x_1, \tfrac12 x_2, \tfrac13 x_3, \dots) \]
is injective, self-adjoint, compact, and contractive,
 \index{operator!with non-closed range}%
but its range, while dense in $l_2$, is not all of~$l_2$.
\end{exam}

Although it is incidental to our present purposes this is a convenient place to note the
fact that the sum of two subspaces of a Hilbert space need not be a subspace.

\begin{exam}\label{004071} Let $T$ be the operator on the Hilbert space $l_2$ defined in
 \index{subspace!sum of subspaces need not be a}%
example~\ref{004066}, $M = l_2 \oplus \{\vc 0\}$, and $N$ be the graph of~$T$.  Then $M$
and $N$ are both (closed) subspaces of the Hilbert space $l_2 \oplus l_2$ but $M + N$ is
not.
\end{exam}

\begin{proof} Verification of example~\ref{004071} follows easily from the following result
and example~\ref{004066}. \ns
\end{proof}

\begin{prop} Let $H$ be a Hilbert space, $T \in \ofml B(H)$, $M = H \oplus \{\vc 0\}$, and $N$
be the graph of~$T$. Then
 \begin{enumerate}
   \item[(a)] The set $N$ is a subspace of $H \oplus H$.
   \item[(b)] The operator $T$ is injective if and only if $M \cap N = \{\,(\vc 0,\vc 0)\,\}$.
   \item[(c)] The range of $T$ is dense in $H$ if and only if $M + N$ is dense in $H \oplus H$.
   \item[(d)] The operator $T$ is surjective if and only if $M + N = H \oplus H$.
 \end{enumerate}
\end{prop}

The good news for the theory of Fredholm operators is that operators with finite
dimensional cokernels automatically have closed range.

\begin{prop}\label{00411} If a bounded linear map $A\colon H \sto K$ between Hilbert spaces
has finite dimensional cokernel, then $\ran A$ is closed in~$K$.
\end{prop}

We observe in \emph{Fredholm alternative IIIb}~\ref{0040343} that condition (1) is
redundant and also that (2) holds for \emph{any} Banach space operator with closed range.
This enables us to rephrase~\ref{0040343} more economically.

\begin{prop}[Fredholm alternative IV]\label{00413}
 \index{Fredholm alternative!version IV}%
If $T$ is a Riesz-Schauder operator on a Banach space, then
 \begin{enumerate}
    \item $T$ has closed range and
    \item  $\dim\ker T = \dim \ker T^* < \infty$.
 \end{enumerate}
\end{prop}














\section{Fredholm Operators}
\begin{defn} Let $H$ be a Hilbert space and $\ofml K(H)$ be the ideal of compact operators in
the $C^*$-algebra~$\ofml B(H)$.  Then the quotient algebra (see proposition~\ref{001902})
$\ofml Q(H) := \ofml B(H)/\ofml K(H)$ is the
 \index{Calkin algebra}%
 \index{algebra!Calkin}%
 \index{Q@$\ofml Q(H)$ (the Calkin algebra)}%
\df{Calkin algebra}.  As usual the quotient map taking $\ofml B(H)$ onto $\ofml
B(H)/\ofml K(H)$ is
 \index{pi@$\pi$ (quotient map)}%
 \index{pi@$\pi(T)$ (element of the Calkin algebra containing $T$)}%
denoted by $\pi$ so that if $T \in \ofml B(H)$ then $\pi(T) = [T] = T + \ofml K(H)$ is
its corresponding element in the Calkin algebra.  An element $T \in \ofml B(H)$ is a
 \index{Fredholm!operator}%
 \index{operator!Fredholm}%
\df{Fredholm operator} if $\pi(T)$ is invertible in~$\ofml Q(H)$.  We denote the
 \index{F@$\ofml F(H)$ (Fredholm operators on $H$)}%
family of all Fredholm operators on $H$ by~$\ofml F(H)$.
\end{defn}

\begin{prop} If $H$ is a Hilbert space, then $\ofml F(H)$ is a self-adjoint open subset
of~$\ofml B(H)$ which is closed under compact
 \index{perturbation}%
perturbations (that is, if $T$ is Fredholm and $K$ is compact, then $T + K$ is Fredholm).
\end{prop}

\begin{thm}[Atkinson's theorem]\label{004352}
 \index{Atkinson's theorem}%
A Hilbert space operator is Fredholm if and only if it has finite dimensional kernel and
cokernel.
\end{thm}

\begin{proof} See \cite{Arveson:2002}, theorem 3.3.2; \cite{Blackadar:2006}, theorem I.8.3.6;
\cite{Douglas:1972}, theorem 5.17; \cite{HigsonR:2000}, theorem 2.1.4;
or~\cite{Wegge-Olsen:1993}, theorem 14.1.1. \ns
\end{proof}

Interestingly the dimensions of the kernel and cokernel of a Fredholm operator are not
very important quantities.  Their difference however turns out to be very important.

\begin{defn} If $T$ is a Fredholm operator on a Hilbert space then its
 \index{index!of a Fredholm operator}%
 \index{Fredholm!index (\seeonly{index}))}%
\df{Fredholm index} (or just \df{index}) is defined by
   \[ \ind T := \dim\ker T - \dim\coker T\,. \]
\end{defn}

\begin{exam}\label{0044125} Every invertible Hilbert space operator is Fredholm
 \index{index!of an invertible operator}%
 \index{operator!invertible!index of}%
 \index{invertible!operator!index of}%
with index zero.
\end{exam}

\begin{exam}\label{0044128} The Fredholm index of the unilateral shift
 \index{index!of the unilateral shift}%
 \index{unilateral shift!index of}%
 \index{operator!unilateral shift!index of}%
operator is~$-1$.
\end{exam}

\begin{exam} Every linear map $T\colon V \sto W$ between finite dimensional vector spaces is
Fredholm and $\ind T = \dim V - \dim W$.
\end{exam}

\begin{exam}\label{004413} If $T$ is a Fredholm operator on a Hilbert space,
 \index{index!of the adjoint of an operator}%
 \index{operator!adjoint!index of}%
 \index{adjoint!index of}%
then $\ind T^* = -\ind T$.
\end{exam}

\begin{exam} The index of any normal Fredholm
 \index{index!of a normal Fredholm operator}%
 \index{operator!normal Fredholm!index of}%
 \index{normal!Fredholm operator!index of}%
operator is~$0$.
\end{exam}

\begin{lem}\label{004432} Let $S\colon U \sto V$ and $T\colon V \sto W$ be linear
transformations between vector spaces.  Then (there exist linear mappings such that) the
following sequence is exact.
 \[ \vc 0 \to \ker S \to \ker TS \to \ker T \to \coker S
                            \to \coker TS \to \coker T \to \vc 0\,. \]
\end{lem}

\begin{lem}\label{004433} If $V_0, V_1, \dots, V_n$ are finite dimensional vector spaces
and the sequence
  \[ \vc 0 \to V_0 \to V_1 \to \dots \to V_n \to \vc 0 \]
is exact, then
  \[ \sum_{k=0}^n (-1)^k \dim V_k = 0\,. \]
\end{lem}

\begin{prop} Let $H$ be a Hilbert space. Then the set $\ofml F(H)$ of Fredholm operators
on $H$ is a semigroup under composition and the index function $\ind$ is an epimorphism
from $\ofml F(H)$ onto the additive semigroup $\Z$ of integers.
\end{prop}

\begin{proof} \emph{Hint.} Use \ref{004432}, \ref{004433}, \ref{0044125}, \ref{0044128},
and~\ref{004413}. \ns
\end{proof}





















\section{The Fredholm Alternative -- Concluded}
The next result implies that every Fredholm operator of index zero is of the form
invertible plus compact.

\begin{prop}\label{004712} If $T$ is a Fredholm operator of index zero on a Hilbert space,
then there exists a finite rank partial isometry $V$ such that $T - V$ is invertible.
\end{prop}

\begin{proof} See \cite{Pedersen:1995}, lemma 3.3.14 or \cite{Wegge-Olsen:1993}, proposition
14.1.3.  \ns
\end{proof}

\begin{lem} If $F$ is a finite rank operator on a Hilbert space, then $I + F$ is Fredholm with
index zero.
\end{lem}

\begin{proof} See \cite{Pedersen:1995}, lemma 3.3.13 or \cite{Wegge-Olsen:1993}, lemma 14.1.4. \ns
\end{proof}

\begin{notn} Let $H$ be a Hilbert space.  For each integer $n$ we denote the
 \index{F@$\ofml F_n(H)$, $\ofml F_n$ (Fredholm operators of index~$n$)}%
family of all Fredholm operators of index $n$ on $H$ by $\ofml F_n(H)$ or just $\ofml
F_n$.
\end{notn}

\begin{prop}\label{004724} In every Hilbert space $\ofml F_0 + \ofml K = \ofml F_0$.
\end{prop}

\begin{proof} See \cite{Douglas:1972}, lemma 5.20 or \cite{Wegge-Olsen:1993}, 14.1.5. \ns
\end{proof}

\begin{cor}[Fredholm alternative V]\label{004725}
 \index{Fredholm alternative!version V}%
Every Riesz-Schauder operator on a Hilbert space is Fredholm of index zero.
\end{cor}

\begin{proof} This follows immediately from the preceding proposition~\ref{004724} and
example~\ref{0044125}.
\end{proof}

We have actually proved a stronger result: our final (quite general) version of the
\emph{Fredholm alternative}.

\begin{cor}[Fredholm alternative VI]\label{0047253} A Hilbert space operator is
 \index{Fredholm alternative!version VI}%
Riesz-Schauder if and only if it is Fredholm of index zero.
\end{cor}

\begin{proof} Proposition~\ref{004712} and corollary~\ref{004725}.
\end{proof}

\begin{prop} If $T$ is a Fredholm operator of index $n \in \Z$ on a Hilbert space $H$
and $K$ is a compact operator on~$H$, then $T + K$ is also Fredholm of index~$n$; that is
  \[ \ofml F_n + \ofml K = \ofml F_n\,. \]
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, proposition 2.1.6; \cite{Pedersen:1995}, theorem
3.3.17; or \cite{Wegge-Olsen:1993}, proposition 14.1.6. \ns
\end{proof}

\begin{prop} Let $H$ be a Hilbert space. Then $\ofml F_n(H)$ is an open subset of $\ofml B(H)$
for each integer~$n$.
\end{prop}

\begin{proof} See \cite{Wegge-Olsen:1993}, proposition 14.1.8. \ns \end{proof}

\begin{defn}\label{004735} A
 \index{path}%
\df{path} in a topological space $X$ is a continuous map from the interval $[0,1]$
into~$X$. Two points $p$ and $q$ in $X$ are said to be
 \index{path!points connected by a}%
 \index{connected!by a path}%
\df{connected by a path} (or
 \index{homotopic!points in a topological space}%
\df{homotopic in~$X$}) if there exists a path $f\colon [0,1] \sto X$ in $X$ such that
$f(0) = p$ and $f(1) = q$.   In this case we
 \index{<binrelequivh@$p \sim_h q$ (homotopy of points)}%
write $p \sim_h q$ in~$X$ (or just $p \sim_h q$ when the space $X$ is clear from context).
\end{defn}


\begin{exam} Let $a$ be an invertible element in a unital $C^*$-algebra $A$ and $b$ be an element of $A$
such that $\norm{a - b} < \norm{a^{-1}}^{-1}$.  Then $a \sim_h b$ in~$\inv(A)$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Apply corollary~\ref{cor2_Neumann_series} to points in the closed segment~$[a,b]$.  \ns
\end{proof}

\begin{prop}\label{K002011} The relation $\sim_h$ of homotopy equivalence defined above is an equivalence
 \index{equivalence!homotopy!of points}%
relation on the set of points of a topological space.
\end{prop}

\begin{defn} If $X$ is a topological space and $\sim_h$ is the relation of homotopy equivalence,
then the resulting equivalence classes are the
 \index{path!components}%
 \index{components!path}%
\df{path components} of~$X$.
\end{defn}

The next proposition identifies the path components of the set of Fredholm operators as
the sets $\ofml F_n$ of operators with index~$n$.

\begin{prop} Operators $S$ and $T$ in the space $\ofml F(H)$ of Fredholm operators on a
Hilbert space $H$ are homotopic in $\ofml F(H)$ if and only if they have the same index.
\end{prop}

\begin{proof} See \cite{Wegge-Olsen:1993}, corollary 14.1.9. \ns \end{proof}







\endinput
 \documentclass[11pt,reqno]{amsbook}
 \usepackage[dvips]{graphicx}
% \usepackage{epstopdf}
 \usepackage{amssymb}
 \usepackage{amscd}
 \usepackage[all]{xy}
 \usepackage{url}
 \usepackage{srcltx}
 \usepackage{enumitem}
 \usepackage{showkeys}
 \usepackage[plainpages=false,pagebackref=true,hypertex]{hyperref}
     %plainpages=false [corrects page numbers in index]
     %pagebackref=true [provides back references from bibliography]



\makeindex


 \setlength{\textwidth}{6.5in}
 \setlength{\oddsidemargin}{0in}
 \setlength{\evensidemargin}{0in}
 \setlength{\textheight}{9.25in}
 \setlength{\topmargin}{-0.35in}

 \input{table}
 \input diagxy


%\includeonly{}


\swapnumbers \theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ax}[thm]{Axiom}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{notn}[thm]{Notation}
\newtheorem{conv}[thm]{Convention}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem*{cau}{CAUTION}
\newtheorem{fact}[thm]{Fact}
\newtheorem{exam}[thm]{Example}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{prob}[thm]{Problem}
\newtheorem{proj}[thm]{Project}



\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}


\renewcommand{\labelenumi}{\rm(\alph{enumi})}
%to get enumeration in lower case roman




\numberwithin{equation}{section}





 \newcommand{\field}[1]{\mathbb{#1}}
 \newcommand{\C}{\field{C}}
 \newcommand{\Di}{\field{D}}
 \newcommand{\Dsk}{\field{D}}
 \newcommand{\E}{\field{E}}
 \newcommand{\K}{\field{K}}
 \newcommand{\N}{\field{N}}
 \newcommand{\Po}{\field{P}}
 \newcommand{\Q}{\field{Q}}
 \newcommand{\R}{\field{R}}
 \newcommand{\Sp}{\field{S}}
 \newcommand{\T}{\field{T}}
 \newcommand{\Z}{\field{Z}}



 \newcommand{\sto}{\rightarrow}
      %Diagxy redefined \to . This shortens the arrow.
 \newcommand{\ofml}[1]{\mathfrak{#1}}
      % for families of operators
 \newcommand{\fml}[1]{\mathcal{#1}}
      % for families of functions
 \newcommand{\sfml}[1]{\mathfrak{#1}}
      % for families of sets
 \newcommand{\ftr}[1]{\mathsf{#1}}
      % for functors
 \newcommand{\vc}[1]{\mathbf{#1}}
      % for vectors
 \newcommand{\df}[1]{\textsc{#1}}
      % for terms being defined
 \newcommand{\bs}[1]{\boldsymbol{#1}}
      % to create bold symbols and Greek letters
 \newcommand{\D}{\displaystyle}
 \newcommand{\ns}{\renewcommand{\qed}{}}
 \newcommand{\abs}[1]{\lvert#1\rvert}
      % absolute value
 \newcommand{\bigabs}[1]{\left\lvert#1\right\rvert}
      % large absolute value
  \newcommand{\norm}[1]{\lVert#1\rVert}
      % norm
 \newcommand{\bignorm}[1]{\bigl\lVert#1\bigr\rVert}
      % large norm
 \newcommand{\biggnorm}[1]{\biggl\lVert#1\biggr\rVert}
      % larger norm
 \newcommand{\trinorm}[1]{\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert#1\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert}
      % 3-bar norm
 \newcommand{\cstariso}[2]{#1\overset{\,{}_\ast}{\cong}#2}
      % #1 is *-isomorphic to #2
 \newcommand{\cat}[1]{\mathbf{#1}}
      % for categories
 \newcommand{\clo}[1]{\overline{#1}}
      % closure
 \newcommand{\cloco}[1]{\overline{\operatorname{co}}(#1)}
      % closed convex hull
 \newcommand{\co}[1]{\operatorname{co}(#1)}
      % convex hull
 \newcommand{\conj}[1]{\overline{#1}}
      % complex conjugate
 \newcommand{\id}[1]{\operatorname{id}_{#1}}
      % Identity on #1
 \newcommand{\intr}[1]{{#1}^{\circ}}
      % interior
  \newcommand{\lfs}[1]{\mathrm{L_{#1}}}
      % Lebesgue function spaces
 \newcommand{\locint}[1]{\mathrm{L_1^{loc}}({#1})}
      % locally integrable functions on #1
 \newcommand{\M}[2]{\mathbf{M}_{#1}({#2})}
      % for matrix algebras
 \newcommand{\mor}[2]{\operatorname{\mathfrak{Mor}}(#1,#2)}
       % Morphisms from #1 to #2
 \newcommand{\open}[2]{#1\overset{\thickspace{}_\circ}{\subseteq}#2}
      % #1 is an open subset of #2
 \newcommand{\sbsb}[2]{#1_{{}_\sst{#2}}}
      % makes smaller, lower subscript
 \newcommand{\spsp}[2]{#1^{{}^\sst{#2}}}
      % makes smaller higher superscript
 \newcommand{\sst}[1]{{\scriptstyle{#1}}}
      % sub- and superscript size type
 \newcommand{\ssst}[1]{{\scriptscriptstyle{#1}}}
      % second level sub- and superscripts
 \newcommand{\pd}[2]{\dfrac{\partial#1}{\partial#2}}
      % first partial, 1 variable
 \newcommand{\ppd}[2]{\dfrac{\partial^2#1}{\partial#2^2}}
      % second partial, 1 variable
 \newcommand{\ppdd}[3]{\dfrac{\partial^2#1}{\partial#2\partial#3}}
      % second partial, 2 variables
 \newcommand{\wt}[1]{\widetilde{#1}}
 \newcommand{\wh}[1]{\widehat{#1}}



 \DeclareMathOperator{\ad}{Ad}
 \DeclareMathOperator{\ba}{ba}
 \DeclareMathOperator{\ca}{ca}
 \DeclareMathOperator{\card}{card}
 \DeclareMathOperator{\cloin}{cl}
 \DeclareMathOperator{\codim}{codim}
 \DeclareMathOperator{\coker}{coker}
 \DeclareMathOperator{\diag}{diag}
 \DeclareMathOperator{\diam}{diam}
 \DeclareMathOperator{\dist}{dist}
 \DeclareMathOperator{\dom}{dom}
 \DeclareMathOperator{\essran}{essran}
 \DeclareMathOperator{\ext}{Ext}
 \DeclareMathOperator{\fin}{Fin}
 \DeclareMathOperator{\Hom}{Hom}
 \DeclareMathOperator{\im}{im}
 \DeclareMathOperator{\ind}{ind}
 \DeclareMathOperator{\intrin}{int}
 \DeclareMathOperator{\inv}{inv}
 \DeclareMathOperator{\mx}{Max}
 \DeclareMathOperator{\ran}{ran}
 \DeclareMathOperator{\rca}{rca}
 \DeclareMathOperator{\spn}{span}
 \DeclareMathOperator{\supp}{supp}
 \DeclareMathOperator{\tr}{tr}





\begin{document}


 \frontmatter
 \title{FUNCTIONAL ANALYSIS \\ AND \\ OPERATOR ALGEBRAS: \\ An Introduction}
 \author{John M. Erdman \\
      Portland State University \\
      \mbox{\hphantom{P}} \\
      Version October 4, 2015  \\
      \mbox{\hphantom{P}} \\
    %  \copyright 2014 John M. Erdman \\
      \mbox{\hphantom{P}} \\
            \mbox{\hphantom{P}} \\
                  \mbox{\hphantom{P}} }


\vskip 2 in

 \email{erdman@pdx.edu}

\maketitle


\vskip 2 in

 \begin{figure}[h]
  \includegraphics{by-sa}
 \end{figure}

\vskip 1 in

This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/.



\vfill\eject

\thispagestyle{empty}
 \include{Wiener_quote}
\cleardoublepage

\tableofcontents


\include{preface}

 \cleardoublepage
 \phantomsection
        %allows TOC links to find preface correctly




\mainmatter




 \include{linalg}
 \include{categories}
 \include{normlinspaces}
 \include{Hilbert_spaces}
 \include{Hilbert_space_operators}
 \include{Banach_spaces}
 \include{compact_operators}
 \include{spectrum}
 \include{topvecspaces}
 \include{distributions}
 \include{Gelfand_Naimark}
 \include{no_identity}
 \include{GNS_construction}
 \include{multiplier_algebras}
 \include{fredholm_theory}
 \include{extensions}
 \include{K0_functor}
% \include{K1_functor}
% \include{groupoids}


%\enlargethispage{\baselineskip}




\backmatter


 \clearpage
 \phantomsection
        %allows TOC links to find bibliography correctly
 \bibliographystyle {amsplain}
 \bibliography{functional_analysis_op_algs_bib}
     %\addcontentsline{toc}{chapter}{Bibliography}


 \cleardoublepage
 \phantomsection
         %allows TOC links to find index correctly
 \printindex
 \addcontentsline{toc}{chapter}{Index}




\end{document}
 \documentclass[11pt,reqno]{amsbook}
 \usepackage[dvips]{graphicx}
 \usepackage{amssymb}
 \usepackage{amscd}
 \usepackage[all]{xy}
 \usepackage{url}
 \usepackage{srcltx}
 \usepackage{enumitem}
 \usepackage[pdftex,
            pdfauthor={John M Erdman},
            pdftitle={Functional Analysis and Operator Algebras: An Introduction},
            pdfsubject={functional analysis, operator algebras},
            pdfkeywords={linear, functional, analysis, operator, algebras, Hilbert spaces, Banach spaces, spectrum, projections, K-theory, distributions, Gelfand, Naimark, Topological vector spaces, locally convex, compact operators, spectral theorem, unitary, self-adjoint, Fredholm},
            plainpages=false,pdftex,pagebackref=true,colorlinks,bookmarks=false]{hyperref}
     %plainpages=false corrects page numbers in index
     %pagebackref=true provides back references from bibliography
     %bookmarks=false prevents PDF bookmarks and the accompanying
     %     error messages about tokens (like $)







\makeindex


 \setlength{\textwidth}{6.5in}
 \setlength{\oddsidemargin}{0in}
 \setlength{\evensidemargin}{0in}
 \setlength{\textheight}{9.25in}
 \setlength{\topmargin}{-0.35in}

 \input{table}
 \input diagxy


%\includeonly{}


\swapnumbers \theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ax}[thm]{Axiom}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{notn}[thm]{Notation}
\newtheorem{conv}[thm]{Convention}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem*{cau}{CAUTION}
\newtheorem{fact}[thm]{Fact}
\newtheorem{exam}[thm]{Example}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{prob}[thm]{Problem}
\newtheorem{proj}[thm]{Project}



\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\thechapter.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}


\renewcommand{\labelenumi}{\rm(\alph{enumi})}
%to get enumeration in lower case roman




\numberwithin{equation}{section}





 \newcommand{\field}[1]{\mathbb{#1}}
 \newcommand{\C}{\field{C}}
 \newcommand{\Di}{\field{D}}
 \newcommand{\Dsk}{\field{D}}
 \newcommand{\E}{\field{E}}
 \newcommand{\K}{\field{K}}
 \newcommand{\N}{\field{N}}
 \newcommand{\Po}{\field{P}}
 \newcommand{\Q}{\field{Q}}
 \newcommand{\R}{\field{R}}
 \newcommand{\Sp}{\field{S}}
 \newcommand{\T}{\field{T}}
 \newcommand{\Z}{\field{Z}}



 \newcommand{\sto}{\rightarrow}
      %Diagxy redefined \to . This shortens the arrow.
 \newcommand{\ofml}[1]{\mathfrak{#1}}
      % for families of operators
 \newcommand{\fml}[1]{\mathcal{#1}}
      % for families of functions
 \newcommand{\sfml}[1]{\mathfrak{#1}}
      % for families of sets
 \newcommand{\ftr}[1]{\mathsf{#1}}
      % for functors
 \newcommand{\vc}[1]{\mathbf{#1}}
      % for vectors
 \newcommand{\df}[1]{\textsc{#1}}
      % for terms being defined
 \newcommand{\bs}[1]{\boldsymbol{#1}}
      % to create bold symbols and Greek letters
 \newcommand{\D}{\displaystyle}
 \newcommand{\ns}{\renewcommand{\qed}{}}
 \newcommand{\abs}[1]{\lvert#1\rvert}
      % absolute value
 \newcommand{\bigabs}[1]{\left\lvert#1\right\rvert}
      % large absolute value
  \newcommand{\norm}[1]{\lVert#1\rVert}
      % norm
 \newcommand{\bignorm}[1]{\bigl\lVert#1\bigr\rVert}
      % large norm
 \newcommand{\biggnorm}[1]{\biggl\lVert#1\biggr\rVert}
      % larger norm
 \newcommand{\trinorm}[1]{\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert#1\vert\mspace{-2mu}\vert
     \mspace{-2mu}\vert}
      % 3-bar norm
 \newcommand{\cstariso}[2]{#1\overset{\,{}_\ast}{\cong}#2}
      % #1 is *-isomorphic to #2
 \newcommand{\cat}[1]{\mathbf{#1}}
      % for categories
 \newcommand{\clo}[1]{\overline{#1}}
      % closure
 \newcommand{\cloco}[1]{\overline{\operatorname{co}}(#1)}
      % closed convex hull
 \newcommand{\co}[1]{\operatorname{co}(#1)}
      % convex hull
 \newcommand{\conj}[1]{\overline{#1}}
      % complex conjugate
 \newcommand{\id}[1]{\operatorname{id}_{#1}}
      % Identity on #1
 \newcommand{\intr}[1]{{#1}^{\circ}}
      % interior
  \newcommand{\lfs}[1]{\mathrm{L_{#1}}}
      % Lebesgue function spaces
 \newcommand{\locint}[1]{\mathrm{L_1^{loc}}({#1})}
      % locally integrable functions on #1
 \newcommand{\M}[2]{\mathbf{M}_{#1}({#2})}
      % for matrix algebras
 \newcommand{\mor}[2]{\operatorname{\mathfrak{Mor}}(#1,#2)}
       % Morphisms from #1 to #2
 \newcommand{\open}[2]{#1\overset{\thickspace{}_\circ}{\subseteq}#2}
      % #1 is an open subset of #2
 \newcommand{\sbsb}[2]{#1_{{}_\sst{#2}}}
      % makes smaller, lower subscript
 \newcommand{\spsp}[2]{#1^{{}^\sst{#2}}}
      % makes smaller higher superscript
 \newcommand{\sst}[1]{{\scriptstyle{#1}}}
      % sub- and superscript size type
 \newcommand{\ssst}[1]{{\scriptscriptstyle{#1}}}
      % second level sub- and superscripts
 \newcommand{\pd}[2]{\dfrac{\partial#1}{\partial#2}}
      % first partial, 1 variable
 \newcommand{\ppd}[2]{\dfrac{\partial^2#1}{\partial#2^2}}
      % second partial, 1 variable
 \newcommand{\ppdd}[3]{\dfrac{\partial^2#1}{\partial#2\partial#3}}
      % second partial, 2 variables
 \newcommand{\wt}[1]{\widetilde{#1}}
 \newcommand{\wh}[1]{\widehat{#1}}



 \DeclareMathOperator{\ad}{Ad}
 \DeclareMathOperator{\ba}{ba}
 \DeclareMathOperator{\ca}{ca}
 \DeclareMathOperator{\card}{card}
 \DeclareMathOperator{\cloin}{cl}
 \DeclareMathOperator{\codim}{codim}
 \DeclareMathOperator{\coker}{coker}
 \DeclareMathOperator{\diag}{diag}
 \DeclareMathOperator{\diam}{diam}
 \DeclareMathOperator{\dist}{dist}
 \DeclareMathOperator{\dom}{dom}
 \DeclareMathOperator{\essran}{essran}
 \DeclareMathOperator{\ext}{Ext}
 \DeclareMathOperator{\fin}{Fin}
 \DeclareMathOperator{\Hom}{Hom}
 \DeclareMathOperator{\im}{im}
 \DeclareMathOperator{\ind}{ind}
 \DeclareMathOperator{\intrin}{int}
 \DeclareMathOperator{\inv}{inv}
 \DeclareMathOperator{\mx}{Max}
 \DeclareMathOperator{\ran}{ran}
 \DeclareMathOperator{\rca}{rca}
 \DeclareMathOperator{\spn}{span}
 \DeclareMathOperator{\supp}{supp}
 \DeclareMathOperator{\tr}{tr}





\begin{document}


 \frontmatter
 \title{FUNCTIONAL ANALYSIS \\ AND \\ OPERATOR ALGEBRAS: \\ An Introduction}
 \author{John M. Erdman \\
      Portland State University \\
      \mbox{\hphantom{P}} \\
      Version October 4, 2015  \\
      \mbox{\hphantom{P}} \\
    %  \copyright 2014 John M. Erdman \\
      \mbox{\hphantom{P}} \\
            \mbox{\hphantom{P}} \\
                  \mbox{\hphantom{P}} }


\vskip 2 in

 \email{erdman@pdx.edu}

\maketitle


 \begin{figure}[h]
  \includegraphics[scale=0.4]{by-sa.pdf}
 \end{figure}


This work is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-sa/4.0/.



\vfill\eject

\thispagestyle{empty}
 \include{Wiener_quote}
\cleardoublepage

\tableofcontents


\include{preface}

 \cleardoublepage
 \phantomsection
        %allows TOC links to find preface correctly




\mainmatter




 \include{linalg}
 \include{categories}
 \include{normlinspaces}
 \include{Hilbert_spaces}
 \include{Hilbert_space_operators}
 \include{Banach_spaces}
 \include{compact_operators}
 \include{spectrum}
 \include{topvecspaces}
 \include{distributions}
 \include{Gelfand_Naimark}
 \include{no_identity}
 \include{GNS_construction}
 \include{multiplier_algebras}
 \include{fredholm_theory}
 \include{extensions}
 \include{K0_functor}


%\enlargethispage{\baselineskip}




\backmatter


 \clearpage
 \phantomsection
        %allows TOC links to find bibliography correctly
 \bibliographystyle {amsplain}
 \bibliography{functional_analysis_op_algs_bib}
     %\addcontentsline{toc}{chapter}{Bibliography}


 \cleardoublepage
 \phantomsection
         %allows TOC links to find index correctly
 \printindex
 \addcontentsline{toc}{chapter}{Index}




\end{document}
\chapter{THE GELFAND-NAIMARK THEOREM}\label{Gelfand_Naimark}

As in Chapter~\ref{spectrum} we will in this chapter---and for the rest of these notes---assume that our field
  \index{conventions!in chapter~\ref{Gelfand_Naimark} and those following scalars are complex}%
of scalars is the field $\C$ of complex numbers.



\section{Maximal Ideals in $\fml C(X)$}
\begin{prop}\label{0006801} If $J$ is a proper ideal in a unital Banach algebra, then
so is its closure.
\end{prop}

\begin{cor} Every maximal ideal in a unital Banach algebra is closed.
\end{cor}

\begin{exam} For every subset $C$ of a topological space~$X$ the set
 \index{ideal!in $\fml C(X)$}%
 \index{J@$J_C$ (continuous functions vanishing on~$C$)}%
   \[ J_C := \bigl\{f \in \fml C(X)\colon f^{\sto}(C) = \{0\}\,\bigr\} \]
is an ideal in $\fml C(X)$.  Furthermore, $J_C \supseteq J_D$ whenever
$C~\subseteq~D~\subseteq~X$.  (In the following we will write $J_x$ for the ideal
$J_{\{x\}}$.)
\end{exam}

\begin{prop}\label{0006832} Let $X$ be a compact topological space and $I$ be a proper
ideal in $\fml C(X)$. Then there exists $x \in X$ such that $I \subseteq J_x$.
\end{prop}

\begin{prop} Let $x$ and $y$ be points in a compact Hausdorff space.  If $J_x \subseteq J_y$,
then $x = y$.
\end{prop}

\begin{prop}\label{0006836} Let $X$ be a compact Hausdorff space. A subset $I$ of $\fml C(X)$
is a maximal ideal in $\fml C(X)$ if and only if $I = J_x$ for some $x~\in~X$.
\end{prop}

\begin{cor} If $X$ is a compact Hausdorff space, then the map $x \mapsto J_x$ from $X$ to
$\mx\,\fml C(X)$ is bijective.
\end{cor}

Compactness is an important ingredient in proposition~\ref{0006836}.

\begin{exam} In the Banach algebra $\fml C_b\bigl(\,(0,1)\,\bigr)$ of bounded continuous
functions on the interval $(0,1)$ there exists a maximal ideal $I$ such that for
\emph{no} point $x \in (0,1)$ is $I = J_x$. Let $I$ be a maximal ideal containing the
ideal $S$ of all functions $f$ in $\fml C_b\bigl(\,(0,1)\,\bigr)$ for which there exists
a neighborhood $U_f$ of $0$ in $\R$ such that $f(x) = 0$ for all $x \in U_f \cap (0,1)$.
\end{exam}













\section{The Character Space}
\begin{defn} A
 \index{character}%
\df{character} (or
 \index{multiplicative!linear functional}%
 \index{linear!functional!multiplicative}%
\df{nonzero multiplicative linear functional}) on an algebra $A$ is a nonzero
homomorphism from $A$ into~$\C$.  The set of all characters on $A$ is denoted
 \index{delta@$\Delta(A)$!character space of~$A$}%
by~$\Delta A$.
\end{defn}

\begin{prop}\label{00068511} Let $A$ be a unital algebra and $\phi \in \Delta A$.  Then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $\phi(\vc 1) = 1$;
  \item if $a \in \inv A$, then $\phi(a) \ne 0$;
  \item if $a$ is
 \index{nilpotent}%
  \df{nilpotent} (that is, if $a^n = 0$ for some $n \in \N$), then $\phi(a) = 0$;
 \index{idempotent}%
  \item if $a$ is \df{idempotent} (that is, if $a^2 = a)$, then $\phi(a)$ is $0$ or~$1$; and
  \item $\phi(a) \in \sigma(a)$ for every $a \in A$.
 \end{enumerate}
\end{prop}
We note in passing that part (e) of the preceding proposition does not give us an easy
way of showing that the spectrum $\sigma(a)$ of an algebra element is nonempty. This
would depend on knowing that $\Delta(A)$ is nonempty.

\begin{exam} The identity map is the only character on the algebra~$\C$.
\end{exam}

\begin{exam} Let $A$ be the algebra of $2 \times 2$ matrices $a = \bigl[a_{ij}\bigr]$ such
that $a_{12} = 0$. This algebra has exactly two characters $\phi(a) = a_{11}$ and
$\psi(a) = a_{22}$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Write $\begin{bmatrix} a & 0 \\ b & c \end{bmatrix}$ as a linear combination
of the matrices $ u = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$, $v = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$,
and \smash[t]{$w = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$}. Use proposition~\ref{00068511}.   \ns
\end{proof}

\begin{exam} The algebra of all $2 \times 2$ matrices of complex numbers has no characters.
\end{exam}

\begin{prop} Let $A$ be a unital algebra and $\phi$ be a linear functional on~$A$. Then $\phi
\in \Delta A$ if and only if $\ker\phi$ is closed under multiplication and $\phi(\vc 1) =
1$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For the converse apply $\phi$ to the product of
$a - \phi(a)\vc 1$ and $b - \phi(b)\vc 1$ for $a$, $b \in A$. \ns
\end{proof}

\begin{prop}\label{00068522} Every multiplicative linear functional on a unital Banach algebra
$A$ is continuous.  In fact, if $\phi \in \Delta(A)$, then $\phi$ is contractive and
$\norm\phi = 1$.
\end{prop}

\begin{exam}\label{00068524} Let $X$ be a topological space and $x \in X$. We define the
 \index{evaluation!functional}%
 \index{evaluation@$\sbsb EXx$ or $E_x$ (evaluation functional at~$x$)}%
\df{evaluation functional at $x$}, denoted by $\sbsb EXx$, by
 \[ \sbsb EXx\colon \fml C(X) \sto \C\colon f \mapsto f(x)\,. \]
This functional is a character on~$\fml C(X)$ and its kernel is $J_x$. When there is only
one topological space under discussion we simplify the notation from $\sbsb EXx$
to~$E_x$. Thus, in particular, for $f \in \fml C(X)$ we often write $E_x(f)$ for the more
cumbersome~$\sbsb EXx(f)$.
\end{exam}

\begin{defn} In proposition~\ref{00068522} we discovered that every character on a unital
Banach algebra $A$ lives on the unit sphere of the dual~$A^*$. Thus we may give the set
$\Delta A$ of characters on $A$ the relative $w^*$-topology it inherits from~$A^*$.
This is
 \index{Gelfand!topology}%
 \index{topology!Gelfand}%
the \df{Gelfand topology} on $\Delta A$ and the resulting topological space we call the
 \index{character!space}%
 \index{space!character}%
 \index{structure!space}%
 \index{space!structure}%
\df{character space} (or \emph{carrier space} or \emph{structure space} or \emph{maximal ideal space} or \emph{spectrum}) of~$A$.
\end{defn}

\begin{prop} The character space of a unital Banach algebra is a compact Hausdorff space.
\end{prop}

\begin{exam}\label{000633} Let $l_1(\Z)$ be the family of all bilateral sequences
   \[ (\dots,a_{-2},a_{-1},a_0,a_1,a_2,\dots) \]
which are
 \index{absolutely!summable!bilateral sequence}%
\df{absolutely summable}; that is, such that $\sum_{k=-\infty}^{\infty} \abs{a_k} <
\infty$. This is a Banach space under pointwise operations of addition and scalar
multiplication and norm given by
   \[ \norm a = \sum_{k=-\infty}^{\infty} \abs{a_k}\,. \]
For $a$, $b \in l_1(\Z)$ define $a \ast b$ to be the sequence whose $n^{\text{th}}$ entry
is given by
  \[ (a \ast b)_n = \sum_{k=-\infty}^{\infty} a_{n-k}\,b_k\,. \]
The operation $\ast$ is called
 \index{convolution!in $l_1(\Z)$}%
\df{convolution}.  (To see where the definition comes from try multiplying the power
series $\sum_{-\infty}^{\infty}a_kz^k$ and $\sum_{-\infty}^{\infty}b_kz^k$.)  With this
additional operation $l_1(\Z)$ becomes a unital commutative Banach algebra.

The maximal ideal space of this Banach algebra
 \index{maximal ideal!space!of $l_1(\Z)$}%
 \index{character!space!of $l_1(\Z)$}%
is (homeomorphic to) the unit circle~$\T$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] For each $z \in \T$ define
   \[ \psi_z\colon l_1(\Z) \sto \C\colon a \mapsto \sum_{k=-\infty}^\infty a_kz^k\,. \]
Show that $\psi_z \in \Delta l_1(\Z)$.  Then show that the map
   \[ \psi\colon \T \sto \Delta l_1(\Z)\colon z \mapsto \psi_z \]
is a homeomorphism. \ns
\end{proof}

\begin{prop} If $\phi \in \Delta A$ where $A$ is a unital algebra, then $\ker\phi$ is a
maximal ideal in~$A$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] To show maximality, suppose $I$ is an ideal in $A$
which properly contains $\ker\phi$.  Choose $z \in I \setminus \ker\phi$. Consider the
element $x - \bigl(\phi(x)/\phi(z)\bigr)\,z$ where $x$ is an arbitrary element of~$A$.
\ns
\end{proof}

\begin{prop} A character on a unital algebra is completely determined by its kernel.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $a$ be an element of the algebra and $\phi$ be
a character. For how many complex numbers $\lambda$ can $a^2 - \lambda a$ belong to the
kernel of~$\phi$? \ns
\end{proof}

\begin{cor} If $A$ is a unital algebra, then the map $\phi \mapsto \ker\phi$ from
$\Delta A$ to $\mx A$ is injective.
\end{cor}

\begin{prop}\label{0007207} Let $I$ be a maximal ideal in a unital commutative Banach
algebra~$A$. Then there exists a character on $A$ whose kernel is~$I$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use corollary~\ref{C073147} Why can we think of the
quotient map as a character?  \ns
\end{proof}

\begin{cor}\label{0007207a} If $A$ is a unital commutative Banach algebra, then the map
$\phi \mapsto \ker\phi$ is a bijection from $\Delta A$ onto~$\mx A$.
\end{cor}

\begin{defn} Let $A$ be a unital commutative Banach algebra.  In light of the preceding
corollary we can give $\mx A$ a topology under which it is homeomorphic to the character
space~$\Delta A$. This is the
 \index{maximal ideal!space}%
 \index{space!maximal ideal}%
 \index{maximal@$\mx A$!homeomorphic to $\Delta(A)$}%
\df{maximal ideal space} of~$A$.  Since $\Delta A$ and $\mx A$ are homeomorphic it is
common practice to identify them and so $\Delta A$ is often called the \emph{maximal
ideal space} of~$A$.
\end{defn}

\begin{defn}\label{00073} Let $X$ be a compact Hausdorff space and $x \in X$.
Recall  that in example~\ref{00068524} we defined $\sbsb EXx$, the \emph{evaluation
functional} at~$x$ by
   \[ \sbsb EXx(f) := f(x) \]
for every $f \in \fml C(X)$.  The map
   \[ \sbsb EX \colon X \sto \Delta\fml C(X)\colon x \mapsto \sbsb EXx \]
is the
 \index{evaluation map}%
 \index{evaluation@$\sbsb EX$!evaluation map}%
\df{evaluation map} on~$X$.  As was mentioned earlier when only one topological space is
being considered we usually shorten $\sbsb EX$ to $E$ and $\sbsb EXx$ to~$E_x$.
\end{defn}

\begin{notn} To indicate that two topological spaces $X$ and $Y$ are homeomorphic we
 \index{<binrel@$X \approx Y$ ($X$ and $Y$ are homeomorphic)}%
write $X \approx Y$.
\end{notn}

\begin{prop}\label{000731} Let $X$ be a compact Hausdorff space.  Then the evaluation
 \index{maximal ideal!space!of $\fml C(X)$}%
 \index{character!space!of $\fml C(X)$}%
map on~$X$
   \[ \sbsb EX \colon X \sto \Delta\fml C(X) \colon x \mapsto \sbsb EXx \]
is a homeomorphism. Thus we have
   \[ X \approx \Delta\fml C(X) \approx \mx\fml C(X)\,. \]
More is true: not only is each $\sbsb EX$ a homeomorphism between compact Hausdorff
spaces, but $E$ itself is a \emph{natural equivalence} between functors---the identity
functor and the $\Delta\fml C$ functor.
\end{prop}

The identification between a compact Hausdorff space $X$ and its character space and its maximal ideal space is so strong
that many people speak of them as if they were actually equal.  It is very common to hear, for example, that ``the
maximal ideals in $\fml C(X)$ are just the points of~$X$''. Although not literally true, it does sound a bit less intimidating
than ``the maximal ideals of $\fml C(X)$ are the kernels of the evaluation functionals at points of~$X$''.

\begin{prop}\label{000735} Let $X$ and $Y$ be compact Hausdorff spaces and
 \index{functor!$\fml C$ as a}%
 \index{C@$\fml C$ (the functor)}%
$F \colon X \sto Y$ be continuous.  Recall that in example~\ref{C029717} we defined $\fml C(F)$ on $\fml C(Y)$ by
   \[ \fml C(F)(g) = g \circ F\]
for all $g \in \fml C(Y)$.  Then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $\fml C(F)$ maps $\fml C(Y)$ into $\fml C(X)$.
  \item The map $\fml C(F)$ is a contractive unital Banach algebra homomorphism.
  \item $\fml C(F)$ is injective if and only if $F$ is surjective.
  \item $\fml C(F)$ is surjective if and only if $F$ is injective.
  \item If $X$ is homeomorphic to $Y$, then $\fml C(X)$ is isometrically isomorphic
to~$\fml C(Y)$.
 \end{enumerate}
\end{prop}

\begin{prop}\label{000736} Let $A$ and $B$ be unital commutative Banach algebras and
 \index{functor!$\Delta$ as a}%
 \index{delta@$\Delta$ (the functor)}%
$T\colon A \sto B$ be a unital algebra homomorphism. Define $\Delta T $ on $\Delta B$ by
   \[ \Delta T(\psi) = \psi \circ T \]
for all $\psi \in \Delta B$.  Then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $\Delta T$ maps $\Delta B$ into $\Delta A$.
  \item The map $\Delta T\colon \Delta B \sto \Delta A$ is continuous.
  \item If $T$ is surjective, then $\Delta T$ is injective.
  \item If $T$ is an (algebra) isomorphism, then $\Delta T$ is a homeomorphism.
  \item If $A$ and $B$ are (algebraically) isomorphic, then $\Delta A$ and $\Delta B$
are homeomorphic.
 \end{enumerate}
\end{prop}

\begin{cor} Let $X$ and $Y$ be compact Hausdorff spaces.  If $\fml C(X)$ and $\fml C(Y)$ are
algebraically isomorphic, then $X$ and $Y$ are homeomorphic.
\end{cor}

\begin{cor}\label{nasc_homeomor_CHSps} Two compact Hausdorff spaces are homeomorphic if and only if their algebras of
continuous complex valued functions are (algebraically) isomorphic.
\end{cor}

Corollary~\ref{nasc_homeomor_CHSps} gives rise to quite an array of intriguing problems.  The corollary suggests that
for each topological property of a compact Hausdorff space $X$ there ought to be a corresponding algebraic property of
the algebra $\fml C(X)$, and \emph{vice versa}.  But it does not provide a usable recipe for finding this correspondence.
One very simple example of this type of result is given below as proposition~\ref{prop_connected_idempotent}.  For a
beautiful discussion of this question see Semadeni's monograph~\cite{Semadeni:1971}, in particular, see the table on
pages~132--133.

\begin{cor} Let $X$ and $Y$ be compact Hausdorff spaces.  If $\fml C(X)$ and $\fml C(Y)$ are
algebraically isomorphic, then they are isometrically isomorphic.
\end{cor}

\begin{prop}\label{prop_connected_idempotent} A topological space $X$ is connected if and only if the algebra $\fml C(X)$
contains no nontrivial idempotents.
\end{prop}

(The \emph{trivial idempotents} of an algebra are $\vc 0$ and~$\vc 1$.)




















\section{The Gelfand Transform}
\begin{defn} Let $A$ be a commutative Banach algebra and $a\in A$. Define
   \[\sbsb{\Gamma}A a\colon \Delta A \sto \C \colon \phi \mapsto \phi(a) \]
for every $\phi \in \Delta(A)$. (Alternative notations: when no confusion seems likely we
frequently write $\Gamma a$ or $\wh a$ for $\sbsb{\Gamma}A a$.) The map $\sbsb{\Gamma}A$
is the
 \index{Gelfand!transform!on a Banach algebra}%
 \index{Gamma@$\sbsb\Gamma A$, $\Gamma$ (Gelfand transform)}%
 \index{<unoptop@$\wh a$ (Gelfand transform of~$a$)}%
\df{Gelfand transform on~$A$}.
\end{defn}

Since $\Delta A \subseteq A^*$ it is clear that $\sbsb{\Gamma}A a$ is just the
restriction of $a^{**}$ to the character space of~$A$. Furthermore the Gelfand topology
on $\Delta A$ is the relative $w^*$-topology, the weakest topology under which $a^{**}$
is continuous on $\Delta A$ for each $a \in A$, so  $\sbsb{\Gamma}A a$ is a continuous
function on $\Delta A$.  Thus $\sbsb{\Gamma}A \colon A \sto \fml C(\Delta A)$.

As a matter of brevity and convenience the element $\sbsb{\Gamma}A a = \Gamma a = \wh a$
is usually called just the
 \index{Gelfand!transform!of an element}%
\emph{Gelfand transform of~$a$}---because the phrase \emph{the Gelfand transform on $A$
evaluated at~$a$} is awkward.

\begin{defn} We say that a family $\fml F$ of functions on a set $S$
 \index{separation!of points}%
\df{separates points} of $S$ (or is a
 \index{separating!family of functions}%
\df{separating family} of functions on $S$) if for every pair of distinct elements $x$
and $y$ of $S$ there exists $f \in \fml F$ such that $f(x) \ne f(y)$.
\end{defn}

\begin{prop} Let $X$ be a compact topological space.  Then $\fml C(X)$ is separating if and
only if $X$ is Hausdorff.
\end{prop}

\begin{prop}\label{0007402} If $A$ is a commutative Banach algebra, then $\sbsb{\Gamma}A
\colon A \sto \fml C(\Delta A)$ is a contractive algebra homomorphism and the
range of $\sbsb{\Gamma}A$ is a separating subalgebra of $\fml C(\Delta A)$.  If, in addition,
$A$ is unital, then $\Gamma$ has norm one, its range is a unital subalgebra of $\fml C(\Delta A)$,
and it is a unital homomorphism.
\end{prop}

\begin{prop} Let $a$ be an element of a unital commutative Banach algebra~$A$. Then
$a$ is invertible in $A$ if and only if $\hat a$ is invertible in~$\fml C(\Delta A)$.
\end{prop}

\begin{prop}\label{00074043} Let $A$ be a unital commutative Banach algebra and $a$ be an
element of~$A$.  Then $\ran\hat a = \sigma(a)$ and $\norm{\hat a}_u~=~\rho(a)$.
\end{prop}

\begin{defn} An element $a$ of a Banach algebra is
 \index{quasinilpotent}%
\df{quasinilpotent} if $\lim\limits_{n \sto \infty}\norm{a^n}^{1/n} = 0$.
\end{defn}

\begin{prop} Let $a$ be an element of a unital commutative Banach algebra~$A$. Then the
following are equivalent:
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $a$ is quasinilpotent;
  \item $\rho(a) = 0$;
  \item $\sigma(a) = \{0\}$;
  \item $\Gamma a = 0$;
  \item $\phi(a) = 0$ for every $\phi \in \Delta A$;
  \item $a \in \bigcap\mx A$.
 \end{enumerate}
\end{prop}

\begin{defn} A Banach algebra is
 \index{semisimple}%
\df{semisimple} if it has no nonzero quasinilpotent elements.
\end{defn}

\begin{prop} Let $A$ be a unital commutative Banach algebra. Then the following are
equivalent:
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $A$ is semisimple;
  \item if $\rho(a) = 0$, then $a = 0$;
  \item if $\sigma(a) = \{0\}$, then $a = 0$;
 \index{monomorphism}%
  \item the Gelfand transform $\sbsb{\Gamma}A$ is a monomorphism (that is, an
injective homomorphism);
  \item if $\phi(a) = 0$ for every $\phi \in \Delta A$, then $a = 0$;
  \item $\bigcap\mx A = \{0\}$.
 \end{enumerate}
\end{prop}

\begin{prop}\label{00074083} Let $A$ be a unital commutative Banach algebra.  Then the
following are equivalent:
 \begin{enumerate}[label=\rm{(\alph*)}]
   \item $\|a^2\| = \|a\|^2$ for all $a \in A$;
   \item $\rho(a) = \|a\|$ for all $a \in A$; and
   \item the Gelfand transform is an isometry; that is, $\norm{\wh a}_u = \|a\|$ for
all $a \in A$.
 \end{enumerate}
\end{prop}

\begin{exam} The Gelfand transform on $l_1(\Z)$ is not an isometry.
\end{exam}

Recall from proposition~\ref{000731} that when $X$ is a compact Hausdorff space the
evaluation mapping $E_X$ identifies the space $X$ with the maximal ideal space of the
Banach algebra~$\fml C(X)$.  Thus according to proposition~\ref{000735} the mapping $\fml
CE_X$ identifies the algebra $\fml C(\Delta(\fml C(X)))$ of continuous functions on this
maximal ideal space with the algebra $\fml C(X)$ itself.  It turns out that the Gelfand
transform on the algebra $\fml C(X)$ is just the inverse of this identification map.

\begin{exam} Let $X$ be a compact Hausdorff space. Then the Gelfand transform on the Banach
algebra $\fml C(X)$ is an isometric isomorphism.  In fact, on $\fml C(X)$ the Gelfand
transform \smash[b]{$\sbsb\Gamma X$} is $(\fml CE_X)^{-1}$.
\end{exam}

\begin{exam}\label{0007472} The maximal ideal space of the Banach algebra $L_1(\R)$
 \index{maximal ideal!space!for $L_1(\R)$}%
is $\R$ itself and the Gelfand transform on $L_1(\R)$ is the Fourier transform.
\end{exam}

\begin{proof} See~\cite{BaggettF:1979}, pages 169--171.  \ns
\end{proof}

The function $t \mapsto e^{it}$ is a bijection from the interval $[-\pi,\pi)$ to the
 \index{T@$\T$ (unit circle in the complex plane)}%
unit circle $\T$ in the complex plane. One consequence of this is that we need not
distinguish between
 \begin{enumerate}[label=\rm{(\alph*)}]
    \item $2\pi$-periodic functions on $\R$,
    \item all functions on $[-\pi,\pi)$,
    \item functions $f$ on $[-\pi,\pi]$ such that $f(-\pi) = f(\pi)$, and
    \item functions on $\T$.
 \end{enumerate}
In the sequel we will frequently without further explanation identify these classes of
functions.

Another convenient identification is the one between the unit circle $\T$ in $\C$ and the
maximal ideal space of the algebra $l_1(\Z)$.  The homeomorphism $\psi$ between these two
compact Hausdorff space was defined in example~\ref{000633}. It is often technically
more convenient in working with the Gelfand transform $\Gamma_a$ of an element $a \in
l_1(\Z)$ to treat it as a function, let's call it $G_a$, whose domain is~$\T$ as the
following diagram suggests.
 \[
   \xy
     \btriangle[\T`\Delta l_1(\Z)`\C;\psi`G_a`\Gamma_a]
   \endxy
 \]
Thus for $a \in l_1(\Z)$ and $z \in \T$ we have
  \[ G_a(z) = \Gamma_a(\psi_z) = \psi_z(a) = \sum_{k=-\infty}^\infty a_nz^n\,. \]

\begin{defn} If $f \in \fml L_1([-\pi,\pi))$, the
 \index{Fourier!series}%
\df{Fourier series} for $f$ is the series
   \[ \sum_{n=-\infty}^\infty\wt f(n)\exp(int) \qquad -\pi \le t \le \pi \]
where the sequence $\wt f$ is defined by
   \[ \wt f(n) = {\textstyle\frac1{2\pi}}\int_{-\pi}^\pi f(t)\exp(-int)\,dt \]
for all $n \in \Z$. The doubly infinite sequence $\wt f$ is the
 \index{Fourier!transform!on $L_1([-\pi,\pi])$}%
\df{Fourier transform} of $f$, and the number $\wt f(n)$ is the $n^{\text{th}}$
 \index{Fourier!coefficient}%
\df{Fourier coefficient} of $f$. If $\wt f \in l_1(\Z)$ we say that $f$ has an
 \index{absolutely!convergent!Fourier series}%
 \index{Fourier!series!absolutely convergent}%
\df{absolutely convergent Fourier series}. The set of all continuous functions on $\T$
with absolutely convergent Fourier series is denoted by
 \index{absolutely@$\fml{A\,C}(\T)$ (absolutely convergent Fourier series}%
$\fml{A\,C}(\T)$.
\end{defn}

\begin{prop} If $f$ is a continuous $2\pi$-periodic function on $\R$ whose Fourier
transform is zero, then $f = 0$.
\end{prop}

\begin{cor} The Fourier transform on $\fml C(\T)$ is injective.
\end{cor}

\begin{prop} The Fourier transform on $\fml C(\T)$ is a left inverse of the Gelfand transform
on~$l_1(\Z)$.
\end{prop}

\begin{prop} The range of the Gelfand transform on $l_1(\Z)$ is the unital commutative Banach
algebra $\fml{A\,C}(\T)$.
\end{prop}

\begin{prop} There are continuous functions whose Fourier series diverge at~$0$.
\end{prop}

\begin{proof} See, for example, \cite{HewittS:1965}, exercise 18.45.)  \ns
\end{proof}

What does the preceding result say about the Gelfand transform $\Gamma\colon l_1(\Z) \sto
\fml C(\T)$?

\vskip 3 pt

Suppose a function $f$ belongs to $\fml{AC}(\T)$ and is never zero.  Then $1/f$ is
certainly continuous on $\T$, but does it have an absolutely convergent Fourier series?
One of the first triumphs of the abstract study of Banach algebras was a very simple
proof of the answer to this question given originally by Norbert Wiener.  Wiener's
original proof by comparison was quite difficult.

\begin{thm}[Wiener's theorem] Let $f$ be a continuous function on $\T$ which is never
 \index{Wiener's theorem}%
zero. If $f$ has an absolutely convergent Fourier series, then so does its
reciprocal~$1/f$.
\end{thm}

\begin{exam} The Laplace transform can also be viewed as a special case of the Gelfand
transform.  For details see~\cite{BaggettF:1979}, pages 173--175.
\end{exam}














\section{Unital $C^*$-algebras}

\begin{prop} If $a$ is an element of a $*\,$-algebra, then $\sigma(a^*) = \conj{\sigma(a)}$.
\end{prop}


\begin{prop}\label{001311} In every $C^*$-algebra involution is an isometry.  That
 \index{involution!is an isometry}%
is, $\norm{a^*} = \norm{a}$ for every element $a$ in the algebra.
\end{prop}

In definition~\ref{def_norm_alg} of \emph{normed algebra} we made the special requirement that
the identity element of a unital normed algebra have norm one.  In $C^*$-algebras this
requirement is redundant.

\begin{cor} In a unital $C^*$-algebra $\norm{\vc 1} = 1$.
\end{cor}

\begin{cor}\label{001311005fa} Every unitary element in a unital $C^*$-algebra has norm one.
\end{cor}

\begin{cor}\label{001311008} If $a$ is an element of a $C^*$-algebra $A$ such that
$ab = \vc 0$ for every $b \in A$, then $a = \vc 0$.
\end{cor}

\begin{prop}\label{00131101} Let $a$ be a normal element of a $C^*$-algebra.
Then $\norm{a^2} = {\norm a}^2$. And if the algebra is unital, then $\rho(a)~=~\norm a$.
\end{prop}

\begin{cor} If $p$ is a nonzero projection in a $C^*$-algebra, then $\norm p = 1$.
\end{cor}

\begin{cor}\label{00131102} Let $a \in A$ where $A$ is a commutative $C^*$-algebra. Then
$\norm{a^2} = {\norm a}^2$ and if additionally $A$ is unital, then $\rho(a)~=~\norm a$.
\end{cor}

\begin{cor}\label{00131103} On a unital commutative $C^*$-algebra $A$ the Gelfand transform
$\Gamma$ is an isometry; that is, $\norm{\Gamma_a}_u = \norm{\wh a}_u = \norm a$ for
every $a \in A$.
\end{cor}

\begin{cor}\label{00131104} The norm of a unital $C^*$-algebra is unique in the sense that
given a unital algebra $A$ with involution there is at most one norm which makes $A$ into a $C^*$-algebra.
\end{cor}

\begin{prop}\label{001312} If $a$ is a self-adjoint element of a unital $C^*$-algebra,
 \index{spectrum!of a self-adjoint element}%
 \index{self-adjoint!element!spectrum of}%
then $\sigma(a) \subseteq \R$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Write $\lambda \in \sigma(a)$ in the form $\alpha + i\beta$ where $\alpha$, $\beta \in \R$.
For every $n \in \N$ let $b_n := a - \alpha \vc 1 + i n\beta\vc 1$.  Show that for every $n$ the scalar $i(n+1)\beta$ belongs to
the spectrum of $b_n$ and that therefore
   \[ {\abs{i(n+1)\beta}}^2 \le {\norm{a - \alpha\vc 1}}^2 + n^2\beta^2\,. \]   \ns
\end{proof}

\begin{cor}\label{001406} Let $A$ be a unital commutative $C^*$-algebra.  If $a \in A$ is
self-adjoint, then its Gelfand transform $\wh a$ is real valued.
\end{cor}

\begin{cor}\label{0014062} Every character on a unital $C^*$-algebra is a $*\,$-homomorphism.
\end{cor}

\begin{cor}\label{0014062fa} The Gelfand transform on a unital commutative $C^*$-algebra $A$ is a unital $*\,$-homomorphism
into~$\fml C(\Delta A)$.
\end{cor}

\begin{cor} The range of the Gelfand transform on a unital commutative $C^*$-algebra $A$ is a unital self-adjoint separating
subalgebra of~$\fml C(\Delta A)$.
\end{cor}

\begin{prop}\label{001313} If $u$ is a unitary element of a unital $C^*$-algebra,
 \index{spectrum!of a unitary element}%
 \index{unitary!element!spectrum of}%
then $\sigma(u) \subseteq \T$.
\end{prop}

\begin{prop} The Gelfand transform $\Gamma$ is a natural equivalence between the identity
functor and the $\fml C\Delta$ functor on the category of commutative unital
$C^*$-algebras and unital $*\,$-homomorphisms.
\end{prop}



















\section{The Gelfand-Naimark Theorem}

We recall a standard theorem from real analysis.

\begin{thm}[Stone-Weierstrass Theorem]\label{00141} Let $X$ be a compact Hausdorff space. Every
 \index{Stone-Weierstrass theorem}%
unital separating $*\,$-subalgebra of $\fml C(X)$ is dense.
\end{thm}

\begin{proof} See \cite{Douglas:1972}, theorem 2.40. \ns \end{proof}

The first version of the \emph{Gelfand-Naimark theorem} says that any unital commutative
$C^*$-algebra is the algebra of all continuous functions on some compact Hausdorff space.
Of course the word \emph{is} in the preceding sentence means \emph{is isometrically
$*\,$-isomorphic to}. The compact Hausdorff space referred to is the character
space of the algebra.

\begin{thm}[Gelfand-Naimark theorem I]\label{00142} Let $A$ be a unital commutative
 \index{Gelfand-Naimark!theorem I}%
$C^*$-algebra.  Then the Gelfand transform $\Gamma_A\colon a \mapsto \wh a$ is an
isometric $*\,$-isomorphism of $A$ onto $\fml C(\Delta A)$.
\end{thm}





\begin{defn} Let $S$ be a nonempty subset of a $C^*$-algebra~$A$.  The intersection of the
family of all $C^*$-subalgebras of $A$ which contain $S$ is the
 \index{C*@$C^*$-algebra!generated by a set}%
\df{$C^*$-subalgebra generated by~$S$}. We denote it by
 \index{C*@$C^*(S)$ ($C^*$-subalgebra generated by~$S$)}%
$C^*(S)$. (It is easy to see that the intersection of a family of $C^*$-subalgebras
really is a $C^*$-algebra.) In some cases we shorten the notation slightly: for example,
if $a \in A$ we write $C^*(a)$ for~$C^*(\{a\})$ and $C^*(\vc 1,a)$ for $C^*(\{\vc 1,a\})$.
\end{defn}

\begin{prop} Let $S$ be a nonempty subset of a $C^*$-algebra~$A$.  For each natural number
$n$ define the set $W_n$ to be the set of all elements $a$ of $A$ for which there exist
$x_1$, $x_2$, \dots, $x_n$ in $S \cup S^*$ such that $a = x_1x_2\cdots x_n$. Let $W =
\bigcup_{n=1}^\infty W_n$.  Then
   \[ C^*(S) = \clo{\spn W}\,. \]
\end{prop}











\begin{thm}[Abstract Spectral Theorem]\label{00144} If $a$ is a normal element of a
 \index{abstract!spectral theorem}%
 \index{spectral!theorem!abstract}%
unital $C^*$-algebra $A$, then the $C^*$-algebra $\fml C(\sigma(a))$ is isometrically
$*\,$-isomorphic to $C^*(\vc 1,a)$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Use the Gelfand transform of $a$ to identify the maximal
ideal space of $C^*(\vc 1,a)$ with the spectrum of~$a$.  Apply the functor~$\fml C$.
Compose the resulting map with $\Gamma^{-1}$ where $\Gamma$ is the Gelfand transform on
the $C^*$-algebra~$C^*(\vc 1,a)$.   \ns
\end{proof}

If $\psi\colon \fml C(\sigma(a)) \sto C^*(\vc 1,a)$ is the $*\,$-isomorphism in the preceding theorem and $f$
is a continuous function on the spectrum of $a$, then the element $\psi(f)$ in the algebra $A$ is usually
denoted by~$f(a)$.  This operation of associating with the continuous function $f$ an element $f(a)$ in $A$ is
referred to as the
 \index{functional!calculus}%
 \index{calculus!functional}%
\emph{functional calculus} associated with~$a$.

\begin{exam} Suppose that in the preceding theorem $\psi\colon \fml C(\sigma(a)) \sto
C^*(\vc 1,a)$ implements the isometric $*\,$-isomorphism.  Then the image under $\psi$ of
the constant function~$\vc 1$ on the spectrum of~$a$ is $\vc 1_A$ and the image under
$\psi$ of the identity function $\lambda \mapsto \lambda$ on the spectrum of~$a$ is~$a$.
\end{exam}

\begin{exam}\label{X_sqroot_op} Let $T$ be a normal operator on a Hilbert space $H$ whose spectrum is contained
in~$[0,\infty)$. Suppose that $\psi\colon \fml C(\sigma(T)) \sto C^*(I,T)$ implements the
isometric ${}^*$-isomorphism between these two $C^*$-algebras.  Then there is at least
one operator $\sqrt T$ whose square is~$T$.  Indeed, whenever $f$ is a continuous
function on the spectrum of a normal operator~$T$, we may meaningfully speak of the
operator~$f(T)$.
\end{exam}

\begin{exam} If $N$ is a normal Hilbert space operator whose spectrum is $\{0,1\}$, then
$N$ is an orthogonal projection.
\end{exam}

\begin{exam} If $N$ is a normal Hilbert space operator whose spectrum is is contained in the unit circle $\T$, then
$N$ is a unitary operator.
\end{exam}

\begin{prop}[Spectral mapping theorem]\label{001446} Let $a$ be a self-adjoint element
 \index{spectral!mapping theorem}%
in a unital $C^*$-algebra~$A$ and $f$ be a continuous complex valued function on the
spectrum of~$a$.  Then
  \[ \sigma(f(a)) = f^{\sto}(\sigma(a))\,. \]
\end{prop}

The functional calculus sends continuous functions to continuous functions in  the following sense.

\begin{prop}\label{001449} Suppose $A$ is a unital $C^*$-algebra, $K$ is a nonempty compact subset of $\R$, and $f\colon K \sto \C$ is a
continuous function. Let $\ofml H_K$ be the set of all self-adjoint elements of $A$ whose spectra are subsets of~$K$.  Then
the function $f\colon \ofml H_K \sto A\colon h \mapsto f(h)$ is continuous.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Approximate $f$ uniformly by a polynomial and use an ``$\epsilon$ over $3$'' argument and
the \emph{spectral mapping theorem}~\ref{001446}.   \ns
\end{proof}

















\endinput
\chapter{THE GELFAND-NAIMARK-SEGAL CONSTRUCTION}\label{gelfand_naimark_segal}

\section{Positive Linear Functionals}
\begin{defn} Let $A$ be an algebra with involution. For each linear functional $\tau$ on
$A$ and each $a \in A$ define
 \index{<unopurstar@$\tau^\star$ (conjugate of $\tau$)}%
$\tau^\star(a) = \conj{\tau(a^*)}$.  We say that $\tau$ is
 \index{Hermitian!linear functional}%
 \index{linear!functional!Hermitian}%
\df{Hermitian} if $\tau^\star = \tau$.  Notice that a linear functional $\tau\colon A
\sto \C$ is Hermitian if and only if it preserves involution; that is, $\tau^\star =
\tau$ if and only if $\tau(a^*) = \conj{\tau(a)}$ for all $a \in A$.
\end{defn}

\begin{cau} The $\tau^\star$ defined above should not be confused with the usual adjoint
mapping $\tau^*\colon \C^* \sto A^*$. Context (or use of a magnifying glass) should make
it clear which is intended.
\end{cau}

\begin{prop} A linear functional $\tau$ on a $C^*$-algebra $A$ is Hermitian if and only if
$\tau(a)~\in~\R$ whenever $a$ is self-adjoint.
\end{prop}

As is always the case with maps between ordered vector spaces, \emph{positive} maps are
the ones that take positive elements to positive elements.

\begin{defn} A linear functional $\tau$ on a $C^*$-algebra $A$ is
 \index{positive!linear functional}%
 \index{linear!functional!positive}%
\df{positive} if $\tau(a) \ge 0$ whenever $a \ge \vc 0$ in~$A$ for all $a \in A$.
\end{defn}


\begin{prop} Every positive linear functional on a $C^*$-algebra is Hermitian.
\end{prop}

\begin{prop} The family of positive linear functionals is a proper convex cone in the real
vector space of all Hermitian linear functionals on a $C^*$-algebra.  The cone induces a
partial ordering on the vector space: $\tau_1 \le \tau_2$ whenever $\tau_2 - \tau_1$ is
positive.
\end{prop}

\begin{defn} A
 \index{state}%
\df{state} of a $C^*$-algebra $A$ is a positive linear functional $\tau$ on $A$ such that
$\tau(\vc 1) = 1$.
\end{defn}

\begin{exam} Let $x$ be a vector in a Hilbert space $H$. Define
   \[ \omega_x:\ofml B(H) \sto \C\colon T \mapsto \langle Tx,x \rangle\,. \]
Then $\omega_x$ is a positive linear functional on $\ofml B(H)$.  If $x$ is a unit
vector, then $\omega_x$ is a state of~$\ofml B(H)$.  A state $\tau$ is a
 \index{vector!state}%
 \index{state!vector}%
\df{vector state} if $\tau = \omega_x$ for some unit vector~$x$.
\end{exam}

\begin{prop}[Schwarz inequality]\label{0025105} If $\tau$ is a positive linear functional on
a $C^*$-algebra $A$, then
   \[ \abs{\tau(b^*a)}^2 \le \tau(a^*a)\tau(b^*b) \]
for all $a$, $b \in A$.
 \index{Schwarz inequality}%
 \index{inequality!Schwarz}%
\end{prop}

\begin{prop} A linear functional $\tau$ on a $C^*$-algebra $A$ is positive if and only if it
is bounded and $\norm\tau = \tau(\vc 1_A)$.
\end{prop}

\begin{proof} See \cite{KadisonR:1983}, pages 256--257. \ns
\end{proof}



















\section{Representations}
\begin{defn}\label{0028} Let $A$ be a $C^*$-algebra. A
 \index{representation}%
\df{representation} of $A$ is a pair $(\pi,H)$ where $H$ is a Hilbert space and $\pi
\colon A \sto \ofml B(H)$ is a $*\,$-homomorphism.  Usually one says simply that $\pi$ is
a representation of~$A$.  When we wish to emphasize the role of the particular Hilbert
space we say that $\pi$ \emph{is a representation of $A$ on~$H$.} Depending on context we
may write either $\pi_a$ or $\pi(a)$ for the Hilbert space operator which is the image of
the algebra element $a$ under~$\pi$.  A representation $\pi$ of $A$ on $H$ is
 \index{representation!nondegenerate}%
 \index{nondegenerate!representation}%
\df{nondegenerate} if $\pi(A)H$ is dense in~$H$.
\end{defn}

 \index{conventions!representations of unital $C*$-algebras are unital}%
\begin{conv} We add to the preceding definition the following requirement: if the $C^*$-algebra
$A$ is unital, then a representation of $A$ must be a \emph{unital} $*\,$-homomorphism.
\end{conv}

\begin{defn} A representation $\pi$ of a $C^*$-algebra $A$ on a Hilbert space $H$ is
 \index{representation!faithful}%
 \index{faithful representation}%
\df{faithful} if it is injective. If there exists a vector $x \in H$ such that
$\pi^{\sto}(A)x = \{\pi_a(x)\colon a \in A\}$ is dense in $H$, then we say that the
representation $\pi$ is
 \index{representation!cyclic}%
 \index{cyclic!representation}%
\df{cyclic} and that $x$ is a
 \index{representation!cyclic vector for a}%
 \index{cyclic!vector}%
\df{cyclic vector} for~$\pi$.
\end{defn}

\begin{exam}\label{002802} Let $(S, \sfml A, \mu)$ be a $\sigma$-finite measure space and
$L_\infty = L_\infty(S, \sfml A, \mu)$ be the $C^*$-algebra of essentially bounded
$\mu$-measurable functions on~$S$. As we saw in example~\ref{C063527} for each $\phi \in
L_\infty$ the corresponding multiplication operator $M_\phi$ is an operator on the
Hilbert space $L_2 = L_2(S, \sfml A, \mu)$.  The mapping $M\colon L_\infty \sto \ofml
B(L_2)\colon \phi \mapsto M_\phi$ is a faithful representation of the $C^*$-algebra
$L_\infty$ on the Hilbert space~$L_2$.
\end{exam}

\begin{exam} Let $\fml C([0,1])$ be the $C^*$-algebra of continuous
functions on the interval~$[0,1]$. For each $\phi \in \fml C([0,1])$ the corresponding
multiplication operator $M_\phi$ is an operator on the Hilbert space $L_2 = L_2([0,1])$
of functions on~$[0,1]$ which are square-integrable with respect to Lebesgue measure.
The mapping $M\colon \fml C([0,1]) \sto \ofml B(L_2)\colon \phi \mapsto M_\phi$ is a
faithful representation of the $C^*$-algebra $\fml C([0,1])$ on the Hilbert space~$L_2$.
\end{exam}

\begin{exam} Suppose that $\pi$ is a representation of a unital $C^*$-algebra $A$ on a
Hilbert space~$H$ and $x$ is a unit vector in~$H$. If $\omega _x$ is the corresponding
vector state of $\ofml B(H)$, then $\omega_x \circ \pi$ is a state of~$A$.
\end{exam}

\begin{exer} Let $X$ be a locally compact Hausdorff space.  Find an isometric
(therefore faithful) representation $(\pi,H)$ of the $C^*$-algebra $C_0(X)$ on some
Hilbert space~$H$..
\end{exer}

\begin{defn} Let $\rho$ be a state of a $C^*$-algebra $A$. Then
   \[ L_\rho := \{a \in A\colon \rho(a^*a) = 0\} \]
is called the
 \index{left!kernel}%
 \index{kernel!left}%
\df{left kernel} of~$\rho$.
\end{defn}

Recall that as part of the proof of \emph{Schwarz inequality}~\ref{0025105} for positive
linear functionals we verified the following result.

\begin{prop} If $\rho$ is a state of a $C^*$-algebra $A$, then $\langle a,b \rangle_0 :=
\rho(b^*a)$ defines a semi-inner product on~$A$.
\end{prop}

\begin{cor} If $\rho$ is a state of a $C^*$-algebra $A$, then its left kernel $L_\rho$ is a
vector subspace of $A$ and $\langle\,[a],[b]\,\rangle := \langle a,b \rangle_0$ defines
an inner product on the quotient vector space~$A/L_\rho$.
\end{cor}

\begin{prop} Let $\rho$ be a state of a $C^*$-algebra $A$ and $a \in L_\rho$. Then $\rho(b^*a)
= 0$ for every $b \in A$.
\end{prop}

\begin{prop} If $\rho$ is a state of a $C^*$-algebra $A$, then its left kernel $L_\rho$ is a
closed left ideal in~$A$.
\end{prop}


























\section{The GNS-Construction and the Third Gelfand-Naimark Theorem}
The following theorem is known as the
 \index{Gelfand-Naimark-Segal construction}%
 \index{construction!Gelfand-Naimark-Segal}%
\emph{Gelfand-Naimark-Segal construction} (the \emph{GNS-construction}).

\begin{thm}[GNS-construction]\label{0029} Let $\rho$ be a state of a $C^*$-algebra $A$.  Then
there exists a cyclic representation $\pi_\rho$ of $A$ on a Hilbert space $H_\rho$ and a
unit cyclic vector $x_\rho$ for $\pi_\rho$ such that $\rho = \omega_{x_\rho} \circ
\pi_\rho$.
\end{thm}

\begin{notn} In the following material $\pi_\rho$, $H_\rho$, and $x_\rho$ are the cyclic
representation, the Hilbert space, and the unit cyclic vector guaranteed by the
GNS-construction starting with a given state $\rho$ of a $C^*$-algebra
\end{notn}

\begin{prop} Let $\rho$ be a state of a $C^*$-algebra $A$ and $\pi$ be a cyclic representation
of $A$ on a Hilbert space $H$ such that $\rho = \omega_x \circ \pi$ for some unit cyclic
vector $x$ for~$\pi$.  Then there exists a unitary map $U$ from $H_\rho$ to~$H$ such that
$x = Ux_\rho$ and $\pi(a) = U \pi_\rho(a) U^*$ for all $a \in A$.
\end{prop}

\begin{defn} Let $\{H_\lambda\colon \lambda \in \Lambda\}$ be a family of Hilbert spaces.
Denote by $\bigoplus\limits_{\lambda \in \Lambda} H_\lambda$ the set of all functions
$x\colon \Lambda \sto \bigcup\limits_{\lambda \in \Lambda} H_\lambda\colon \lambda
\mapsto x_\lambda$ such that $x_\lambda \in H_\lambda$ for each $\lambda \in \Lambda$ and
$\D\sum\limits_{\lambda \in \Lambda} \norm{x_\lambda}^2 < \infty$.  On
$\bigoplus\limits_{\lambda \in \Lambda} H_\lambda$ define addition and scalar
multiplication pointwise; that is. $(x + y)_\lambda = x_\lambda + y_\lambda$ and $(\alpha
x)_\lambda = \alpha x_\lambda$ for all $\lambda \in \Lambda$, and define an inner product
by $\langle x,y \rangle = \sum_{\lambda \in \Lambda} \langle x_\lambda,y_\lambda\rangle$.
These operations (are well defined and) make $\bigoplus\limits_{\lambda \in \Lambda}
H_\lambda$ into a Hilbert space.  It is the
 \index{direct!sum!of Hilbert spaces}%
\df{direct sum} of the Hilbert spaces~$H_\lambda$. Various notations for elements of this
direct sum occur in the literature: $x$, $(x_\lambda)$, $(x_\lambda)_{\lambda \in
\Lambda}$, and $\oplus_\lambda x_\lambda$ are common.

Now suppose that $\{T_\lambda\colon \lambda \in \Lambda\}$ is a family of Hilbert space
operators where $T_\lambda \in \ofml B(H_\lambda)$ for each $\lambda \in \Lambda$.
Suppose further that $\sup\{\norm{T_\lambda}\colon \lambda \in \Lambda\} < \infty$.  Then
$T(x_\lambda)_{\lambda \in \Lambda} = (T_\lambda x_\lambda)_{\lambda \in \Lambda}$
defines an operator on the Hilbert space $\bigoplus_\lambda H_\lambda$. The operator $T$
is usually denoted by $\bigoplus_\lambda T_\lambda$ and is called the
 \index{direct!sum!of Hilbert space operators}%
\df{direct sum} of the operators~$T_\lambda$.
\end{defn}

\begin{prop} The claims made in the preceding definition that
\smash[b]{$\bigoplus\limits_{\lambda \in \Lambda} H_\lambda$} is a Hilbert space and
$\bigoplus_\lambda T_\lambda$ is an operator on $\bigoplus\limits_{\lambda \in \Lambda}
H_\lambda$ are correct.
\end{prop}

\begin{exam} Let $A$ be a $C^*$-algebra and $\{\pi_\lambda\colon \lambda \in \Lambda\}$ be a
family of representations of $A$ on Hilbert spaces $H_\lambda$ so that $\pi_\lambda(a)
\in \ofml B(H_\lambda)$ for each $\lambda \in \Lambda$ and each $a \in A$.  Then
  \[ \pi = \textstyle{ \bigoplus_\lambda \pi_\lambda\colon A \sto
       \ofml B\bigl(\bigoplus_\lambda H_\lambda\bigr)
       \colon a \mapsto \bigoplus_\lambda \pi_\lambda(a)} \]
is a representation of $A$ on the Hilbert space $\bigoplus_\lambda H_\lambda$. It is the
 \index{direct!sum!of representations}%
\df{direct sum} of the representations~$\pi_\lambda$.
\end{exam}

\begin{thm}\label{thm_exist_faith_rep} Every $C^*$-algebra has a faithful representation.
\end{thm}

\begin{proof} See \cite{Blackadar:2006}, Corollary II.6.4.10; \cite{Conway:1990}, page 253;
\cite{DoranB:1986}, Theorem 19.1; \cite{Fillmore:1996}, Theorem 5.4.1;
\cite{KadisonR:1983}, page 281; or \cite{Murphy:1990}, Theorem 3.4.1. \ns
\end{proof}

An obvious restatement of the preceding theorem is a third version of the
\emph{Gelfand-Naimark theorem}, which says that every $C^*$-algebra is (essentially) an
algebra of Hilbert space operators.

\begin{cor}[Gelfand-Naimark Theorem III]\label{002973} Every $C^*$-algebra is
 \index{Gelfand-Naimark!Theorem III}
isometrically $*\,$-isomorphic to a $C^*$-subalgebra of $\ofml B(H)$ for some Hilbert
space~$H$.
\end{cor}


\endinput
\chapter{HILBERT SPACE OPERATORS}

\section{Invertible Linear Maps and Isometries}

\begin{defn} A linear map $T\colon H \sto K$ between two inner product spaces is
 \index{inner product!preserving}%
\df{inner product preserving} if $\langle Tx, Ty \rangle = \langle x, y \rangle$ for all $x$, $y \in H$.
\end{defn}

\begin{prop}\label{prop_isometry_equiv} Let $T\colon H \sto K$ be a linear map between inner product spaces.  Then
the following are equivalent:
 \begin{enumerate}[label=\rm{(\alph*)}]
   \item $T$ is inner product preserving;
   \item $T$ is norm preserving; and
   \item $T$ is an isometry.
 \end{enumerate}
\end{prop}

Just as was the case with normed linear spaces (see the very end of section~\ref{sec_bdd_lin_maps})
there are (at least) two important categories in which the objects are Hilbert spaces:
 \begin{enumerate}
  \item[(a)] $\cat{HSp} =$ Hilbert spaces and bounded linear maps, and
   \index{HSp@$\cat{HSp}$!the category}%
   \index{category!$\cat{HSp}$ as a}%
  \item[(b)] $\cat{HSp_1} =$ Hilbert spaces and linear contractions.
   \index{HSp1@$\cat{HSp_1}$!the category}%
   \index{category!$\cat{HSp_1}$ as a}%
 \end{enumerate}
In any category it is usual to define \emph{isomorphism} to be an \emph{invertible morphism}. Since it is the
 \index{category!topological}%
(topological) category of Hilbert spaces and bounded linear maps that we encounter
nearly exclusively in these notes it might seem reasonable to apply the word ``isomorphism'' to
the isomorphisms in this category---in other words, to invertible maps; while an isomorphism
in the more restrictive
 \index{category!geometric}%
(geometric) category of Hilbert spaces and linear contractions might reasonably be
called an \emph{isometric isomorphism}.  But this is \emph{not} the convention.  When most mathematicians think of
a ``Hilbert space isomorphism'' they think of a map which preserves all the Hilbert space structure---including
the inner product.  Thus (invoking~\ref{prop_isometry_equiv}) the word
 \index{isomorphism!between Hilbert spaces}%
 \index{isomorphism!in the category $\cat{HSp}$}%
 \index{HSp@$\cat{HSp}$!isomorophisms in}%
 \index{isomorphism!in the category $\cat{HSp_1}$}%
 \index{HSp1@$\cat{HSp_1}$!isomorophisms in}%
``isomorphism'' when applied to maps between Hilbert spaces has come, in most places, to mean \emph{isometric isomorphism}.
And consequently, the isomorphisms in the more common category $\cat{HSp}$ are called \emph{invertible (bounded linear) maps}.
Recall also: We reserve the word
 \index{operator}%
 \index{conventions!all operators are bounded and linear}%
``operator'' for bounded linear maps from a space \emph{into itself}.

\begin{exam} Define an operator $T$ on the Hilbert space $H = L_2([0,\infty))$ by
   \[ Tf(t) = \begin{cases}   f(t-1),  &\text{ if $t \ge 1$} \\
                                   0,  &\text{ if $0 \le t < 1$.}
           \end{cases} \]
Then $T$ is an isometry but not an isometric isomorphism. If, on the other hand,
$H = L_2(\R)$ and $T$ maps each $f \in H$ to the function $t \mapsto f(t-1)$, then $T$ is an isometric isomorphism.
\end{exam}

\begin{exam} Let $H$ be the Hilbert space defined in example~\ref{exam_Hsp_abs_cont}.  Then the differentiation mapping
$D\colon f \mapsto f\,'$ from $H$ into $L_2([0,1])$ is an isometric isomorphism.  What is its inverse?
\end{exam}

\begin{exam} Let $\{(X_i,\mu_i) \colon i \in I\}$ be a family of $\sigma$-finite measure spaces. Make the disjoint union
$\D X = \biguplus_{i \in I}X_i$ into a measure space $(X,\mu)$ in the usual way.  Then $L_2(X,\mu)$ is isometrically
isomorphic to the direct sum of the Hilbert spaces $L_2(X_i,\mu_i)$.
\end{exam}

\begin{exam} Let $\mu$ be Lebesgue measure on $[-\frac\pi2, \frac\pi2]$ and $\nu$ be Lebesgue measure on $\R$.
Let $(Uf)(x) = \dfrac{f(\arctan x)}{\sqrt{1+x^2}}$ for all $f \in L_2(\mu)$ and all $x \in \R$.  Then $U$ is an
isometric isomorphism between $L_2([-\frac\pi2, \frac\pi2], \mu)$ and $L_2(\R, \nu)$.
\end{exam}

\begin{prop} Every inner product preserving surjection from one Hilbert space to another is automatically linear.
\end{prop}

\begin{prop}\label{prop_norm_lin_map} If $T\colon H \sto K$ is a bounded linear map between Hilbert spaces, then
   \[ \norm T = \sup\{\abs{\langle Tx,y \rangle}\colon \norm x = \norm y = 1\}\,. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Show first that $\norm x = \sup\{\abs{\langle x,y \rangle}\colon \norm y = 1\}$.  \ns
\end{proof}

\begin{defn} A
 \index{curve}%
\df{curve} in a Hilbert space $H$ is a continuous map from $[0,1]$ into~$H$. A curve is
 \index{simple!curve}%
\df{simple} if it is injective.  If $c$ is a curve and $0 \le a < b \le 1$, then the
 \index{chord}%
\df{chord} of $c$ corresponding to the interval $[a,b]$ is the vector $c(b) - c(a)$.  Two chords are
\df{non-overlapping} if their associated parameter intervals have at most an endpoint in common.
\end{defn}

Halmos, in \cite{Halmos:1982}, illustrates the roominess of infinite dimensional Hilbert spaces by means of the
following elegant example.

\begin{exam} In every infinite dimensional Hilbert space there exists a simple curve which makes a right-angle
turn at each point, in the sense that every pair of non-overlapping chords are perpendicular.
\end{exam}

\begin{proof}[\emph{Hint for proof}]  Consider characteristic functions in $L_2([0,1])$.  \ns
\end{proof}
























\section{Operators and their Adjoints}

\begin{prop}\label{C0635103} Let $S$, $T \in \ofml B(H,K)$ where $H$ and $K$ are Hilbert
spaces. If $\langle Sx,y \rangle = \langle Tx,y \rangle$ for every $x \in H$ and $y \in
K$, then $S = T$.
\end{prop}

\begin{defn} Let $T$ be an operator on a Hilbert space~$H$.  Then $Q_T$, the
 \index{quadratic form}%
 \index{form!quadratic}%
 \index{Q@$Q_T$ (quadratic form associated with~$T$)}%
\df{quadratic form associated with~$T$}, is the scalar valued function defined by
   \[ Q_T(x) := \langle Tx,x \rangle \]
for all $x \in H$.
\end{defn}

An operator on complex Hilbert spaces is zero if and only if its associated quadratic forms is.

\begin{prop}\label{C0635105} If $H$ is a \emph{complex} Hilbert space and $T \in \ofml B(H)$, then
$T = \vc 0$ if and only if $Q_T = 0$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] In the hypothesis $Q_T(z) = 0$ for all $z \in H$ replace $z$ first by $y + x$
and then by $y + ix$.  \ns
\end{proof}

The preceding proposition is one of the few we will encounter that does not hold for both real and complex Hilbert spaces.

\begin{exam} Proposition~\ref{C0635105} is not true if in the hypothesis the word ``real'' is substituted for ``complex''.
\end{exam}

\begin{defn} A function $T\colon V \sto W$ between complex vector spaces is
 \index{conjugate!linear}%
 \index{linear!conjugate}%
\df{conjugate linear} if it is additive ($T(x+y) = Tx + Ty$) and satisfies $T(\alpha x) =
\conj\alpha\, Tx$ for all $x \in V$ and $\alpha \in \C$. A complex valued function of two
variables $\phi\colon V \times W \sto \C$ is a
 \index{sesquilinear functional}%
 \index{functional!sesquilinear}%
\df{sesquilinear functional} if it is linear in its first variable and conjugate linear
in its second variable.

If $H$ and $K$ are normed linear spaces, a sesquilinear functional $\phi$ on $H \times K$
is
 \index{bounded!sesquilinear functional}%
 \index{sesquilinear functional!bounded}%
 \index{functional!bounded sesquilinear}%
\df{bounded} if there exists a constant $M > 0$ such that
    \[ \abs{\phi(x,y)} \le M\norm x\norm y \]
for all $x \in H$ and $y \in K$.
\end{defn}

\begin{prop} If $\phi \colon H \oplus K \sto \C$ is a bounded sesquilinear functional on
the direct sum of two Hilbert spaces, then the following numbers (exist and) are equal:
 \begin{enumerate}[label=\rm{(\alph*)}]
    \item $\sup\{\abs{\phi(x,y)}\colon \norm x \le 1, \norm y \le 1\}$
 \vskip 3 pt
    \item $\sup\{\abs{\phi(x,y)}\colon \norm x = \norm y = 1\}$
 \vskip 3 pt
    \item $\sup\left\{\dfrac{\abs{\phi(x,y)}}{\norm x\,\norm y}\colon x,y \ne 0\right\}$
 \vskip 3 pt
    \item $\inf\{M > 0\colon \abs{\phi(x,y)} \le M\norm x\,\norm y
                    \text{ for all $x$, $y \in H$}\}$.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] The proof is virtually identical to the corresponding
result for linear maps (see~\ref{C023221}). \ns
\end{proof}

\begin{defn} Let $\phi \colon H \oplus K \sto \C$ be a bounded sesquilinear functional on
the direct sum of two Hilbert spaces. We define $\norm \phi$, the
 \index{norm!of a bounded sesquilinear functional}%
\emph{norm} of $\phi$, to be any of the (equal) expressions in the preceding result.
\end{defn}

\begin{prop} Let $T\colon H \sto K$ be a bounded linear map between Hilbert spaces.  Then
   \[ \phi \colon H \oplus K \sto \C\colon (x,y) \mapsto \langle Tx,y \rangle \]
is a bounded sesquilinear functional on $H \oplus K$ and $\norm\phi = \norm T$.
\end{prop}

\begin{prop} Let $\phi \colon H \times K \sto \C$ be a bounded sesquilinear functional on the
product of two Hilbert spaces. Then there exist unique bounded linear maps $T \in \ofml B(H,K)$ and
$S \in \ofml B(K,H)$ such that
    \[ \phi(x,y) = \langle Tx,y \rangle = \langle x,Sy \rangle \]
for all $x \in H$ and $y \in K$. Also, $\norm T = \norm S = \norm\phi$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Show that for every $x \in H$ the map
$y \mapsto \conj{\phi(x,y)}$ is a bounded linear functional on~$K$. \ns
\end{proof}

\begin{defn}\label{def000926} Let $T\colon H \sto K$ be a bounded linear map between Hilbert spaces.  The
mapping $(x,y) \mapsto \langle Tx,y \rangle$ from $H \oplus K$ into $\C$ is a bounded
sesquilinear functional. By the preceding proposition there exists an unique bounded
linear
 \index{adjoint!of a linear map!between Hilbert spaces}%
 \index{linear!map!adjoint of a}%
 \index{<unopurstar@$T^*$ (Hilbert space adjoint)}%
map $T^*\colon K \sto H$ called the \df{adjoint} of $T$ such that
   \[ \langle Tx,y \rangle = \langle x,T^*y \rangle \]
for all $x \in H$ and $y \in K$.
\end{defn}

\begin{exam}\label{C063526} Recall from example~\ref{exam150013} that the family $l_2$ of all
square summable sequences of complex numbers is a Hilbert space. Let
    \[ S\colon l_2 \sto l_2\colon (x_1,x_2,x_3,\dots) \mapsto (0,x_1,x_2,\dots)\,. \]
Then $S$ is an operator on $l_2$,called the
 \index{unilateral shift}%
 \index{S@$S$ (unilateral shift operator)}%
 \index{operator!unilateral shift}%
\df{unilateral shift operator}, and $\norm S = 1$.  The adjoint $S^*$ of the unilateral
shift
 \index{unilateral shift!adjoint of}%
 \index{adjoint!of the unilateral shift}%
 \index{operator!unilateral shift!adjoint of}%
is given by
  \[ S^*\colon l_2 \sto l_2\colon (x_1,x_2,x_3,\dots) \mapsto (x_2,x_3,x_4,\dots). \]
\end{exam}

\begin{exam} Let $\bigl(e^n\bigr)_{n=1}^\infty$ be an orthonormal basis for a separable Hilbert space~$H$ and
$\bigl(\alpha_n\bigr)_{n=1}^\infty$ be a bounded sequence of complex numbers. Let $Te^n = \alpha_n e^n$ for every~$n$.
Then $T$ can be extended uniquely to a operator on~$H$.  What is the norm of this operator?  What is its adjoint?
\end{exam}

\begin{defn} The operator discussed in the preceding example is known as a
 \index{operator!diagonal}%
 \index{diagonal!operator}%
\df{diagonal operator}.   A common notation for this operator is
 \index{diag@$\diag(\alpha_1,\alpha_2,\dots)$ (diagonal operator)}%
$\diag(\alpha_1,\alpha_2,\dots)$.
\end{defn}

In the category of inner product spaces and bounded linear maps both the kernel and range of a morphism are objects in the category.
However, as the next example shows, this need not be true of Hilbert spaces.  While a Hilbert space operator always has a kernel
which is itself a Hilbert space, its range may fail to be closed and consequently fail to be complete.

\begin{exam}\label{exam_ran_nonclosed} The range of the diagonal operator $\diag\bigl(1,\frac12,\frac13, \dots\bigr)$ on the
 \index{closed!range!operator without}%
 \index{operator!without closed range}%
Hilbert space $l_2$ is not closed.
\end{exam}

\begin{exam}\label{C063527} Let $(S,\fml A, \mu)$ be a sigma-finite positive measure space and $L_2(S,\mu)$
be the Hilbert space of all (equivalence classes of) complex valued functions on $S$
which are square integrable with respect to~$\mu$.  Let $\phi$ be an essentially bounded
complex valued $\mu$-measurable function on~$S$. Define
 \index{M@$M_\phi$ (multiplication operator)}%
$M_\phi$ on $L_2(S,\mu)$ by $M_\phi(f) := \phi f$.  Then $M_\phi$ is an operator on $L_2(S,\mu)$; it is called a
 \index{multiplication operator!on $L_2(S,\mu)$}%
 \index{operator!multiplication!on $L_2(S,\mu)$}%
\df{multiplication operator}. Its norm is given by $\norm{M_\phi} = \norm\phi_\infty$ and its
 \index{multiplication operator!adjoint of a}%
 \index{operator!multiplication!adjoint of a}%
 \index{adjoint!of a multiplication operator}%
adjoint by ${M_\phi}^*~=~M_{\conj\phi}$.
\end{exam}

It will be convenient to expand definition~\ref{0001612} slightly.

\begin{defn} Operators $S$ and $T$ on Hilbert spaces $H$ and $K$, respectively, are said to be
 \index{unitary!equivalence}%
 \index{equivalent!unitarily}%
\df{unitarily equivalent} if there is an isometric isomorphism $U\colon H \sto K$ such that $T = USU^{-1}$.

\end{defn}

\begin{exam} Let $\mu$ be Lebesgue measure on $[-\frac\pi2, \frac\pi2]$ and $\nu$ be Lebesgue measure on $\R$.
Let $\phi(x) = x$ for $x \in [-\frac\pi2, \frac\pi2]$ and $\psi(x) = \arctan x$ for $x \in \R$.  Then the
multiplication operators $M_\phi$ and $M_\psi$ (on $L_2([-\frac\pi2, \frac\pi2], \mu)$ and $L_2(\R, \nu)$,
respectively) are unitarily equivalent.
\end{exam}

\begin{exam} Let Let $M_\phi$ be a multiplication operator on $L_2(S,\mu)$ where $(S,\fml A,\mu)$ is a
$\sigma$-finite measure space as in example~\ref{C063527}.  Then $M_\phi$ is idempotent if and only if
$\phi$ is the characteristic function of some set in~$\fml A$.
\end{exam}

One may regard the
 \index{spectral!theorem}%
\df{spectral theorem} in beginning linear algebra as saying: \textbf{Every normal operator on a finite
dimensional inner product space is unitarily equivalent to a multiplication operator.}  What is truly
remarkable is the way in which this generalizes to infinite dimensional Hilbert spaces---a subject
for much later discussion.

\begin{exer} Let $\N_3 = \{1,2,3\}$ and $\mu$ be counting measure on $\N_3$.
 \begin{enumerate}
  \item[(a)] Identify the Hilbert space~$L_2(\N_3,\mu)$.
  \item[(b)] Identify the multiplication operators on~$L_2(\N_3,\mu)$.
  \item[(c)] Let $T$ be the linear operator on $\C^3$ whose matrix
representation is
   \[ \begin{bmatrix}
          2  &  0  &  1 \\
          0  &  2  & -1 \\
          1  & -1  &  1
      \end{bmatrix}.\]
Use the operator $T$ to illustrate the preceding formulation of the spectral theorem.
 \end{enumerate}
\end{exer}

\begin{exam}\label{exam_int_op} Let $L_2 = L_2([0,1],\lambda)$ be the Hilbert space of all (equivalence classes of)
complex valued functions on $[0,1]$ which are square integrable with respect to Lebesgue measure~$\lambda$.
If $k\colon [0,1] \times [0,1] \sto \C$ is a bounded Borel measurable function, define  $K$ on $L_2$ by
   \[ (Kf)(x) = \int_0^1 k(x,y)f(y)\,dy\,. \]
Then $K$ is an
 \index{integral!operator}%
 \index{operator!integral}%
\df{integral operator} and its
 \index{kernel!of an integral operator}%
 \index{integral!operator!kernel of a}%
\df{kernel} is the function~$k$.  (This is another use of the word ``kernel''; it has nothing whatever to do
with the more common use of the word: $\ker K = K^{\gets}(\{\vc 0\})$---see definition~\ref{defn_op_vsp}\,).
The adjoint $K^*$ of $K$ is also an integral operator on $L_2$ and its
 \index{adjoint!of an integral operator}%
 \index{integral!operator!adjoint of an}%
 \index{operator!integral!adjoint of an}%
kernel is the function $k^*$ defined on $[0,1] \times [0,1]$ by $k^*(x,y) = \conj{k(y,x)}$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Verify that for $f \in L_2$
   \[ \abs{(Kf)(x)} \le \norm{k}_\infty^{1/2}\left[ \int_0^1 \abs{k(x,y)}\abs{f(y)}^2\,dy\right]^{1/2}.\]
\ns
\end{proof}

\begin{exam}\label{000319}  Let $H = L_2([0,1])$ be the real Hilbert space of all (equivalence classes of) real
valued functions on $[0,1]$ which are square integrable with respect to Lebesgue measure. Define $V$ on $H$ by
   \begin{equation}\label{000319i}
       Vf(x) = \int_0^x f(t)\,dt\,.
   \end{equation}
Then $V$ is an operator on~$H$. This is a
 \index{Volterra operator}%
 \index{operator!Volterra}%
\df{Volterra operator} and is an example of an integral operator. (What is its kernel~$k$\,?)
\end{exam}

\begin{exer} Let $V$ be the \emph{Volterra operator} defined above.
 \begin{enumerate}
  \item[(a)] Show that $V$ is injective.
  \item[(b)] Compute $V^*$.
  \item[(c)] What is $V + V^*$?  What is its range?
 \end{enumerate}
\end{exer}

\begin{prop}\label{prop_adj_invol} If $S$ and $T$ are operators on a Hilbert space~$H$ and $\alpha \in \C$, then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $(S + T)^* = S^* + T^*$;
  \item $(\alpha T)^* = \conj\alpha T^*$;
  \item $T^{**} = T$; and
  \item $(TS)^* = S^*T^*$.
 \end{enumerate}
\end{prop}

\begin{prop}\label{C063544} Let $T$ be an operator on a Hilbert space~$H$. Then
   \begin{enumerate}[label=\rm{(\alph*)}]
     \item $\norm{T^*} = \norm T$ and
     \item $\norm{T^*T} = {\norm T}^2$.
   \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] Notice that since $T = T^{**}$ to prove (a) it is sufficient to show that $\norm T \le \norm{T^*}$.
To this end observe that ${\norm{Tx}}^2 = \langle T^*Tx,x \rangle \le \norm{T^*}\norm T$ whenever $x \in H$ and $\norm x \le 1$.  From
this conclude that ${\norm T}^2 \le \norm{T^*T} \le \norm{T^*}\norm T$.  \ns
\end{proof}

The rather innocent looking condition (b) in the preceding proposition will turn out to be \emph{the} property of fundamental interest
when $C^*$-algebras are first introduced in chapter~\ref{chap_cpt_ops}.

\begin{prop}\label{prop_ker_adj_perp_ran_op} If $T$ is an operator on a Hilbert space, then
 \begin{enumerate}[label=\rm{(\alph*)}]
   \item $\ker T^* = (\ran T)^\perp$,
   \item $\clo{\ran T^*} = (\ker T)^\perp$,
   \item $\ker T = (\ran T^*)^\perp$, and
   \item $\clo{\ran T} = (\ker T^*)^\perp$.
 \end{enumerate}
\end{prop}

Compare the preceding result with theorem~\ref{00016025}.

\begin{defn}\label{def_bdd_away_zero} A bounded linear map $T \colon V \sto W$ between normed linear spaces is
 \index{bounded!away from zero}%
 \index{zero!bounded away from}%
\df{bounded away from zero} (or
 \index{bounded!below}%
\df{bounded below}) if there exists a number $\delta > 0$ such that $\norm{Tx} \ge \delta\norm{x}$ for all $x \in V$.
\end{defn}

\begin{exam} Clearly if a bounded linear map is bounded away from zero, then it is injective. The operator
$T\colon x \mapsto (x_1, \frac12 x_2, \frac13 x_3, \dots)$ defined on the Hilbert space $l_2$ shows that being bounded
away from zero is a strictly stronger condition than being injective.
\end{exam}

\begin{prop}\label{prop_bafz_closed_rng} Every bounded linear map between Hilbert spaces which is bounded away from
zero has closed range.
\end{prop}

\begin{prop}\label{prop_nasc_op_invert} An operator on a Hilbert space is invertible if and only if
it is bounded away from zero and has dense range.
\end{prop}

\begin{prop} The pair of maps $H \mapsto H$ and $T \mapsto T^*$ taking every Hilbert space to
 \index{adjoint!as a functor on $\cat{HSp}$}%
 \index{functor!adjoint}%
itself and every bounded linear map between Hilbert spaces to its adjoint is a contravariant functor
from the category $\cat{HSp}$ of Hilbert spaces and bounded linear maps to itself.
 \index{HSp@$\cat{HSp}$!adjoint functor on}%
\end{prop}

\begin{prop} Let $H$ be a Hilbert space, $T \in \ofml B(H)$, $M = H \oplus \{0\}$, and $N$ be the graph of~$T$.
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item Then $N$ is a closed linear subspace of $H \oplus H$.
  \item $T$ is injective if and only if $M \cap N = \{(0,0)\}$.
  \item $\ran T$ is dense in $H$ if and only if $M + N$ is dense in $H \oplus H$.
  \item $T$ is surjective if and only if $M + N = H \oplus H$.
 \end{enumerate}
\end{prop}

\begin{exam} There exists a Hilbert space $H$ with subspaces $M$ and $N$ such that $M + N$ is not closed.
\end{exam}

\begin{prop} Let $H$ and $K$ be Hilbert spaces and $V$ be a vector subspace of~$H$.  Every bounded linear map
$T\colon V \sto K$ can be extended to a bounded linear map from $\clo V$, the closure of $V$, without increasing its norm.
\end{prop}

\begin{exer} Let $\fml E$ be an orthonormal basis for a Hilbert space~$H$. Discuss linear transformations $T$ on $H$
such that $T(e) = 0$ for every $e \in \fml E$.  Give a nontrivial example.
\end{exer}






















\section{Algebras with Involution}
\begin{defn}\label{00109}  An
 \index{involution}%
\df{involution} on an algebra $A$ is a map $x \mapsto x^\ast$ from $A$ into $A$ which satisfies
  \begin{enumerate}
    \item[(a)] $(x + y)^\ast = x^\ast + y^\ast$,
    \item[(b)] $(\alpha x)^* = \clo\alpha\, x^*$,
    \item[(c)] $x^{\ast\ast} = x$, and
    \item[(d)] $(xy)^\ast = y^\ast x^\ast$
  \end{enumerate}
for all $x$, $y \in A$ and $\alpha \in \C$.  An algebra on which an involution has been defined is a
 \index{star@$*\,$-algebra}%
 \index{<star@$*\,$-algebra}%
 \index{algebra!with involution}%
 \index{algebra!$*\,$-}%
\df{$*\,$-algebra } (pronounced ``star algebra''). If $a$ is an element of a $*\,$-algebra, then $a^*$ is called the
 \index{adjoint}%
\df{adjoint} of~$a$.
\end{defn}

\begin{exam} In the algebra $\C$ of complex numbers the map $z \mapsto \conj z$ of a
number to its complex conjugate is an involution.
\end{exam}

\begin{exam}\label{0010902} The map of an $n \times n$ matrix to its conjugate transpose
is an involution on the unital algebra~$M_n$.
\end{exam}

\begin{exam} Let $X$ be a compact Hausdorff space.  The map $f \mapsto \conj f$ of a
function to its complex conjugate is an involution in the algebra~$\fml C(X)$.
\end{exam}

\begin{exam} The map $T \mapsto T^*$ of a Hilbert space operator to its adjoint is an
involution in the algebra $\ofml B(H)$ (see proposition~\ref{prop_adj_invol}).
\end{exam}

\begin{prop} Let $a$ and $b$ be elements of a $*\,$-algebra.  Then $a$ commutes with $b$
if and only if $a^*$ commutes with $b^*$.
\end{prop}

\begin{prop} In a unital $*\,$-algebra $\vc 1^* = \vc 1$.
\end{prop}

\begin{prop}\label{00109125} If a $*\,$-algebra $A$ has a left multiplicative identity $e$,
then $A$ is unital and $e = \vc 1_A$.
\end{prop}

\begin{prop}\label{00109128fa} Let $a$ be an element of a unital $*\,$-algebra.  Then $a^*$ is invertible if and only if $a$ is.
 \index{inverse!of an adjoint}%
 \index{inverse!adjoint of an}%
 \index{adjoint!inverse of an}%
 \index{adjoint!of an inverse}%
And when $a$ is invertible we have
   \[ (a^*)^{-1} = \bigl(a^{-1}\bigr)^*\,. \]
\end{prop}

\begin{defn} An algebra homomorphism $\phi$ between $*\,$-algebras which preserves
involution (that is, such that $\phi(a^*) = (\phi(a))^*$) is a
 \index{<star@$*\,$-homomorphism}%
 \index{star@$*\,$-homomorphism}%
 \index{homomorphism!$*\,$-}%
\df{$*\,$-homomorphism} (pronounced ``star homomorphism''.  A $*\,$-homomorphism $\phi\colon A \sto B$ between unital algebras is said to be
 \index{unital!$*\,$-homomorphism}%
 \index{bijective morphisms!are invertible!in $*\,$-algebras}%
 \index{<star@$*\,$-homomorphism!unital}%
\df{unital} if $\phi(\vc 1_A) = \vc 1_B$.  In the category of $*\,$-algebras and $*\,$-homomorphisms, the isomorphisms (called for emphasis
 \index{<star@$*\,$-isomorphism}%
 \index{star@$*\,$-isomorphism}%
 \index{isomorphism!$*\,$-}
\df{$*\,$-isomorphisms}) are the bijective $*\,$-homomorphisms.
\end{defn}

\begin{prop}\label{0010952} Let $\phi\colon A \sto B$ be a $*\,$-homomorphism between $*\,$-algebras. Then
the kernel of $\phi$ is a $*\,$-ideal in $A$ and the range of $\phi$ is a
$*\,$-subalgebra of~$B$.
\end{prop}
















\section{Self-Adjoint Operators}
\begin{defn} An element $a$ of a $*\,$-algebra $A$ is
 \index{self-adjoint!element}%
\df{self-adjoint} (or
 \index{Hermitian!element}%
\df{Hermitian}) if $a^* = a$.  It is
 \index{normal!element}%
\df{normal} if $a^*a = aa^*$. And it is
 \index{unitary!element}%
\df{unitary} if $a^*a = aa^* = \vc 1$. The set of all self-adjoint elements of $A$ is
denoted by
 \index{H@$\fml H(A)$ (self-adjoint elements of a $*\,$-algebra)}%
$\fml H(A)$, the set of all normal elements
 \index{N@$\fml N(A)$ (normal elements of a $*\,$-algebra)}%
by~$\fml N(A)$, and the set of all unitary elements
 \index{U@$\fml U(A)$ (unitary elements of a $*\,$-algebra)}%
by~$\fml U(A)$.
\end{defn}

\begin{prop}\label{001093} Let $a$ be an element of a complex $*\,$-algebra.  Then there exist
unique self-adjoint elements $u$ and $v$ such that $a = u + iv$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Think of the special case of writing a complex
number in terms of its real and imaginary parts.  \ns
\end{proof}

\begin{defn} Let $S$ be a subset of a $*\,$-algebra~$A$.  Then
$S^*~=~\{s^*\colon~s~\in~S\}$. The subset $S$ is
 \index{self-adjoint!subset of a $*\,$-algebra}%
 \index{<unopurstar@$S^*$ (set of adjoints of elements of~$S$)}%
\df{self-adjoint} if $S^* = S$.

A nonempty self-adjoint subalgebra of $A$ is a
 \index{<star@$*\,$-subalgebra (self-adjoint subalgebra)}%
 \index{star@$*\,$-subalgebra (self-adjoint subalgebra}%
 \index{subalgebra!$*\,$-}%
 \index{self-adjoint!subalgebra}%
\df{$*\,$-subalgebra} (or a \df{sub-$*\,$-algebra}).
\end{defn}

\begin{cau} The preceding definition does \emph{not} say that the elements of a
self-adjoint subset of a $*\,$-algebra are themselves self-adjoint.
\end{cau}

 \begin{prop} An operator $T$ on a complex Hilbert space $H$ is self-adjoint if and only if
 its associated quadratic form $Q_T$ is real valued.
 \end{prop}

\begin{proof}[\emph{Hint for proof}] Use the same trick as in~\ref{C0635105}.  In the hypothesis that
$Q_T(z)$ is always real, replace $z$ first by $y + x$ and then by $y + ix$.   \ns
\end{proof}

\begin{defn} Let $T$ be an operator on a Hilbert space~$H$.  The
 \index{numerical!range}%
 \index{range!numerical}%
 \index{W@$W(T)$ (numerical range)}%
\df{numerical range} of $T$, denoted by $W(T)$, is the range of the restriction to the unit sphere of $H$ of the
quadratic form associated with~$T$.  That is,
   \[ W(T) := \{Q_T(x) \colon x \in H \text{ and } \norm x = 1\}. \]
The
 \index{numerical!radius}%
 \index{radius!numerical}%
 \index{w@$w(T)$ (numerical radius)}%
\df{numerical radius} of $T$, denoted by $w(T)$, is $\sup\{\abs \lambda\colon \lambda \in W(T)\}$.
\end{defn}

\begin{prop} If $T$ is a self-adjoint Hilbert space operator, then $\norm T = w(T)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] The hard part is showing that $\norm T \le w(T)$.  To this end show first that
   \begin{equation}
             Q_T(x + y) - Q_T(x - y) =   4 \Re\langle Tx,y \rangle
   \end{equation}
for all vectors $x$ and $y$ in the Hilbert space, where $Q_T$ is the quadratic form associated with $T$ and $\Re z$ is
the real part of the complex number~$z$.  Use this to verify that if $\norm x = \norm y = 1$, then
   \begin{equation}\label{num_ran_saop_eqn2}
           \Re\langle Tx,y \rangle \le w(T).
   \end{equation}
Write the complex number $\langle Tx,y \rangle$ in polar form $re^{i\theta}$.  Clearly if we replace
$x$ by $e^{-i\theta}x$ in inequality~\eqref{num_ran_saop_eqn2} the resulting inequality still holds whenever $\norm x = \norm y = 1$.

Finally notice that if $\norm x = 1$ and $y$ is any vector in $H$ such that $Tx = \norm{Tx}\,y$, then
   \begin{equation}
             \norm{Tx} = \langle Tx,y \rangle\,.
   \end{equation}
Then the desired result follows immediately.         \ns
\end{proof}

In proposition~\ref{C0635105} we saw that an operator $T$ on a \emph{complex} Hilbert space is zero if and only if its
associated quadratic form is.  It is an easy corollary of  the preceding proposition that this equivalence also holds
for self-adjoint operators on a real Hilbert space.

\begin{cor}\label{real_HSp_zero_Qform} Let $T$ be a self-adjoint operator on a Hilbert space.  Then $T = \vc 0$ if and
only if its associated quadratic form $Q_T$ is zero.
\end{cor}

\begin{defn} An operator $T$ on a Hilbert space $H$ is
 \index{positive!operator}%
 \index{operator!positive}%
\df{positive} if it is self-adjoint and its associated quadratic form is positive ($Q_T(x) \ge 0$ for every $x \in H$).
\end{defn}

\begin{exer} Let $H$ be a complex Hilbert space and $T$ be a positive operator on~$H$.  Define
   \[ \langle x,y \rangle_1 := \langle Tx,y \rangle \qquad \text{ and } \qquad \norm{x}_1 \equiv \sqrt{\langle x,x \rangle_1} \]
for all $x \in H$.
 \begin{enumerate}
  \item[(a)] Prove that $\norm{\hphantom O}_1$ is a norm if and only if $\ker T = \{0\}$.
  \item[(b)] Suppose $\ker T = \{0\}$. Show that the norms $\norm{\hphantom O}$ and
$\norm{\hphantom O}_1$ induce the same topology on~$H$ if and only if $T$ is invertible in
$\ofml B(H)$. \emph{Hint.}  When there are two topologies on~$H$, it is frequently useful to
consider the identity operator on~$H$.
  \item[(c)] Suppose $T$ is invertible in $\ofml B(H)$.  For $S \in \ofml B(H)$ find an expression for the
adjoint of $S$ with respect to the inner product $\langle \hphantom x , \hphantom y \rangle_1$.
 \end{enumerate}
\end{exer}


















\section{Projections}
\begin{conv} At the moment our attention is focused primarily on
 \index{conventions!projections in Hilbert spaces are orthogonal}%
operators on Hilbert spaces.  In this context the term \emph{projection} is always taken
 \index{projection}%
to mean \emph{orthogonal projection} (see definition~\ref{000164}).  Thus a Hilbert space
operator is called a projection if $P^2 = P$ and $P^* = P$.
\end{conv}

We generalize the definition of ``(orthogonal) projection'' from Hilbert spaces to $*\,$-algebras.

\begin{defn} A
 \index{projection!in a $*\,$-algebra}%
\df{projection} in a $*\,$-algebra $A$ is an element $p$ of the algebra which is
idempotent ($p^2 = p$) and self-adjoint ($p^* = p$).  The set of all projections in $A$
is denoted
 \index{P@$\fml P(A)$!projections in an algebra}%
by~$\fml P(A)$.  In the case of $\ofml B(H)$, the bounded operators on a Hilbert space,
we write $\fml P(H)$,
 \index{p@$\fml P(H)$ or $\fml P(\ofml B(H))$!orthogonal projections on Hilbert space}%
or, if $H$ is understood, just $\fml P$, for the more informative $\fml P(\ofml B(H))$.
\end{defn}

\begin{prop} Every operator on a Hilbert space that is an isometry on the orthogonal complement of its kernel has closed range.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Notice first that if $M = (\ker T)^\perp$, where $T$ is the operator in question, then
$\ran T = T^{\sto}(M)$.    \ns
\end{proof}

\begin{prop} Let $P$ be a projection on a Hilbert space $H$. Then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $Px = x$ if and only if $x \in \ran P$\,;
  \item $\ker P = (\ran P)^\perp$; and
  \item $H = \ker P \oplus \ran P$.
 \end{enumerate}
\end{prop}

In part (c) of the preceding proposition the symbol $\oplus$ stands of course for
\emph{orthogonal} direct sum (see the paragraph following~\ref{000164}).

\begin{prop} Let $M$ be a subspace of a Hilbert space $H$.  If $P \in \ofml B(H)$, if $Px = x$
for every $x \in M$, and if $Px = \vc 0$ for every $x \in M^\perp$, then $P$ is the
projection of $H$ onto~$M$.
\end{prop}

In the remainder of this section it is important to keep firmly in mind that the spaces $\ofml B(H)$ of Hilbert
space operators are \emph{examples} of $*\,$-algebras, and that (orthogonal) projections on a Hilbert space are
\emph{examples} of projections in $*\,$-algebras.  Thus on  the one hand, everything that is claimed for
such projections in $*\,$-algebras holds for Hilbert space projections.  For lack of a better name we will refer to
these results as documenting the
 \index{abstract view of projections}%
\df{abstract} structure of projections.  On the other hand, Hilbert space projections have additional structure which
makes no sense in general $*\,$-algebras: they have \emph{domains} and \emph{ranges}, and they \emph{act on vectors}.
When we investigate such matters we will say we are dealing with the
 \index{spatial view of projections}%
\df{spatial} (or \df{concrete}) structure of projections.  Notice how these two views alternate in the remainder of this section.

Our first result gives necessary and sufficient conditions for the sum of projections to be a projection.

\begin{prop}\label{prop_sum_projs} Let $p$ and $q$ be projections in a $*\,$-algebra. Then the following are equivalent:
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $pq = \vc 0$;
  \item $qp = \vc 0$;
  \item $qp = -pq$;
 \index{projections!sum of}%
  \item $p + q$ is a projection.
 \end{enumerate}
\end{prop}

\begin{defn}\label{def_perp_projs} Let $p$ and $q$ be projections in a $*\,$-algebra.  If any of the conditions
in the preceding result holds, then we say that $p$ and $q$ are
 \index{projection!orthogonal}%
 \index{projections!orthogonality of two}%
 \index{orthogonal!projections!in a $*\,$-algebra}%
\df{orthogonal} and write
 \index{<binrel@$p \perp q$ (orthogonality of projections)}%
$p \perp q$.  (Thus for operators on a Hilbert space we could correctly, if unpleasantly, speak of
\emph{orthogonal orthogonal projections}!)
\end{defn}

Next is the corresponding spatial result for the sum of two projections.

\begin{prop} Let $P$ and $Q$ be projections on a Hilbert space~$H$. Then $P \perp Q$
 \index{orthogonal!Hilbert space projections}%
if and only if $\ran P \perp \ran Q$. In this case $P + Q$ is a projection
whose kernel is $\ker P \cap \ker Q$ and whose range is $\ran P + \ran Q$.
\end{prop}

\begin{exam} On a Hilbert space (orthogonal) projections need not commute. For example
let $P$ be the projection of the (real) Hilbert space $R^2$ onto the line $y = x$ and $Q$
be the projection of $R^2$ onto the $x$-axis.  Then $PQ \ne QP$.
\end{exam}

The product of two projections is a projection if and only if they commute.

\begin{prop} Let $p$ and $q$ be projections in a $*\,$-algebra. Then $pq$ is a
 \index{projections!product of}%
projection if and only if $pq = qp$.
\end{prop}

\begin{prop} Let $P$ and $Q$ be projections on a Hilbert space~$H$. If $PQ = QP$, then
$PQ$ is a projection whose kernel is $\ker P + \ker Q$ and whose range is $\ran P \cap
\ran Q$.
\end{prop}

\begin{prop} Let $p$ and $q$ be projections in a $*\,$-algebra. Then the following are equivalent:
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $pq = p$;
  \item $qp = p$;
 \index{projections!difference of}%
  \item $q - p$ is a projection.
 \end{enumerate}
\end{prop}

\begin{defn}\label{0022251} Let $p$ and $q$ be projections in a $*\,$-algebra.  If any
of the conditions in the preceding result holds, then we
 \index{<binrelle@$p \preceq q$ (ordering of projections in a $*\,$-algebra)}%
 \index{projections!ordering of}%
 \index{ordering!partial!of projections}%
 \index{partial!ordering!of projections}%
write $p \preceq q$.
\end{defn}

\begin{prop} Let $P$ and $Q$ be projections on a Hilbert space~$H$. Then the following are equivalent:
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $P \preceq Q$;
  \item $\norm{Px} \le \norm{Qx}$ for all $x \in H$; and
  \item $\ran P \subseteq \ran Q$.
 \end{enumerate}
In this case $Q - P$ is a projection whose kernel is $\ran P + \ker Q$ and whose range is $\ran Q \ominus \ran P$.
\end{prop}

\noindent\emph{Notation:} Let $H$, $M$, and $N$ be subspaces of a Hilbert space. The assertion
 \index{<binopdirectdiff@$\ominus$ (direct difference)}%
$H = M \oplus N$, may be rewritten as $M = H \ominus N$ (or $N = H \ominus M$).

\begin{prop} The operation $\preceq$ defined in~\ref{0022251} for projections on a
$*\,$-algebra $A$ is a partial ordering on $\fml P(A)$.  If $p$ is a projection in $A$,
then $\vc 0 \preceq p \preceq \vc 1$.
\end{prop}

\begin{prop} Suppose $p$ and $q$ are projections on a $*\,$-algebra~$A$. If $pq = qp$, then
the infimum of $p$ and $q$, which we denote
 \index{<binrel@$p \curlywedge q$ (infimum of projections)}%
 \index{infimum!of projections in a $*\,$-algebra}%
 \index{projections!infimum of}%
by $p \curlywedge q$, exists with respect to the partial ordering $\preceq$ and $p
\curlywedge q = pq$.  The infimum $p \curlywedge q$ may exist even when $p$ and $q$ do
not commute.  A necessary and sufficient condition that $p \perp q$ hold is that both $p
\curlywedge q = \vc 0$ and $pq = qp$ hold.
\end{prop}

\begin{prop} Suppose $p$ and $q$ are projections on a $*\,$-algebra~$A$. If $p \perp q$, then
the supremum of $p$ and $q$, which we denote
 \index{<binrel@$p \curlyvee q$ (supremum of projections)}%
 \index{supremum!of projections in a $*\,$-algebra}%
 \index{projections!supremum of}%
by $p \curlyvee q$, exists with respect to the partial ordering $\preceq$ and $p
\curlyvee q = p + q$.  The supremum $p \curlyvee q$ may exist even when $p$ and $q$ are
not orthogonal.
\end{prop}

\begin{prop} Let $A$ be $C^*$-algebra, $a \in A^+$, and $0 < \epsilon \le \frac12$.  If
$\norm{a^2 - a} < \epsilon/2$, then there exists a projection $p$ in $A$ such that
$\norm{p - a} < \epsilon$.
\end{prop}











\section{Normal Operators}

\begin{prop}\label{prop_nasc_normal} An operator $T$ on a Hilbert space $H$ is normal if and only if
$\norm{Tx} = \norm{T^*x}$ for all $x \in H$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use corollary~\ref{real_HSp_zero_Qform}.  \ns
\end{proof}

\begin{cor} If $T$ is a normal operator on a Hilbert space, then $\ker T = \ker T^*$.
\end{cor}

\begin{prop} If $T$ is a normal operator on a Hilbert space, then $\norm{T^2} = {\norm T}^2$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Substitute $Tx$ for $x$ in proposition~\ref{prop_nasc_normal}. Use
proposition~\ref{C063544}.  \ns
\end{proof}

\begin{prop} If $T$ is a normal operator on a Hilbert space $H$, then
   \[ H = \clo{\ran T} \oplus \ker T\,.  \]
\end{prop}

\begin{cor} A normal operator on a Hilbert space has dense range if and only if it is injective.
\end{cor}

\begin{exam} The \emph{unilateral shift} operator (\ref{C063526}) is an example of a Hilbert space operator that is injective
but does not have dense range.  Its adjoint has dense range (in fact, is surjective) but is not injective.
\end{exam}

\begin{cor} A normal operator on a Hilbert space is invertible if and only if it is bounded away from zero.
\end{cor}
















\section{Operators of Finite Rank}

\begin{defn} The
 \index{rank}%
\df{rank} of a linear map is the (vector space) dimension of its range.  Thus an operator of
 \index{finite!rank}%
 \index{operator!of finite rank}%
\df{finite rank} is one which has finite dimensional range. We denote by $\ofml{FR}(V)$
 \index{fr@$\ofml{FR}(V)$ (operators of finite rank on~$V$)}%
the collection of finite rank operators on a vector space $V$.
\end{defn}

\begin{exam}\label{0022431fa} For vectors $x$ and $y$ in a Hilbert space $H$ define
   \[ x \otimes y\colon H \sto H \colon z \mapsto \langle z,y \rangle x\,. \]
If $x$ and $y$ are nonzero vectors in $H$, then $x \otimes y$ is a rank-one operator on~$H$.
\end{exam}

The next two propositions identify the adjoints and composites of these rank-one operators.

\begin{prop} If $u$ and $v$ are vectors in a Hilbert space, then
   \[ (u \otimes v)^* = v \otimes u\,. \]
\end{prop}

\begin{prop} If $u$, $v$, $x$, and $y$ are vectors in a Hilbert space, then
    \[ (u \otimes v)(x \otimes y) = \langle x,v \rangle (u \otimes y)\,. \]
\end{prop}

\begin{prop} If $x$ is a vector in a Hilbert space $H$, then $x \otimes x$ is a rank-one projection if and
only if $x$ is a unit vector.
\end{prop}

\begin{prop} Suppose that $\{e^1, e^2, e^3, \dots, e^n\}$ is an orthonormal set in a Hilbert space~$H$.  Let
$M = \spn\{e^1, \dots, e^n\}$.  Then $P = \sum_{k=1}^n e^k \otimes e^k$ is the orthogonal projection of $H$ onto~$M$.
\end{prop}

\begin{prop}\label{prop_op_act_otimes1} If $u$ and $v$ are vectors in a Hilbert space and $T$ is an operator on $H$, then
   \[ T\,(u \otimes v) = (Tu) \otimes v\,. \]
\end{prop}

\begin{prop}\label{prop_op_act_otimes2} If $u$ and $v$ are vectors in a Hilbert space and $T$ is an operator on $H$, then
   \[ (u \otimes v)\,T = u \otimes T^*v\,. \]
\end{prop}

We will see shortly that the family of all finite rank operators on a Hilbert space is a minimal $*\,$-ideal in the
$*\,$-algebra $\ofml B(H)$.  In preparation for this we review a few basic facts about ideals in algebras.

\begin{defn}\label{000551} An
 \index{ideal!left}%
 \index{left!ideal}%
\df{left ideal} in an algebra $A$ is a vector subspace $J$ of $A$ such that $AJ \subseteq J$. (For
 \index{ideal!right}%
 \index{right!ideal}%
\df{right ideals}, of course, we require $JA \subseteq J$.)  We say that $J$ is an
 \index{ideal!in an algebra}%
\df{ideal} if it is a two-sided ideal, that is, both a left and a right ideal.  A
 \index{proper!ideal}%
 \index{ideal!proper}%
\df{proper} ideal is an ideal which is a proper subset of~$A$.

The ideals $\{0\}$ and $A$ are often referred to as the
 \index{trivial!ideal}%
 \index{ideal!trivial}%
\df{trivial ideals} of~$A$. The algebra $A$ is
 \index{simple!algebra}%
 \index{algebra!simple}%
\df{simple} if it has no nontrivial ideals.

A
 \index{maximal ideal}%
 \index{ideal!maximal}%
\df{maximal} ideal is a proper ideal that is properly contained in no other proper ideal.
We denote the family of all maximal ideals in an algebra $A$
 \index{maximal@$\mx A$!set of maximal ideals in an algebra}%
by~$\mx A$.  A
 \index{minimal ideal}%
 \index{ideal!minimal}%
\df{minimal} ideal is a nonzero ideal that properly contains no other nonzero ideal.
\end{defn}

 \index{conventions!ideals are two-sided}%
\begin{conv} Whenever we refer to an \emph{ideal} in an algebra we understand it to be a
two-sided ideal (unless the contrary is stated).
\end{conv}

\begin{prop} No invertible element in a unital algebra can belong to a proper ideal.
\end{prop}

 \begin{prop}\label{000552} Every proper ideal in a unital algebra $A$ is contained in a
 maximal ideal. Thus, in particular, $\mx A$ is nonempty whenever $A$ is a unital algebra.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Zorn's lemma. \ns  \end{proof}

\begin{prop}\label{000553} Let $a$ be an element of a commutative algebra~$A$.  If $A$ is
unital and $a$ is not invertible, then $aA$ is a proper ideal in~$A$.
\end{prop}

\begin{defn}\label{000554} The ideal $aA$ in the preceding proposition is the
 \index{principal ideal}%
 \index{ideal!principal}%
\df{principal ideal} generated by~$a$.
\end{defn}

\begin{defn}\label{000555} Let $J$ be an ideal in an algebra~$A$.  Define an
equivalence relation $\sim$ on $A$ by
  \[ a \sim b \qquad \text{if and only if } \qquad b-a \in J. \]
For each $a \in A$ let $[a]$ be the equivalence class containing~$a$.  Let $A/J$ be the
set of all equivalence classes of elements of~$A$.  For $[a]$ and $[b]$ in $A/J$ define
  \[ [a] + [b] := [a+b] \qquad \text{ and } \qquad [a][b] := [ab] \]
and for $\alpha \in \C$ and $[a] \in A/J$ define
  \[\alpha[a] := [\alpha a] \,. \]
Under these operations $A/J$ becomes an algebra.  It is the
 \index{quotient!algebra}%
 \index{algebra!quotient}%
\df{quotient algebra} of $A$ by~$J$.  The notation $A/J$ is usually read ``$A$ mod~$J$''.
The surjective algebra homomorphism
  \[ \pi\colon A \sto A/J\colon a \mapsto [a]\]
is called the
 \index{quotient!map!for algebras}%
\df{quotient map}.
\end{defn}

\begin{prop} The preceding definition makes sense and the claims made therein are correct.
Furthermore, if the ideal $J$ is proper, then the quotient algebra $A/J$ is unital if $A$ is.
\end{prop}

\begin{proof}[\emph{Hint for proof}] You will need to show that:
 \begin{enumerate}
  \item[(a)] $\sim$ is an equivalence relation.
  \item[(b)] Addition and multiplication of equivalence classes is well defined.
  \item[(c)] Multiplication of an equivalence class by a scalar is well defined.
  \item[(d)] $A/J$ is an algebra.
  \item[(e)] The ``quotient map'' $\pi$ really is a surjective algebra homomorphism.
 \end{enumerate}
(At what point is it necessary that we factor out an ideal and not just a subalgebra?) \ns
\end{proof}

\begin{defn} In an algebra $A$ with involution
 \index{<star@$*\,$-ideal}%
 \index{star@$*\,$-ideal}%
 \index{ideal!$*\,$-}%
a \df{$*\,$-ideal} is a self-adjoint ideal in~$A$.
\end{defn}

\begin{prop}\label{0010953} Suppose that $J$ is a $*\,$-ideal in a $*\,$-algebra $A$.  In the quotient
algebra $A/J$, as developed in~\ref{000555}, define $[a]^* := [a^*]$ for every $[a] \in A/J$.
 \index{quotient!$*\,$-algebra}%
 \index{<star@$*\,$-algebra!quotient}%
This is a well-defined unary operation on the resulting quotient algebra $A/J$, which becomes a $*\,$-algebra, and the
quotient map $a \mapsto [a]$ is a $*\,$-homomorphism. The $*\,$-algebra $A/J$ is,of
course, the
 \index{quotient!$*\,$-algebra}%
 \index{<star@$*\,$-algebra!quotient}%
 \index{star@$*\,$-algebra!quotient}%
\df{quotient} of $A$ by~$J$.
\end{prop}

\begin{prop} Let $H$ be a Hilbert space. Then the family $\ofml{FR}(H)$ of all finite rank operators on $H$
 \index{star@$*\,$-ideal!$\ofml{FR}(H)$ is a}%
 \index{FR@$\ofml{FR}(H)$!as a $*\,$-ideal}%
is a $*\,$-ideal in the $*\,$-algebra~$\ofml B(H)$.
\end{prop}

\begin{prop}\label{prop_frop_sum_rnk1} Let $T$ be an operator of rank-$n$ on a Hilbert space~$H$. Then there exist
vectors $u^1$, \dots, $u^n$, $v^1$, \dots, $v^n$ in $H$ such that
   \[ T = \sum_{k=1}^n u^k \otimes v^k\,. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $\{e^1, \dots, e^n\}$ be an orthonormal basis for the range of~$T$.  For an
arbitrary vector $x \in H$ write out the \emph{Fourier expansion} of $Tx$ (see proposition~\ref{C063144}(d)\,).  \ns
\end{proof}

\begin{cor} Every rank-one operator on a Hilbert space is of the form $u \otimes v$ for some nonzero vectors
$u$ and $v$ in~$H$.
\end{cor}

\begin{cor} Every finite rank operator on a Hilbert space is a sum of (finitely many) rank-one operators.
\end{cor}

\begin{prop} If $H$ is a Hilbert space, then the family $\ofml{FR}(H)$ of finite rank operators on $H$
is a minimal ideal in~$\ofml B(H)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $\ofml J$ be a nonzero ideal in $\ofml B(H)$.  We need to show that $\ofml J$
contains~$\ofml{FR}(H)$.  By proposition~\ref{prop_frop_sum_rnk1} it suffices to prove that $\ofml J$ contains every
operator of the form $u \otimes v$.  To this end let $T$ be any nonzero member of $\ofml J$, let $y$ be a unit vector in
the range of $T$, and let $x$ be any vector in $H$ such that $y = Tx$.  Consider the operator $(u \otimes y)T(x \otimes v)$. \ns
\end{proof}

Although the set of finite rank operators is an ideal in the Banach algebra of operators on a Hilbert space, it is not a closed
ideal and, as a consequence, turns out not to be an appropriate set to factor out in hopes of getting a quotient object in the
category of Banach algebras.  The closure of the ideal of finite rank operators on a Hilbert space is the ideal of \emph{compact
operators}.  In a sense it would be a natural next step to examine the properties of this class of operators, among which we find
some of the most important operators in applications, integral operators, for example.  However, a smoother and more perspicuous
exposition is possible with a bit more technical apparatus at our disposal.  So in the next chapter we will concentrate on
developing some of the standard tools used in functional analysis.
























\endinput
\chapter{HILBERT SPACES}

For the most elementary facts about Hilbert spaces I can think of no better introduction than the first two chapters
of Halmos's classic text~\cite{Halmos:1951}.  Another good introduction is~\cite{Young:1988}.  You may also find
chapters 3 and 9 of~\cite{Kreyszig:1978} and chapter 4 of~\cite{Ha:2006} helpful.  For a start towards more serious
explorations into the mysteries of Hilbert space look at~\cite{Conway:1990}, \cite{Conway:2000}, and~\cite{Helmberg:1969}.


\section{Definition and Examples}

A Hilbert space is a \emph{complete inner product space}.  In more detail:
\begin{defn} In proposition~\ref{00015025} we showed how an inner product on a vector space induces a norm on the space and in
proposition~\ref{0001503} how a norm in turn induces a metric.  If an inner product space is complete with respect to this metric
it is a
  \index{Hilbert!space}%
  \index{space!Hilbert}%
\df{Hilbert space}.  Similarly, a
  \index{Banach!space}%
  \index{space!Banach}%
\df{Banach space} is a complete normed linear space and a
  \index{Banach!algebra}%
  \index{algebra!Banach}%
\df{Banach algebra} is a complete normed algebra.
\end{defn}

\begin{exam} Under the inner product defined in
 \index{K@$\K^n$!as a Hilbert space}%
 \index{Hilbert!space!$\K^n$ as a}%
example~\ref{000150012} $\K^n$ is a Hilbert space.
\end{exam}

\begin{exam}\label{exam150013} Under the inner products in
 \index{l@$l_2 = l_2(\N)$!as a Hilbert space}%
 \index{l@$l_2(\Z)$!as a Hilbert space}%
 \index{Hilbert!space!$l_2 = l_2(\N)$, $l_2(\Z)$ as a}%
example~\ref{000150013} $l_2 = l_2(\N)$ and $l_2(\Z)$ are Hilbert spaces.
\end{exam}

\begin{exam} Under the inner product defined in example~\ref{000150014} $\fml C([a,b])$ is \emph{not} a Hilbert space.
\end{exam}

\begin{exam}\label{exam_CX} Let $\fml C(X) = \fml C(X,\C)$ be the family of all continuous complex valued functions on a compact
Hausdorff space~$X$.  Under the
 \index{uniform!norm!on $\fml C(X)$}%
 \index{norm!uniform}%
 \index{C@$\fml C(X)$!as a Banach space}%
 \index{C@$\fml C(X)$!continuous complex valued functions on~$X$}%
\df{uniform norm}
   \[ \norm f_u := \sup\{\abs{f(x)}\colon 0 \le x \le 1\} \]
$\fml C(X)$ is a Banach space.  If the space $X$ is not compact we can make the set $\fml C_b(X)$
 \index{C@$\fml C_b(X)$!as a Banach space}%
of all \emph{bounded} continuous complex valued functions on $X$ into a Banach space in the same way.
When the scalar field is $\R$ we will
 \index{C@$\fml C(X,\R)$!continuous real valued functions on~$X$}%
 \index{C@$\fml C(X,\R)$!as a Banach space}%
write $\fml C(X,\R)$ for the family of continuous real valued functions on~$X$.  It too is a Banach space.
\end{exam}

\begin{exam}\label{000213} The inner product space $l_c$ of all sequences $(a_n)$ of complex numbers which are eventually zero
(see example~\ref{exam13122}) is not a Hilbert space.
\end{exam}

\begin{exam}\label{00022} Let $\mu$ be a positive measure on a $\sigma$-algebra $\sfml A$ of
subsets of a set~$S$.  If you are unacquainted with general measure theory, a casual introduction to Lebesgue measure
on the real line should suffice for examples which appear in these notes.  A complex valued function $f$ on $S$ is
 \index{measurable}%
\df{measurable} if the inverse image under $f$ of every Borel set (equivalently, every open
set) in $\C$ belongs to~$\sfml A$. We define an equivalence relation $\sim$ on the family of
measurable complex valued functions by setting $f \sim g$ whenever $f$ and $g$ differ on a set
of measure zero, that is, whenever $\mu(\{x \in S\colon f(x) \ne g(x)\}) = 0$.  We adopt
conventional notation and denote the equivalence class containing $f$ by $f$ itself (rather
than something more logical such as~$[f]$).  We denote the family of (equivalence classes of)
measurable complex valued functions on $S$ by $\fml M(S,\mu)$. A function $f \in \fml M(S,\mu)$ is
 \index{square integrable}%
 \index{M@$\fml M(S,\mu)$ (equivalence classes of measurable functions)}%
 \index{L@$\fml L_2(S,\mu)$!square integrable functions}%
\df{square integrable} if $\int_S \abs{f(x)}^2\,d\mu(x) < \infty$.  We denote the family of
(equivalence classes of) square integrable functions on $S$ by~$\lfs 2(S,\mu)$.
 \index{L@$\lfs 2(S,\mu)$!classes of square integrable functions}%
 \index{L@$\lfs 2(S,\mu)$!as a Hilbert space}%
 \index{Hilbert!space!$\lfs 2(S,\mu)$ as a}%
For every $f$, $g \in \lfs 2(S,\mu)$ define
   \[ \langle f,g \rangle := \int_S f(x) \conj{g(x)}\,d\mu(x)\,. \]
With this inner product (and the obvious pointwise vector space operations) $\lfs 2(S,\mu)$ is a Hilbert space.
\end{exam}

\begin{exam}\label{exam_L1S} Notation as in the preceding example.  A function $f \in \fml M(S,\mu)$ is
 \index{integrable}%
 \index{L@$\fml L_1(S,\mu)$!integrable functions}%
\df{integrable} if $\int_S \abs{f(x)}\,d\mu(x) < \infty$.  We denote the family of
(equivalence classes of) integrable functions on $S$ by~$\lfs 1(S,\mu)$.
 \index{L@$\lfs 1(S,\mu)$!classes of integrable functions}%
 \index{L@$\lfs 1(S,\mu)$!as a Banach space}%
 \index{Banach!space!$\lfs 1(S,\mu)$ as a}%
For every $f \in \lfs 1(S,\mu)$ define
    \[ {\norm f}_1 := \int_S \abs{f(x)}\,d\mu(x)\,. \]
With this norm (and the obvious pointwise vector space operations) $\lfs 1(S,\mu)$ is a Banach space.
\end{exam}

\begin{defn} If $J$ is any interval in $\R$ a real (or complex) valued function $f$ on $J$ is
 \index{absolute!continuity!of a function}%
 \index{continuity!absolute!of a function}%
 \index{absolutely!continuous}%
\df{absolutely continuous} if it satisfies the following condition:
  \quote for every $\epsilon > 0$ there exists $\delta > 0$ such
 that if $(a_1,b_1), \dots,(a_n,b_n)$ \\
 are disjoint nonempty subintervals of $J$ with $\sum_{k=1}^n(b_k - a_k) < \delta$, \\
 then $\sum_{k=1}^n\abs{f(b_k) - f(a_k)}~<~\epsilon$.
  \endquote
\end{defn}

\begin{exam}\label{exam_Hsp_abs_cont} Let $\lambda$ be Lebesgue measure on the interval $[0,1]$ and $H$ be the set of all
absolutely continuous functions on $[0,1]$ such that $f\,'$ belongs to $\lfs 2([0,1],\lambda)$ and $f(0) = 0$.
 \index{Hilbert!space!of absolutely continuous functions}%
 \index{absolutely!continuous functions!Hilbert space of}%
For $f$ and $g$ in $H$ define
   \[ \langle f,g \rangle = \int_0^1 f\,'(t)\clo{g\,'(t)}\,dt. \]
This is an inner product on $H$ under which $H$ becomes a Hilbert space.
\end{exam}

\begin{exam} If $H$ and $K$ are Hilbert spaces, then their (external orthogonal) direct sum $H \oplus K$
 \index{Hilbert!space!direct sum of Hilbert spaces as a}%
 \index{direct!sum!of Hilbert spaces}%
(as defined in~\ref{0001504}) is also a Hilbert space.
\end{exam}


















\section{Unordered Summation}

Defining \emph{unordered summation} is no more difficult in normed linear spaces than in~$\R$.  We generalize
definition~\ref{def_summable_R}.

\begin{defn} Let $V$ be a normed linear space and $A \subseteq V$.
For every $F \in \fin A$ define
  \[ \sbsb sF = \sum F\]
where $\sum F$ is the sum of the (finitely many) vectors belonging to~$F$.
Then $s = \bigl(\sbsb sF\bigr)_{F \in \fin A}$ is a net in~$V$. If this net converges, the set
$A$ is said to be
 \index{summable!set of vectors}%
\df{summable}; the limit of the net is the
 \index{sum!of a summable set}%
 \index{<sum@$\sum A$ (sum of a set of vectors)}%
\df{sum} of $A$ and is denoted by~$\sum A$. If $\{\norm a\colon a \in A\}$ is summable, we say that the set $A$ is
 \index{absolutely!summable}%
 \index{summable!absolutely}%
\df{absolutely summable}.

To accommodate sums in which the same vector appears more than once, we need to make use of indexed families of vectors.  They
require a slightly different approach. Suppose, for example, that $A = \bigl(x_i\bigr)_{i \in I}$ where $I$ is
an arbitrary index set. Then for each $F \in \fin I$
  \[ \sbsb sF = \sum_{i \in F}x_i . \]
As above $s$ is a net in~$V$. If it converges $\bigl(x_i\bigr)_{i \in I}$ is
 \index{summable!indexed family of vectors}%
summable and its limit is the
 \index{sum!of an indexed family}%
 \index{<sum@$\sum_{i \in I} x_i$ (sum of an indexed family)}%
sum of the indexed family, and is denoted by $\sum_{i \in I} x_i$.  An alternative way of saying that $\bigl(x_i\bigr)_{i \in I}$
is summable is to say that the ``series'' $\sum_{i \in I}x_i$
 \index{convergence!of infinite series}%
 \index{sum!of an infinite series}%
\df{converges} or that the ``sum'' $\sum_{i \in I}x_i$ \df{exists}.  An indexed family $\bigl(x_i\bigr)_{i \in I}$ is
 \index{absolutely!summable}%
 \index{summable!absolutely}%
\df{absolutely summable} if the indexed family $\bigl(\norm{x_i}\bigr)_{i \in I}$ is summable.  An alternative way of saying
that $\bigl(x_i\bigr)_{i \in I}$ is absolutely summable is to say that the ``series'' $\sum_{i \in I}x_i$ is
 \index{convergence!absolute}%
 \index{absolute!convergence}%
\df{absolutely convergent}.

Sequences, as usual, are treated as a separate case.  Let $(a_k)$ be a sequence in~$V$.  If the infinite series
$\sum_{k=1}^\infty a_k$ (that is, the sequence of
 \index{partial!sum}%
 \index{sum!partial}%
 \index{infinite series!partial sum of an}%
 \index{series!infinite}%
partial sums $s_n = \sum_{k=1}^n a_k$) converges to a
vector $b$ in $V$, then we say that the sequence $(a_k)$ is
 \index{infinite series!in a normed linear space}%
 \index{summable!sequence}%
 \index{sequence!summable}%
\df{summable} or, equivalently, that the series $\sum_{k=1}^\infty a_k$ is a
 \index{convergence!of a series}%
 \index{series!convergent}%
\df{convergent series}.  The vector $b$ is called the
 \index{sum!of an infinite series}%
 \index{series!sum of a}%
 \index{infinite series!sum of an}%
\df{sum} of the series $\sum_{k=1}^\infty a_k$ and we write
  \[ \sum_{k=1}^\infty a_k = b\,. \]
It is clear that a necessary and sufficient condition for a series $\sum_{k=1}^\infty a_k$ to
be convergent or, equivalently, for the sequence $(a_k)$ to be summable, is that there exist a
vector $b$ in $V$ such that
  \begin{equation}\label{cond_ser_conv}
     \biggnorm{b - \sum_{k=1}^n a_k} \sto 0 \qquad \text{as} \qquad n \sto \infty.
  \end{equation}
\end{defn}

\begin{prop}\label{prop_abs_sum_implies_sum} A normed linear space $V$ is complete if and only if every absolutely
summable sequence in $V$ is summable.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $(x_n)$ be a Cauchy sequence in a normed linear space in which every absolutely
summable sequence is summable.  Find a subsequence $\bigl(x_{n_k}\bigr)$ of $(x_n)$ such that $\norm{x_n - x_m} < 2^{-k}$
whenever $m$, $n \ge n_k$.  If we let $y_1 = x_{n_1}$ and $y_j = x_{n_j} - x_{n_{j-i}}$ for $j > 1$, then $(y_j)$ is
absolutely summable.   \ns
\end{proof}

\begin{prop} If $\bigl(x_i\bigr)_{i \in I}$ and $\bigl(y_i\bigr)_{i \in I}$ are
summable indexed families in a normed linear space and $\alpha$ is a scalar, then
$\bigl(\alpha x_i\bigr)_{i \in I}$ and $\bigl(x_i + y_i\bigr)_{i \in I}$
are summable; and
  \[ \sum_{i \in I} \alpha x_i = \alpha \, \sum_{i \in I} x_i \]
and
  \[ \sum_{i \in I} x_i + \sum_{i \in I} y_i =
                         \sum_{i \in I} (x_i + y_i). \]
\end{prop}

\begin{rem} One frequently finds some version of the
following ``proposition'' in textbooks.
\vskip 10 pt
\begin{quote}
If $\sum_{i \in I}x_i = u$ and $\sum_{i \in I}x_i = v$ in a
Hilbert (or Banach) space, then $u = v$.
\end{quote}

 \vskip 10 pt
\noindent (I once saw a 6 line proof of this.) Of which result is
this a trivial consequence?
\end{rem}

\begin{prop} If $\bigl(x_i\bigr)_{i \in I}$ is a summable indexed family of vectors in a Hilbert space $H$ and
$y$ is a vector in $H$, then $\bigl(\langle x_i,y \rangle\bigr)_{i \in I}$ is a summable indexed family of scalars and
   \[ \sum_{i \in I}\langle x_i,y \rangle  = \biggl < \sum_{i \in I} x_i, y \biggr >. \]
\end{prop}

The following is a generalization of example~\ref{exam150013}.

\begin{exam} Let $I$ be an arbitrary nonempty set and $l^2(I)$ be the set of all complex valued functions $x$ on $I$ such that
$\sum_{i \in I}\abs{x_i}^2 < \infty$. For all $x$, $y \in l^2(I)$ define
    \[ \langle x,y \rangle = \sum_{i \in I}x(i)\conj{y(i)}.\]
  \begin{enumerate}
    \item If $x \in l^2(I)$, then $x(i) = 0$ for all but countably many $i$.
    \item The map $(x,y) \mapsto \langle x,y \rangle$ is an inner product on the vector space $l^2(I)$.
    \item The space $l^2(I)$ is a Hilbert space.
  \end{enumerate}
\end{exam}

\begin{prop}\label{prop_sum_conv} Let $(x_n)$ be a sequence in a Hilbert space.  If the sequence regarded as an indexed family
is summable, then the infinite series $\sum_{k=1}^\infty x_k$ converges.
\end{prop}

\begin{exam} The converse of proposition~\ref{prop_sum_conv} does not hold.
\end{exam}

The direct sum of two Hilbert spaces is a Hilbert space.

\begin{prop} If $H$ and $K$ are Hilbert spaces, then their (orthogonal) direct sum (as defined in~\ref{0001504})
is a Hilbert space.
\end{prop}

We can also take the product of more than two Hilbert spaces---even an infinite family of them.

\begin{prop} Let $\bigl(H_i\bigr)_{i \in I}$ be an indexed family of Hilbert spaces.  A function $x\colon I \sto \bigcup H_i$
belongs to $H = \D\bigoplus_{i \in I}H_i$ if $x(i) \in H_i$ for every $i$ and if $\sum_{i \in I}\norm{x(i)}^2 < \infty$.
Defining $\langle \hphantom x, \hphantom y \rangle$ on $H$ by $\langle x, y \rangle := \sum_{i \in I} \langle
x(i), y(i) \rangle$ makes $H$ into an Hilbert space.
\end{prop}

\begin{defn} The Hilbert space defined in the preceding proposition is the
 \index{external!othogonal direct sum}%
 \index{orthogonal!direct sum!of Hilbert spaces}%
 \index{direct!sum!of Hilbert spaces}%
 \index{Hilbert!space!direct sum}%
 \index{sum!direct}%
\df{(external orthogonal) direct sum} of the Hilbert spaces~$H_i$.
\end{defn}

\begin{exer}  Is the direct sum defined above (with appropriate morphisms) a product in the category $\cat{HSp}$
of Hilbert spaces and bounded linear maps?  Is it a coproduct?
\end{exer}


























\section{Hilbert space Geometry}

\begin{conv}  In the context of Hilbert spaces the word ``subspace'' will always mean
 \index{conventions!subspaces of Hilbert spaces are closed}%
 \index{subspace!of a Hilbert space}%
\emph{closed vector subspace}.  The reason for this is that we want a subspace of a Hilbert space to be a Hilbert space in its own right; that is to say,
a subspace of a Hilbert space should be a subobject in the category of Hilbert spaces (and appropriate morphisms).  To indicate that $M$ is a subspace of $H$ we write
 \index{<binrelle@$\preccurlyeq$ (subspace of a Banach or Hilbert space)}%
$M \preccurlyeq H$.  A (not necessarily closed) vector subspace of a Hilbert space is also
 \index{linear!subspace}%
 \index{vector!subspace}%
 \index{linear!manifold}%
 \index{manifold!linear}%
called a \emph{linear subspace} or a \emph{linear manifold}.
\end{conv}

\begin{defn} Let $A$ be a nonempty subset of a Banach (or Hilbert) space~$B$.  We define the
 \index{closed!linear span}%
 \index{linear!span!closed}%
 \index{<unops@$\bigvee A$ (closed linear span of~$A$)}%
\df{closed linear span} of $A$ (denoted by $\bigvee A$) to be the smallest subspace of $B$ containing~$A$; that is, the
intersection of all subspaces of $B$ which contain~$A$.
\end{defn}

As with definitions \ref{smplx005def}, \ref{def_interior}, and \ref{def_closure}, we need to show that the preceding one makes sense.

\begin{prop} The preceding definition makes sense.  Furthermore, a vector is in the closed linear span of $A$ if and only if it belongs
to the closure of the linear span of~$A$.
\end{prop}

\begin{thm}[Minimizing Vector Theorem] If $C$ is a nonempty closed convex subset of a
 \index{minimizing vector theorem}%
Hilbert space $H$ and $a \in C^c$, then there exists a unique $b \in C$ such that
$\norm{b - a} \le \norm{x - a}$ for every $x \in C$.
 \[\xymatrix{
     &&&&&{}  \\
     &&&& x   \\
     a\ar[rrrru]\ar[rrr] &&& b  \\
     &&&&&C   \\
     &&&&{}\ar@{-}@/^3.5pc/[uuuu]       }\]
\end{thm}

\begin{proof}[\emph{Hint for proof}]  For the existence part let $d = \inf\{\norm{x - a}\colon x \in C\}$.  Choose a sequence
$(y_n)$ of vectors in $C$ such that $\norm{y_n - a} \sto d$.  To prove that the sequence $(y_n)$ is Cauchy, apply the
\emph{parallelogram law}~\ref{parallelogram_law} to the vectors $y_m - a$ and $y_n - a$ for $m$, $n \in \N$.  The uniqueness
part is similar: apply the \emph{parallelogram law} to the vectors $b - a$ and $c - a$ where $b$ and $c$ are two vectors
satisfying the conclusion of the theorem.  \ns
\end{proof}

\begin{exam} The vector space $\R^2$ under the uniform metric is a Banach space. To see
that in this space the \emph{minimizing vector theorem} does not hold take $C$ to be the
closed unit ball about the origin and $a$ to be the point~$(2,0)$.
\end{exam}

\begin{exam} The sets
   \[ C_1 = \biggl\{f \in \fml C([0,1])\colon \int_0^{1/2}f - \int_{1/2}^1f = 1\biggr\} \qquad \text{and} \qquad
                              C_2 = \biggl\{f \in \lfs 1([0,1],\lambda)\colon \int_0^1f = 1\biggr\} \]
are examples that show that neither the existence nor the uniqueness claims of the \emph{minimizing vector theorem}
necessarily holds in a Banach space. (The symbol $\lambda$ denotes Lebesgue measure.)
\end{exam}

\begin{thm}[Vector Decomposition Theorem]\label{thm_vec_decomp} Let $H$ be a Hilbert space and
 \index{vector!decomposition theorem}%
$M$ be a (closed linear) subspace of~$H$.
Then for every $x \in H$ there exist unique vectors $y \in M$ and $z \in M^\perp$ such that $x = y + z$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Assume $x \notin M$ (otherwise the result is trivial). Explain how we
know that there exists a unique vector $y \in M$ such that
   \begin{equation} \norm{x - y} \le \norm{x - m} \end{equation}
for all $m \in M$.  Explain why it is enough, for the existence part of the proof, to prove
that $x - y$ is perpendicular to every \emph{unit} vector $m \in M$.  Explain why it is true
that for every unit vector $m \in M$
   \begin{equation} \norm{x - y}^2 \le \norm{x - (y + \lambda m)}^2 \end{equation}
where $\lambda := \langle x - y, m \rangle$.  \ns
\end{proof}

\begin{cor} If $M$ is a subspace of a Hilbert space~$H$, then $H = M \oplus M^\perp$.
\end{cor}

\begin{exam}The preceding result says that every subspace of a Hilbert space is a direct
summand. This result need not be true if $M$ is assumed to be just a linear subspace of the space.
For example, notice that $M = l_c$ (see example~\ref{000213}) is a linear subspace of the
Hilbert space $l_2$ (see example~\ref{exam150013}) but $M^\perp = \{\vc 0\}$.
\end{exam}

\begin{cor} A vector subspace $A$ of a Hilbert space $H$ is dense in $H$ if and only if $A^\perp = \{0\}$.
\end{cor}

\begin{prop} Every proper subspace of a Hilbert space has empty interior.
\end{prop}

\begin{prop} If a Hamel basis for a Hilbert space is not finite, then it is uncountable.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Suppose the space has a countably infinite basis $\{e^1, e^2, e^3, \dots \}$.
For each $n \in \N$ let $M_n = \spn\{e^1, e^2, \dots, e^n\}$.  Apply the \emph{Baire category theorem} (see~\cite{Erdman:2007},
theorem 29.3.20) to $\bigcup_{n=1}^\infty M_n$ keeping in mind the preceding proposition.   \ns
\end{proof}

\begin{prop}\label{00023} Let $H$ be a Hilbert space. Then the following hold:
 \begin{enumerate}
  \item if $A \subseteq H$, then $A \subseteq A^{\perp\perp}$;
  \item if $A \subseteq B \subseteq H$, then $B^\perp \subseteq A^\perp$;
  \item if $M$ is a subspace of $H$, then $M = M^{\perp\perp}$; and
  \item if $A \subseteq H$, then $\bigvee A = A^{\perp\perp}$.
 \end{enumerate}
\end{prop}

\begin{prop} Let $M$ and $N$ be subspaces of a Hilbert space.  Then
 \begin{enumerate}
  \item  $(M + N)^\perp = (M \cup N)^\perp =  M^\perp \cap N^\perp$, and
  \item  $(M \cap N)^\perp = M^\perp + N^\perp$.
 \end{enumerate}
\end{prop}

\begin{prop} Let $V$ be a normed linear space.  There exists an inner product on $V$ which induces the norm on $V$
if and only if the norm satisfies the \emph{parallelogram law}~\ref{parallelogram_law}.
\end{prop}

\begin{proof}[\emph{Hint for proof}]  We say that an inner product $\langle \,\cdot\,\,,\,\cdot\, \rangle$
on $V$ induces the norm $\norm{\,\cdot\,}$ if $\norm x = \sqrt{\langle x,x \rangle}$ holds for every $x$ in $V$.)
To prove that if a norm satisfies the \emph{parallelogram law} then it is induced by an inner product,
use the equation in the \emph{polarization identity}~\ref{0001507} as a definition.
Prove first that $\langle y,x \rangle = \conj{\langle x,y \rangle}$ for all $x$, $y\in V$ and that
$\langle x,x \rangle > 0$ whenever $x \ne 0$.  Next, for arbitrary $z \in V$, define a function
$f\colon V \sto \C\colon x \mapsto \langle x,z \rangle$.  Prove that
   \[ f(x + y) + f(x - y) = 2f(x) \tag{$\ast$} \]
for all $x$, $y \in V$.  Use this to prove that $f(\alpha x) = \alpha f(x)$ for all
$x \in V$ and $\alpha \in \R$.  (Start with $\alpha$ being a natural number.)  Then
show that $f$ is additive. (If $u$ and $v$ are arbitrary elements of $V$ let $x = u + v$
and $y = u - v$.  Use~$(\ast)$.)  Finally prove that $f(\alpha x) = \alpha f(x)$ for
complex $\alpha$ by showing that $f(ix) = i\,f(x)$ for all $x \in V$.  \ns
\end{proof}
















\section{Orthonormal Sets and Bases}\label{sec_onbases}
\begin{defn} A nonempty subset $E$ of a Hilbert space is
 \index{orthonormal}%
\df{orthonormal} if $e \perp f$ for every pair of distinct vectors $e$ and $f$ in $E$ and
$\norm e = 1$ for every $e \in E$.
\end{defn}

\begin{prop} In an inner product space every orthonormal set is linearly independent.
\end{prop}

\begin{prop}[Bessel's inequality]\label{C063141} Let $H$ be a Hilbert space and $E$ be an
 \index{Bessel's inequality}%
orthonormal subset of~$H$.  For every $x \in H$
  \[ \sum_{e \in E} \norm{\langle x,e \rangle}^2 \le \norm x^2\,. \]
\end{prop}

\begin{cor} If $E$ is an orthonormal subset of a Hilbert space $H$ and $x \in H$, then
$\{e \in E\colon \langle x,e \rangle~\ne~0\}$ is countable.
\end{cor}

\begin{prop}\label{C063144} Let $E$ be an orthonormal set in a Hilbert space~$H$. Then the
following are equivalent.
 \begin{enumerate}
  \item $E$ is a maximal orthonormal set.
  \item If $x \perp E$, then $x = 0$ ($E$ is \emph{total}).
 \index{total!orthonormal set}%
  \item $\bigvee E = H$ ($E$ is a \emph{complete orthonormal set}).
 \index{complete!orthonormal set}%
  \item $\D x = \sum_{e \in E} \langle x,e \rangle e$ for all $x \in H$. \emph{(Fourier
expansion)}
 \index{Fourier!expansion}%
  \item $\D \langle x,y \rangle = \sum_{e \in E} \langle x,e \rangle \langle e,y \rangle$
for all $x,y \in H$. \emph{(Parseval's identity.)}
 \index{Parseval's identity}%
  \item $\D \norm{x}^2 = \sum_{e \in E} \abs{\langle x,e \rangle}^2$ for all $x \in H$.
\emph{(Parseval's identity.)}
 \end{enumerate}
\end{prop}

\begin{defn}\label{C063147} If $H$ is a Hilbert space, then an orthonormal set $E$ satisfying
any one (hence all) of the conditions in proposition~\ref{C063144} is called an
 \index{orthonormal!basis}%
 \index{basis!orthonormal}%
\df{orthonormal basis} for~$H$ (or a
 \index{complete!orthonormal set}%
 \index{orthonormal!set!complete}%
\df{complete orthonormal set} in~$H$, or a
 \index{Hilbert!space!basis for}%
 \index{basis!for a Hilbert space}%
\df{Hilbert space basis} for~$H$).  Notice that this is a very different thing from the
usual (Hamel) basis for a vector space (see~\ref{defn_Hamel_basis}).  For a Hilbert space
the basis vectors are required to be of length one and to be mutually perpendicular
(conditions which make no sense in a vector space); but it is \emph{not} necessary that
every vector in the space be a linear combination of the basis vectors.
\end{defn}

\begin{prop} If $E$ is an orthonormal subset of a Hilbert space $H$, then there exists an
orthonormal basis for $H$ which contains~$E$.
\end{prop}

\begin{cor} Every nonzero Hilbert space has a basis.
\end{cor}

\begin{prop}  If $E$ is an orthonormal subset of a Hilbert space $H$ and $x \in H$, then
$\{\langle x,e \rangle e: e \in E\}$ is summable.
\end{prop}

\begin{exam}\label{C063149} For each $n \in \N$ let $\vc e^n$ be the sequence in $l_2$ whose
$n^{\text{th}}$ coordinate is $1$ and all the other coordinates are~$0$. Then $\{\vc e^n\colon n \in \N\}$
is an orthonormal basis for~$l_2$.  This is the
 \index{usual!orthonormal basis for~$l_2$}%
 \index{orthonormal!basis!usual (or standard) for~$l_2$}%
 \index{basis!standard (or usual)}%
\df{usual} (or
 \index{standard!orthonormal basis}%
\df{standard}) \df{orthonormal basis} for~$l_2$.
\end{exam}

\begin{exer} A function $f\colon \R \sto \C$ of the form
   \[ f(t) = a_0 + \sum_{k=1}^n (a_k \cos kt + b_k \sin kt) \]
where $a_0, \dots, a_n, b_1, \dots, b_n$ are complex numbers is a
 \index{trigonometric polynomial}%
 \index{polynomial!trigonometric}%
\df{trigonometric polynomial}.
 \begin{enumerate}
  \item Explain why complex valued $2\pi$-periodic functions on the real line $\R$ are
often identified with complex valued functions on the unit circle~$\T$.
  \item Some authors say that a \emph{trigonometric polynomial} is a function of the form
    \[ f(t) = \sum_{k=-n}^n c_k e^{ikt} \]
where $c_1, \dots, c_n$ are complex numbers. Explain carefully why this is exactly the
same as the preceding definition.
  \item Justify the use of the term \emph{trigonometric polynomial}.
  \item Prove that every continuous $2\pi$-periodic function on $\R$ is the uniform limit
of a sequence of trigonometric polynomials.
 \end{enumerate}
\end{exer}

\begin{prop}\label{prop_unique_onbasis} Let $E$ be a basis for a Hilbert space $H$ and $x \in H$. 
If $x = \sum_{e \in E} \alpha_e e$, then $\alpha_e = \langle x,e \rangle$ for each $e \in E$.
\end{prop}

\begin{prop} Consider the Hilbert space $\lfs 2([0,2\pi],\C)$ of (equivalence classes of complex valued,
 \index{L@$\lfs 2([0,2\pi],\C)$}%
square integrable functions on $[0,2\pi]$ with inner product
  \[ \langle f,g \rangle = \frac1{2\pi}\int_0^{2\pi} f(x) \conj{g(x)}\,dx. \]
For every integer $n$ (positive, negative, or zero) let $\vc e^n(x) = e^{inx}$ for all $x
\in [0,2\pi]$.  Then the set of these functions $\vc e^n$ for $n \in \Z$ is an
orthonormal basis for $\lfs 2([0,2\pi],\C)$.
\end{prop}

\begin{exam} The sum of the infinite series $\D\sum_{k=1}^\infty \frac1{k^2}$ is $\dfrac{\pi^2}6$\,.
\end{exam}

\begin{proof}[\emph{Hint for proof}]   Let $f(x) = x$ for $0 \le x \le 2\pi$.  Regard $f$ as a member of
$\lfs 2([0,2\pi])$.  Find $\norm{f}$ in two ways.  \ns
\end{proof}

The next proposition allows us to define the notion of the \emph{dimension} of a Hilbert space.  (In finite dimensional
spaces it agrees with the vector space concept.)

\begin{prop} Any two orthonormal bases of a Hilbert space have the same cardinality (that is, they are in
one-to-one correspondence).
\end{prop}

\begin{proof}  See \cite{Halmos:1951}, page 29, Theorem 1.   \ns
\end{proof}

\begin{defn} The
 \index{dimension}%
\df{dimension} of a Hilbert space is the cardinality of any orthonormal basis for the space.
\end{defn}

\begin{prop}\label{prop_Schauder_basis_Hsp} A Hilbert space is separable if and only if it has a
 \index{separable!Hilbert space}%
 \index{Hilbert!space!separable}%
countable orthonormal basis.
\end{prop}

\begin{proof}  See \cite{Kreyszig:1978}, page 171, Theorem 3.6-4.   \ns
\end{proof}





















\section{The Riesz-Fr\'echet Theorem}
\begin{exam}\label{000341} Let $H$ be a Hilbert space and $a \in H$.  Define $\psi_a\colon
H \sto \C\colon x \mapsto \langle x,a \rangle$.  Then $\psi_a \in H^*$.
\end{exam}

Now we generalize theorem~\ref{0001601} to the infinite dimensional setting. This result
is sometimes called the \emph{Riesz representation theorem} (which invites confusion with
 \index{representation!of continuous linear functionals}%
 \index{representation!Riesz's little theorem}%
the more substantial result about representing  certain linear functionals as measures)
or the \emph{little Riesz representation theorem}.  This theorem says that the \emph{only}
continuous linear functionals on a Hilbert space are the functions $\psi_a$ defined in~\ref{000341}.
In other words, given a continuous linear functional $f$ on a Hilbert space there is a unique vector
$a$ in the space such that the action of $f$ on a vector $x$ can be represented simply by taking the
inner product of $x$ with~$a$.

\begin{thm}[Riesz-Fr\'echet Theorem]\label{000342} If $f \in H^*$ where $H$ is a Hilbert
 \index{Riesz-Fr\'echet theorem}%
space, then there exists a unique vector $a$ in $H$ such that $f = \psi_a$. Furthermore,
$\norm a = \norm{\psi_a}$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] For the case $f \ne 0$, choose a unit vector $z$ in $(\ker f)^\perp$.
Notice that for every $x \in H$ the vector $x - \frac{f(x)}{f(z)}\,z$ belongs to $\ker f$.  \ns
\end{proof}

\begin{cor} The kernel of every nonzero bounded linear functional on a Hilbert space has codimension~$1$.
\end{cor}

\begin{proof}[\emph{Hint for proof}] The
 \index{codimension}%
\df{codimension} of a subspace of a Hilbert space is the dimension of its orthogonal complement.  This is really
a corollary to the \emph{proof} of the \emph{Riesz-Fr\'echet theorem}.  In our proof we found that the vector
representing the bounded linear functional $f$ was a (nonzero) multiple of a unit vector in $(\ker f)^\perp$.
What would happen if there were two linearly independent vectors in $(\ker f)^\perp$?   \ns
\end{proof}

\begin{defn} Recall that a mapping $T\colon V \sto W$ between complex vector spaces is
 \index{conjugate linear}%
 \index{linear!conjugate}%
\df{conjugate linear} if $T(x + y) = Tx + Ty$ and $T(\alpha x) = \conj{\alpha}\,Tx$ for all
$x$, $y \in V$ and $\alpha \in \C$. A bijective conjugate linear map is an
 \index{anti-isomorphism}%
 \index{isomorphism!anti-}%
\df{anti-isomorphism}.
\end{defn}

\begin{exam}\label{0022057} For a Hilbert space $H$, the mapping
$\psi\colon H \sto H^*\colon a \mapsto \psi_a$ defined in example~\ref{000341} is an
isometric anti-isomorphism between $H$ and its dual space.
\end{exam}

\begin{defn} A conjugate linear mapping $C\colon V \sto V$ from a vector space into itself
which satisfies $C^2 = \id V$ is called a
 \index{conjugation}
\df{conjugation} on~$V$.
\end{defn}

\begin{exam} Complex conjugation $z \mapsto \conj z$ is an example of a conjugation in the
vector space~$\C$.
\end{exam}

\begin{exam}\label{0022067} Let $H$ be a Hilbert space and $E$ be a basis for~$H$.  Then the
map $x \mapsto \sum_{e \in E}\langle e,x \rangle e$ is a conjugation and an isometry on~$H$.
\end{exam}

The term ``anti-isomorphism'' is a bit misleading. It suggests something entirely
different from an isomorphism.  In fact, an anti-isomorphism is nearly as good as an
isomorphism. The next proposition says in essence that a Hilbert space and its dual are
isomorphic \emph{because} they are anti-isomorphic.

\begin{prop}\label{0022071} Let $H$ be a Hilbert space, $\psi\colon H \sto H^*$ be the
anti-isomorphism defined in~\ref{000341} (see~\ref{0022057}), and $C$ be the conjugation
defined in~\ref{0022067}. Then the composite $C \psi^{-1}$ is an isometric isomorphism
from $H^*$ onto~$H$.
\end{prop}

\begin{cor}\label{cor_Hsp_iso_dual} Every Hilbert space is isometrically isomorphic to its dual.
\end{cor}

\begin{prop} Let $H$ be a Hilbert space and $\psi\colon H \sto H^*$ be the anti-isomorphism
defined in~\ref{000341} (see~\ref{0022057}).  If we define $\langle f,g \rangle :=
\langle \psi^{-1}g, \psi^{-1}f \rangle$ for all $f$, $g \in H^*$, then $H^*$ becomes a
Hilbert space isometrically isomorphic to~$H$.  The resulting norm on $H^*$ is its usual
norm.
\end{prop}

\begin{cor} Every Hilbert space is reflexive.
\end{cor}

\begin{exam} Let $H$ be the collection of all absolutely continuous complex
valued functions $f$ on $[0,1]$ such that $f(0) = 0$ and $f' \in
\lfs 2([0,1],\lambda)$.  Define an inner product on $H$ by
 \[\langle f,g \rangle := \int_0^1 f'(t)\,\clo{g'(t)}\,dt.\]
We already know that $H$ is a Hilbert space (see example~\ref{exam_Hsp_abs_cont})
Fix $t \in (0,1]$.  Define
   \[ E_t\colon H \sto \C\colon f \mapsto f(t). \]
Then $E_t$ is a bounded linear functional on~$H$. What is its $\norm{E_t}$?
What vector $g$ in $H$ represents the functional~$E_t$?
\end{exam}

\begin{exer} One afternoon your pal Fred R. Dimm comes to you with a problem. ``Look,'' he says,
``On $\lfs 2 = \lfs 2(\R,\R)$ the evaluation functional $E_0$, which evaluates each member of $\lfs 2$
at~$0$, is clearly a bounded linear functional.  So by the Riesz representation theorem there
should be some function $g$ in $\lfs 2$ such that $f(0) = \langle f,g \rangle = \int_{-\infty}^\infty fg$
for all $f$ in~$\lfs 2$.  But that's a property of the `$\delta$-function', which I was told
doesn't exist.  What am I doing wrong?'' Give Freddy some (helpful) advice.
\end{exer}

\begin{exer} Later the same afternoon Freddy's back.  After considering the
advice you gave him (in the preceding problem) he has revised his
question by letting the domain of $E_0$ be the set of bounded
continuous real valued functions defined on~$\R$.  Patiently you
explain to Freddy that there are now at least two things wrong his
invocation of the `$\delta$-function'.  What are they?
\end{exer}

\begin{exer} The nightmare continues. It's 11 P. M. and Freddy's back again.
This time (happily having given up on $\delta$-functions) he wants
to apply the representation theorem to the functional ``evaluation
at zero'' on the space~$l_2(\Z)$.  Can he do it?  Explain.
\end{exer}



























\section{Strong and Weak Topologies on Hilbert Spaces}
We recall briefly the definition and crucial facts about \emph{weak topologies}.  For a more detailed introduction
to this class of topologies see any good textbook on topology or section 11.4 of my notes~\cite{Erdman:2007}.

\begin{defn}\label{C021414} Suppose that $S$ is a set, that for every $\alpha \in A$ (where
$A$ is an arbitrary index set) $X_\alpha$ is a topological space, and that $f_\alpha
\colon S \sto X_\alpha$ for every $\alpha \in A$. Let
   \[ \sfml S = \{{f_\alpha}^\gets(U_\alpha) \colon \open{U_\alpha}{X_\alpha}
                                             \text{ and } \alpha \in A\}\,. \]
Use the family $\sfml S$ as a subbase for a topology on~$S$.  This topology is called the
 \index{weak!topology!induced by a family of functions}%
 \index{topology!weak!induced by a family of functions}%
\df{weak topology} induced by (or determined by) the functions~$f_\alpha$.
\end{defn}

\begin{prop}\label{C021421} Under the weak topology (defined above) on a set $S$ each of the functions $f_\alpha$ is continuous;
in fact the weak topology is  the weakest topology on
 \index{weak!topology!characterization of}%
 \index{topology!weak!characterization of}%
$S$ under which these functions are continuous.
\end{prop}

\begin{prop}\label{C021424}  Let $X$ be a topological space with the weak topology determined
by a family $\fml F$ of functions. Prove that a function $g\colon W \sto X$, where $W$ is
a topological space, is continuous if and only if $f \circ g$ is continuous for every $f
\in \fml F$.
\end{prop}

\begin{defn}\label{C021437}  Let $\bigl(A_\lambda\bigr)_{\lambda \in \Lambda}$ be an indexed
family of topological spaces. The weak topology on the Cartesian product $\prod
A_\lambda$ induced by the family of coordinate projections $\pi_\lambda$ (see
definition~\ref{C015531}) is called the
 \index{product!topology}%
 \index{topology!product}%
\df{product topology}.
\end{defn}

\begin{prop} The product of a family of Hausdorff spaces is Hausdorff.
\end{prop}

\begin{exam}\label{C031473} Let $Y$ be a topological space with the weak topology (see
definition~\ref{C021414}) determined by an indexed family $(f_\alpha)_{\alpha \in A}$ of
functions where for each $\alpha \in A$ the codomain of $f_\alpha$ is a topological
space~$X_\alpha$. Then a net $\bigl(y_{{}_\lambda}\bigr)_{\lambda \in \Lambda}$ in $Y$
converges to a point $a \in Y$ if and only if
\mbox{$f_\alpha(y_{{}_\lambda})~\to_{\lambda \in \Lambda}~f_\alpha(a)$} for every $\alpha
\in A$.
\end{exam}

\begin{exam}\label{C031475} If $Y = \prod_{\alpha \in A} X_\alpha$ is a product of nonempty
topological spaces with the product topology (see definition~\ref{C021437}), then a net
$\bigl(\sbsb y\lambda\bigr)$ in $Y$ converges to $a \in Y$ if and only if $\bigl(\sbsb
y\lambda\bigr)_\alpha \sto a_\alpha$ for every $\alpha \in A$.
\end{exam}

\begin{exam}\label{exam_prod_top_norm} If $V$ and $W$ are normed linear spaces, then the product topology on
$V \times W$ is the same as the topology induced by the product norm on $V \oplus W$.
\end{exam}

The next example illustrates the inadequacy of sequences when dealing with general topological spaces.

\begin{exam} Let $\fml F = \fml F([0,1])$ be the set of all functions $f\colon [0,1] \sto \R$ and give
$\fml F$ the product topology, that is, the weak topology determined by the evaluation
functionals
    \[ E_x\colon \fml F \sto [0,1]\colon f \mapsto f(x) \]
where $x \in [0,1]$.  Thus a basic open neighborhood of a point $g \in \fml F$ is determined
by a finite set of points $A \in \fin[0.1]$ and a number $\epsilon > 0$:
  \[ U(g\,;\,A\,;\, \epsilon) := \{f \in \fml F\colon \abs{f(t) - g(t)} < \epsilon
                           \text{ for all $t \in A$}\}. \]
(What is usual name for this topology?) Let $\fml G$ be the set of all functions in $\fml F$
having finite support. Then
  \begin{enumerate}
    \item the constant function $\vc 1$ belongs to $\clo{\fml G}$,
    \item there is a net of functions in $\fml G$ which converges to~$\vc 1$, but
    \item no sequence in $\fml G$ converges to~$\vc 1$.
  \end{enumerate}
\end{exam}

\begin{defn} A net $(x_{{}_{\sst\lambda}})$ in a Hilbert space $H$ is said to
 \index{convergence!in Hilbert space!weak}%
 \index{weak!convergence!in Hilbert space}%
\df{converge weakly} to a point $a$ in $H$ if $\langle x{{}_{\sst\lambda}}, y \rangle \to \langle a,y \rangle$
for every $y \in H$. In this case we write $x_{{}_{\sst\lambda}} \to^w a$. In a Hilbert space a net is said to
 \index{convergence!in Hilbert space!strong}%
 \index{strong!convergence!in Hilbert space}%
\df{converge strongly} (or
 \index{convergence!in norm}%
 \index{norm!convergence}%
\df{converge in norm}) if it converges with respect to the topology induced by the norm. This is the usual type of
convergence in a Hilbert space. If we wish to emphasize a distinction between modes of convergence we may write
$x_{{}_{\sst\lambda}} \to^s a$ for strong convergence.
\end{defn}

\begin{exer} Explain why the topology on a Hilbert space generated by the weak convergence of nets is in fact a
weak topology in the usual topological sense of the term.
\end{exer}

When we say that a subset of a Hilbert space $H$ is
 \index{weak!closure}%
 \index{closed!weakly}%
\df{weakly closed} we mean that it is closed with respect to the weak topology on $H$; a set is
 \index{weak!compactness}%
 \index{compact!weakly}%
\df{weakly compact} if it is compact in the weak topology on~$H$; and so on. A function
$f\colon H \sto H$ is
 \index{weak!continuity}%
 \index{continuous!weakly}%
\df{weakly continuous} if it is continuous as a map from $H$ with the weak topology into $H$
with the weak topology.

\begin{exer} Let $H$ be a Hilbert space and $(x_{{}_{\sst\lambda}})$ be a net in $H$.
 \begin{enumerate}
  \item If $x_{{}_{\sst\lambda}} \to^s  a$ in $H$, then $x_{{}_{\sst\lambda}} \to^w a$ in $H$
  \item The strong (norm) topology on $H$ is stronger (bigger) than the weak topology.
  \item Show by example that the converse of (a) does not hold.
  \item Is the norm on $H$ weakly continuous?
  \item If $x_{{}_{\sst\lambda}} \to^w  a$ in $H$ and if $\norm{x_{{}_{\sst\lambda}}} \to \norm{a}$, then
$x_{{}_{\sst\lambda}}\to^s a$ in~$H$.
  \item If a linear map $T\colon H \sto H$ is (strongly) continuous, then it is weakly continuous.
 \end{enumerate}
\end{exer}

\begin{exer} Let $H$ be an infinite dimensional Hilbert space. Consider the following subsets of~$H$:
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item the open unit ball;
  \item the closed unit ball;
  \item the unit sphere;
  \item a closed linear subspace.
 \end{enumerate}
Determine for each of these sets whether it is each of the following: strongly closed, weakly
closed, strongly open, weakly open, strongly compact, weakly compact.  (One part of this is
too hard at the moment: \emph{Alaoglu's theorem}~\ref{C067441} will tell us that the
closed unit ball is weakly compact.)
\end{exer}

\begin{exer} Let $\{e^n\colon n \in \N\}$ be the usual basis for $l_2$. For
each $n \in \N$ let $a_n = \sqrt n\, e^n$.  Which of the following
are correct?
 \begin{enumerate}
  \item $0$ is a weak accumulation point of the set
$\{a_n\colon n \in \N\}$.
  \item $0$ is a weak cluster point of the sequence~$(a_n)$.
  \item $0$ is the weak limit of a subsequence of~$(a_n)$.
 \end{enumerate}
\end{exer}










\section{Universal Morphisms}
Much of mathematics involves the construction of new objects from old ones---things such
as products, coproducts, quotients, completions, compactifications, and unitizations.
Often it is possible---and highly desirable---to characterize such a construction by
means of a diagram which describes what the constructed object ``does'' rather than
telling what it ``is'' or how it is constructed.  Such a diagram is a
 \index{universal!mapping!diagram}%
 \index{mapping!universal}%
\df{universal mapping diagram} and it describes the
 \index{universal!mapping!property}%
 \index{mapping!diagram!universal}%
\df{universal property} of the object being constructed.
In particular it is usually possible to characterize such a construction by the existence
of a unique morphism having some particular property. Because this morphism and its
corresponding property uniquely characterize the construction in question, they are referred to as a
\emph{universal morphism} and a \emph{universal property}, respectively. The following definition
is one way of describing the action of such a morphism.  If this is your first meeting with the
concept of \emph{universal} don't be alarmed by its rather abstract nature.  Few people, I think,
feel comfortable with this definition until they have encountered a half dozen or more examples in
different contexts.  (There exist even more technical definitions for this important concept. For
example, \emph{an initial or terminal object in a comma category}. To unravel this, if you are
interested, search for ``universal property'' in Wikipedia~\cite{wiki:xxx}.)

\begin{defn}\label{00078} Let $\cat A \to^{\ftr{F}} \cat B$ be a functor between categories
$\cat A$ and $\cat B$ and $B$ be an object in~$\cat B$.  A pair $(A,u)$ with $A$ an
object in $\cat A$ and $u$ a $\cat B$-morphism from $B$ to~$\ftr F(A)$ is a
 \index{universal!morphism}%
 \index{morphism!universal}%
\df{universal morphism} for $B$ (with respect to the functor~$\ftr F$) if for every
object $A'$ in $\cat A$ and every $\cat B$-morphism $B \to^f \ftr F(A')$ there exists a
unique $\cat A$-morphism $A \to^{\tilde f} A'$ such that the following diagram commutes.
 \begin{equation}\label{00078i}
   \xy
     \qtriangle/{>}`{>}`{-->}/[B`\ftr F(A)`\ftr F(A');u`f`\ftr F(\tilde f)]
     \morphism(1000,500)|r|/{-->}/<0,-500>[A`A';\tilde f]
   \endxy
 \end{equation}
In this context the object $A$ is often referred to as a
 \index{universal!object}%
 \index{object!universal}%
\df{universal object} in~$\cat A$.
\end{defn}

Here is a first example of such a property.

\begin{defn} Let $F$ be an object in a concrete category $\cat C$ and
$\iota\colon S \sto \abs F$ be an inclusion map whose domain is a nonempty set~$S$.  We say that the
object $F$ is
 \index{free!object}%
 \index{object!free}%
\df{free on} the set $S$ (or that $F$ is the \df{free object generated by}~$S$) if for
every object $A$ in $\cat C$ and every map $f\colon S \sto \abs A$ there exists a unique
morphism $\widetilde f_\iota\colon F \sto A$ in $\cat C$ such that $\abs{\wt f_\iota}
\circ \iota = f$.
 \[\xy
     \qtriangle/{>}`{>}`{-->}/[S`\abs F`\abs A;\iota`f`\abs{\wt f_\iota}]
     \morphism(1000,500)|r|/{-->}/<0,-500>[F`A;\widetilde f_\iota]
   \endxy\]
We will be interested in
 \index{free!vector space}%
 \index{vector!space!free}%
\df{free vector spaces}; that is, free objects in the category $\cat{VEC}$ of vector
spaces and linear maps.  Naturally, merely \emph{defining} a concept does not guarantee
its existence. It turns out, in fact, that free vector spaces exist on arbitrary sets.
(See exercise~\ref{ump001z}.)
\end{defn}

\vskip 5 pt

\begin{exer} In the preceding definition reference to the forgetful functor is often
omitted and the accompanying diagram is often drawn as follows:
 \[\xy
     \qtriangle/{>}`{>}`{-->}/[S`F`A;\iota`f`\wt f_\iota]
   \endxy\]
It certainly looks a lot simpler.  Why do you suppose I opted for the more complicated
version?
\end{exer}

\vskip 5 pt

\begin{prop} If two objects in some concrete category are free on the same set,
then they are isomorphic.
\end{prop}

\vskip 5 pt

\begin{defn} Let $A$ be a subset of a nonempty set~$S$ and $\K$ be either $\R$ or~$\C$. Define
$\sbsb\chi A \colon S \sto \K$, the
 \index{characteristic!function}%
 \index{<@$\sbsb{\chi}{A}$ (characteristic function of $A$)}%
 \index{function!characteristic}%
\df{characteristic function} of~$A$, by
  \[ \sbsb{\chi}{A}(x) = \begin{cases}
                                  1, &\text{if $x \in A$} \\
                                  0, &\text{otherwise}
                              \end{cases} \]
\end{defn}

\begin{exam}\label{ump001z} If $S$ is an arbitrary nonempty set,
then there exists a vector space $V$ over~$\K$ which is free on~$S$.  This vector space
is unique (up to isomorphism).
\end{exam}

\begin{proof}[\emph{Hint for proof}] Given the set $S$ let $V$ be the set of all
$\K$-valued functions on $S$ which have finite support. Define addition and scalar
multiplication pointwise. The map $\iota\colon s \mapsto \sbsb \chi {\{s\}}$ of each
element $s \in S$ to the characteristic function of~$\{s\}$ is the desired injection. To
verify that $V$ is free over $S$ it must be shown that for every vector space $W$ and
every function \hbox{$S~\to^{f}~\abs W$} there exists a unique linear map $V \to^{\wt f}
W$ which makes the following diagram commute.
 \[\xy
 \index{l@$l_2 = l_2(\N)$!square summable sequences}%
 \index{l@$l_2 = l_2(\N)$!as an inner product space}%
 \index{inner product!space!$l_2$ as a}%
     \qtriangle/{>}`{>}`{-->}/[S`\abs V`\abs W;\iota`f`\abs{\wt f}]
     \morphism(1100,500)|r|/{-->}/<0,-500>[V`W;\wt f]
   \endxy\]
\ns \end{proof}

\begin{prop} Every vector space is free.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Of course, part of the problem is to specify a set
$S$ on which the given vector space is free. \ns
\end{proof}


\begin{exam} Let $S$ be a nonempty set and let $S' = S \cup \{\vc 1\}$ where $\vc 1$ is any
element not belonging to~$S$.  A
 \index{word}%
\df{word} in the language~$S$ is a sequence $s$ of elements of $S'$ which is eventually
$\vc 1$ and satisfies: if $s_k = \vc 1$, then $s_{k+1} = \vc 1$.  The constant sequence
$(\vc 1, \vc 1, \vc 1, \dots)$ is called the
 \index{empty word}%
 \index{word!empty}%
\df{empty word}. Let $F$ be the set of all words of members in the language~$S$. Suppose
that $s = (s_1, \dots, s_m, \vc 1, \vc 1, \dots)$ and $t = (t_1, \dots, t_n, \vc 1, \vc
1, \dots)$ are words (where $s_1$, \dots, $s_m$, $t_1$, \dots, $t_n \in S$).  Define
   \[ x \ast y := (s_1, \dots, s_m, t_1, \dots, t_n, \vc 1, \vc 1, \dots). \]
This operation is called
 \index{concatenation}%
\df{concatenation}. It is not difficult to see that the set $F$ under concatenation is a
 \index{monoid}%
monoid (a semigroup with identity) where the empty word is the identity element.  This is the
 \index{free!monoid}%
 \index{monoid!free}%
\df{free monoid} generated by~$S$.  If we exclude the empty word we have the
 \index{free!semigroup}%
 \index{semigroup!free}%
\df{free semigroup} generated by~$S$. The associated diagram is
 \[\xy
     \qtriangle/{>}`{>}`{-->}/[S`\abs F`\abs G;\iota`f`\abs{\tilde f}]
     \morphism(1100,500)|r|/{-->}/<0,-500>[F`G;\tilde f]
   \endxy\]
where $\iota$ is the obvious injection $s \mapsto (s,\vc1, \vc 1, \dots)$ (usually
treated as an inclusion mapping), $G$ is an arbitrary semigroup, $f\colon S \sto G$ is an
arbitrary function, and $\abs{\hphantom{O}}$ is the forgetful functor from the category of monoids and
homomorphisms (or the category of semigroups and homomorphisms) to the category
$\cat{SET}$.
\end{exam}

\begin{exam} Recall from definition~\ref{C015611} the usual presentation of the coproduct of two objects in a category. If
$A_1$ and $A_2$ are two objects in a category $\cat C$, then a
 \index{coproduct}%
\df{coproduct} of $A_1$ and $A_2$ is a triple $(Q,\iota_1,\iota_2)$ with $Q$ an object in
$\cat C$ and $A_k \to^{\iota_k} Q$ ($k = 1,2$) morphisms in $\cat C$ which satisfies the
following condition: if $B$ is an arbitrary object in $\cat C$ and $A_k \to^{f_k} B$ ($k
= 1$,$2$) are arbitrary morphisms in~$\cat C$, then there exists a unique $\cat C$-morphism
$Q \to^f B$ which makes the following diagram commute.
  \begin{equation}
    \xy
      \Atrianglepair/<-`<-`<-`>`<-/[B`A_1`Q`A_2;f_1`f`f_2`\iota_1`\iota_2]
    \endxy
   \end{equation}

It may not be obvious at first glance that this construction is \emph{universal} in the
sense of definition~\ref{00078}.  To see that it in fact is, let $\ftr D$ be the diagonal
functor from a category $\cat C$ to the category of pairs $\cat{C^2}$ (see
example~\ref{0007606}). Suppose that $(Q,\iota_1,\iota_2)$ is a coproduct of the $\cat
C$-objects $A_1$ and $A_2$ in the sense defined above.  Then $A = (A_1,A_2)$ is an object
in $\cat{C^2}$, $A \to^{\iota} \ftr D(Q)$ is a $\cat{C^2}$-morphism, and the pair
$(A,\iota)$ is universal in the sense of~\ref{00078}.  The diagram corresponding to
diagram~\eqref{00078i} is
 \begin{equation}
   \xy
     \qtriangle/{>}`{>}`{-->}/[A`\ftr D(Q)`\ftr D(B);\iota`f`\ftr D(\tilde f)]
     \morphism(1000,500)|r|/{-->}/<0,-500>[Q`B;\tilde f]
   \endxy
 \end{equation}
where $B$ is an arbitrary object in $\cat C$ and (for $k = 1,2$) $A_k \to^{f_k} B$ are
arbitrary $\cat C$-morphisms so that $f = (f_1,f_2)$ is a $\cat{C^2}$-morphism.
\end{exam}

\begin{exam} The coproduct of two objects $H$ and $K$ in the category $\cat{HIL}$ of
 \index{coproduct!in $\cat{HIL}$}%
 \index{hil@$\cat{HIL}$!coproducts in}%
Hilbert spaces and bounded linear maps (and more generally in the category of inner
product spaces and linear maps) is their (external orthogonal) direct sum $H \oplus K$
(see~\ref{0001504}).
\end{exam}


\begin{exam}\label{00078036} Let $A$ and $B$ be Banach spaces. On the Cartesian
 \index{coproduct!in $\cat{BAN_\infty}$}%
 \index{ban@$\cat{BAN_\infty}$!coproducts in}%
product $A \times B$ define addition and scalar multiplication pointwise. For every
$(a,b) \in A \times B$ let $\norm{(a,b)} = \max\{\norm a,\norm b\}$. This makes $A \times
B$ into a Banach space, which is denoted by $A \oplus B$ and is called the
 \index{direct!sum!of Banach spaces}%
\df{direct sum} of $A$ and~$B$.  The direct sum is a coproduct in the topological
category $\cat{BAN_\infty}$ of Banach spaces but not in the corresponding geometrical
category~$\cat{BAN_1}$.
\end{exam}

\begin{exam}\label{00078037} To construct a coproduct on the geometrical category
 \index{coproduct!in $\cat{BAN_1}$}%
 \index{ban@$\cat{BAN_1}$!coproducts in}%
$\cat{BAN_1}$ of Banach spaces define the vector space operations pointwise on $A \times
B$ but as a norm use $\norm{(a,b)}_1 = \norm a + \norm b$ for all $(a,b) \in A \times B$.
\end{exam}

Virtually everything in category theory has a dual concept---one that is obtained by
reversing all the arrows. We can, for example, reverse all the arrows in
diagram~\eqref{00078i}.


\begin{defn}\label{000781} Let $\cat A \to^{\ftr{F}} \cat B$ be a functor between categories
$\cat A$ and $\cat B$ and $B$ be an object in~$\cat B$.  A pair $(A,u)$ with $A$ an
object in $\cat A$ and $u$ a $\cat B$-morphism from $\ftr F(A)$ to~$B$  is a
 \index{co-universal!morphism}%
 \index{morphism!co-universal}%
\df{co-universal morphism for $B$ (with respect to}~$\ftr F$) if for every object $A'$ in
$\cat A$ and every $\cat B$-morphism $\ftr F(A') \to^f B$ there exists a unique $\cat
A$-morphism $A' \to^{\tilde f} A$ such that the following diagram commutes.
 \begin{equation}\label{000781i}
   \xy
     \qtriangle/{<-}`{<-}`{<--}/[B`\ftr F(A)`\ftr F(A');u`f`\ftr F(\tilde f)]
     \morphism(1000,500)|r|/{<--}/<0,-500>[A`A';\tilde f]
   \endxy
 \end{equation}
Some authors reverse the convention and call the morphism in~\ref{00078}
\emph{co-universal} and the one here \emph{universal}.  Other authors, this one included,
call both \emph{universal morphisms}.
\end{defn}

\begin{conv} Morphisms of both the types defined in~\ref{00078} and~\ref{000781} will
 \index{conventions!about universality}%
be referred to as \emph{universal morphisms}.
\end{conv}

\begin{exam}\label{0007812} Recall from definition~\ref{C015511} the categorical approach to products.  If
$A_1$ and $A_2$ are two objects in a category $\cat C$, then a
 \index{product}%
\df{product} of $A_1$ and $A_2$ is a triple $(P,\pi_1,\pi_2)$ with $P$ an object in $\cat
C$ and $Q \to^{\iota_k} A_k$ ($k = 1,2$) morphisms in $\cat C$ which satisfies the
following condition: if $B$ is an arbitrary object in $\cat C$ and $B \to^{f_k} A_k$ ($k
= 1,2$) are arbitrary morphisms in~$\cat C$, then there exists a unique $\cat C$-morphism
$B \to^f P$ which makes the following diagram commute.
  \begin{equation}
    \xy
      \Atrianglepair/>`>`>`<-`>/[B`A_1`P`A_2;f_1`f`f_2`\pi_1`\pi_2]
    \endxy
   \end{equation}
This is (co)-universal in the sense of definition~\ref{000781}.
\end{exam}

\begin{exam} The product of two objects $H$ and $K$ in the category $\cat{HIL}$ of
 \index{product!in $\cat{HIL}$}%
 \index{hil@$\cat{HIL}$!products in}%
Hilbert spaces and bounded linear maps (and more generally in the category of inner
product spaces and linear maps) is their (external orthogonal) direct sum $H \oplus K$
(see~\ref{0001504}).
\end{exam}


\begin{exam} If $A$ and $B$ are Banach algebras their direct sum $A \oplus B$ is a
 \index{product!in $\cat{BA_\infty}$}%
 \index{ba@$\cat{BA_\infty}$!products in}%
 \index{product!in $\cat{BA_1}$}%
 \index{ba@$\cat{BA_1}$!products in}%
product in both the topological and geometric categories, $\cat{BA_\infty}$ and
$\cat{BA_1}$, of Banach algebras.  Compare this to the situation discussed in
examples~\ref{00078036} and~\ref{00078037}.
\end{exam}

Universal constructions in any category whatever produce objects that are unique up to isomorphism.

\begin{prop}\label{C035562} Universal objects in a category are
 \index{universal!object!uniqueness of}%
essentially unique.
\end{prop}















\section{Completion of an Inner Product Space}
A little review of the topic of completion of metric spaces may be useful here.  Occasionally
it may be tempting to think that the completion of a metric space $M$ is a complete metric space
of which some dense subset is homeomorphic to $M$.  The problem with this characterization is that
it may very well produce two ``completions'' of a metric space which are not even homeomorphic.

\begin{exam} Let $M$ be the interval $(0,\infty)$ with its usual metric. There exist two complete
metric spaces $N_1$ and $N_2$ such that
  \begin{enumerate}
    \item $N_1$ and $N_2$ are \emph{not} homeomorphic,
    \item $M$ is homeomorphic to a dense subset of $N_1$, and
    \item $M$ is homeomorphic to a dense subset of~$N_2$.
  \end{enumerate}
\end{exam}

Here is the correct definition.

\begin{defn}\label{C035511} Let $M$ and $N$ be metric spaces.  We say that $N$ is a
 \index{completion!of a metric space}%
 \index{metric!space!completion of a}%
\df{completion} of $M$ if $N$ is complete and $M$ is isometric to a dense subset of~$N$.
\end{defn}

In the construction of the real numbers from the rationals one approach is to treat the
set of rationals as a metric space and define the set of reals to be its completion. One
standard way of producing the completion involves equivalence classes of Cauchy sequences
of rational numbers. This technique extends to metric spaces: an elementary way of
completing an arbitrary metric space starts with defining an equivalence relation on the
set of Cauchy sequences of elements of the space.  Here is a slightly less elementary but
considerably simpler approach.

\begin{prop} Every metric space $M$ is isometric to subspace of~$\fml B(M)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] If $(M,d)$ is a metric space fix $a \in M$. For each
$x \in M$ define $\phi_x\colon M \sto \R$ by $\phi_x(u) = d(x,u) - d(u,a)$.  Show first
that $\phi_x \in \fml B(M)$ for every $x \in M$.  Then show that $\phi\colon M \sto \fml
B(M)$ is an isometry. \ns
\end{proof}

\begin{cor}\label{C035521} Every metric space has a completion.
\end{cor}

The next proposition shows that the completion of a metric space is universal in the sense of
definition~\ref{00078}.  In this result the categories of interest are the category of all metric spaces
and uniformly continuous maps and its subcategory consisting of all complete metric spaces and uniformly
continuous maps.  Here the forgetful functor $\abs{\hphantom{O}}$ forgets only about completeness of objects
(and does not alter morphisms).

\begin{prop}\label{C035557} Let $M$ be a metric space and $\clo M$ be its completion. If $N$
is a complete metric space and $f\colon M \sto N$ is uniformly continuous, then there
exists a unique uniformly continuous function $\clo f\colon \clo M \sto N$ which makes
the following diagram commute.
 \[\xy
     \qtriangle/{>}`{>}`{-->}/[M`\abs{\clo M}`\abs N;\iota`f`\abs{\clo f}]
     \morphism(1100,500)|r|/{-->}/<0,-500>[\clo M`N;\clo f]
   \endxy\]
\end{prop}

The following consequence of propositions~\ref{C035557} and~\ref{C035562} allows us to
speak of \emph{the} completion of a metric space.

\begin{cor}\label{C035564} Metric space completions are unique (up to isometry).
\end{cor}

Now, what about the completion of inner product spaces?  As we have seen, every inner product space is a
metric space; so it has a (metric space) completion.  The question is: Is this completion a Hilbert space?
The answer is, happily, \emph{yes}.

\begin{thm} Let $V$ be an inner product space and $H$ be its metric space completion.  Then there exists
an inner product on $H$ which
  \begin{enumerate}
    \item is an extension of the one on~$V$ and
    \item induces the metric on~$H$.
  \end{enumerate}
\end{thm}




















\endinput
\chapter{THE $\mathbf{\emph{K}_0}$-FUNCTOR}

We now take a brief look at the so-called $K$-theory of $C^*$-algebras.  Perhaps the best introduction to the subject is~\cite{RordamLL:2000},
some of the more elementary parts of which these notes follow rather closely.  Another very readable introduction is~\cite{Wegge-Olsen:1993}.

Given a $C^*$-algebra $A$ we will be interested primarily in two groups known as $K_0(A)$ and~$K_1(A)$.  The present chapter examines
the first of these, a group of projections in $A$ under addition.  Of course if we wish to add arbitrary projections, we have already
encountered a serious difficulty.  We showed earlier (in proposition~\ref{prop_sum_projs}) that in order to add projections they must commute
(and have zero product)!  The solution to this dilemma is typical of the thinking that goes in $K$-theory in general.  If two projections don't
commute, remove the obstruction that prevents them from doing so.  If there isn't enough space for them to get past each other, give them more
room.  Don't insist on regarding them as creatures trying to live in a hopelessly narrow world of $1 \times 1$ matrices of elements of~$A$.
Allow them, for example, to be roam about the much roomier world of $2 \times 2$ matrices.  To accomplish this technically we search for an
equivalence relation $\sim$ that identifies a projection $p$ in $A$ with the matrices $\begin{bmatrix} p & \vc 0 \\ \vc 0 & \vc 0 \end{bmatrix}$
and $\begin{bmatrix} \vc 0 & \vc 0 \\ \vc 0 & p \end{bmatrix}$.  Then the problem is solved: if $p$ and $q$ are projections in~$A$, then
    \[ pq \sim \begin{bmatrix} p & \vc 0 \\ \vc 0 & \vc 0 \end{bmatrix} \begin{bmatrix} \vc 0 & \vc 0 \\ \vc 0 & q \end{bmatrix}  \\
                   = \begin{bmatrix} \vc 0 & \vc 0 \\ \vc 0 & \vc 0 \end{bmatrix}           \\
                   =  \begin{bmatrix} \vc 0 & \vc 0 \\ \vc 0 &  q \end{bmatrix}\begin{bmatrix} p & \vc 0 \\ \vc 0 & \vc 0 \end{bmatrix}  \\
                   \sim qp\,. \]
Now $p$ and $q$ commute modulo the equivalence relation $\sim$ and we may define addition by something like
$p \oplus q = \begin{bmatrix} p & \vc 0 \\ \vc 0 & q \end{bmatrix}$.

Pretty clearly, the preceding simple-minded device is not going to produce anything like an Abelian group, but it's a start.  If you suspect
that, in the end, the construction of $K_0(A)$ may be a bit complicated, you are entirely correct.




\section{Equivalence Relations on Projections}

We define several equivalence relations, all of which are appropriate to projections in a unital $C^*$-algebra.

\begin{defn} In a unital algebra elements $a$ and $b$ are
 \index{similar!elements of an algebra}%
\df{similar} if there exists an invertible element $s$ such that $b = sas^{-1}$.
 \index{<binrelequivs@$a \sim_s b$ (similarity of elements of an algebra)}%
In this case we write $a \sim_s b$.
\end{defn}

\begin{prop} The relation $\sim_s$ of similarity defined above is an equivalence relation on the elements of a unital algebra.
\end{prop}

\begin{defn} In a $C^*$-algebra elements $a$ and $b$ are
 \index{unitary!equivalence}%
\df{unitarily equivalent} if there exists an unitary element $u \in \wt A$ such that $b = uau^*$.
 \index{<binrelequivu@$a \sim_u b$ (unitary equivalence)}%
In this case we write $a \sim_u b$.
\end{defn}

\begin{prop} The relation $\sim_u$ of unitary equivalence defined above is an equivalence relation on the elements of a $C^*$-algebra.
\end{prop}

\begin{prop}\label{0060224fa} Elements $a$ and $b$ of a unital $C^*$-algebra $A$ are unitarily equivalent if and only if
there exists an element $u \in \ofml U(A)$ such that $b = uau^*$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Suppose there exists $v \in \ofml U(\wt A)$ such that $b = vav^*$.  By proposition~\ref{0015213}
there exist $u \in A$ and $\alpha \in \C$ such that $v = u + \alpha \vc j$ (where $\vc j = \vc 1_{\wt A} - \vc 1_A$).  Show
that $\abs\alpha = 1$, $u$ is unitary, and $b = uau^*$.

For the converse suppose that there exists $u \in \ofml U(A)$ such that $b = uau^*$. Let $v = u + \vc j$.  \ns
\end{proof}

\begin{prop}[Polar decomposition]\label{0060141} For every invertible element $s$ of a
 \index{polar decomposition!of an invertible $C^*$-algebra element}%
 \index{decomposition!polar}%
unital $C^*$-algebra, then there exists a unitary element $\omega(s)$ in the algebra such that
   \[   s = \omega(s) \abs s\,. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use Corollary~\ref{00183313}.   \ns
\end{proof}

\begin{prop}\label{K002021} If $A$ is a unital $C^*$-algebra, then the function $\omega\colon \inv A \sto \ofml U(A)$
defined in the preceding proposition is continuous.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use propositions~\ref{000646fa} and~\ref{001449} .   \ns
\end{proof}

\begin{prop}\label{K002024} Let $s = u\abs s$ be the polar decomposition of an invertible element $s$ in a unital $C^*$-algebra~$A$.
Then $u \sim_h s$ in $\inv A$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For $0 \le t \le 1$ let $c_t = u(t\abs s + (1 - t)\vc 1)$.  Conclude from
proposition~\ref{0018301} that there exists $\epsilon \in (0,1]$ such that $\abs s \ge \epsilon \vc 1$.  Use the same
proposition to show that $t\abs s + (1 - t)\vc 1$ is invertible for every $t \in [0,1]$.   \ns
\end{proof}

\begin{prop}\label{K002027} Let $u$ and $v$ be unitary elements in a unital $C^*$-algebra~$A$.  If $u \sim_h v$ in $\inv A$,
then $u \sim_h v$ in~$\ofml U(A)$.
\end{prop}

\begin{prop}\label{K002031} Let $u_1$, $u_2$, $u_3$, and $u_4$ be unitary elements in a unital $C^*$-algebra~$A$.
If $u_1 \sim_h u_2$ in $\ofml U(A)$ and $u_3 \sim_h u_4$ in $\ofml U(A)$, then $u_1u_3 \sim_h u_2u_4$ in~$\ofml U(A)$.
\end{prop}

\begin{exam}\label{0060116} Let $h$ be a self-adjoint element of a unital $C^*$-algebra~$A$.  Then $\exp(ih)$ is unitary and
homotopic to~$\vc 1$ in~$\ofml U(A)$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Consider the path $c\colon [0,1] \sto \T\colon t \mapsto \exp(ith)$.  Use
proposition~\ref{001449}.  \ns
\end{proof}

\begin{notn} If $p$ and $q$ are projections in a $C^*$-algebra $A$, we write $p \sim q$
 \index{Murray-von Neumann equivalence}%
 \index{equivalence!Murray-von Neumann}%
 \index{<binrelequiv@$p \sim q$ (Murray-von Neumann equivalence)}%
($p$ is \df{Murray-von Neumann equivalent} to~$q$) if there exists an element $v \in A$
such that $v^*v = p$ and $vv^* = q$.  Note that such a $v$ is automatically a partial
isometry.  We will refer to it as a
 \index{implementation of Murray-von Neumann equivalence}%
 \index{equivalence!Murray-von Neumann!implementation of}%
 \index{Murray-von Neumann equivalence!implementation of}%
\emph{partial isometry that implements the equivalence}.
\end{notn}

\begin{prop}\label{0060216} The relation $\sim$ of Murray-von Neumann equivalence is an
equivalence relation on the family $\fml P(A)$ of projections in a $C^*$-algebra~$A$.
\end{prop}

\begin{prop}\label{0060144} Let $a$ and $b$ be self-adjoint elements of a unital $C^*$-algebra.  If $a \sim_s b$, then $a \sim_u b$.
In fact, if $b = sas^{-1}$ and $s = u\abs s$ is the polar decomposition of $s$, then $b = uau^*$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Suppose there is an invertible element $s$ such that $b = sas^{-1}$.  Let $s = u\abs s$
be the polar decomposition of~$s$.  Show that $a$ commutes with ${\abs s}^2$ and therefore with anything in the algebra $C^*(\vc 1,{\abs s}^2)$.
In particular, $a$ commutes with~${\abs s}^{-1}$.  From this it follows that $uau^* = b$.   \ns
\end{proof}

\begin{prop}\label{0060221} If $p$ and $q$ are projections in a $C^*$-algebra, then
   \[ p \sim_h q \implies p \sim_u q \implies p \sim q\,. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] In this hint $\vc 1 = \vc 1_{\wt A}$.  For the first implication show that there is no loss
of generality in supposing that $\norm{p - q} < \frac12$.  Let $s = pq + (\vc 1 - p)(\vc 1 - q)$.  Prove that $p \sim_s q$.  To
this end write $s - \vc 1$ as $p(q - p) + (\vc 1 - p)((\vc 1 - q) - (\vc 1 - p))$ and use corollary~\ref{cor_Neumann_series}.
Then use proposition~\ref{0060144}.

To prove the second implication notice that if $upu^* = q$ for some unitary element in~$\wt A$, then $up$ is a partial isometry
in~$A$.  \ns
\end{proof}

In general, the converse of the second implication, $p \sim_u q \implies p \sim q$, in the preceding proposition does not hold (see
example~\ref{0060233} below).  However, for projections $p$ and $q$ in a unital $C^*$-algebra, if we have both $p \sim  q$ and
$\vc 1 - p \sim \vc 1 - q$, then we can conclude $p \sim_u q$.

\begin{prop}\label{0060224} Let $p$ and $q$ be projections in a unital $C^*$-algebra~$A$.  Then $p \sim_u q$ if and only if
$p \sim q$ and $\vc 1 - p \sim \vc 1 - q$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For the converse suppose that a partial isometry $v$ implements the equivalence $p \sim q$
and $w$ implements $\vc 1 - p \sim \vc 1 - q$.  Consider the element $u = v + w + \vc j$ in~$\wt A$.   \ns
\end{proof}

\begin{defn} An element $s$ of a unital $C^*$-algebra is an
 \index{isometry!in a unital $C^*$-algebra}%
\df{isometry} if $s^*s = \vc 1$.
\end{defn}

\begin{exer} Explain why the terminology in the preceding definition is reasonable.
\end{exer}

It is not difficult to see that the converse of the second implication in proposition~\ref{0060221} fails in general.

\begin{exam}\label{0060233} If $p$ and $q$ are projections in a $C^*$-algebra, then
   \[ p \sim q \,\,\,\,\not\!\!\!\!\implies p \sim_u q\,. \]
For example, if $s$ is a nonunitary isometry (such as the unilateral shift), then $s^*s \sim ss^*$, but $s^*s \nsim_u ss^*$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Prove, and keep in mind, that no nonzero projection can be Murray-von Neumann equivalent
to the zero projection.   \ns
\end{proof}

\begin{rem} The converse of the first implication, $p \sim_h q \implies p \sim_u q$, in proposition~\ref{0060221} also does not hold
in general for projections in a $C^*$-algebra.  However, an example illustrating this phenomenon is not easy to come by. To see
what is involved consult~\cite{RordamLL:2000}, examples 2.2.9 and 11.3.4.
\end{rem}

It would be nice if the implications in proposition~\ref{0060221} were reversible.  But as we have seen, they are not.
One way of dealing with recalcitrant facts, as was noted at the beginning of this chapter, is to give the mathematical objects
we are dealing with more room to move around in.  Pass to matrices.  Propositions~\ref{0060236} and~\ref{0060244} are examples
of how this technique works.  We can, in a sense, get the implications in~\ref{0060221} to reverse.

\begin{notn} If $a_1$, $a_2$, \dots $a_n$ are elements of a $C^*$-algebra $A$, then
 \index{diagonal@$\diag(a_1,\dots,a_n)$ (diagonal matrix in $\M nA$)}%
$\diag(a_1,a_2,\dots,a_n)$ is the diagonal matrix in $\M nA$ whose main diagonal consists
of the elements $a_1$, \dots, $a_n$. We also use this notation for block matrices.  For
example if $a$ is an $m \times m$ matrix and $b$ is an $n \times n$ matrix, then
$\diag(a,b)$ is the $(m+n) \times (m+n)$ matrix $\begin{bmatrix} a & \vc 0 \\ \vc 0 & b
\end{bmatrix}$.  (Of course, the $\vc 0$ in the upper right corner of this matrix is the $m \times n$ matrix all of whose
entries are zero and the $\vc 0$ in the lower left corner is the $n \times m$ matrix with each entry zero.)
\end{notn}

\begin{prop}\label{0060236} Let $p$ and $q$ be projections in a $C^*$-algebra~$A$. Then
   \[ p \sim q \implies \diag(p,\vc 0) \sim_u \diag(q,\vc 0) \text{ in $\M 2A$.} \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $v$ be a partial isometry in $A$ that implements the Murray-von Neumann equivalence $p \sim q$.
Consider the matrix $u = \begin{bmatrix} v  &  \vc 1_{\wt A} - q  \\  \vc 1_{\wt A} - p  &  v^* \end{bmatrix}$.  \ns
\end{proof}

Recall from corollary~\ref{001311005fa} that the spectrum of any unitary element of a unital $C^*$-algebra lies in the unit circle.
Remarkably, if its spectrum is not the entire circle, then it is homotopic to~$\vc 1$ in~$\ofml U(A)$.

\begin{prop}\label{0060121} Let $u$ be a unitary element in a unital $C^*$-algebra. If $\sigma(u) \ne \T$, then $u \sim_h \vc 1$ in~$\ofml U(A)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Choose $\theta \in \R$ such that $\exp(i\theta) \notin \sigma(u)$.  Then there is a (unique continuous) function
   \[ \phi\colon \sigma(u) \sto (\theta, \theta + 2\pi)\colon \exp(it) \mapsto t\,. \]
Let $h = \phi(u)$ and use example~\ref{0060116}.   \ns
\end{proof}

\begin{exam}\label{0060129a} If $A$ is a unital $C^*$-algebra, then $\begin{bmatrix} \vc 0  &  \vc 1  \\  \vc 1  &  \vc 0  \end{bmatrix} \sim_h
\begin{bmatrix} \vc 1  &  \vc 0  \\  \vc 0  &  \vc 1  \end{bmatrix}$ in~$\ofml U(\M 2A)$.
\end{exam}

The matrix $J = \begin{bmatrix} \vc 0  &  \vc 1  \\  \vc 1  &  \vc 0  \end{bmatrix}$ is very useful when used in conjunction
with proposition~\ref{K002031} in establishing homotopies between matrices in $\ofml U(A)$.  In verifying the next few examples
it is quite helpful.  Notice that multiplication of a $2 \times 2$ matrix on the right by $J$ interchanges its columns; multiplication
on the left by $J$ interchanges rows; and multiplication on both left and right by $J$ interchanges elements on both diagonals.

\begin{exam}\label{0060129b} If $u$ and $v$ are unitary elements in a unital $C^*$-algebra~$A$, then
   \[ \diag(u,v) \sim_h \diag(v,u) \]
in~$\ofml U(\M 2A)$.
\end{exam}

\begin{exam}\label{0060129c} If $u$ and $v$ are unitary elements in a unital $C^*$-algebra $A$, then
  \[ \diag(u,v) \sim_h \diag(uv, \vc 1) \]
in $\ofml U(\M 2A)$.
\end{exam}

\begin{exam}\label{0060129d}  If $u$ and $v$ are unitary elements in a unital $C^*$-algebra~$A$, then
   \[ \diag(uv,\vc 1) \sim_h \diag(vu, \vc 1) \]
in~$\ofml U(\M 2A)$.
\end{exam}

\begin{prop}\label{0060244} Let $p$ and $q$ be projections in a $C^*$-algebra~$A$. Then
   \[ p \sim_u q \implies \diag(p,\vc 0) \sim_h \diag(q,\vc 0) \text{ in $\ofml P(\M 2A)$.} \]
\end{prop}

\begin{exam}\label{0060247fa} Two projections in $\mathbf M_n$ are Murray-von Neumann equivalent if and only if they have equal traces,
which is, in turn, equivalent to their having ranges of equal dimension.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Use propositions~\ref{tr0121}--\ref{tr0124} and~\ref{pi0117}--\ref{pi0121}.   \ns
\end{proof}

In example~\ref{000533a} we introduced the $n \times n$ matrix algebra $\M nA$ where $A$ is an algebra.  If $A$ is a
$*\,$-algebra and $n \in \N$ we can equip $\M nA$ with an involution in a natural way.  It is defined, as you would expect,
as the analog of ``conjugate transposition'': if $\vc a \in \M nA$, then
   \[ \vc a^* = \bigl[a_{ij}\bigr]^* := \bigl[{a_{ji}}^*\bigr]. \]

If $A$ is a $C^*$-algebra we can introduce a norm on $\M nA$ under which it also is a $C^*$-algebra.
 \index{Ma@$\M nA$!as a $C^*$-algebra}%
 \index{C*@$C^*$-algebra!$\M nA$ as a}%
Choose a faithful representation $\phi\colon A \sto \ofml B(H)$ of $A$, where $H$ is a Hilbert space.  For each $n \in \N$
define
   \[ \phi_n\colon \M nA \sto \ofml B(H^n)\colon \bigl[a_{ij}\bigr] \mapsto \bigl[ \phi(a_{ij})\bigr] \]
where $H^n$ is the $n$-fold direct sum of $H$ with itself.  Then define the norm of an element $\vc a \in \M nA$ to be the
norm of the operator $\phi_n(\vc a) \in \ofml B(H^n)$.  That is, $\norm{\vc a} := \norm{\phi_n(\vc a)}$.

This norm is a $C^*$-norm on $\M nA$.  It does not depend on the particular representation we choose by virtue of the
uniqueness of $C^*$-norms (see corollary~\ref{00152144}).

\begin{prop} Let $A$ be a $C^*$-algebra and $\vc a \in \M nA$. Then
   \[ \max\{\norm{a_{ij}}\colon 1 \le i,j \le n\} \le \norm{\vc a} \le \sum_{i,j = 1}^n \norm{a_{ij}}. \]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Since any faithful representation is an isometry (see proposition~\ref{00152181})
you may as well simplify things by regarding the representation $\phi\colon A \sto \ofml B(H)$ discussed above as an
inclusion map.

For the first inequality define, for $v \in H$, the vector $\vc E_j(v) \in H^n$ to be the $n$-tuple
all of whose entries are zero except for the $j^{\text{th}}$ one which is~$v$.  Then verify that
   \[ \norm{a_{ij}v} \le \norm{\vc a\vc E_j(v)} \le \norm{\vc a} \]
whenever $\vc a \in \M nA$, $v \in H$, and $\norm v \le 1$.

For the second inequality show that
   \[ {\norm{\vc a\vc v}}^2 \le \sum_{i=1}^n\biggl(\sum_{j=1}^n \norm{a_{ij}}\biggr)^2 \le \biggl(\sum_{i=1}^n\sum_{j=1}^n \norm{a_{ij}}\biggr)^2 \]
whenever $\vc a \in \M nA$, $\vc v \in H^n$, and $\norm{\vc v} \le 1$.    \ns
\end{proof}


























\section{A Semigroup of Projections}

\begin{notn} When $A$ is a $C^*$-algebra and $n \in \N$ we let
 \index{P@$\fml P_n(A)$, $\fml P_\infty(A)$ (projections in matrix algebras)}%
  \begin{align*}
       \fml P_n(A) &= \fml P(\M nA) \quad\text{ and} \\
       \fml P_\infty(A) &= \bigcup_{n=1}^\infty \fml P_n(A).
  \end{align*}
\end{notn}

We now extend Murray-von Neumann equivalence to matrices of different sizes.

\begin{notn} If $A$ is a $C^*$-algebra let
 \index{Ma@$\M {n,m}A$ (matrices with entries from~$A$)}%
$\M {n,m}A$ denote the set of $n \times m$ matrices with entries belonging to~$A$.
\end{notn}

Next we extend Murray-von Neumann equivalence to projection matrices which are not necessarily of the same size.

\begin{defn}\label{0060316fa} If $A$ is a $C^*$-algebra, $p \in \fml P_m(A)$, and $q \in \fml P_n(A)$, we set
 \index{Murray-von Neumann equivalence}%
 \index{equivalence!Murray-von Neumann}%
 \index{<binrelequiv@$p \sim q$ (Murray-von Neumann equivalence)}%
$p \sim q$ if there exists $v \in \M {n,m}A$ such that
  \[ v^*v = p \qquad\text{ and } \qquad  vv^* = q. \]
We will extend the use of the name \emph{Murray-von Neumann equivalence} to this new relation on~$\fml P_\infty(A)$.
\end{defn}

\begin{prop}\label{0060317} The relation $\sim$ defined above is an equivalence
relation on~$\fml P_\infty(A)$.
\end{prop}

\begin{defn} For each $C^*$-algebra $A$ we define a binary operation $\oplus$ on $\fml
P_\infty(A)$ by
   \[ p \oplus q = \diag(p,q)\,. \]
Thus if $p \in \fml P_m(A)$ and $q \in \fml P_n(A)$, then $p \oplus q \in \fml
P_{m+n}(A)$.
\end{defn}

In the next proposition $\vc 0_n$ is the additive identity in~$\fml P_n(A)$.

\begin{prop}\label{0060324} Let $A$ be a $C^*$-algebra and $p \in \fml P_\infty(A)$.
 \index{<sum@$p \oplus q$ (binary operation in $\fml P_\infty(A)$)}%
Then $p \sim p \oplus \vc 0_n$ for every $n \in \N$.
\end{prop}

\begin{prop}\label{0060327} Let $A$ be a $C^*$-algebra and $p$, $p'$, $q$, $q' \in
\fml P_\infty(A)$.  If $p \sim p'$ and $q \sim q'$, then $p \oplus q \sim p' \oplus q'$.
\end{prop}

\begin{prop}\label{0060331} Let $A$ be a $C^*$-algebra and $p$, $q \in \fml P_\infty(A)$.
Then $p \oplus q \sim q \oplus p$.
\end{prop}

\begin{prop}\label{0060334} Let $A$ be a $C^*$-algebra and $p$, $q \in \fml P_n(A)$ for
some $n$.  If $p \perp q$, then $p + q$ is a projection in $\M nA$ and $p + q \sim p \oplus q$.
\end{prop}

\begin{prop}\label{0060337} If $A$ is a $C^*$-algebra, then $\fml P_\infty(A)$ is a
commutative semigroup under the operation~$\oplus$.
\end{prop}

\begin{notn}\label{0060411} If $A$ is a $C^*$-algebra and $p \in \fml P_\infty(A)$, let
 \index{<bracsqr@$\sbsb{[p]}{\fml D}$ ($\sim$ equivalence class of $p$)}%
$\sbsb{[p\,]}{\fml D}$ be the equivalence class containing $p$ determined by the
equivalence relation~$\sim$. Also let
 \index{D@$\fml D(A)$ (set of equivalence classes of projections)}%
$\fml D(A) := \{\,\sbsb{[p\,]}{\fml D}\colon p \in \fml P_\infty(A)\,\}$.
\end{notn}

\begin{defn}\label{0060414} Let $A$ be a $C^*$-algebra. Define a binary operation $+$ on
$\fml D(A)$ by
 \index{<binop@$\sbsb{[p\,]}{\fml D} + \sbsb{[q\,]}{\fml D}$ (addition in $\fml D(A)$)}%
   \[ \sbsb{[p\,]}{\fml D} + \sbsb{[q\,]}{\fml D} := \sbsb{[p \oplus q\,]}{\fml D} \]
where $p$, $q \in \fml P_\infty(A)$.
\end{defn}

\begin{prop}\label{0060417} The operation $+$ defined in~\ref{0060414} is well defined
and makes $\fml D(A)$ into a commutative semigroup.
\end{prop}

\begin{exam}\label{0060421} The semigroup $\fml D(\C)$ is isomorphic to the additive group of positive integers; that is,
    \[ \fml D(\C) \cong \Z^+ = \{0,1,2,\dots\}\,.  \]
\end{exam}

\begin{proof}[\emph{Hint for proof}] Use example~\ref{0060247fa}. \ns
\end{proof}

\begin{exam}\label{0060424} If $H$ is a Hilbert space, then
   \[ \fml D(\ofml B(H)) \cong \Z^+ \cup \{\infty\}. \]
(Use the usual addition on $\Z^+$ and let $n + \infty = \infty + n = \infty$ whenever $n
\in \Z^+ \cup \{\infty\}$.)
\end{exam}

\begin{exam}\label{0060427} For the $C^*$-algebra $\C \oplus \C$ we have
   \[ \fml D(\C \oplus \C) \cong \Z^+ \oplus \Z^+\,. \]
\end{exam}


























\section{The Grothendieck Construction}
In the construction of the field of real numbers we may use the same technique to get from the (additive semigroup of) natural numbers to
the (additive group of all) integers as we do to get from the (multiplicative semigroup of) nonzero integers to the (multiplicative group
of all) nonzero rational numbers.  It is called the \emph{Grothendieck construction}.

\begin{defn}\label{0062111} Let $(S,+)$ be a commutative semigroup. Define a relation
 \index{<binrelequiv@$(a,b) \sim (c,d)$ (Grothendieck equivalence)}%
$\sim$ on $S \times S$ by
   \[ (a,b) \sim (c,d) \text{ if there exists $k \in S$ such that } a + d + k = b + c + k. \]
\end{defn}

\begin{prop}\label{0062114} The relation $\sim$ defined above is an equivalence relation.
\end{prop}

\begin{notn} For the equivalence relation $\sim$ defined in~\ref{0062111} the equivalence
class containing the pair $(a,b)$ will be denote by
 \index{<bracpnt@$\langle a,b \rangle$ (pair in Grothendieck construction)}%
$\langle a,b \rangle$ rather than by~$[(a,b)]$.
\end{notn}

\begin{defn} Let $(S,+)$ be a commutative semigroup. On $G(S)$ define a binary operation
 \index{addition!on the Grothendieck group}%
(also denoted by $+$) by:
  \[ \langle a,b \rangle + \langle c,d \rangle := \langle a + c,b + d\rangle\,. \]
\end{defn}

\begin{prop}\label{0062119} The operation $+$ defined above is well defined and under
this operation $G(S)$ becomes and Abelian group.
\end{prop}

The Abelian group $(G(S),+)$ is called the
 \index{G@$G(S)$ (Grothendieck group of $S$)}%
 \index{Grothendieck!group}%
 \index{group!Grothendieck}%
\df{Grothendieck group} of~$S$.

\begin{prop}\label{0062124} For a semigroup $S$ and an arbitrary $a \in S$ define a
mapping
 \index{gamma@$\sbsb{\gamma}S$ (Grothendieck map)}%
   \[ \sbsb{\gamma}S \colon S \sto G(S) \colon s \mapsto \langle s + a, a \rangle\,. \]
The mapping $\sbsb{\gamma}S$, called the
 \index{Grothendieck!map}%
\df{Grothendieck map}, is well defined and is a semigroup homomorphism.
\end{prop}

Saying that the Grothendieck map is well defined means that its definition is independent
of the choice of~$a$.  We frequently write just $\gamma$ for~$\sbsb{\gamma}S$.

\begin{exam}\label{0062126} Both $\N = \{1,2,3,\dots\}$ and $\Z^+ = \{0,1,2,\dots\}$ are
commutative semigroups under addition.  They generate the same Grothendieck group
   \[G(\N) = G(\Z^+) = \Z\,. \]
\end{exam}

Nothing guarantees that the Grothendieck group of an arbitrary semigroup will be of much interest.

\begin{exam}\label{0062127} Let $S$ be the commutative additive semigroup $\Z^+ \cup \{\infty\}$.  Then  $G(S) = \{\vc 0\}$.
\end{exam}

\begin{exam}\label{0062128} Let $\Z_0$ be the (commutative) multiplicative semigroup of nonzero
integers.  Then  $G(\Z_0) = \Q_0$, the Abelian multiplicative group of nonzero rational
numbers.
\end{exam}

\begin{prop}\label{0062141} If $S$ is a commutative semigroup, then
   \[ G(S) = \{\gamma(s) - \gamma(t)\colon s,t \in S\}\,. \]
\end{prop}

\begin{prop}\label{0062142} If $r$, $s \in S$, where $S$ is a commutative semigroup, then $\gamma(r) = \gamma(s)$ if and only if
there exists $t \in S$ such that $r + t = s + t$.
\end{prop}

\begin{defn} A commutative semigroup $S$ has the
 \index{cancellation property}%
\df{cancellation property} if whenever $r$, $s$, $t \in S$ satisfy $r + t = s + t$, then $r = s$.
\end{defn}

\begin{cor}\label{0062144} Let $S$ be a commutative semigroup. The Grothendieck map $\sbsb\gamma S\colon S \sto G(S)$ is injective
if and only if $S$ has the cancellation property.
\end{cor}

The next proposition asserts the universal property of the Grothendieck group.

\begin{prop}\label{0062151} Let $S$ be a commutative (additive) semigroup and $G(S)$ be its Grothendieck group.
 \index{universal!property!of the Grothendieck map}%
 \index{Grothendieck!map!universal property of}%
If $H$ is an Abelian group and $\phi\colon S \sto H$ is an additive map, then there
exists a unique group homomorphism $\psi\colon G(S) \sto H$ such that the following
diagram commutes.
 \begin{equation}\label{0062151i}
   \xy
     \qtriangle/{>}`{>}`{-->}/[S`\abs{G(S)}`\abs{H};\gamma`\phi`\abs{\psi}]
     \morphism(1100,500)|r|/{-->}/<0,-500>[G(S)`H;\psi]
   \endxy
 \end{equation}
\end{prop}

In the preceding diagram $\abs{G(S)}$ and $\abs{H}$ are just $G(S)$ and $H$ regarded as
semigroups and $\abs{\psi}$ is the corresponding semigroup homomorphism.  In other words,
the forgetful functor $\abs{\hphantom{G}}$ ``forgets'' only about identities and inverses but not about
the operation of addition. Thus the triangle on the left is a commutative diagram in the
category of semigroups and semigroup homomorphisms.

\begin{prop}\label{0062154} Let $\phi\colon S \sto T$ be a homomorphism of commutative
semigroups. Then the map $\sbsb{\gamma}T \circ \phi\colon S \sto G(T)$ is additive. By
proposition~\ref{0062151} there exists a unique group homomorphism $G(\phi)\colon G(S)
\sto G(T)$ such that the following diagram commutes.
   \[ \xy
          \Square[S`T`G(S)`G(T);\phi`\sbsb{\gamma}S`\sbsb{\gamma}T`G(\phi)]
      \endxy \]
\end{prop}

\begin{prop}\label{0062155} The pair of maps $S \mapsto G(S)$, which takes commutative semigroups to their corresponding
 \index{functor!Grothendieck}%
 \index{Grothendieck!construction!functorial property of}%
Grothendieck groups, and $\phi \mapsto G(\phi)$, which takes semigroup homomorphisms to
group homomorphism (as defined in~\ref{0062154}) is a covariant functor from the category
of commutative semigroups and semigroup homomorphisms to the category of Abelian groups
and group homomorphisms.
\end{prop}

One slight advantage of the rather pedantic inclusion of a forgetful functor in
diagram~\eqref{0062151i} is that it makes it possible to regard the Grothendieck map
$\gamma\colon S \mapsto \sbsb{\gamma}S$ as a natural transformation of functors.

\begin{cor}[Naturality of the Grothendieck map]\label{0062157} Let $\abs{\hphantom{G}}$ be the forgetful
functor on Abelian groups which ``forgets'' about identities and inverses but not the
group operation as in~\ref{0062151}. Then $\abs{G(\hphantom{S})}$ is a covariant functor from the
category of commutative semigroups and semigroup homomorphisms to itself.
 \index{Grothendieck!map!naturality of}%
 \index{natural!transformation!Grothendieck map is a}%
Furthermore, the Grothendieck map $\gamma\colon S \mapsto \sbsb{\gamma}S$ is a natural
transformation from the identity functor to the functor~$\abs{G(\hphantom{S})}$.
  \[ \xy
          \Square[S`T`\abs{G(S)}`\abs{G(T)};\phi`\sbsb{\gamma}S`\sbsb{\gamma}T`\abs{G(\phi)}]
      \endxy \]
\end{cor}





\section{The $\mathbf{\emph{K}_0}$-Group for Unital $C^*$-Algebras}

\begin{defn} Let $A$ be a unital $C^*$-algebra. Let
 \index{K0@$K_0(A)$!for unital $A$}%
$K_0(A) := G(\fml D(A))$, the Grothendieck group of the semigroup $\fml D(A)$ defined in~\ref{0060411} and~\ref{0060414},
and define
 \index{<bracsqr@$[p\,]$ (image under the Grothendieck map of $[p\,]_{\fml D}$)}%
   \[ [\hphantom{P}]\colon \fml P_\infty(A) \sto K_0(A)
           \colon p \mapsto \sbsb{\gamma}{\fml D(A)}\bigl([p\,]_{\fml D}\bigr)\,. \]
\end{defn}

The next proposition is an obvious consequence of this definition.

\begin{prop}\label{0062188fa} Let $A$ be a unital $C^*$-algebra and $p$, $q \in \fml P_\infty(A)$.  If $p$ and $q$ are
Murray-von Neumann equivalent, then $[p\,] = [q\,]$ in~$K_0(A)$.
\end{prop}

\begin{defn} Let $A$ be a unital $C^*$-algebra and $p$, $q \in \fml P_\infty(A)$. We say that $p$ is
 \index{equivalence!stable}%
 \index{stable equivalence}%
\df{stably equivalent} to $q$ and write
 \index{<binrelequivst@$p \sim_{st} q$ (stable equivalence)}%
$p \sim_{st} q$ if there exists a projection $r \in \fml P_\infty(A)$ such that $p \oplus r \sim q \oplus r$.
\end{defn}

\begin{prop} Stable equivalence $\sim_{st}$ is an equivalence relation on $\fml P_\infty(A)$.
\end{prop}

\begin{prop} Let $A$ be a unital $C^*$-algebra and $p$, $q \in \fml P_\infty(A)$.  Then $p
\sim_{st} q$ if and only if $p \oplus \vc 1_n \sim q \oplus \vc 1_n$ for some $n \in \N$.
\end{prop}

Here, of course, $\vc 1_n$ is the multiplicative identity in $\M nA$.

\begin{prop}[Standard Picture of $K_0(A)$ when $A$ is unital]\label{0062224} If $A$ is a
unital $C^*$-algebra, then
 \index{standard!picture of $K_0(A)$!when $A$ is unital}%
 \index{picture (\seeonly{standard picture}))}%
   \begin{align*}
         K_0(A) &= \{ [p\,] - [q\,]\colon p,q \in \fml P_\infty(A)\}  \\
                &= \{ [p\,] - [q\,]\colon  p,q \in \fml P_n(A) \text{ for some $n \in \N$}\}\,.
   \end{align*}
\end{prop}

\begin{prop} Let $A$ be a unital $C^*$-algebra and $p$, $q \in \fml P_\infty(A)$. Then $[p
\oplus q\,] = [p\,] + [q\,]$.
\end{prop}

\begin{prop} Let $A$ be a unital $C^*$-algebra and $p$, $q \in \fml P_n(A)$. If $p \sim_h q$
in $\fml P_n(A)$, then $[p\,] = [q\,]$.
\end{prop}

\begin{prop} Let $A$ be a unital $C^*$-algebra and $p$, $q \in \fml P_n(A)$. If $p \perp q$
in $\fml P_n(A)$, then $p + q \in \fml P_n(A)$ and $[p + q\,] = [p\,] + [q\,]$.
\end{prop}

\begin{prop} Let $A$ be a unital $C^*$-algebra and $p$, $q \in \fml P_\infty(A)$. Then
$[p\,] = [q\,]$ if and only if $p \sim_{st} q$.
\end{prop}

 The next proposition specifies a universal property of $K_0(A)$ when $A$ is unital.  In it the forgetful functor $\abs{\hphantom{G}}$
 is the one described in proposition~\ref{0062151}.

\begin{prop}\label{0062244} Let $A$ be a unital $C^*$-algebra, $G$ be an Abelian group, and $\nu\colon \fml P_\infty(A) \sto \abs G$
be a semigroup homomorphism that satisfies
 \index{universal!property!of $K_0$ (unital case)}%
 \index{K0@$K_0$!universal property of}%
 \begin{enumerate}[label=\rm{(\alph*)}]
    \item $\nu(\vc 0_A) = \vc 0_G$ and
    \item if $p \sim_h q$ in $\fml P_n(A)$ for some $n$, then $\nu(p) = \nu(q)$.
 \end{enumerate}
Then there exists a unique group homomorphism $\wt\nu\colon K_0(A) \sto G$ such that the following diagram commutes.
   \[\xy
      \qtriangle/{>}`{>}`{-->}/<1000,700>[\fml P_\infty(A)`\abs{K_0(A)}`\abs G;{[\hphantom{P}]}`\nu`\abs{\wt\nu}]
      \morphism(1600,700)|r|/{-->}/<0,-700>[K_0(A)`G;\wt\nu]
     \endxy\]
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $\tau\colon \fml D(A) \sto K_0(A)\colon \sbsb{[p\,]}{\fml D} \mapsto \nu(p)$.
Verify that $\tau$ is well-defined and that it is a semigroup homomorphism. Then use proposition~\ref{0062151}.  \ns
\end{proof}

\begin{defn} A $*\,$-homomorphism $\phi\colon A \sto B$ between $C^*$-algebras extends, for
every $n \in \N$, to a $*\,$-homomorphism $\phi\colon \M nA \sto \M nB$ and also (since $*\,$-homomorphisms take projections
to projections) to a $*\,$-homomorphism $\phi$ from $\fml P_\infty(A)$ to $\fml P_\infty(B)$.  For such a $*\,$-homomorphism
$\phi$ define
   \[ \nu\colon \fml P_\infty(A) \sto K_0(B)\colon p \mapsto [\phi(p)]\,. \]
Then $\nu$ is a semigroup homomorphism  satisfying conditions (a) and (b) of proposition~\ref{0062244} according to which there
exists a unique group homomorphism
 \index{K0@$K_0(\phi)$!unital case}%
$K_0(\phi)\colon K_0(A) \sto K_0(B)$ such that $K_0(\phi)\bigl([p\,]\bigr) = \nu(p)$ for every $p \in \fml P_\infty(A)$.
\end{defn}

\begin{prop} The pair of maps $A \mapsto K_0(A)$, $\phi \mapsto K_0(\phi)$ is a covariant
functor from the category of unital $C^*$-algebras and $*\,$-homomorphisms to the category of Abelian groups and group
homomorphisms.  Furthermore, for all $*\,$-homomorphisms $\phi\colon A \sto B$ between unital $C^*$-algebras the following
diagram commutes.
  \[\xy
         \Square[\fml P_\infty(A)`\fml
         P_\infty(B)`K_0(A)`K_0(B);\phi`{[\hphantom{P}]}`{[\hphantom{P}]}`K_0(\phi)]
  \endxy\]
\end{prop}

\begin{notn} For $C^*$-algebras $A$ and $B$ let
 \index{hom@$\Hom(A,B)$ (set of $*\,$-homomorphisms between $C^*$-algebras)}%
$\Hom(A,B)$ be the family of all $*\,$-homomorphisms from $A$ to~$B$.
\end{notn}

\begin{defn} Let $A$ and $B$ be $C^*$-algebras and $a \in A$. For $\phi$, $\psi \in \Hom(A,B)$
let
   \[ d_a(\phi,\psi) = \norm{\phi(a) - \psi(a)}\,. \]
Then $d_a$ is a pseudometric on $\Hom (A,B)$.  The topology generated by the family $\{d_a\colon a \in A\}$ is the
 \index{point-norm topology}%
 \index{topology!point-norm}%
\df{point-norm topology} on~$\Hom(A,B)$. (For each $a \in A$ let $\sfml B_a$ be the family of open balls in $\Hom(A,B)$ generated by
the pseudometric~$d_a$. The family $\bigcup\{\sfml B_a\colon a \in A\}$ is a subbase for the point-norm topology.)
\end{defn}

\begin{defn} Let $A$ and $B$ be $C^*$-algebras.  We say that $*\,$-homomorphisms $\phi$, $\psi\colon A \sto B$ are
 \index{homotopic!$*\,$-homomorphisms}%
 \index{<star@$*\,$-homomorphism!homotopy of}%
 \index{<binrelequivh@$\phi\sim_h\psi$ (homotopy of $*\,$-homomorphisms)}%
\df{homotopic}, and write $\phi \sim_h \psi$, if there exists a function
   \[ c\colon [0,1] \times A \sto B\colon (t,a) \mapsto c_t(a) \]
such that
  \begin{enumerate}[label=\rm{(\alph*)}]
     \item for every $t \in [0,1]$ the map $c_t\colon A \sto B \colon a \mapsto c_t(a)$ is a $*\,$-homomorphism,
     \item for every $a \in A$ the map $c(a)\colon [0,1] \sto B\colon t \mapsto c_t(a)$ is a (continuous) path in~$B$,
     \item $c_0 = \phi$, and
     \item $c_1 = \psi$.
  \end{enumerate}
\end{defn}

\begin{prop} Two $*\,$-homomorphisms $\phi_0$, $\phi_1\colon A \sto B$ between $C^*$-algebras
are homotopic if and only if there exists a point-norm continuous path from $\phi_0$ to $\phi_1$ in~$\Hom(A,B)$.
\end{prop}

\begin{defn} We say that $C^*$-algebras are
 \index{homotopic!equivalence!of $*\,$-homomorphisms}%
 \index{equivalence!homotopy!of $*\,$-homomorphisms}%
\df{homotopically equivalent} if there exist $*\,$-homomorphisms $\phi\colon A \sto B$ and $\psi\colon B \sto A$ such that $\psi
\circ \phi \sim_h \id A$ and $\phi \circ \psi \sim_h \id B$. A $C^*$-algebra is
 \index{contractible!$C^*$-algebras}%
\df{contractible} if it is homotopically equivalent to~$\{\vc 0\}$.
\end{defn}

\begin{prop}\label{0062327} Let $A$ and $B$ be unital $C^*$-algebras. If $\phi \sim_h \psi$
in $\Hom(A,B)$, then $K_0(\phi) = K_0(\psi)$.
\end{prop}

%\begin{proof} See \cite{RordamLL:2000}, proposition 3.2.6.  \ns \end{proof}

\begin{prop}\label{0062331} If unital $C^*$-algebras $A$ and $B$ are
 \index{homotopy!invariance of $K_0$}%
 \index{K0@$K_0$!homotopy invariance of}%
homotopically equivalent, then $K_0(A) \cong K_0(B)$.
\end{prop}

\begin{prop}\label{0062334} If $A$ is a unital $C^*$-algebra, then the split exact sequence
   \begin{equation}\label{0062334i}
      \vc 0 \to A \to^\iota \wt A \two/->`<-/^Q_\psi \C \to \vc 0
   \end{equation}
\emph{(see \ref{0015213}, Case 1, item (7)\,)} induces another split exact sequence
   \begin{equation}\label{0062334ii}
      \vc 0 \to K_0(A) \to^{K_0(\iota)} K_0(\wt A) \two/->`<-/^{K_0(Q)}_{K_0(\psi)} K_0(\C) \to \vc 0 \,.
   \end{equation}
\end{prop}

\begin{proof}[\emph{Hint for proof}] Define two additional functions
   \[ \mu\colon \wt A \sto A\colon a + \lambda \vc j \mapsto a \]
and
   \[ \psi' \colon \C \sto \wt A \colon \lambda \mapsto \lambda \vc j\,. \]
Then verify that
   \begin{enumerate}[label=\rm{(\alph*)}]
     \item $\mu \circ \iota = \id A$,
     \item $\iota \circ \mu + \psi' \circ Q = \id{\wt A}$,
     \item $Q \circ \iota = \vc 0$, and
     \item $Q \circ \psi = \id{\C}$.
   \end{enumerate}   \ns
\end{proof}

\begin{exam} For every $n \in \N$,
 \index{K0@$K_0(A)$!is $\Z$ when $A = \mathbf M_n$}%
$K_0(\mathbf M_n) \cong \Z$.
\end{exam}

\begin{exam} If $H$ is a separable infinite dimensional Hilbert space,
 \index{K0@$K_0(A)$!is $\vc 0$ when $A = \ofml B(H)$}%
then $K_0(\ofml B(H)) \cong \vc 0$.
\end{exam}

\begin{defn} Recall that a topological space $X$ is
 \index{contractible!topological space}%
\df{contractible} if there is a point $a$ in the space and a continuous function $f\colon [0,1] \times X \sto X$ such that $f(1,x) = x$
and $f(0,x) = a$ for every $x \in X$.
\end{defn}

\begin{exam} If $X$ is a contractible compact Hausdorff space, then
 \index{K0@$K_0(A)$!is $\Z$ when $A = \fml C(X)$, $X$ contractible}%
$K_0(\fml C(X)) \cong \Z$.
\end{exam}


















\section{$\mathbf{\emph{K}_0}(A)$---the Nonunital Case}
\begin{defn} Let $A$ be a nonunital $C^*$-algebra. Recall that the split exact
sequence~\eqref{0062334i} for the unitization of $A$ induces a split exact
sequence~\eqref{0062334ii} between the corresponding $K_0$ groups.
 \index{K0@$K_0(A)$!for nonunital $A$}%
Define
   \[ K_0(A) = \ker(K_0(\pi))\,. \]
\end{defn}

\begin{prop}\label{0062413} For a nonunital $C^*$-algebra $A$ the mapping $[\hphantom{P}] \colon \fml
P_\infty(A) \sto K_0(\wt A)$ may be regarded as a mapping from $\fml P_\infty(A)$ into~$K_0(A)$.
\end{prop}

\begin{prop}\label{0062416} For both unital and nonunital $C^*$-algebras the sequence
  \[ \vc 0 \to K_0(A) \to K_0(\wt A) \to K_0(\C) \to \vc 0 \]
is exact.
\end{prop}

\begin{prop}\label{0062418} For both unital and nonunital $C^*$-algebras the group $K_0(A)$ is (isomorphic
to) $\ker(K_0(\pi))$.
\end{prop}

\begin{prop}\label{0062424} If $\phi\colon A \sto B$ is a $*\,$-homomorphism between $C^*$-algebras, then
there exists a unique $*\,$-homomorphism
 \index{K0@$K_0(\phi)$!nonunital case}%
$K_0(\phi)$ which makes the following diagram commute.
   \[\xy
     \hSquares/>`>`.>`>`=`>`>/[K_0(A)`K_0(\wt A)`K_0(\C)`K_0(B)`K_0(\wt
     B)`K_0(\C);`K_0(\pi_A)`K_0(\phi)`K_0(\wt\phi)```K_0(\pi_B)]
   \endxy\]
\end{prop}

\begin{prop}\label{0062511} The pair of maps $A \mapsto K_0(A)$, $\phi \mapsto K_0(\phi)$ is a
 \index{functor!$K_0$}%
 \index{K0@$K_0$!as a covariant functor}%
covariant functor from the category $\cat{CSA}$ of $C^*$-algebras and $*\,$-homomorphisms to
the category of Abelian groups and group homomorphisms.
\end{prop}

In propositions~\ref{0062327} and~\ref{0062331} we asserted the homotopy invariance of the
functor $K_0$ for unital $C^*$-algebras.  We now extend the result to arbitrary
$C^*$-algebras.

\begin{prop}\label{0062521} Let $A$ and $B$ be $C^*$-algebras. If $\phi \sim_h \psi$
in $\Hom(A,B)$, then $K_0(\phi) = K_0(\psi)$.
\end{prop}

%\begin{proof} See \cite{RordamLL:2000}, proposition 4.1.4.  \ns \end{proof}

\begin{prop}\label{0062524} If $C^*$-algebras $A$ and $B$ are homotopically equivalent,
then $K_0(A) \cong K_0(B)$.
\end{prop}

%\begin{proof} See \cite{RordamLL:2000}, proposition 4.1.4.  \ns \end{proof}

\begin{defn} Let $\pi$ and $\lambda$ be the $*\,$-homomorphisms in the split exact
sequence~\eqref{0062334i} for the unitization of as $C^*$-algebra~$A$. Define the
 \index{scalar!mapping}%
\df{scalar mapping} $s\colon \wt A \sto \wt A$ for $\wt A$ by $s :=  \lambda \circ \pi$. Every
member of $\wt A$ can be written in the form $a + \alpha \vc 1_{\wt A}$ for some $a \in A$ and
$\alpha \in \C$. Notice that $s(a + \alpha \vc 1_{\wt A}) = \alpha \vc 1_{\wt A}$ and that $x
- s(x) \in A$ for every $x \in \wt A$.  For each natural number $n$ the scalar mapping induces
a corresponding map $s = s_n\colon \M n{\wt A} \sto \M n{\wt A}$. An element $x \in \M n{\wt
A}$ is a
 \index{scalar!element}%
\df{scalar element} of $\M n{\wt A}$ if $s(x) = x$.
\end{defn}

\begin{prop}[Standard Picture of $K_0(A)$ for arbitrary $A$]\label{0062544} If $A$ is a
$C^*$-algebra,
 \index{standard!picture of $K_0(A)$!for arbitrary $A$}%
 \index{picture (\seeonly{standard picture}))}%
then
     \[ K_0(A) = \{\,[p\,] - [s(p)]\colon p,q \in \fml P_\infty(\wt A)\}\,. \]
\end{prop}























\section{Exactness and Stability Properties of the $K_0$ Functor}
\begin{defn} A covariant functor $F$ from a category $\cat A$ to a category $\cat B$ is
 \index{split exact!functor}%
 \index{functor!split exact}%
\df{split exact} if it takes split exact sequences to split exact sequences. And it is
 \index{half exact functor}%
 \index{functor!half exact}%
\df{half exact} provided that whenever the sequence
   \[ \vc 0 \to A_1 \to^j A_2 \to^k A_3 \to \vc 0 \]
is exact in $\cat A$, then
   \[ F(A_1) \to^{F(j)} F(A_2) \to^{F(k)} F(A_3) \]
is exact in~$\cat B$.
\end{defn}

\begin{prop}\label{0062664} The functor $K_0$ is half exact.
\end{prop}

%\begin{proof} See~\cite{RordamLL:2000}, proposition 4.3.2.  \ns \end{proof}

\begin{prop}\label{0062667} The functor $K_0$ is split exact.
\end{prop}

%\begin{proof} See~\cite{RordamLL:2000}, proposition 4.3.3.  \ns \end{proof}

\begin{prop}\label{0062671} The functor $K_0$ preserves direct sums.  That is, if $A$ and $B$ are
$C^*$-algebras, then $K_0(A \oplus B) = K_0(A) \oplus K_0(B)$.
\end{prop}

\begin{exam}\label{0062674} If $A$ is a $C^*$-algebra, then $K_0(\wt A) = K_0(A) \oplus \Z$.
\end{exam}

Despite being both split exact and half exact the functor $K_0$ is not exact. Each of the next
two examples is sufficient to demonstrate this.

\begin{exam}\label{0062677} The sequence
   \[ \vc 0 \to \fml C_0\bigl((0,1)\bigr) \to^\iota \fml C\bigl([0,1]\bigr)
                               \to^\psi \C \oplus \C \to \vc 0 \]
where $\psi(f) = \bigl(f(0),f(1)\bigr)$, is clearly exact; but $K_0(\psi)$ is not surjective.
\end{exam}

\begin{exam}\label{0062679} If $H$ is a Hilbert space the exact sequence
  \[ \vc 0 \to \ofml K(H) \to^\iota \ofml B(H) \to^\pi \ofml Q(H) \to \vc 0 \]
associated with the Calkin algebra $\ofml Q(H)$ is exact but $K_0(\iota)$ is not injective.
(This example requires a fact we have not derived: $K_0(\ofml K(H)) \cong \Z$, for which see~\cite{RordamLL:2000}, Corollary~6.4.2.)
\end{exam}

Next is an important stability property of the functor~$K_0$.
\begin{prop}\label{0062681} If $A$ is a $C^*$-algebra, then $K_0(A) \cong K_0(\M nA)$.
\end{prop}

%\begin{proof} See~\cite{RordamLL:2000}, proposition 4.3.8.   \ns \end{proof}





















\section{Inductive Limits}
In section~\ref{sec_ind_limits} we defined \emph{inductive limits} for arbitrary categories and in
section~\ref{sec_LFsps} we restricted our attention to strict inductive limits of locally convex
topological vector spaces, which were useful in the context of the theory of distributions.  In the
current section we look at inductive limits of sequences of $C^*$-algebras.

\begin{defn} In any category an
 \index{inductive!sequence}%
 \index{sequence!inductive}%
\df{inductive sequence} is a pair $(A,\phi)$ where $A = (A_j)$ is a sequence of objects and
$\phi = (\phi_j)$ is a sequence of morphisms such that $\phi_j\colon A_j \sto A_{j+1}$ for
each~$j$. An
 \index{inductive!limit}%
 \index{limit!inductive}%
\df{inductive limit} (or
 \index{direct!limit}%
 \index{limit!direct}%
\df{direct limit}) of the sequence $(A,\phi)$ is a pair $(L,\mu)$ where $L$ is an object and
$\mu = (\mu_n)$ is a sequence of morphisms $\mu_j\colon A_j \sto L$ which satisfy
 \begin{enumerate}
  \item[(i)] $\mu_j = \mu_{j+1} \circ \phi_j$ for each $j \in \N$, and
  \item[(ii)] if $(M,\lambda)$ is a pair where $M$ is an object and $\lambda = (\lambda_j)$ is
a sequence of morphisms $\lambda_j\colon A_j \sto M$ satisfying $\lambda_j = \lambda_{j+1}
\circ \phi_j$, then there exists a unique morphism $\psi\colon L \sto M$ such that $\lambda_j
= \psi \circ \mu_j$ for each $j \in \N$.
 \end{enumerate}
Abusing language we usually say that $L$ is the inductive limit of the sequence $(A_j)$ and
 \index{limit@$\underrightarrow{\lim} A_j$ (inductive limit)}%
write $L = \underrightarrow{\lim} A_j$.

\vskip 15 pt

   \[\xymatrix@+15pt@C+25pt{&&&& L\ar@{-->}[dddd]^\psi \\
                   &&&&   \\
                  {\cdots}\ar[r] & A_j \ar[uurrr]^{\mu_j}\ar[ddrrr]^{\lambda_j}\ar[r]^(.6){\phi_j} &
                       A_{j+1}\ar[uurr]^{\mu_{j+1}}\ar[ddrr]^{\lambda_{j+1}}\ar[r] & {\cdots} &  \\
                   &&&&   \\
                   &&&& M
    }\]
\end{defn}

\begin{prop} Inductive limits of inductive sequences (if they exist in a category) are unique (up to isomorphism).
\end{prop}

\begin{prop} Every inductive sequence $(A,\phi)$ of $C^*$-algebras
 \index{cstaralg@$C^*$-algebra!inductive limit}%
has an inductive limit. (And so does every inductive sequence of Abelian groups.)
\end{prop}

\begin{proof} See \cite{Blackadar:2006}, II.8.2.1; \cite{RordamLL:2000}, proposition 6.2.4; or
\cite{Wegge-Olsen:1993}, appendix~L.  \ns
\end{proof}

\begin{prop} If $(L,\mu)$ is the inductive limit of an inductive sequence $(A,\phi)$ of
$C^*$-algebras, then $L = \clo{\bigcup {\mu_n}^\sto(A_n)}$ and $\norm{\mu_m(a)} = \lim_{n \sto
\infty}\norm{\phi_{n,m}(a)} = \inf_{n \ge m}\norm{\phi_{n,m}(a)}$ for all $m \in \N$ and $a
\in A$.
\end{prop}

\begin{proof} See \cite{RordamLL:2000}, proposition 6.2.4.   \ns \end{proof}

\begin{defn}  An
 \index{approximately finite dimensional $C^*$-algebra}%
 \index{AF-algebra}%
 \index{finite!dimensional!approximately}%
 \index{cstaralg@$C^*$-algebra!AF-algebras}%
\df{approximately finite dimensional $C^*$-algebra} (an AF-algebra for short) is the inductive
limit of a sequence of finite dimensional $C^*$-algebras.
\end{defn}

\begin{exam} If $(A_n)$ is an increasing sequence of $C^*$-subalgebras of a $C^*$-algebra $D$
(with $\iota_n\colon A_n \sto A_{n+1}$ being the inclusion map for each~$n$), then $(A,\iota)$
is an inductive sequence of $C^*$-algebras whose inductive limit is $(B,j)$ where $B =
\bigcup_{n=1}^\infty A_n$ and $j_n\colon A_n \sto B$ is the inclusion map for each~$n$.
\end{exam}

\begin{exam} The sequence $\mathbf M_1 = \C \to^{\phi_1} \mathbf M_2 \to^{\phi_2} \mathbf M_3
\to^{\phi_3} \dots$ (where $\phi_n(a) = \diag(a,0)$ for each $n \in \N$ and $a \in \mathbf
M_n$) is an inductive sequence of $C^*$-algebras whose inductive limit is the $C^*$-algebra
$\ofml K(H)$ of compact operators on a Hilbert space~$H$.
\end{exam}

\begin{proof} See \cite{RordamLL:2000}, section 6.4.  \ns \end{proof}

\begin{exam} The sequence $\Z \to^{\vc 1} \Z \to^{\vc 2} \Z \to^{\vc 3} \Z \to^{\vc 4} \dots$
(where $\vc n \colon \Z \sto \Z$ satisfies $\vc n(1) = n$) is an inductive sequence of Abelian
groups whose inductive limit is the set $\Q$ of rational numbers.
\end{exam}

\begin{exam} The sequence $\Z \to^{\vc 2} \Z \to^{\vc 2} \Z \to^{\vc 2} \Z \to^{\vc 2} \dots$
is an inductive sequence of Abelian groups whose inductive limit is the set of dyadic rational
numbers.
\end{exam}

The next result is referred to the
 \index{continuity!of $K_0$}%
 \index{K0@$K_0$!continuity of}%
\emph{continuity property of $K_0$}.

\begin{prop} If $(A,\phi)$ is an inductive sequence of $C^*$-algebras, then
   \[ K_0\bigl(\,\underrightarrow{\lim}\, A_n\bigr) = \underrightarrow{\lim}\,K_0(A_n)\,. \]
\end{prop}

\begin{proof} See \cite{RordamLL:2000}, theorem 6.3.2.   \ns \end{proof}

\begin{exam} If $H$ is a Hilbert space,
 \index{K0@$K_0(A)$!is $\Z$ when $A = \ofml K(H)$}%
then $K_0(\ofml K(H)) \cong \Z$.
\end{exam}





















\section{Bratteli Diagrams}
\begin{prop} Nonzero algebra homomorphisms from $M_k$ into $M_n$ exist only if $n \ge k$, in
which case they are precisely the mappings of the form
   \[ a \mapsto u\,\diag(a, a, \dots, a, \vc 0)\, u^* \]
where $u$ is a unitary matrix.  Here there are $m$ copies of $a$ and $\vc 0$ is the $r \times
r$ zero matrix where $n = mk + r$. The number $m$ is the
 \index{multiplicity!of an algebra homomorphism}%
\df{multiplicity} of~$\phi$.
\end{prop}

\begin{proof} See \cite{Power:1992}, corollary 1.3.  \ns \end{proof}

\begin{exam} An example of a homomorphism from $M_2$ into $M_7$ is
 \[  A =
  \begin{bmatrix} a & b \\ c & d  \end{bmatrix} \mapsto
    \begin{bmatrix}
      a & b & 0 & 0 & 0 & 0 & 0 \\
      c & d & 0 & 0 & 0 & 0 & 0 \\
      0 & 0 & a & b & 0 & 0 & 0 \\
      0 & 0 & c & d & 0 & 0 & 0 \\
      0 & 0 & 0 & 0 & 0 & 0 & 0 \\
      0 & 0 & 0 & 0 & 0 & 0 & 0 \\
      0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{bmatrix} = \diag(A,A, \vc 0)\]
 where $m = 2$ and $r = 3$.
\end{exam}

\enlargethispage{\baselineskip}

\begin{prop}\label{0068117}  Every finite dimensional $C^*$-algebra $A$ is isomorphic to a
direct sum of matrix algebras
   \[ A \simeq M_{k_1} \oplus \dots \oplus M_{k_r}.\]
Suppose $A$ and $B$ are finite dimensional $C^*$-algebras, so that
   \[ A \simeq M_{k_1} \oplus \dots \oplus M_{k_r} \qquad \text{and}
                          \qquad B \simeq M_{n_1} \oplus \dots \oplus M_{n_s} \]
and suppose that $\phi\colon A \sto B$ is a unital ${}^*$-homomorphism.  Then $\phi$ is
determined (up to unitary equivalence in~$B$) by an $s \times r$ matrix $\vc m = \bigl[
m_{ij}\bigr]$ of positive integers such that
   \begin{equation}\label{0068117i}
    \vc m\, \vc k = \vc n.
   \end{equation}
\emph{Here $\vc k = (k_1, \dots,k_r)$, $\vc n = (n_1, \dots, n_s)$, and the number $m_{ij}$ is
the multiplicity of the map}
 \[\xymatrix@1{M_{k_j} \ar[r] & A \ar[r]^\phi & B \ar[r] & M_{n_i}
 }\]
\end{prop}

\begin{proof} See \cite{Davidson:1996}, theorem III.1.1.  \ns \end{proof}

\begin{defn}  Let $\phi$ be as in the preceding proposition~\ref{0068117}.  A
 \index{Bratteli diagram}%
 \index{diagram!Bratteli}%
\df{Bratteli diagram} for $\phi$  consists of two rows (or columns) of vertices labeled by the
$k_j$ and the $n_i$ together with $m_{ij}$ edges connecting $k_j$ to $n_i$ (for $1 \le j \le
r$ and $1 \le i \le s$).
\end{defn}

\begin{exam} Suppose $\phi\colon \C \oplus \C \sto M_4 \oplus M_3$ is given by
 \[ (\lambda, \mu) \mapsto \left(
      \begin{bmatrix}
        \lambda & 0 & 0 & 0 \\
        0 & \lambda & 0 & 0 \\
        0 & 0 & \lambda & 0 \\
        0 & 0 & 0 & \mu
      \end{bmatrix}\, , \,
      \begin{bmatrix}
        \lambda & 0 & 0 \\
        0 & \lambda & 0 \\
        0 & 0 & \mu
      \end{bmatrix}\right)\, .\]
Then $m_{11} = 3$, $m_{12} = 1$, $m_{21} = 2$, and $m_{22} = 1$. Notice that
   \[\vc m \,\vc k = \begin{bmatrix} 3 & 1 \\ 2 & 1 \end{bmatrix}\,
             \begin{bmatrix} 1 \\ 1 \end{bmatrix} =
             \begin{bmatrix} 4 \\ 3 \end{bmatrix} = \vc n.\]
A Bratteli diagram for $\phi$ is
    \[\xymatrix@=90pt{1 \ar@<.7ex>[r]\ar[r]\ar@<-.7ex>[r]\ar@<.5ex>[dr]\ar@<-.5ex>[dr] & 4 \\
                1 \ar[ur]\ar[r] & 3
     }\]
\end{exam}

\begin{exam} Suppose $\phi\colon \C \oplus M_2 \sto M_3 \oplus M_5 \oplus M_2$
is given by
 \[ (\lambda, b) \mapsto \left(
    \begin{bmatrix}
        \lambda & 0  \\
            0 &  b
      \end{bmatrix}\, , \,
      \begin{bmatrix}
        \lambda & 0 & 0  \\
              0 & b & 0  \\
              0 & 0 &  b
      \end{bmatrix} \,,\, b \, \right).
    \]
Then $m_{11} = 1$, $m_{12} = 1$, $m_{21} = 1$, $m_{22} = 2$, $m_{31} = 0$, and $m_{32} = 1$.
Notice that
 \[\vc m \,\vc k = \begin{bmatrix} 1 & 1 \\ 1 & 2 \\ 0 & 1 \end{bmatrix}\,
             \begin{bmatrix} 1 \\ 2 \end{bmatrix} =
             \begin{bmatrix} 3 \\ 5 \\2 \end{bmatrix} = \vc n.\]
A Bratteli diagram for $\phi$ is
 \[\xymatrix@=20pt@C+30pt{&& 3                    \\
                   1 \ar[urr]\ar[drr] &&   \\
                   && 5                    \\
         2\ar[uuurr]\ar@<+.5ex>[urr]\ar@<-.5ex>[urr]\ar[drr] && \\
                   && 2
   }\]
\end{exam}

\begin{exam} Suppose $\phi\colon M_3 \oplus M_2 \oplus M_2 \sto M_9 \oplus M_7$
is given by

\vskip 10 pt

 \[ (a,b,c) \mapsto \left(
    \begin{bmatrix}
            a & \vc 0 & \vc 0 \\
        \vc 0 &     a & \vc 0 \\
        \vc 0 & \vc 0 & \vc 0
      \end{bmatrix}\, , \,
      \begin{bmatrix}
            a & \vc 0 & \vc 0  \\
        \vc 0 &     b & \vc 0  \\
        \vc 0 & \vc 0 &     c
      \end{bmatrix}\, \right)\, .\]
Then $m_{11} = 2$, $m_{12} = 0$, $m_{13} = 0$, $m_{21} = 1$, $m_{22} = 1$, and $m_{23} = 1$.
Notice that this time something peculiar happens: we have $\vc m\,\vc k \ne \vc n$:
 \[\vc m \,\vc k = \begin{bmatrix} 2 & 0 & 0 \\ 1 & 1 & 1 \end{bmatrix}\,
             \begin{bmatrix} 3 \\ 2 \\ 2 \end{bmatrix} =
             \begin{bmatrix} 6 \\ 7 \end{bmatrix} \le
             \begin{bmatrix} 9 \\ 7 \end{bmatrix} = \vc n.\]
What's the problem here?  Well, the result stated earlier was for \emph{unital}
${}^*$-homomorphisms, and this $\phi$ is \emph{not} unital.  In general, this will be the best
we can expect: $\vc m\,\vc k \le \vc n$.

\noindent Here is the resulting Bratteli diagram for $\phi$:
   \[\xymatrix@=+20pt@C+20pt@R-7pt{
        3\ar@<+.5ex>[drr]\ar@<-.5ex>[drr]\ar[dddrr] && \\
                            && 9                   \\
                           2\ar[drr] &&            \\
                            && 7                   \\
                           2\ar[urr] &&
   }\]
\end{exam}

\begin{exam} If $K$ is the Cantor set, then $\fml C(K)$ is an $AF$-algebra.
 \index{AF@$AF$-algebra!$\fml C(K)$ is an ($K$ is the Cantor set)}%
To see this write $K$ as the intersection of a decreasing family of closed subsets $K_j$ each
of which consists of $2^j$ disjoint closed subintervals of $[0,1]$.  For each $j \ge 0$ let
$A_j$ be the subalgebra of functions in $\fml C(K)$ which are constant on each of the
intervals making up~$K_j$.  Thus $A_j \simeq \C^{2^j}$ for each~$j \ge 0$.  The imbedding
 \[\phi_j \colon A_j \sto A_{j+1}\colon
      (a_1, a_2, \dots, a_{2^j}) \mapsto
        (a_1, a_1, a_2, a_2,\dots, a_{2^j}, a_{2^j})\]
splits each minimal projection into the sum of two minimal projections.  For $\phi_0$ the
corresponding matrix $\vc m_0$ of ``partial multiplicities'' is $\begin{bmatrix} 1 \\ 1
\end{bmatrix}$; the matrix $\vc m_1$ corresponding to $\phi_1$ is
$\begin{bmatrix} 1 & 0\\ 1 & 0 \\ 0 & 1 \\ 0 & 1 \end{bmatrix}$; and so on.  Thus the Bratteli
diagram for the inductive limit $\fml C(K) = \underrightarrow{\lim}A_j$ is:

\vskip 20 pt

 \[\xymatrix@=+7pt@C+20pt@R-7pt{
     &&& 1 & \\
     && 1\ar[ur]\ar[dr] && \\
     &&& 1 & \\
     & 1\ar[uur]\ar[ddr] &&& \\
     &&& 1 & \\
     && 1\ar[ur]\ar[dr] && \\
     &&& 1 & \\
     1\ar[uuuur]\ar[ddddr] &&&& \cdots \\
     &&& 1 & \\
     && 1\ar[ur]\ar[dr] && \\
     &&& 1 & \\
     & 1\ar[uur]\ar[ddr] &&& \\
     &&&1 & \\
     && 1\ar[ur]\ar[dr] && \\
     &&& 1 &
   }\]
\end{exam}

\begin{exam} Here is an example of a so-called
 \index{CAR-algebras}%
 \index{cstaralg@$C^*$-algebra!CAR-algebras}%
CAR\textbf{}-algebra (CAR = Canonical Anticommutation Relations).  For $j \ge 0$ let $A_j =
M_{\spsp{2}j}$ and
 \[\phi_j\colon A_j \to A_{j+1}\colon \vc a \mapsto
      \begin{bmatrix} \vc a & 0 \\ 0 & \vc a \end{bmatrix}.\]
The multiplicity ``matrix'' $\vc m$ for each $\phi_j$ is just the $1 \times 1$ matrix~$[2]$.
We see that for each $j$
 \[\vc m\, \vc k(j) = [2]\,[2^j] = [2^{j+1}] = \vc n(j).\]
This leads to the following Bratteli diagram

\vskip 15pt

 \[\xymatrix{1 \ar@<.5ex>[r]\ar@<-.5ex>[r] & 2 \ar@<.5ex>[r]\ar@<-.5ex>[r] & 4
        \ar@<.5ex>[r]\ar@<-.5ex>[r] & 8 \ar@<.5ex>[r]\ar@<-.5ex>[r] & 16
        \ar@<.5ex>[r]\ar@<-.5ex>[r] & 32
        \ar@<.5ex>[r]\ar@<-.5ex>[r]& {\cdots}
   }\]
\vskip 15pt
\noindent for the inductive limit $C = \underrightarrow{\lim}A_j$.

\textbf{However} the range of $\phi_j$ is contained in a subalgebra $B_{j+1} \simeq M_{2^j}
\oplus M_{2^j}$ of~$A_{j+1}$. (Take $B_0 = A_0 = \C$.)  Thus for $j \in \N$ we may regard
$\phi_j$ as a mapping from $B_j$ to~$B_{j+1}$:
 \[ \phi_j\colon(\vc b, \vc c) \mapsto
    \left(\begin{bmatrix} \vc b & 0 \\ 0 & \vc c \end{bmatrix},
          \begin{bmatrix} \vc b & 0 \\ 0 & \vc c \end{bmatrix}
              \right)\,.\]
Now the multiplicity matrix $\vc m$ for each $\phi_j$ is $\begin{bmatrix} 1 & 1 \\ 1 & 1
\end{bmatrix}$ and we see that for each $j$
 \[\vc m\, \vc k(j) =
   \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}\,
        \begin{bmatrix} 2^{j-1} \\ 2^{j-1} \end{bmatrix} =
   \begin{bmatrix} 2^j \\ 2^j \end{bmatrix} = \vc n(j).\]
Then, since $C = \underrightarrow{\lim}A_j = \underrightarrow{\lim}B_j$, we have a second
(quite different) Bratteli diagram for the AF-algebra~$C$.
   \[\xymatrix@-10pt{&& 1 \ar[rr]\ar[rrdd] && 2 \ar[rr]\ar[rrdd] && 4 \ar[rr]\ar[rrdd]
                                && 8 \ar[rr]\ar[rrdd] && 16 \ar[rr]\ar[rrdd]&& \cdots\\
              1 \ar[urr]\ar[drr]&&&&&&&&&&  \\
             && 1 \ar[rr]\ar[rruu] && 2 \ar[rr]\ar[rruu] && 4 \ar[rr]\ar[rruu]
                                && 8 \ar[rr]\ar[rruu] && 16 \ar[rr]\ar[rruu]&& \cdots \\
   }\]
\end{exam}

\begin{exam}   This is the
 \index{Fibonacci algebra}%
 \index{cstaralg@$C^*$-algebra!Fibonacci algebra}%
\df{Fibonacci algebra}.  For $j \in \N$ define sequences $\bigl(p_j\bigr)$ and
$\bigl(q_j\bigr)$ by the familiar recursion relations:
 \begin{align*}
       p_1 = &q_1 = 1, \\
       p_{j+1} = &p_j + q_j, \text{ and } \\
       q_{j+1} &= p_j.
 \end{align*}
For all $j \in \N$ let $A_j = M_{p_j} \oplus M_{q_j}$ and

\vskip 10 pt

 \[\phi_j\colon A_j \sto A_{j+1}\colon (a,b) \mapsto
     \left(\begin{bmatrix} a & 0 \\ 0 & b
          \end{bmatrix},  a \right). \]
The multiplicity matrix $\vc m$ for each $\phi_j$ is $\begin{bmatrix} 1 & 1 \\ 1 & 0
\end{bmatrix}$ and for each $j$
 \[\vc m\, \vc k(j) =
   \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}\,
        \begin{bmatrix} p_j \\ q_j \end{bmatrix} =
   \begin{bmatrix} p_j + q_j \\ p_j \end{bmatrix} =
        \begin{bmatrix} p_{j+1} \\ q_{j+1} \end{bmatrix} =
                          \vc n(j).\]
The resulting Bratteli diagram for $F = \underrightarrow{\lim}A_j$ is
   \[\xymatrix@-10pt{&& 1 \ar[rr]\ar[rrdd] && 2 \ar[rr]\ar[rrdd] &&
         3 \ar[rr]\ar[rrdd] && 5 \ar[rr]\ar[rrdd] && 8 \ar[rr]\ar[rrdd]&& \cdots\\
              1 \ar[urr]\ar[drr]&&&&&&&&&&  \\
             && 1 \ar[rruu] && 1 \ar[rruu] && 2 \ar[rruu] &&
              3 \ar[rruu] && 5 \ar[rruu]&& \cdots \\
   }\]
\end{exam}













\endinput
\chapter{LINEAR ALGEBRA AND THE SPECTRAL THEOREM}

The emphasis in beginning analysis courses is on the behavior of individual (real or complex valued) functions.  In a \emph{functional analysis}
course the focus is shifted to \emph{spaces} of such functions and certain classes of mappings between these spaces.  It turns out that a concise,
perhaps overly simplistic, but certainly not misleading, description of such material is \emph{infinite dimensional linear algebra}.  From an
analyst's point of view the greatest triumphs of linear algebra were, for the most part, theorems about operators on finite dimensional vector
spaces (principally $\R^n$ and $\C^n$).  However, the vector spaces of scalar-valued mappings defined on various domains are typically infinite
dimensional.  Whether one sees the process of generalizing the marvelous results of finite dimensional linear algebra to the infinite dimensional
realm as a depressingly unpleasant string of abstract complications or as a rich source of astonishing insights, fascinating new questions, and
suggestive hints towards applications in other fields depends entirely on one's orientation towards mathematics in general.

In light of the preceding remarks it may be helpful to begin our journey into this new terrain with a brief review of some of the classical
successes of linear algebra.  It is not unusual for students who successfully complete a beginning linear algebra to have at the end only the
vaguest idea of what the course was about.  Part of the blame for this may be laid on the many elementary texts which make an unholy conflation
of two quite different, if closely related subjects: the study of \emph{vector spaces} and the linear maps between them on the one hand, and
\emph{inner product spaces} and their linear maps on the other.  Vector spaces have none of the geometric/topological notions of \emph{distance}
or \emph{length} or \emph{perpendicularity} or \emph{open} set or \emph{angle} between vectors; there is only addition and scalar multiplication.
Inner product spaces are vector spaces endowed with these additional structures.  Let's look at them separately.


\section{Vector Spaces and the Decomposition of Diagonalizable Operators}
\begin{conv} In these notes all vector
 \index{conventions!vector spaces are complex or real}%
 \index{C@$\C$!field of complex numbers}%
 \index{R@$\R$!field of real numbers}%
spaces will be assumed to be vector spaces over the field~$\C$ of complex numbers (in which case it is called a \emph{complex vector space}) or the
field~$\R$ of real numbers (in which case it is a \emph{real vector space}).  No other fields will appear.  When $\K$ appears it can be taken
 \index{K@$\K$ (field of real or complex numbers)}%
to be either $\C$ or~$\R$. A
 \index{scalar}%
\df{scalar} is a member of $\K$; that is, either a \emph{complex number} or a \emph{real number}.
\end{conv}

\begin{defn}\label{00001} The triple $(V,+,M)$ is a
 \index{vector!space}%
 \index{space!vector}%
\df{vector space} over $\K$ if $(V,+)$ is an Abelian group and $M\colon \K \sto \Hom(V)$
is a unital ring homomorphism (where $\Hom(V)$ is the ring of group homomorphisms
on~$V$).
\end{defn}

To check on the meanings of any the terms in the preceding definition, take a look at the first three sections of the first chapter of my
linear algebra notes~\cite{Erdman:2010}.

\begin{exer} The definition of \emph{vector space} found in many elementary texts is
something like the following: a \emph{vector space over} $\K$ is a set $V$ together with operations
of addition and scalar multiplication which satisfy the following axioms:
 \begin{enumerate}
  \item[(1)] if $x$, $y \in V$, then $x + y \in V$;
  \item[(2)] $(x + y) + z = x + (y + z)$ for every
 \index{associative}%
$x$, $y$, $z \in V$ (associativity);
  \item[(3)] there exists $\vc 0 \in V$ such that $x + \vc 0 = x$ for every
 \index{identity!additive}%
$x \in V$ (existence of additive identity);
  \item[(4)] for every $x\in V$ there exists $-x \in V$ such that
 \index{additive!inverses}%
 \index{inverse!additive}%
$x + (-x) = \vc 0$ (existence of additive inverses);
  \item[(5)] $x + y = y + x$ for every $x$, $y \in V$
 \index{commutative}%
(commutativity);
  \item[(6)] if $\alpha \in \K$ and $x \in V$, then $\alpha x \in V$;
  \item[(7)] $\alpha (x + y) = \alpha x + \alpha y$ for every $\alpha \in \K$
and every $x$, $y \in V$;
  \item[(8)] $(\alpha + \beta)x = \alpha x + \beta x$ for every $\alpha$,
$\beta \in \K$ and every $x \in V$;
  \item[(9)] $(\alpha\beta)x = \alpha(\beta x)$ for every $\alpha$,
$\beta \in \K$ and every $x \in V$; and
  \item[(10)] $1\,x = x$ for every $x \in V$.
 \end{enumerate}
Verify that this definition is equivalent to the one given above in~\ref{00001}.
\end{exer}

\begin{defn} A subset $M$ of a vector space $V$ is a
 \index{subspace!vector}%
 \index{vector!subspace}%
\df{vector subspace} of $V$ if it is a vector space under the operations it inherits from~$V$.
\end{defn}

\begin{prop} A nonempty subset of $M$ of a vector space $V$ is a vector subspace of $V$ if and
only if it is closed under addition and scalar multiplication. (That is: if $\vc x$ and
$\vc y$ belong to $M$, so does $\vc x + \vc y$; and if $\vc x$ belongs to $M$ and $\alpha
\in \K$, then $\alpha \vc x$ belongs to~$M$.)
\end{prop}

\begin{defn} A vector $y$ is a
 \index{linear!combination}%
 \index{combination!linear}%
\df{linear combination} of vectors $x_1$, \dots, $x_n$ if there exist scalars $\alpha_1$,
\dots $\alpha_n$ such that $y = \sum_{k=1}^n \alpha_k x_k$. The linear combination $\sum_{k=1}^n \alpha_k x_k$ is
 \index{linear!combination!trivial}%
 \index{trivial!linear combination}%
\df{trivial} if all the coefficients $\alpha_1$, \dots $\alpha_n$ are zero.  If at least
one $\alpha_k$ is different from zero, the linear combination is \df{nontrivial}.
\end{defn}

\begin{defn} If $A$ is a nonempty subset of a vector space~$V$, then $\spn A$, the
 \index{span}%
\df{span} of $A$, is the set of all linear combinations of elements of~$A$.  This is also frequently called the
 \index{span!linear}%
 \index{linear!span}%
\df{linear span} of~$A$.
\end{defn}

\begin{notn}\label{bases111} Let $A$ be a set which spans a vector space~$V$.  For every
$x \in V$ there exists a finite set $S$ of vectors in $A$ and for each element $e$ in $A$
there exists a scalar $x_e$ such that $x = \sum_{e \in S} x_e e$.  If we agree to let $x_e = 0$
whenever $e \in B \setminus S$, we can just as well write $x = \sum_{e \in B} x_e e$.  Although
this notation may make it appear as if we are summing over an arbitrary, perhaps uncountable,
set, the fact of the matter is that all but finitely many of the terms are zero, so no
``convergence'' problems arise. Treat $\sum_{e \in B} x_e e$ as a finite sum.
Associativity and commutativity of addition in $V$ make the expression unambiguous.
\end{notn}

\begin{defn}\label{defn_lin_dep} A subset $A$ of a vector space is
 \index{linear!dependence}%
 \index{dependent}
\df{linearly dependent} if the zero vector $\vc 0$ can be written as a nontrivial linear
combination of elements of~$A$; that is, if there exist vectors $x_1, \dots, x_n \in A$
and scalars $\alpha_1, \dots, \alpha_n$, \textbf{not all zero,} such that $\sum_{k=1}^n
\alpha_kx_k = \vc 0$. A subset of a vector space is
 \index{linear!independence}%
 \index{independent}
\df{linearly independent} if it is not linearly dependent.
\end{defn}

Technically, it is a \emph{set} of vectors that is linearly dependent or independent.
Nevertheless, these terms are frequently used as if they were properties of the vectors
themselves.  For instance, if $S = \{x_1, \dots, x_n\}$ is a finite set of vectors in a
vector space, you may see the assertions ``the set $S$ is linearly independent'' and
``the vectors $x_1$, \dots $x_n$ are linearly independent'' used interchangeably.

\begin{defn}\label{defn_Hamel_basis} A set $B$ of vectors in a vector space $V$ is a
 \index{basis!Hamel}%
 \index{Hamel basis}%
\df{Hamel basis} for $V$ if it is linearly independent and spans~$B$.
\end{defn}

\begin{conv}\label{conv_1_1} The vector space containing a single vector, the zero vector, is the
 \index{convention!basis for the zero vector space}%
 \index{vector!space!trivial (or zero)}%
 \index{trivial!vector space}%
\df{trivial} (or
 \index{zero!vector space}%
\df{zero}) vector space. Associated with this space is a technical, if not very interesting, question.  Does it have a Hamel basis?
It is clear from the definition of \emph{linear dependence}~\ref{defn_lin_dep} that the empty set $\emptyset$ is linearly
independent.  And it is certainly a maximal linearly independent subset of the zero space (which condition, for nontrivial
spaces, is equivalent to being a linearly independent spanning set).  But it is hard to argue that the empty set spans anything.
So, simply as a matter of convention, we will say that the answer to the question is \emph{yes}.  The zero vector space has a Hamel
basis, and it is the empty set!
\end{conv}

In beginning linear algebra texts a linearly independent spanning set is called simply a \emph{basis}.  I use \emph{Hamel basis}
instead to distinguish it from \emph{orthonormal basis} and \emph{Schauder basis}, terms which refer to concepts which are of
more interest in functional analysis.

\begin{prop} Let $B$ be a Hamel basis for a nontrivial vector space~$V$.  Then every element in $V$ can be written in a unique way as a linear
combination of members of~$B$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Existence is clear.  Simplify the proof of uniqueness by making use of the notational
convention suggested in~\ref{bases111}.   \ns
\end{proof}

\begin{prop}\label{bases025} Let $A$ be a linearly independent subset of a vector
space~$V$. Then there exists a Hamel basis for~$V$ which contains~$A$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Show first that, for a nontrivial space, a linearly independent subset of $V$ is a basis for~$V$
if and only if it is a maximal linearly independent subset. Then order the family of linearly independent subsets of
$V$ which contain~$A$ by inclusion and apply \emph{Zorn's lemma}. (If you are not familiar with \emph{Zorn's lemma},
see section 1.7 of my linear algebra notes~\cite{Erdman:2010}.)  \ns
\end{proof}

\begin{cor} Every vector space has a Hamel basis.
\end{cor}

\begin{defn}\label{000082} Let $M$ and $N$ be subspaces of a vector space $V$. If
$M \cap N = \{\vc 0\}$ and $M + N = V$, then $V$ is the
 \index{internal!direct sum!in vector spaces}%
 \index{direct!sum!internal}%
 \index{direct!sum!of vector spaces}%
 \index{<binopdirectsum@$\oplus$ (vector space direct sum)}%
(\df{internal}) \df{direct sum} of $M$ and $N$. In this case we write
   \[ V = M \oplus N\,. \]
We say that $M$ and $N$ are
 \index{complement!of a vector subspace}%
 \index{vector!space!complement}%
\df{complementary vector subspaces} and that each is a (vector space) \df{complement} of the
other. The
 \index{codimension}%
\df{codimension} of the subspace $M$ is the dimension of its complement~$N$.
\end{defn}

\begin{prop}\label{0000823} Let $V$ be a vector space and suppose that $V = M \oplus N$.  Then for
every $v \in V$ there exist unique vectors $m \in M$ and $n \in N$ such that $v = m + n$.
\end{prop}

\begin{exam} Let $\fml C = \fml C[-1,1]$ be the vector space of all continuous real valued
functions on the interval $[-1,1]$. A function $f$ in $\fml C$ is \df{even} if $f(-x) =
f(x)$ for all $x \in [-1,1]$; it is \df{odd} if $f(-x) = -f(x)$ for all $x \in [-1,1]$.
Let $\fml C_o = \{f \in \fml C\colon  f \text{ is odd }\}$ and $\fml C_e = \{f \in \fml
C\colon  f \text{ is even }\}$. Then $\fml C = \fml C_o \oplus \fml C_e$.
\end{exam}

\begin{prop}\label{0000824} If $M$ is a subspace of a vector space $V$, then there exists a subspace $N$ of
$V$ such that $V = M \oplus N$.
\end{prop}

\begin{lem}\label{bases030} Let $V$ be a vector space with a nonempty finite basis  $\{e^1,
\dots,e^n\}$ and let $v = \sum_{k=1}^n \alpha_k e^k$ be a vector in~$V$.  If $p \in \N_n$
and $\alpha_p \ne 0$, then $\{e^1, \dots,e^{p-1},v,e^{p+1}, \dots, e^n\}$ is a basis
for~$V$.
\end{lem}

\begin{prop}\label{bases031} If some basis for a vector space $V$ contains $n$ elements,
then every linearly independent subset of $V$ with $n$ elements is also a basis.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Suppose $\{e^1,\dots,e^n\}$ is a basis for~$V$ and
$\{v^1, \dots,v^n\}$ is linearly independent in~$V$. Start by using lemma~\ref{bases030}
to show that (after perhaps renumbering the $e^k$'s) the set $\{v^1, e^2, \dots, e^n\}$
is a basis for~$V$.  \ns
\end{proof}

\begin{cor}\label{bases032} If a vector space $V$ has a finite basis $B$, then every basis
for $V$ is finite and contains the same number of elements as~$B$.
\end{cor}

\begin{defn} A vector space is
 \index{finite dimensional}%
\df{finite dimensional} if it has a finite basis and the
 \index{dimension!of a vector space}%
 \index{vector!space!dimension of a}%
 \index{dim@$\dim V$ (dimension of a vector space $V$)}%
\df{dimension} of the space is the number of elements in this (hence any) basis for the
space. The dimension of a finite dimensional vector space $V$ is denoted by $\dim V$. By the
convention made above~\ref{conv_1_1}, the zero vector space has dimension zero.  If
the space does not have a finite basis, it is
 \index{infinite dimensional}%
\df{infinite dimensional}.
\end{defn}

\begin{defn}\label{defn_op_vsp} A function $T\colon V \sto W$ between vector spaces (over the same field) is
 \index{linear!map}%
\df{linear} if $T(u + v) = Tu + Tv$ for all $u$, $v \in V$ and $T(\alpha v) = \alpha Tv$
for all $\alpha \in \K$ and $v \in V$. Linear functions are frequently called
\emph{linear transformations} or \emph{linear maps}. When $V = W$ we say that the linear map $T$ is an
 \index{operator!on a vector space}%
\df{operator} on~$V$. Depending on context we denote the identity operator $x \mapsto x$ on $V$
 \index{identity@$\id V$ or $I_V$ or $I$ (identity operator)}%
by $\id V$ or $I_V$ or just~$I$. Recall that if $T\colon V \sto W$ is a linear map, then the
 \index{kernel!of a linear map}%
 \index{<ker@$\ker T$ (kernel of~$T$)}%
\df{kernel} of $T$, denoted by $\ker T$, is $T^{\gets}(\{0\}) := \{x \in V\colon Tx = \vc
0\}$. Also, the
 \index{range!of a linear map}%
 \index{<ran@$\ran T$ (range of~$T$)}%
\df{range} of $T$, denoted by $\ran T$, is $T^{\sto}(V) := \{Tx\colon x \in V\}$.
\end{defn}

\begin{prop} A linear map $T\colon V \sto W$ is injective if and only if its kernel is zero.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be an injective linear map between vector spaces. If $A$ is a linearly independent
subset of~$V$, then $T^\sto(A)$ is a linearly independent subset of~$W$.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be a linear map between vector spaces and $A \subseteq V$.
Then $T^\sto(\spn A) = \spn T^\sto(A)$.
\end{prop}

\begin{prop} Let $T\colon V \sto W$ be an injective linear map between vector spaces. If $B$ is a basis
for a subspace $U$ of~$V$, then $T^\sto(B)$ is a basis for~$T^\sto(U)$.
\end{prop}

\begin{prop} Two finite dimensional vector spaces are isomorphic if and only if they have the same dimension.
\end{prop}

\begin{defn} Let $T$ be a linear map between vector spaces. Then $\textrm{rank }T$, the
 \index{rank}
\df{rank} of $T$ is the dimension of the range of ~$T$, and $\textrm{nullity }T$, the
 \index{nullity}%
\df{nullity} of $T$ is the dimension of the kernel of~$T$.
\end{defn}

\begin{prop} Let $T\colon V \sto W$ be a linear map between vector spaces. If $V$ is finite dimensional, then
  \[ \textrm{rank }T + \textrm{nullity }T = \dim V. \]
\end{prop}

\begin{defn} A linear map $T\colon V \sto W$ between vector spaces is
 \index{invertible!linear map}%
\df{invertible} (or is an
 \index{isomorphism!of vector spaces}%
\df{isomorphism}) if there exists a linear map $T^{-1}\colon W \sto V$ such that $T^{-1}T
= \id V$ and $TT^{-1} = \id W$.  Two vector spaces $V$ and $W$ are
 \index{isomorphic!vectorspaces}%
\df{isomorphic} if there exists an isomorphism from $V$ to~$W$.
\end{defn}

\begin{prop}\label{inv_lin_maps001} A linear map $T\colon V \sto W$ between vector spaces
is invertible if and only if has both a left inverse and a right inverse.
\end{prop}

\begin{prop}\label{inv_lin_maps004} A linear map between vector spaces is invertible if
and only if it is bijective.
\end{prop}

\begin{prop} For an operator $T$ on a finite dimensional vector space the following are equivalent:
 \begin{enumerate}
    \item $T$ is an isomorphism;
    \item $T$ is injective; and
    \item $T$ is surjective.
 \end{enumerate}
\end{prop}

 \begin{defn}\label{defn_similar_vs_ops} Two operators $R$ and $T$ on a vector space $V$ are
 \index{similar!operators}%
\df{similar} if there exists an invertible operator $S$ on $V$ such that $R = STS^{-1}$.
\end{defn}

\begin{prop}\label{sim004thm} If $V$ is a vector space, then similarity is an equivalence
relation on~$\ofml L(V)$.
\end{prop}

Let $V$ and $W$ be finite dimensional vector spaces with ordered bases. Suppose that $V$ is $n$-dimensional with ordered basis
$\{e^1,e^2, \dots,e^n\}$ and $W$ is $m$-dimensional. Recall from beginning linear algebra that if $T\colon V \sto W$ is linear,
then its
 \index{matrix!representation}%
 \index{representation!matrix}%
\emph{matrix representation} $[T]$ (taken with respect to the ordered bases in $V$ and~$W$) is the $m \times n$-matrix $[T]$
whose $k^{\text{th}}$ column ($1 \le k \le n$) is the column vector $Te^k$ in~$W$.  The point here is that the action of $T$ on
a vector $x$ can be \emph{represented} as multiplication of $x$ by the matrix $[T]$; that is,
   \[ Tx = [T]x. \]
Perhaps this equation requires a little interpretation. The left side is the function $T$ evaluated at $x$, the
result of this evaluation being thought of as a column vector in~$W$; the right side is an $m \times n$ matrix multiplied
by an $n \times 1$ matrix (that is, a column vector).  So the asserted equality is of two $m \times 1$ matrices (column vectors).

\begin{defn} Let $V$ be a finite dimensional vector space and $B = \{e^1, \dots, e^n\}$ be a basis for~$V$.  An operator $T$ on $V$ is
 \index{diagonal!operator}%
 \index{operator!diagonal}%
\df{diagonal} if there exist scalars $\alpha_1, \dots, \alpha_n$ such that $Te^k =
\alpha_ke^k$ for each $k \in \N_n$. Equivalently, $T$ is diagonal if its matrix
representation $[T] = [T_{ij}]$ has the property that $T_{ij} = 0$ whenever $i \ne j$.
\end{defn}

Asking whether a particular operator on some finite dimensional vector space is diagonal
is, strictly speaking, nonsense. As defined, the operator property of being diagonal is
definitely \emph{not} a vector space concept. It makes sense only for a vector space
\emph{for which a basis has been specified}.  This important, if obvious, fact seems to
go unnoticed in many beginning linear algebra courses, due, I suppose, to a rather obsessive
fixation on $\R^n$ in such courses. Here is the relevant \emph{vector space} property.

\begin{defn} An operator $T$ on a finite dimensional vector space $V$ is
 \index{diagonalizable!operator}%
 \index{operator!diagonalizable}%
\df{diagonalizable} if there exists a basis for $V$ with respect to which $T$ is
diagonal. Equivalently, an operator on a finite dimensional vector space \emph{with
basis} is diagonalizable if it is similar to a diagonal operator.
\end{defn}

\begin{defn} Let $V$ be a vector space and suppose that $V = M \oplus N$.  We know
from~\ref{0000823} that for each $\vc v \in V$ there exist unique vectors $\vc m \in M$
and $\vc n \in N$ such that $\vc v = \vc m + \vc n$.  Define a function $\sbsb E{MN}\colon V \sto V$ by $\sbsb E{MN}v = n$.
The function $\sbsb E{MN}$ is the
 \index{projection!in a vector space}%
 \index{E@$\sbsb E{MN}$ (projection along $M$ onto $N$)}%
\df{projection of} $V$ \df{along} $M$ \df{onto}~$N$.  (Frequently we write $E$ for $\sbsb E{MN}$. But keep in mind that
$E$ depends on both $M$ and~$N$.)
\end{defn}

\begin{prop} Let $V$ be a vector space and suppose that $V = M \oplus N$. If $E$ is the
projection of $V$ along $M$ onto $N$, then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $E$ is linear;
 \index{idempotent}%
  \item $E^2 = E$ \quad (that is, $E$ is \df{idempotent});
  \item $\ran E = N$; and
  \item $\ker E = M$.
 \end{enumerate}
\end{prop}

\begin{prop} Let $V$ be a vector space and suppose that $E\colon V \sto V$ is a function which
satisfies
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $E$ is linear, and
  \item $E^2 = E$.
\end{enumerate}
Then
 \[V = \ker E \oplus \ran E\]
and $E$ is the projection of $V$ along $\ker E$ onto~$\ran E$.
\end{prop}

It is important to note that an obvious consequence of the last two propositions is that
a function $T\colon V \sto V$ from a finite dimensional vector space into itself is a
projection if and only if it is linear and idempotent.







\begin{prop} Let $T\colon V \sto W$ be a linear transformation between vector spaces. Then
  \begin{enumerate}[label=\rm{(\alph*)}]
    \item $T$ has a left inverse if and only if it is injective; and
    \item $T$ has a right inverse if and only if it is surjective.
  \end{enumerate}
\end{prop}














\begin{prop} Let $V$ be a vector space and suppose that $V = M \oplus N$.  If $E$ is the
projection of $V$ along $M$ onto $N$, then $I - E$ is the projection of $V$ along $N$
onto~$M$.
\end{prop}

As we have just seen, if $E$ is a projection on a vector space $V$, then the identity
operator on $V$ can be written as the sum of two projections $E$ and $I - E$ whose
corresponding ranges form a direct sum decomposition of the space $V = \ran E \oplus \ran
(I - E)$.  We can generalize this to more than two projections.

\begin{defn} Suppose that on a vector space $V$ there exist projection operators $E_1$,
\dots, $E_n$ such that
\begin{enumerate}
    \item $I_V = E_1 + E_2 + \dots + E_n$ and
    \item $E_iE_j = 0$ whenever $i \ne j$.
\end{enumerate}
Then we say that $I_V = E_1 + E_2 + \dots + E_n$ is a
 \index{resolution of the identity!in vector spaces}%
\df{resolution of the identity}.
\end{defn}

\begin{prop} If $I_V = E_1 + E_2 + \dots + E_n$ is a resolution of the identity on a vector
space $V$, then $V = \bigoplus_{k=1}^n \ran E_k$.
\end{prop}

\begin{exam} Let $P$ be the plane in $\R^3$ whose equation is $x - z = 0$ and $L$ be the line
whose equations are $y = 0$ and $x = -z$.  Let $E$ be the projection of $\R^3$ along~$L$
onto $P$ and $F$ be the projection of $\R^3$ along~$P$ onto~$L$.  Then
 \[[E] = \begin{bmatrix}
              \frac12  & 0 & \frac12 \\
                  0    & 1 &   0     \\
              \frac12  & 0 & \frac12
         \end{bmatrix} \qquad \text{ and } \qquad
   [F] = \begin{bmatrix}
              \frac12  & 0 & -\frac12 \\
                  0    & 0 &   0     \\
             -\frac12  & 0 &  \frac12
         \end{bmatrix}\,. \]
\end{exam}

\begin{defn} A complex number $\lambda$ is an
 \index{eigenvalue}%
\df{eigenvalue} of an operator $T$ on a vector space $V$ if $\ker(T - \lambda I_V)$
contains a nonzero vector.  Any such vector is an
 \index{eigenvector}%
\df{eigenvector} of $T$ associated with $\lambda$ and $\ker(T - \lambda I_V)$ is the
 \index{eigenspace}
\df{eigenspace} of $T$ associated with~$\lambda$.  The set of all eigenvalues of the
operator $T$ is its
 \index{point spectrum}%
 \index{spectrum!point}%
\df{point spectrum} and is denoted by~$\sigma_p(T)$.
\end{defn}

If $M$ is an $n \times n$ matrix, then $\det(M - \lambda I_n)$ (where $I_n$ is the $n
\times n$ identity matrix) is a polynomial in $\lambda$ of degree~$n$. This is the
\df{characteristic polynomial} of~$M$. A standard way of computing the eigenvalues of an
operator $T$ on a finite dimensional vector space is to find the zeros of the
characteristic polynomial of its matrix representation. It is an easy consequence of the
multiplicative property of the determinant function that the characteristic polynomial of
an operator $T$ on a vector space $V$ is independent of the basis chosen for $V$ and
hence of the particular matrix representation of~$T$ that is used.

\vskip 5 pt

\begin{exam} The eigenvalues of the operator on (the real vector space) $\R^3$ whose matrix
representation is  $ \begin{bmatrix} 0  &  0  &  2 \\
                                     0  &  2  &  0 \\
                                     2  &  0  &  0
                      \end{bmatrix}$ are $-2$ and $+2$, the latter having (both algebraic and
geometric) multiplicity~$2$. The eigenspace associated with the negative eigenvalue is
$\spn\{(1,0,-1)\}$ and the eigenspace associated with the positive eigenvalue is
$\spn\{(1,0,1), (0,1,0)\}$.
\end{exam}

\vskip 5 pt

\begin{prop} Every operator on a complex finite dimensional vector space has an eigenvalue.
\end{prop}

\begin{exam} The preceding proposition does not hold for real finite dimensional spaces.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Consider rotations of the plane.  \ns
\end{proof}

The central fact asserted by the finite dimensional vector space version of the
\emph{spectral theorem} is that every diagonalizable operator on such a space can be
written as a linear combination of projection operators where the coefficients of the
linear combination are the eigenvalues of the operator and the ranges of the projections
are the corresponding eigenspaces.  Thus if $T$ is a diagonalizable operator on a finite
dimensional vector space $V$, then $V$ has a basis consisting of eigenvectors of~$T$.

Here is a formal statement of the theorem.

\begin{thm}[Spectral Theorem: finite dimensional vector space version] Suppose that $T$ is a
 \index{spectral!theorem!for diagonalizable operators on vector spaces}%
diagonalizable operator on a finite dimensional vector space~$V$.  Let $\lambda_1, \dots, \lambda_n$ be
the (distinct) eigenvalues of~$T$. Then there exists a resolution of the identity $I_V =
E_1 + \dots + E_n$, where for each $k$ the range of the projection $E_k$ is the
eigenspace associated with~$\lambda_k$, and furthermore
     \[ T = \lambda_1E_1 +\dots + \lambda_nE_n\,. \]
\end{thm}

\begin{proof} A proof of this theorem can be found in \cite{HoffmanK:1971} on page~212. \ns
\end{proof}

\begin{exam}  Let $T$ be the operator on (the real vector space) $\R^2$ whose matrix representation
is $\begin{bmatrix} -7 &   8 \\ -16 &  17 \end{bmatrix}$.

\vskip 3 pt

 \begin{enumerate}
  \item The characteristic polynomial for~$T$ is
                     $\sbsb cT(\lambda) = \lambda^2 - 10\lambda + 9$.

\vskip 3 pt

  \item The eigenspace $M_1$ associated with the eigenvalue $1$ is $\spn \{(1,1)\}$.

\vskip 3 pt

  \item The eigenspace $M_2$ associated with the eigenvalue $9$ is $\spn \{(1,2)\}$.

\vskip 3 pt

  \item We can write $T$ as a linear combination of projection operators.  In particular,
   \[ T = 1\cdot E_1 + 9\cdot E_2 \text{ where } [E_1] = \begin{bmatrix} 2 & -1 \\ 2 & -1 \end{bmatrix}
\text{ and } [E_2] = \begin{bmatrix} -1 & 1 \\ -2 & 2 \end{bmatrix}\,. \]

\vskip 3 pt

  \item Notice that the sum of $[E_1]$ and $[E_2]$ is the identity matrix and that their
product is the zero matrix.

\vskip 3 pt

  \item The matrix  $S = \begin{bmatrix} 1 &  1  \\ 1 &  2 \end{bmatrix}$  diagonalizes
$[T]$. That is, $S^{-1}\,[T]\,S = \begin{bmatrix} 1 &  0  \\ 0 &  9 \end{bmatrix}$.


\vskip 3 pt


  \item A matrix representing $\sqrt T$ is $\begin{bmatrix} -1 &  2  \\ -4 &  5 \end{bmatrix}$.
 \end{enumerate}
\end{exam}

\begin{exer} Let $T$ be the operator on $\R^3$ whose matrix representation is
    \smash[b]{$\begin{bmatrix}
               2  &  0  &  0 \\
              -1  &  3  &  2 \\
               1  & -1  &  0 \end{bmatrix}$.}
  \begin{enumerate}
    \item Find the characteristic and minimal polynomials of~$T$.
    \item What can be concluded from the form of the minimal polynomial?
    \item Find a matrix which diagonalizes~$T$. What is the diagonal form of $T$ produced by this matrix?
    \item Find (the matrix representation of)~$\sqrt T$.
  \end{enumerate}
\end{exer}

\begin{defn} A vector space operator $T$ is
 \index{nilpotent}%
\df{nilpotent} if $T^n = \vc 0$ for some $n \in \N$.
\end{defn}

An operator on a finite dimensional vector space need not be diagonalizable.  If it is not,
how close to diagonalizable is it?  Here is one answer.

\begin{thm} Let $T$ be an operator on a finite dimensional vector space~$V$.  Suppose that the minimal
polynomial for $T$ factors completely into linear factors
   \[ m_T(x) = (x - \lambda_1)^{r_1} \dots (x - \lambda_k)^{r_k} \]
where $\lambda_1, \dots \lambda_k$ are the (distinct) eigenvalues of~$T$. For each $j$ let
$W_j = \ker(T - \lambda_jI)^{r_j}$ and $E_j$ be the projection of $V$ onto $W_j$ along
$W_1 + \dots + W_{j-1} + W_{j+1} + \dots + W_k$. Then
   \[ V = W_1 \oplus W_2 \oplus \dots \oplus W_k, \]
each $W_j$ is invariant under $T$, and $I = E_1 + \dots + E_k$. Furthermore, the operator
   \[ D = \lambda_1E_1 + \dots + \lambda_kE_k \]
is diagonalizable, the operator
   \[ N = T - D \]
is nilpotent, and $N$ commutes with~$D$.
\end{thm}

\begin{proof} See~\cite{HoffmanK:1971}, pages 222--223. \ns
\end{proof}

\begin{defn} Since, in the preceding theorem, $T = D + N$ where $D$ is diagonalizable and $N$ is nilpotent,
we say that $D$ is the
 \index{diagonalizable!part}%
 \index{part!diagonalizable}%
\df{diagonalizable part} of $T$ and $N$ is the
 \index{nilpotent!part}%
 \index{part!nilpotent}%
\df{nilpotent part} of~$T$.
\end{defn}

\begin{exer} Let $T$ be the operator on $\R^3$ whose matrix representation is
 $\begin{bmatrix}
        3  &  1  & -1 \\
        2  &  2  & -1 \\
        2  &  2  &  0
  \end{bmatrix}$.

 \begin{enumerate}
  \item Find the characteristic and minimal polynomials of~$T$.
  \item Find the eigenspaces of~$T$.
  \item Find the diagonalizable part $D$ and nilpotent part $N$ of~$T$.
  \item Find a matrix which diagonalizes~$D$. What is the diagonal form
of $D$ produced by this matrix?
  \item Show that $D$ commutes with~$N$.
 \end{enumerate}
\end{exer}














\section{Normal Operators on an Inner Product Space}
\begin{defn} Let $V$ be a vector space.  A function $s$ which associates to each pair of vectors
$x$ and $y$ in $V$ a scalar
 \index{s@$s(x,y)$ (semi-inner product)}%
 \index{<s@$s(x,y)$ (semi-inner product)}%
$s(x,y)$ is an
 \index{semi-inner product}%
 \index{semi-inner product!space}%
 \index{product!semi-inner}%
 \index{space!semi-inner product}%
\df{semi-inner product} on $V$ provided that for every $x$, $y$, $z \in V$ and $\alpha \in \K$ the following four conditions are satisfied:
 \begin{enumerate}
   \item $s(x + y , z) = s(x,z) + s(y,z)$;
   \item $s(\alpha x,y) = \alpha s(x,y)$;
   \item $s(x,y) = \conj{s(y,x)}$; and
   \item] $s(x,x) \ge 0$.
 \end{enumerate}
 If, in addition, the function $s$ satisfies
  \begin{enumerate}
    \item For every nonzero $x$ in $V$ we have $s(x,x) > 0$.
  \end{enumerate}
then $s$ is an
 \index{inner product}%
 \index{inner product!space}%
 \index{product!inner}%
 \index{space!inner product}%
\df{inner product} on~$V$.  We will usually write the inner product of two vectors $x$ and $y$ as
 \index{<bracpnt@$\langle x,y \rangle$ (inner product)}%
$\langle x,y \rangle$ rather than $s(x,y)$.

\noindent The overline in (c) denotes complex conjugation (so is redundant in the case of a real vector space).
Conditions (a) and (b) show that a semi-inner product is linear in its first variable.  Conditions (a) and (b) of
proposition~\ref{00015001} say that a complex inner product is
 \index{conjugate!linear}%
 \index{linear!conjugate}%
\df{conjugate linear} in its second variable.  When a scalar valued function of two variables on a complex semi-inner product
space is linear in one variable and conjugate linear in the other, it is often called
 \index{sesquilinear}%
 \index{linear!sesqui-}%
\df{sesquilinear}.  (The prefix ``sesqui-'' means ``one and a half''.)  We will also use the term \emph{sesquilinear} for a
bilinear function on a real semi-inner product space.  Taken together conditions (a)--(d) say that the inner product is a
\emph{positive definite, conjugate symmetric, sesquilinear form}.
\end{defn}

\begin{notn}\label{norm_notn} If $V$ is a vector space which has been equipped with an
semi-inner product and $x \in V$ we introduce the abbreviation
  \[ \norm x := \sqrt{s(x,x)} \]
which is read \emph{the norm of $x$} or \emph{the length of~$x$}.  (This somewhat
optimistic terminology is justified, at least when $s$ is an inner product, in proposition~\ref{00015025} below.)
\end{notn}

\begin{prop}\label{00015001} If $x$, $y$, and $z$ are vectors in a space with semi-inner product $s$ and $\alpha \in \K$, then
 \begin{enumerate}[label=\rm{(\alph*)}]
   \item $s(x,y + z)  = s(x,y) + s(x,z)$,
   \item $s(x,\alpha y) = \conj\alpha s(x,y)$, and
   \item $s(x,x) = 0$ if and only if $x = \vc 0$.
 \end{enumerate}
\end{prop}

\begin{exam}\label{000150012} For vectors $x = (x_1,x_2,\dots,x_n)$ and $y = (y_1,y_2,\dots,y_n)$
belonging to $\K^n$ define
   \[ \langle x,y \rangle = \sum_{k=1}^n x_k \conj{y_k}\,. \]
Then
 \index{K@$\K^n$!as an inner product space}%
 \index{inner product!space!$\K^n$ as a}%
$\K^n$ is an inner product space.
\end{exam}

\begin{exam}\label{000150013} Let $l_2 = l_2(\N)$ be the set of all square summable sequences of
complex numbers. (A sequence $x = (x_k)_{k=1}^\infty$ is
 \index{square summable}%
 \index{summable!square}%
 \index{l@$l_2 = l_2(\N)$!square summable sequences}%
 \index{l@$l_2 = l_2(\N)$!as an inner product space}%
 \index{inner product!space!$l_2$ as a}%
\df{square summable} if $\sum_{k=1}^\infty \abs{x_k}^2 < \infty$.) (The vector space operations are defined
pointwise.) For vectors $x = (x_1,x_2,\dots)$ and $y = (y_1,y_2,\dots)$ belonging to $l_2$ define
   \[ \langle x,y \rangle = \sum_{k=1}^\infty x_k \conj{y_k}\,. \]
Then $l_2$ is an inner product space.  (It must be shown, among other things, that the
series in the preceding definition actually converges.)  A similar space is $l_2(\Z)$ the set of all square summable
 \index{l@$l_2(\Z)$!square summable bilateral sequences}%
 \index{l@$l_2(\Z)$!as an inner product space}%
 \index{inner product!space!$l_2(\Z)$ as a}%
bilateral sequences $x = (\dots,x_{-2},x_{-1},x_0,x_1,x_2,\dots)$ with the obvious inner product.
\end{exam}

\begin{exam}\label{000150014} For $a < b$ let $\fml C([a,b])$ be the family of all
 \index{C@$\fml C([a,b])$!as an inner product space}%
 \index{inner product!space!$\fml C([a,b])$ as a}%
continuous complex valued functions on the interval $[a,b]$.  For every $f$, $g \in \fml C([a,b])$ define
   \[ \langle f,g \rangle = \int_a^b f(x) \conj{g(x)}\,dx. \]
Then $\fml C([a,b])$ is an inner product space.
\end{exam}

Here is the single most useful fact about semi-inner products. It is an inequality attributed variously to Cauchy,
Schwarz, Bunyakowsky, or combinations of the three.

\begin{thm}[Schwarz inequality]\label{00015002} Let $s$ be a semi-inner product defined on a vector space~$V$.
 \index{Schwarz inequality}%
 \index{inequality!Schwarz}%
Then the inequality
   \[ \abs{s(x,y)} \le \norm x \,\norm y. \]
holds for all vectors $x$ and~$y$ in~$V$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Fix $x$, $y \in V$.  For every $\alpha \in \K$ we know that
  \begin{equation}\label{Schwarz_ineq}
      0 \le s(x - \alpha y, x - \alpha y) .
  \end{equation}
Expand the right hand side of~\eqref{Schwarz_ineq} into four terms and write $s(y,x)$ in polar form: $s(y,x) = r e^{i\theta}$,
where $r > 0$ and $\theta \in \R$.  Then in the resulting inequality consider those $\alpha$ of the form $e^{-i\theta}t$ where
$t \in \R$.  Notice that now the right side of~\eqref{Schwarz_ineq} is a quadratic polynomial in~$t$.  What can you say about its
discriminant?  \ns
\end{proof}

\begin{prop} Let $V$ be a vector space with semi-inner product $s$ and let $z \in V$.  Then $s(z,z) = 0$
if and only if $s(z,y) = 0$ for all $y \in V$.
\end{prop}

\begin{prop} Let $V$ be a vector space on which a semi-inner product $s$ has been defined.  Then the set
$L := \{z \in V\colon s(z,z) = 0\}$ is a vector subspace of $V$ and the quotient vector space $V/L$ can be
made into an inner product space by defining
    \[ \langle\, [x],[y]\, \rangle := s(x,y) \]
for all $[x]$, $[y] \in V/L$.
\end{prop}

The next proposition shows that an inner product is continuous in its first variable.  Conjugate symmetry then guarantees
continuity in the second variable.

\begin{prop} If $(x_n)$ is a sequence in an inner product space $V$ which converges to a
vector $a \in V$, then $\langle x_n,y \rangle \sto \langle a,y \rangle$ for every  $y \in V$.
\end{prop}

\begin{exam} If $(a_k)$ is a square summable sequence of real numbers, then the series $\sum_{k=1}^\infty k^{-1}a_k$
converges absolutely.
\end{exam}

\begin{exer} Let $0 < a < b$.
 \begin{enumerate}
  \item If $f$ and $g$ are continuous real valued functions on the
interval $[a,b]$, then
   \[\left(\int_a^b f(x)g(x)\,dx\right)^2
             \le \int_a^b\bigl(f(x)\bigr)^2\,dx \cdot
                       \int_a^b\bigl(g(x)\bigr)^2\,dx.\]

\vskip 3 pt

  \item Use part (a) to find numbers $M$ and $N$ (depending on $a$
and~$b$) such that
   \[M(b-a) \le \ln\left(\frac ba\right) \le N(b-a).\]

\vskip 3 pt

  \item Use part (b) to show that $2.5 \le e \le 3$.
 \end{enumerate}
\end{exer}

\begin{proof}[\emph{Hint for proof}] For (b) try $f(x) = \sqrt x$ and $g(x) = \dfrac1{\sqrt x}$.  Then try
$f(x) = \dfrac1x$ and $g(x) = 1$.  For (c) try $a = 1$ and $b = 3$.  Then try $a = 2$ and $b = 5$.  \ns
\end{proof}

\begin{defn} Let $V$ be a vector space. A function
 \index{<unopn@$\norm x$ (norm of a vector~$x$)}%
$\norm{\hphantom{x}} \colon V \sto \R \colon x \mapsto \norm{x}$ is a
 \index{norm}%
\df{norm} on $V$ if
 \begin{enumerate}
  \item $\norm{x + y} \le \norm{x} + \norm{y}$ \qquad  for all $x$, $y \in V$;
  \item $\norm{\alpha x} = \abs{\alpha}\,\norm{x}$\qquad for all $x \in V$ and
$\alpha \in \K$; and
  \item if $\norm{x} = 0$, then $x = \vc 0$.
 \end{enumerate}
The expression $\norm{x}$ may be read as ``the \emph{norm} of $x$'' or ``the
 \index{length}%
\emph{length} of $x$''.  If the function $\norm{\hphantom{x}}$ satisfies~(a) and~(b)
above (but perhaps not~(c)) it is a
 \index{seminorm}%
\df{seminorm} on~$V$.

A vector space on which a norm has been defined is a
 \index{normed!linear space}%
 \index{normed!vector space|seeonly{normed linear space}}%
 \index{space!normed linear}%
\df{normed linear space} (or
 \index{vector!space!normed}%
\df{normed vector space}). A vector in a normed linear space which has norm $1$ is a
 \index{unit!vector}%
 \index{vector!unit}%
\df{unit vector}.
\end{defn}

\begin{exer} Show why it is clear from the definition that $\norm{\vc 0} = 0$ and that norms
(and seminorms) can take on only positive values.
\end{exer}

Every inner product space is a normed linear space.  In more precise language, an inner product on a vector space induces
 \index{norm!induced by an inner product}%
 \index{induced!norm}
a norm on the space.

\begin{prop}\label{00015025} Let $V$ be an inner product space. The map $x \mapsto \norm
x$ defined on $V$ in~\ref{norm_notn} is a norm on~$V$.
\end{prop}

And every normed linear space is a metric space.  Recall the following definition.

\begin{defn}\label{C019614} Let $M$ be a nonempty set. A function $d \colon M \times M \sto \R$ is a
 \index{metric}%
\df{metric} (or
 \index{distance!function}%
 \index{d@$d(x,y)$ (distance between points $x$ and $y$)}%
\df{distance function}) on $M$ if for all $x$, $y$, $z \in M$
 \begin{enumerate}
     \item $d(x,y) = d(y,x)$,
     \item $d(x,y) \le d(x,z) + d(z,y)$, \qquad (the
 \index{triangle inequality}%
 \index{inequality!triangle}%
\emph{triangle inequality})
     \item $d(x,x) = 0$, and
     \item $d(x,y) = 0$ only if $x = y$.
 \end{enumerate}
If $d$ is a metric on a set $M$, then we say that the pair $(M,d)$ is a
 \index{metric!space}%
 \index{space!metric}%
\df{metric space}.  The notation ``$d(x,y)$'' is read, ``the distance between $x$
and~$y$''. (As usual we substitute the phrase, ``Let $M$ be a metric space \dots '' for
the correct formulation ``Let $(M,d)$ be a metric space \dots''.

If $d \colon M \times M \sto \R$ satisfies conditions (1)--(3), but not (4), then $d$ is a
 \index{pseudometric}%
\df{pseudometric} on~$M$.
\end{defn}

As mentioned above, every normed linear space is a metric space. More precisely, a norm on a vector space
induces a
 \index{metric!induced by a norm}%
metric $d$, which is defined by $d(x,y) = \norm{x - y}$.  That is, the distance between
two vectors is the length of their difference.
  \[ \xy
       \Atriangle/<-`<-`>/[``;x`x-y`y]
  \endxy \]
If no other metric is specified we always regard a normed linear space as a metric space
 \index{induced!metric}%
under this induced metric.  The metric is referred to as the \emph{metric induced by the norm}.

\begin{exam}\label{0001503}  Let $V$ be a normed linear space.  Define
$d\colon V \times V \sto \R$ by $d(x,y) = \norm{x - y}$.  Then $d$ is a metric on~$V$.
If $V$ is only a seminormed space, then $d$ is a pseudometric.
\end{exam}

\begin{defn} Vectors $x$ and $y$ in an inner product space $V$ are
 \index{orthogonal}%
\df{orthogonal} (or
 \index{perpendicular}%
\df{perpendicular}) if $\langle x,y \rangle = 0$.  In this case we write
 \index{<binrelperp@$x \perp y$ (orthogonal vectors)}%
$x \perp y$.  Subsets $A$ and $B$ of $V$ are \df{orthogonal} if $a \perp b$ for every $a
\in A$ and $b \in B$.  In this case we write $A \perp B$.
\end{defn}

\begin{defn} If $M$ and $N$ are subspaces of an inner product space $V$ we use the notation
$V = M \oplus N$ to indicate not only that $V$ is the (vector space) direct sum of $M$
and $N$ but also that $M$ and $N$ are orthogonal. Thus we say that $V$ is the
 \index{internal!orthogonal direct sum}%
 \index{direct!sum!orthogonal}%
 \index{orthogonal!direct sum!of inner product spaces}%
(\df{internal}) \df{orthogonal direct sum} of $M$ and~$N$.
\end{defn}

\begin{prop} Let $a$ be a vector in an inner product space $V$.  Then $a \perp x$ for every
$x \in V$ if and only if $a = 0$.
\end{prop}

\begin{prop} In an inner product space $x \perp y$ if and only if $\norm{x + \alpha y} = \norm{x - \alpha y}$
for all scalars $\alpha$.
\end{prop}

\begin{prop}[The Pythagorean theorem]\label{00015037}  If $x \perp y$ in an inner
 \index{Pythagorean theorem}%
product space, then
   \[ \norm{x + y}^2 = \norm x^2 + \norm y^2\,. \]
\end{prop}

\begin{defn}\label{0001504} Let $V$ and $W$ be inner product spaces.  For $(v,w)$ and
$(v',w')$ in $V \times W$ and $\alpha \in \C$ define
 \begin{align*}
      (v,w) + (v',w') &= (v + v', w + w') \\
\intertext{and}
         \alpha(v,w) &= (\alpha v,\alpha w)\,.
 \end{align*}
This results in a vector space, which is the \emph{(external) direct sum} of $V$ and~$W$.
To make it into an inner product space define
   \[ \langle (v,w),(v',w') \rangle = \langle v,v' \rangle + \langle w,w' \rangle. \]
This makes the direct sum of $V$ and $W$ into an inner product space.  It is the
(\df{external orthogonal})
 \index{external!direct sum}%
 \index{direct!sum!external}%
 \index{direct!sum!of inner product spaces}%
 \index{orthogonal!direct sum!of inner product spaces}%
 \index{<binopdirectsum@$\oplus$ (orthogonal direct sum)}%
\df{direct sum} of $V$ and $W$ and is denoted by $V \oplus W$.
\end{defn}

Notice that the same notation $\oplus$ is used for both internal and external direct sums
and for both vector space direct sums (see definition~\ref{000082}) and orthogonal direct
sums. So when we see the symbol $V \oplus W$ it is important to know which category we
are in: vector spaces or inner product spaces, especially as it is common practice to
omit the word ``orthogonal'' as a modifier to ``direct sum'' even in cases when it is
intended.

\begin{exam} In $\R^2$ let $M$ be the $x$-axis and $L$ be the line whose equation is $y = x$.
If we think of $\R^2$ as a (real) vector space, then it is correct to write $\R^2 = M
\oplus L$.  If, on the other hand, we regard $\R^2$ as a (real) inner product space, then
$\R^2 \ne M \oplus L$ (because $M$ and $L$ are not perpendicular).
\end{exam}

\begin{prop}  Let $V$ be an inner product space.  The inner product on $V$, regarded as a map
from $V \oplus V$ into $\C$, is continuous. So is the norm, regarded as a map from $V$
into $\R$.
\end{prop}

Concerning the proof of the preceding proposition, notice that the maps $(v,v') \mapsto
\norm v + \norm{v'}$, $(v,v') \mapsto \sqrt{\norm v^2 + \norm{v'}^2}$, and $(v,v')
\mapsto \max\{\norm v, \norm{v'}\}$ are all norms on $V \oplus V$. Which one is induced
by the inner product on $V \oplus V$?  Why does it not matter which one we use in proving
that the inner product is continuous?

\begin{prop}[The parallelogram law]\label{parallelogram_law}  If $x$ and $y$ are vectors in an inner product
 \index{parallelogram law}%
space, then
   \[ \norm{x + y}^2 + \norm{x - y}^2 = 2\norm x^2 + 2\norm y^2\,. \]
\end{prop}

While every inner product induces a norm not every norm comes from an inner product.

\begin{exam}\label{00015061} There is no inner product on space $\fml C([0,1])$ which induces the uniform norm.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Use the preceding proposition. \ns
\end{proof}

\begin{prop}[The polarization identity]\label{0001507} If $x$ and $y$ are vectors
 \index{polarization identity}%
in a complex inner product space, then
 \[ \langle x,y \rangle = \tfrac14 (\norm{x + y}^2 - \norm{x - y}^2
                       + i\,\norm{x + iy}^2 - i\,\norm{x - iy}^2)\,. \]
\end{prop}

What is the correct identity for a \emph{real} inner product space?

\begin{notn}\label{0001511} Let $V$ be an inner product space, $x \in V$, and $A$,
$B \subseteq V$. If $x \perp a$ for every $a \in A$, we write $x \perp A$; and if $a
\perp b$ for every $a \in A$ and $b \in B$, we write $A \perp B$.
We define $A^\perp$, the
 \index{orthogonal!complement}%
 \index{complement!orthogonal}%
 \index{<unopur@$A^\perp$ (orthogonal complement)}%
\df{orthogonal complement} of $A$, to be $\{x \in V\colon x \perp A\}$.  Since the phrase ``orthogonal complement of $A$''
is a bit unwieldy, especially when used repeatedly, the symbol ``$A^\perp$'' is usually pronounced ``A perp''.
We write $A^{\perp\perp}$ for $\left(A^\perp\right)^\perp$.
\end{notn}

\begin{prop}\label{0001512} If $A$ is a subset of an inner product space $V$, then
$A^\perp$ is a closed linear subspace of~$V$.
\end{prop}

\begin{prop}[Gram-Schmidt orthonormalization] If $(v^k)$ is a linearly independent sequence in
 \index{Gram-Schmidt orthonormalization}%
 \index{orthonormalization}%
an inner product space~$V$, then there exists an orthonormal sequence $(e^k)$ in~$V$ such
that $\spn\{v^1, \dots, v^n\} = \spn\{e^1, \dots, e^n\}$ for every $n \in \N$.
\end{prop}

\begin{cor}\label{0001514} If $M$ is a subspace of a finite dimensional inner product
space $V$, then $V = M \oplus M^\perp$.
\end{cor}

It will be convenient to say of sequences that they \emph{eventually} have a certain property or
that they \emph{frequently} have that property.

\begin{defn}\label{C013121}  Let $(x_n)$ be a sequence of elements of a set $S$ and $P$ be some
property that members of $S$ may possess.  We say that the sequence $(x_n)$
 \index{eventually}%
\df{eventually} has property $P$ if there exists $n_0 \in \N$ such that $x_n$ has
property $P$ for every $n \ge n_0$.  (Another way to say the same thing: $x_n$ has
property $P$ for all but finitely many~$n$.)
\end{defn}

\begin{exam}\label{exam13122} Denote by
 \index{l@$l_c$!sequences which are eventually zero}%
 \index{l@$l_c$!as a vector space}%
$l_c$ the vector space consisting of all sequences of complex numbers which are eventually zero;
that is, the set of all sequences with only finitely many nonzero entries.  They are also referred to as the sequences with
 \index{finite!support}%
 \index{support!finite}%
\emph{finite support}.  The vector space operations are defined pointwise.  We make the space $l_c$ into an
 \index{l@$l_c$!as an inner product space}%
 \index{inner product!space!$l_c$ as a}%
inner product space by
defining $\langle a,b \rangle = \langle\, (a_n),(b_n)\, \rangle := \sum_{k=1}^\infty a_n \conj{b_n}$.
\end{exam}

\begin{defn}\label{C013131} Let $(x_n)$ be a sequence of elements of a set $S$ and $P$ be some
property that members of $S$ may possess.  We say that the sequence $(x_n)$
 \index{frequently}%
\df{frequently} has property $P$ if for every $k \in \N$ there exists $n \ge k$ such that
$x_n$ has property~$P$. (An equivalent formulation: $x_n$ has property $P$ for infinitely
many~$n$.)
\end{defn}

\begin{exam}\label{exam13132} Let $V = l_2$ be the inner product space of all square-summable sequences of complex numbers
(see example~\ref{000150013}) and $M = l_c$ (see example~\ref{exam13122}).  Then the conclusion of~\ref{0001514} fails.
\end{exam}

\begin{defn}\label{def_alg_dual} A
 \index{linear!functional}%
 \index{functional!linear}%
\df{linear functional} on a vector space $V$ is a linear map from $V$ into its scalar
field. The set of all linear functionals on $V$ is the
 \index{dual!algebraic}%
 \index{algebraic!dual space}%
 \index{space!algebraic dual}%
(\df{algebraic}) \df{dual space} of~$V$.  We will use the
 \index{<unopur@$V^\#$ (algebraic dual space)}%
 \index{V@$V^\#$ (algebraic dual space)}%
notation $V^{\#}$ for the algebraic dual space.
\end{defn}

\begin{thm}[Riesz-Fr\'echet Theorem]\label{0001601} If $f \in V^{\#}$ where $V$ is a
 \index{Riesz-Fr\'echet theorem!finite dimensional version}%
finite dimensional inner product space, then there exists a unique vector $a$ in $V$ such
that
    \[ f(x) = \langle x,a \rangle \]
for all $x$ in $V$.
\end{thm}

We will prove shortly (in~\ref{000342})that every continuous linear functional on an \emph{arbitrary} inner
product space has the above representation. The finite dimensional version stated here is a special case, since
every linear map on a finite dimensional inner product space is continuous (see proposition~\ref{00015033}).

\begin{defn}\label{0001602} Let $T\colon V \sto W$ be a linear transformation between complex
inner product spaces. If there exists a function $T^*\colon W \sto V$ which satisfies
   \[ \langle T\vc v,\vc w \rangle = \langle \vc v,T^*\vc w \rangle \]
for all $\vc v \in V$ and $\vc w \in W$, then $T^*$ is the
 \index{adjoint!of an operator on an inner product space}%
 \index{<unopurstar@$T^*$ (inner product space adjoint)}%
 \index{T@$T^*$ (inner product space adjoint)}%
\df{adjoint} (or \df{conjugate transpose}, or \df{Hermitian conjugate}) of~$T$.
\end{defn}

\begin{prop}\label{00016021} If $T\colon V \sto W$ is a linear map between finite
dimensional inner product spaces, then $T^*$ exists.
\end{prop}

\begin{proof}[\emph{Hint for proof}] The functional $\phi\colon V \times W \sto \C \colon
(v,w) \mapsto \langle Tv,w \rangle$ is sesquilinear. Fix $w \in W$ and define $\phi_w
\colon V \sto \C \colon v \mapsto \phi(v,w)$. Then $\phi_w \in V^{\#}$. Use the
\emph{Riesz-Fr\'echet theorem}~(\ref{0001601}).  \ns
\end{proof}

\begin{prop} If $T\colon V \sto W$ is a linear map between finite dimensional inner product
spaces, then the function $T^*$ defined above is linear and $T^{**} = T$.
\end{prop}

\begin{thm}[The fundamental theorem of linear algebra]\label{00016025} If
 \index{fundamental!theorem of linear algebra}%
$T\colon V \sto W$ is a linear map between finite dimensional inner product spaces, then
   \[ \ker T^* = (\ran T)^\perp \quad \text{ and } \quad \ran T^* = (\ker T)^\perp\,. \]
\end{thm}

\begin{defn}\label{000161} An operator $U$ on an inner product space is
 \index{unitary!operator}%
 \index{operator!unitary}%
\df{unitary} if $UU^* = U^*U = I$, that is if $U^* = U^{-1}$.
\end{defn}

\begin{defn}\label{0001612} Two operators $R$ and $T$ on an inner product space $V$ are
 \index{unitary!equivalence}%
 \index{equivalent!unitarily}%
\df{unitarily equivalent} if there exists a unitary operator $U$ on $V$ such that $R =
U^*TU$.
\end{defn}

\begin{prop} If $V$ is an inner product space, then unitary equivalence is in fact an
equivalence relation on~$\ofml L(V)$.
\end{prop}

\begin{defn} An operator $T$ on a finite dimensional inner product space $V$ is
 \index{unitary!diagonalization}%
 \index{diagonalizable!unitarily}%
\df{unitarily diagonalizable} if there exists an orthonormal basis for $V$ with respect
to which $T$ is diagonal. Equivalently, an operator on a finite dimensional inner product
space \emph{with basis} is diagonalizable if it is unitarily equivalent to a diagonal
operator.
\end{defn}

\begin{defn} An operator $T$ on an inner product space is
 \index{self-adjoint!operator}%
 \index{operator!self-adjoint}%
\df{self-adjoint} (or
 \index{Hermitian!operator}%
 \index{operator!Hermitian}%
\df{Hermitian}) if $T^* = T$.
\end{defn}

\begin{defn}\label{000164} A projection $P$ in an inner product space is an
 \index{projection!orthogonal}%
 \index{orthogonal!projection!in an inner product space}%
 \index{operator!orthogonal projection}%
\df{orthogonal projection} if it is self-adjoint.  If $M$ is the range of an orthogonal
projection we will adopt the notation
 \index{PM@$\sbsb PM$ (orthogonal projection onto~$M$)}%
$\sbsb PM$ for the projection rather than the more cumbersome~$\sbsb E{M^\perp M}$.
\end{defn}

\begin{cau} A projection on a vector space or a normed linear space is linear and idempotent,
while an orthogonal projection on an inner product space is linear, idempotent, and
self-adjoint.  This otherwise straightforward situation is somewhat complicated by a
common tendency to refer to orthogonal projections simply as ``projections''.  In fact,
later in these notes we will adopt this very convention.  In inner product spaces
$\oplus$ usually indicates orthogonal direct sum and ``projection'' usually means
``orthogonal projection''. In many elementary linear algebra texts, where everything
happens in $\R^n$, it can be quite exasperating trying to divine whether on any
particular page the author is treating $\R^n$ as a vector space or as an inner product
space.
\end{cau}

\begin{prop} If $P$ is an orthogonal projection on an inner product space $V$, then we have
the orthogonal direct sum decomposition $V = \ker P \oplus \ran P$.
\end{prop}

\begin{defn} If $I_V = P_1 + P_2 + \dots + P_n$ is a resolution of the identity in an
inner product space~$V$ and each $P_k$ is an orthogonal projection, then we say that $I =
P_1 + P_2 + \dots + P_n$ is an
 \index{resolution of the identity!orthogonal}%
 \index{orthogonal!resolution of the identity}%
\df{orthogonal resolution of the identity}.
\end{defn}

\begin{prop} If $I_V = P_1 + P_2 + \dots + P_n$ is an orthogonal resolution of the identity
on an inner product space $V$, then $V = \bigoplus_{k=1}^n \ran P_k$.
\end{prop}

\begin{defn} An operator $N$ on an inner product space is
 \index{normal!operator}%
 \index{operator!normal}%
\df{normal} if $NN^* = N^*N$.
\end{defn}

Two great triumphs of linear algebra are the \emph{spectral theorem} for operators on a
(complex) finite dimensional inner product space (see \ref{00017}), which gives a simply
stated necessary and sufficient condition for an operator to be unitarily diagonalizable,
and theorem~\ref{00018}, which gives a complete classification of those operators.

\begin{thm}[Spectral Theorem for Finite Dimensional Complex Inner Product Spaces]\label{00017}
Let $T$ be an operator on a finite dimensional inner product space~$V$ with (distinct)
 \index{spectral!theorem!for normal operators on inner product spaces}%
eigenvalues $\lambda_1, \dots, \lambda_n$.  Then $T$ is unitarily diagonalizable if and
only if it is normal.  If $T$ is normal, then there exists an orthogonal resolution of
the identity $I_V = P_1 + \dots + P_n$, where for each $k$ the range of the orthogonal
projection $P_k$ is the eigenspace associated with~$\lambda_k$, and furthermore
     \[ T = \lambda_1P_1 +\dots + \lambda_nP_n\,. \]
\end{thm}

\begin{proof}  See \cite{Roman:2005}, page 227. \ns
\end{proof}

\begin{thm}\label{00018} Two normal operators on a finite dimensional inner product space
 \index{normal!operator!conditions for unitary equivalence}%
are unitarily equivalent if and only if they have the same eigenvalues each with the same
multiplicity; that is, if and only if they have the same characteristic polynomial.
\end{thm}

\begin{proof} See \cite{HoffmanK:1971}, page 357.   \ns
\end{proof}

Much of the remainder of these notes is devoted to the definitely nontrivial adventure of finding
appropriate generalizations of the preceding two results to the infinite dimensional
setting and to the astonishing mathematical landscapes which come into view along the way.

\begin{exer} Let $N = \dfrac13\begin{bmatrix}
                                4+2i & 1-i  & 1-i  \\
                                1-i  & 4+2i & 1-i  \\
                                1-i  & 1-i  & 4+2i \end{bmatrix}$.

\vskip 3 pt
 \begin{enumerate}
  \item The matrix $N$ is normal.

\vskip 3 pt

  \item Thus according to the \emph{spectral theorem} $N$
can be written as a linear combination of orthogonal projections.
Explain clearly how to do this (and carry out the computation).
 \end{enumerate}
\end{exer}














\endinput
\chapter{MULTIPLIER ALGEBRAS}\label{multiplier_algebras}

\section{Hilbert Modules}
\begin{notn}\label{0038014} The inner products that occur previously in these notes
and that one encounters in standard textbooks and monographs on Hilbert spaces,
functional analysis, and so on, are linear in the first variable and conjugate linear in
the second.  Most contemporary operator algebraists have chosen to work with objects
called \emph{right} Hilbert $A$-modules ($A$ being a $C^*$-algebra).  For such modules it
turns out to be more convenient to have ``inner products'' that are linear in the second
variable and conjugate linear in the first.  While this switch in conventions may provoke
some slight irritation, it is, mathematically speaking, of little consequence. Of course,
we would like Hilbert spaces to be examples of Hilbert $\C$-modules.  To make this
possible we equip a Hilbert space, whose inner product is denoted by
$\langle\hphantom{X},\hphantom{X}\rangle$ with a new ``inner product'' defined
 \index{<bracketspointy@$\langle x\,\vert\,y \rangle$ (inner product)}%
by $\langle x\,\vert\,y\rangle := \langle y,x \rangle$. This ``inner product'' is linear
in the second variable and conjugate linear in the first. I will try to be consistent in
using $\langle\hphantom{X},\hphantom{X}\rangle$ for the standard inner product and
$\langle\hphantom{X}\,\vert\,\hphantom{X}\rangle$ for the one which is linear in its
second variable.

Another (very common) solution to this problem is to insist that an inner product is
always linear in its second variable and ``correct'' standard texts, monographs, and
papers accordingly.
\end{notn}

\begin{conv} In light of the preceding remarks we will in the sequel use the word
``sesquilinear'' to mean either
 \index{sesquilinear}%
 \index{conventions!about \emph{sesquilinear}}%
\emph{linear in the first variable and conjugate linear in the second} or \emph{linear in
the second variable and conjugate linear in the first}.
\end{conv}

\begin{defn}\label{0038017} Let $A$ be a nonzero $C^*$-algebra.  A vector space $V$ is an
\df{$A$-module}
 \index{module}%
 \index{amodule@$A$-module}%
if there is a bilinear map
  \[ B\colon V \times A \sto V\colon (x,a) \mapsto xa \]
such that $x(ab) = (xa)b$ holds for all $x \in V$ and $a$, $b \in A$.  We also require
that $x \vc 1_A = x$ for every $x \in V$ if $A$ is unital. (A function of two variables
 \index{bilinear}%
is \df{bilinear} if it is linear in both of its variables.)
\end{defn}

\begin{defn}\label{0038021} For clarity in some of the succeeding material it will
be convenient to have the following formal definition. A
 \index{vector!space}%
\df{(complex) vector space} is a triple $(V,+,M)$ where $(V,+)$ is an Abelian group and
$M\colon \C \sto \Hom(V,+)$ is a unital ring homomorphism. (As you would guess,
$\Hom(V,+)$ denotes the unital ring of endomorphisms of the group $(V,+)$.) Thus an
 \index{algebra}%
\df{algebra} is an ordered quadruple $(A,+,M,\,\cdot\,)$ where $(A,+,M)$ is a vector
space and $\,\cdot\,\colon A \times A \sto A \colon (a,b) \mapsto a \cdot b = ab$ is a
binary operation on $A$ satisfying the conditions specified in~\ref{defn_alg}.
\end{defn}

\begin{exer} Make sure the preceding definition of \emph{vector space} is equivalent
to the one you are accustomed to.
\end{exer}

\begin{defn} Let $(A,+,M,\,\cdot\,)$ be a (complex) algebra. Then $A^{\textrm{op}}$
 \index{<A@$A^{\textrm{op}}$ (opposite algebra of $A$)}%
is the algebra $(A,+,M,*)$ where $a*b = b\cdot a$ for all $a$, $b \in A$. This is the
 \index{opposite algebra}%
 \index{algebra!opposite}%
\df{opposite algebra} of~$A$.  It is just $A$ with the order of multiplication reversed.
If $A$ and $B$ are algebras, then any function $f \colon A \sto B$ induces in an obvious
fashion a function from $A$ into $B^{\textrm{op}}$ (or from $A^{\textrm{op}}$ into $B$,
or from $A^{\textrm{op}}$ into $B^{\textrm{op}}$). We will denote all these functions
simply by~$f$.
\end{defn}

\begin{defn}\label{0038027} Let $A$ and $B$ be algebras. A function
$\phi\colon A \sto B$ is an
 \index{antihomomorphism}%
\df{antihomomorphism} if the function $f\colon A \sto B^{\textrm{op}}$ is a homomorphism.
A bijective antihomomorphism is an
 \index{anti-isomorphism}%
\df{anti-isomorphism}. In example~\ref{0022057} we encountered \emph{anti-isomorphisms}
of Hilbert spaces. These were a bit different since instead of reversing multiplication
(which Hilbert spaces don't have) they conjugate scalars. In either case an
anti-isomorphism is not something terribly different from an isomorphism but actually
something quite similar.
\end{defn}

The notion of an $A$-module (where $A$ is an algebra) was defined in~\ref{0038017}. You
may prefer the following alternative definition, which is more in line with the
definition of \emph{vector space} given in~\ref{0038021}.

\begin{defn}\label{0038031} Let $A$ be an algebra. An \df{$A$-module} is an ordered quadruple
 \index{module}%
 \index{amodule@$A$-module}%
$(V,+,M,\Phi)$ where $(V,+,M)$ is a vector space and $\Phi\colon A \sto \ofml L(V)$ is an
algebra homomorphism.  If $A$ is unital we require also that $\Phi$ be unital.
\end{defn}

\begin{exer} Check that the definitions of \emph{$A$-module} given in~\ref{0038017}
and~\ref{0038031} are equivalent.
\end{exer}

We now say precisely what it means,when $A$ is a $C^*$-algebra, to give an $A$-module an
$A$-valued inner product.

\begin{defn} Let $A$ be a $C^*$-algebra.  A
 \index{semi-inner product!$A$-module}%
 \index{module!semi-inner product $A$-}%
 \index{amodule@$A$-module!semi-inner product}%
\df{semi-inner product $A$-module} is an $A$-module $V$ together with a mapping
  \[ \beta\colon V \times V \sto A\colon (x,y) \mapsto \langle x\vert\, y\rangle \]
which is linear in its second variable and satisfies
 \begin{enumerate}
    \item[(i)] $\langle x\vert\, ya\rangle = \langle x\vert\, y\rangle a$,
    \item[(ii)] $\langle x\vert\, y\rangle = \langle y\vert\, x\rangle^*$, and
    \item[(iii)] $\langle x\vert\, x\rangle \ge \vc 0$
 \end{enumerate}
for all $x$, $y \in V$ and $a \in A$. It is an
 \index{inner product!$A$-module}%
 \index{module!inner product $A$-}%
 \index{amodule@$A$-module!inner product}%
\df{inner product $A$-module} (or a
 \index{pre-Hilbert!$A$-module}%
 \index{module!pre-Hilbert $A$-}%
 \index{amodule@$A$-module!pre-Hilbert}%
\df{pre-Hilbert $A$-module}) if additionally
 \begin{enumerate}
    \item[(iv)] $\langle x\vert\, x\rangle = \vc 0$ implies that $x = \vc 0$
 \end{enumerate}
when $x \in V$.  We will refer to the mapping $\beta$ as an
 \index{inner product!$A$-valued}%
 \index{Aval@$A$-valued (semi-)inner product}%
\df{$A$-valued (semi-)inner product} on~$V$.
\end{defn}

\begin{exam} Every inner product space is an inner product $\C$-module.
\end{exam}

\begin{prop} Let $A$ be a $C^*$-algebra and $V$ be a semi-inner product $A$-module.
The semi-inner product $\langle\hphantom{x}\vert\,\hphantom{x}\rangle$ is conjugate
linear in its first variable both literally and in the sense that $\langle va \vert\,
w\rangle = a^* \langle v \vert\, w\rangle$ for all $v$, $w \in V$ and $a \in A$.
\end{prop}

\begin{prop}[Schwarz inequality---for inner product $A$-modules]\label{0038038} Let
$V$ be an inner product $A$-module where $A$ is a $C^*$-algebra. Then
  \[ \langle x\vert\, y \rangle^*\langle x\vert\, y \rangle
          \le \norm{\langle x\vert\, x \rangle}\,\langle y\vert\, y \rangle \]
for all $x$, $y \in V$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Show that no generality is lost in assuming that
$\norm{\langle x\vert\, x\rangle} = 1$. Consider the positive element $\langle xa -
y\vert\, xa - y\rangle$ where $a = \langle x \vert\, y \rangle$.  Use
propositions~\ref{0018203} and~\ref{0018204}. \ns
\end{proof}

\begin{defn}\label{0038041} For every element $v$ of an inner product $A$-module
(where $A$ is a $C^*$-algebra) define
  \[ \norm v := \norm{\langle v \vert\, v \rangle}^{1/2}. \]
\end{defn}

\begin{prop}[Yet another Schwarz inequality] Let $A$ be a $C^*$-algebra and $V$ be an
inner product $A$-module.  Then for all $v$, $w \in V$
  \[ \norm{\langle v \vert\, w\rangle} \le \norm v\,\norm w. \]
\end{prop}

\begin{cor} If $v$ and $w$ are elements of an inner product $A$-module (where $A$ is a
$C^*$-algebra), then $\norm{v + w} \le \norm v + \norm w$ and the map $x \mapsto \norm x$
is a norm on~$V$.
\end{cor}

\begin{prop} If $A$ is a $C^*$-algebra and $V$ is an inner product $A$-module, then
  \[ \norm{va} \le \norm v\, \norm a \]
for all $v \in V$ and $a \in A$.
\end{prop}

\begin{defn} Let $A$ be a $C^*$-algebra and $V$ be an inner product $A$-module. If $V$
is complete with respect to (the metric induced by) the norm defined in~\ref{0038041},
then $V$ is a
 \index{amodule@$A$-module!Hilbert}%
 \index{Hilbert!$A$-module}%
\df{Hilbert $A$-module}.
\end{defn}

\begin{exam}\label{0038111} For $a$ and $b$ in a $C^*$-algebra $A$
 \index{Hilbert!$A$-module!$A$ as a}%
 \index{amodule@$A$-module!Hilbert!$A$ as a}%
define
  \[ \langle a \vert\, b\rangle := a^*b\,. \]
Then $A$ is itself a Hilbert $A$-module.  Any closed right ideal in $A$ is also a Hilbert
$A$-module.
\end{exam}

\begin{defn} Let $V$ and $W$ be Hilbert $A$-modules where $A$ is a $C^*$-algebra. A
mapping $T\colon V \sto W$ is
 \index{alinear@$A$-linear}%
 \index{linear!$A$-}%
\df{$A$-linear} if it is linear and if $T(va) = T(v)a$ holds for all $v \in V$ and $a \in
A$.  The mapping $T$ is a
 \index{amodule@$A$-module!morphism}%
 \index{module!morphism}%
 \index{morphism!Hilbert $A$-module}%
 \index{Hilbert!$A$-module!morphism}%
\df{Hilbert $A$-module morphism} if it is bounded and $A$-linear.
\end{defn}

Recall from~\ref{def000926} that every Hilbert space operator has an adjoint.
This is not true for Hilbert $A$-modules

\begin{defn}  Let $V$ and $W$ be Hilbert $A$-modules where $A$ is a $C^*$-algebra. A
function $T\colon V \sto W$ is
 \index{adjointable}%
\df{adjointable} if there exists a function $T^*\colon W \sto V$ satisfying
  \[ \langle Tv \vert\, w \rangle = \langle v \vert\, T^*w \rangle \]
for all $v \in V$ and $w \in W$.  The function $T^*$, if it exists, is the
 \index{adjoint}
\df{adjoint} of~$T$.  Denote by
 \index{L@$\ofml L(V,W)$, $\ofml L(V)$ (adjointable maps)}%
$\ofml L(V,W)$ the family of adjointable maps from $V$ to~$W$.  We shorten $\ofml L(V,V)$
to~$\ofml L(V)$.
\end{defn}

\begin{prop} Let $V$ and $W$ be Hilbert $A$-modules where $A$ is a $C^*$-algebra. If a
function $T\colon V \sto W$ is adjointable, then it is a Hilbert $A$-module morphism.
Furthermore if $T$ is adjointable, then so is its adjoint and $T^{**} = T$.
\end{prop}

\begin{exam} Let $X$ be the unit interval $[0,1]$ and let $Y = \{0\}$.  With its usual topology
$X$ is a compact Hausdorff space and $Y$ is a subspace of~$X$.  Let $A$ be the
$C^*$=algebra $\fml C(X)$ and $J_0$ be the ideal $\{f \in A\colon f(0) = 0\}$ (see
proposition~\ref{0006836}).  Regard $V = A$ and $W = J_0$ as Hilbert $A$-modules (see
example~\ref{0038111}).  Then the inclusion map $\iota\colon V \sto W$ is a Hilbert
$A$-module morphism which is not adjointable.
\end{exam}

\begin{prop} Let $A$ be a $C^*$-algebra. The pair of maps $V \mapsto V$, $T \mapsto T^*$
is a contravariant functor from the category of Hilbert $A$-modules and adjointable maps
into itself.
\end{prop}

\begin{prop}\label{0038244} Let $A$ be a $C^*$-algebra and $V$ be a Hilbert $A$-module.
 \index{C*@$C^*$-algebra!$\ofml L(V)$ as a}%
 \index{L@$\ofml L(V)$!as a $C^*$-algebra}%
Then $\ofml L(V)$ is a unital $C^*$-algebra.
\end{prop}

\begin{notn} Let $V$ and $W$ be Hilbert $A$-modules where $A$ is a $C^*$-algebra.
 \index{theta@$\Theta_{v,w}$}%
For $v \in V$ and $w \in W$ let
  \[ \Theta_{v,w}\colon W \sto V\colon x \mapsto v\langle w \vert\, x\rangle\,. \]
(Compare this with~\ref{0022431fa}.)
\end{notn}

\begin{prop} The map $\Theta$ defined above is sesquilinear.
\end{prop}

\begin{prop} Let $V$ and $W$ be Hilbert $A$-modules where $A$ is a $C^*$-algebra. For
every $v \in V$ and $w \in W$ the map $\Theta_{v,w}$ is adjointable and $(\Theta_{v,w})^*
= \Theta_{w,v}$.
\end{prop}

The next proposition generalizes propositions~\ref{prop_op_act_otimes1} and~\ref{prop_op_act_otimes2}.

\begin{prop} Let $U$, $V$, $W$, and $Z$ be Hilbert $A$-modules where $A$ is a $C^*$-algebra.
If $S \in \ofml L(Z,W)$ and $T \in \ofml L(V,U)$, then
  \[ T \Theta_{v,w} = \Theta_{Tv,w} \qquad \text{ and } \qquad
                               \Theta_{v,w} S = \Theta_{v,S^*w} \]
for all $v \in V$ and $w \in W$.
  \[ Z \to^S W \to^{\Theta_{v,w}} V \to^T U \]
\end{prop}

\begin{prop} Let $U$, $V$, and $W$ be Hilbert $A$-modules where $A$ is a $C^*$-algebra.
Suppose $u \in U$; $v$, $v' \in V$; and $w \in W$.  Then
  \[ \Theta_{u,v}\Theta_{v',w} = \Theta_{u\langle v \vert\,v' \rangle, w}
                               = \Theta_{u, w\langle v'\vert\,v \rangle}\,. \]
\end{prop}

\begin{notn}\label{0038331} Let $A$ be a $C^*$-algebra and $V$ and $W$ be Hilbert
$A$-modules.  We denote by
 \index{K@$\ofml K(V,W)$, $\ofml K(V)$}%
$\ofml K(W,V)$ the closed linear span of $\{\Theta_{v,w}\colon v \in V \text{ and } w \in
W\}$.  As usual we shorten $\ofml K(V,V)$ to~$\ofml K(V)$.
\end{notn}

\begin{prop}\label{0038334} If $A$ is a $C^*$-algebra and $V$ is a Hilbert
$A$-module, then $\ofml K(V)$ is an ideal in the $C^*$-algebra~$\ofml L(V)$.
\end{prop}

The next example is intended as justification for the standard practice of identifying a
$C^*$-algebra $A$ with $\ofml K(A)$.

\begin{exam}\label{0038337} If we regard a $C^*$-algebra $A$ as an $A$-module (see
example~\ref{0038111}), then $\cstariso{\ofml K(A)}A$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] As in corollary~\ref{0013151} define for each
$a \in A$ the left multiplication operator $L_a\colon A \sto A\colon x \mapsto ax$.  Show
that each such operator is adjointable and that the map $L\colon A \sto \ofml L(A) \colon
a \mapsto L_a$ is a $*\,$-isomorphism onto a $C^*$-subalgebra of~$\ofml L(A)$. Then
verify that $\ofml K(A)$ is the closure of the image under $L$ of the span of products of
elements of~$A$. \ns
\end{proof}

\begin{exam}\label{0038341} Let $H$ be a Hilbert space regarded as a $\C$-module.  Then
$\ofml K(H)$ (as defined in~\ref{0038331}) is the ideal of compact operators on~$H$ (see
proposition~\ref{prop_K_is_clo_FR}).
\end{exam}

\begin{proof} See~\cite{RaeburnW:1998}, example 2.27. \ns \end{proof}

The preceding example has lead many researchers, when dealing with an arbitrary Hilbert
module~$V$, to refer to members of $\ofml K(V)$ as \emph{compact operators}.  This is
dubious terminology since such operators certainly need not be compact. (For example, if
we regard an infinite dimensional unital $C^*$-algebra $A$ as an $A$-module, then
$\Theta_{\vc 1,\vc 1} = I_A$, but the identity operator on $A$ is not compact---see
example~\ref{cor_id_op_not_cpt}.)

The fact that in these notes rather limited use is made of Hilbert $C^*$-modules should
not lead you to think that their study is specialized and/or of marginal interest.  To
the contrary, it is currently an important and vigorous research area having applications
to fields as diverse as $K$-theory, graph $C^*$-algebras, quantum groups, quantum
probability, vector bundles, non-commutative geometry, algebraic and geometric topology,
operator spaces and algebras, and wavelets.  Take a look at Michael Frank's
webpage~\cite{Frank:2010}, \emph{Hilbert C*-modules and related subjects---a guided
reference overview}, where he lists 1531 references (as of his 11.09.10 update) to books,
papers, and theses dealing with such modules and categorizes them by application. There
is an interesting graphic (on page 9) illustrating the growth of this field of
mathematics. It covers material from the pioneering efforts in the 50's and early 60's
(0--2 papers per year) to the time of this writing (about 100 papers per year).


































\section{Essential Ideals}
\begin{exam} If $A$ and $B$ are $C^*$-algebras, then $A$ (more precisely, $A \oplus \{\vc
0\}$) is an ideal in $A \oplus B$.
\end{exam}

\begin{conv} As the preceding example suggests, it is conventional to regard $A$ as
 \index{conventions!$A$ is a subset of $A \oplus B$}%
a subset of $A \oplus B$. In the sequel we will do this without further mention.
\end{conv}

\begin{notn} For an element $c$ of an algebra $A$
 \index{Ic@$I_c$}%
let
  \[ I_c := \biggl\{a_0c + cb_0 + \sum_{k=1}^p a_kcb_k \colon
   \text{$p \in \N$, $a_0$, \dots, $a_p$, $b_0$, \dots, $b_p \in A$}\biggr\} \]
\end{notn}

\begin{prop} If $c$ is an element of an algebra $A$, then $I_c$ is an (algebraic) ideal
in~$A$.
\end{prop}

Notice that in the preceding proposition no claim is made that the algebraic ideal $I_c$
must be proper. It may well be the case that $I_c = A$ (as, for example, when $c$ is an
invertible element of a unital algebra).

\vskip 2 pt

\begin{defn} Let $c$ be an element of a $C^*$-algebra $A$.
 \index{Jc@$J_c$ (principal (closed) ideal containing~$c$)}%
 \index{principal!ideal}%
 \index{ideal!principal}%
Define $J_c$, the \df{principal ideal} containing $c$, to be the intersection of the
family of all (closed) ideals of $A$ which contain~$c$.  Clearly, $J_c$ is the smallest
ideal containing~$c$.
\end{defn}

\begin{prop} In a $C^*$-algebra the closure of an algebraic ideal is an ideal.
\end{prop}

\begin{exam} The closure of a proper algebraic ideal in a $C^*$-algebra need not be a
proper ideal. For example, $l_c$, the set of sequences of complex numbers which are
eventually zero, is dense in the $C^*$-algebra $l_0 = \fml C_0(\N)$.  (But recall
proposition~\ref{0006801}.)
\end{exam}

\begin{prop} If $c$ is an element of a $C^*$-algebra, then $J_c = \clo{I_c}$.
\end{prop}

\begin{notn} We adopt a standard notational convention.  If $A$ and $B$ are nonempty
subsets of an algebra. By $AB$ we mean the linear span of products of elements in $A$ and
elements in $B$; that is,
 \index{<AB@$AB$ (in algebras the span of products of elements)}%
 \index{conventions!in an algebra $AB$ denotes the span of products}%
$AB = \spn\{ab \colon \text{$a \in A$ and $b \in B$}\}$.  (Note that in
definition~\ref{000551} it makes no difference whether we take $AJ$ to mean the set of
products of elements in $A$ with elements in $J$ or the span of that set.)
\end{notn}

\begin{prop} If $I$ and $J$ are ideals in a $C^*$-algebra, then $\clo{IJ} =
I \cap J$.
\end{prop}

\vskip 2 pt

A nonunital $C^*$-algebra $A$ can be embedded as an ideal in a unital $C^*$-algebra in
different ways.  The smallest unital $C^*$-algebra containing $A$ is its unitization~$\wt
A$ (see proposition~\ref{0015213}). Of course there is no largest unital $C^*$-algebra in
which $A$ can be embedded as an ideal because if $A$ is embedded as an ideal in a unital
$C^*$-algebra $B$ and $C$ is \emph{any} unital $C^*$-algebra, then $A$ is an ideal in the
still larger unital $C^*$-algebra $B \oplus C$.  The reason this larger unitization is
not of much interest is that the intersection of the ideal $C$ with $A$ is~$\{\vc 0\}$.
This motivates the following definition.

\vskip 2 pt

\begin{defn} An ideal $J$ in a $C^*$-algebra $A$ is
 \index{essential!ideal}%
 \index{ideal!essential}%
\df{essential} if and only if $I \cap J \ne \vc 0$ for every nonzero ideal $I$ in~$A$.
\end{defn}

\begin{exam} A $C^*$-algebra $A$ is an essential ideal in its unitization $\wt A$ if and
only if $A$ is \emph{not} unital.
\end{exam}

\begin{exam} If $H$ is a Hilbert space the ideal of compact operators $\ofml K(H)$
is an essential ideal in the $C^*$-algebra~$\ofml B(H)$.
\end{exam}

\begin{defn} Let $J$ be an ideal in a $C^*$-algebra~$A$. Then we define
$J^\perp$, the
 \index{annihilator}%
 \index{<superscript@$J^\perp$ (annihilator of $J$)}%
\df{annihilator} of $J$, to be $\bigl\{a \in A\colon Ja = \{\vc 0\}\bigr\}$.
\end{defn}

\begin{prop} If $J$ is an ideal in a $C^*$-algebra, then so is~$J^\perp$.
\end{prop}

\begin{prop}\label{0038465} An ideal $J$ in a $C^*$-algebra $A$ is essential if and only
if $J^\perp = \{\vc 0\}$.
\end{prop}

\begin{prop}\label{0038468} If $J$ is an ideal in a $C^*$-algebra, then $\bigl(J \oplus
J^\perp\bigr)^\perp = \{\vc 0\}$.
\end{prop}

\begin{notn} Let $f$ be a (real or) complex valued function on a set $S$. Then
  \[ Z_f := \{ s \in S\colon f(s) = 0\}. \]
This is the
 \index{zero set}%
 \index{Zf@$Z_f$ (zero set of~$f$)}%
\df{zero set} of~$f$.
\end{notn}

 \vskip 2 pt

Suppose that $A$ is a nonunital commutative $C^*$-algebra. By the second
\emph{Gelfand-Naimark theorem}~\ref{00152156} there exists a noncompact locally compact
Hausdorff space $X$ such that $A = \fml C_0(X)$. (Here, of course, we are permitting
ourselves a conventional abuse of language: for literal correctness the indicated
equality should be an isometric $*\,$-isomorphism.)  Now let $Y$ be a compact Hausdorff
space in which $X$ is an open subset and let $B = \fml C(Y)$.  Then $B$ is a unital
commutative $C^*$-algebra. Regard $A$ as embedded as an ideal in $B$ by means of the map
$\iota\colon A \sto B\colon f \mapsto \wt f$ where
  \[ \wt f(y) = \left\{%
                \begin{array}{ll}
                         f(y), & \hbox{if $y \in X$;} \\
                         0, & \hbox{otherwise.} \\
                \end{array}%
                \right.\]
Notice that the closed set $X^c$ is $\bigcap\{Z_{\wt f}\,\colon f \in \fml C_0(X)\}$.

\vskip 2 pt

\begin{prop}\label{0038474} Let the notation be as in the preceding paragraph. Then the
ideal $A$ is essential in $B$ if and only if the open subset $X$ is dense in~$Y$.
\end{prop}

Thus the property of an ideal being essential in the context of unitizations of nonunital
commutative $C^*$-algebras corresponds exactly with the property of an open subspace
being dense in the context of compactifications of noncompact locally compact Hausdorff
spaces.





















\section{Compactifications and Unitizations}
In definition~\ref{00152131} we called the object whose existence was proved in the
preceding proposition~\ref{0015213} ``the'' unitization of a $C^*$-algebra. The definite
article there is definitely misleading.  Just as a topological space may have many
different compactifications, a $C^*$-algebra may have many unitizations. If $A$ is a
nonunital commutative $C^*$-algebra, it is clear from corollary~\ref{001924} that the
unitization $\wt A$ is the smallest possible unitization of~$A$.  Similarly, in topology,
if $X$ is a noncompact locally compact Hausdorff space, then its one-point
compactification is obviously the smallest possible compactification of~$X$.  Recall that
in proposition~\ref{00152194} we established the fact that constructing the smallest
unitization of $A$ is ``essentially'' the same thing as constructing the smallest
compactification of~$X$.

 \vskip 2 pt

It is sometimes convenient (as, for example, in the preceding paragraph) to take a
``unitization'' of an algebra that is already unital and sometimes convenient to take a
``compactification'' of a space that is already compact. Since there appears to be no
universally accepted terminology, I introduce the following (definitely nonstandard, but
I hope helpful) language.

\begin{defn}\label{0038614} Let $A$ and $B$ be $C^*$-algebras and $X$ and $Y$ be
Hausdorff topological spaces. We will say that
 \begin{enumerate}
    \item $B$ is a \df{unitization} of $A$ if $B$ is unital and $A$ is ($*\,$-isomorphic
to) a $C^*$-subalgebra of~$B$;
 \index{unitization}%
 \index{unitization!essential}%
 \index{essential!unitization}%
    \item $B$ is an \df{essential unitization} of $A$ if $B$ is unital and $A$ is
($*\,$-isomorphic to) an essential ideal of~$B$;
 \index{compactification}%
 \index{compactification!essential}%
 \index{essential!compactification}%
    \item $Y$ is a \df{compactification} of $X$ if it is compact and $X$ is (homeomorphic
to) a subspace of~$Y$; and
    \item $Y$ is an \df{essential compactification} of $X$ if it is compact and $X$ is
(homeomorphic to) a dense subspace of~$Y$
 \end{enumerate}
\end{defn}

Perhaps a few words are in order concerning the bits in parentheses in the preceding
definition.  It is seldom the case that a topological space $X$ is literally a
\emph{subset} of a particular compactification of $X$ or that a $C^*$-algebra $A$ is a
\emph{subset} of a particular unitization of~$A$. While certainly true that it is
frequently convenient to regard one $C^*$-algebra as a subset of another when in fact the
first is merely $*\,$-isomorphic to a subset of the second, there are also occasions when
it clarifies matters to specify the actual embeddings involved.  If the details of these
distinctions are not entirely familiar, the next two definitions are intended to help.

\begin{defn} Let $A$ and $B$ be $C^*$-algebras.  We say that $A$ is
\df{embedded} in~$B$ if there exists an injective $*\,$-homomorphism $\iota\colon A \sto
B$; that is, if $A$ is $*\,$-isomorphic to a $C^*$-subalgebra of~$B$ (see
propositions~\ref{0019023} and~\ref{00152181}).  The injective $*\,$-homomorphism $\iota$
is an
 \index{C*@$C^*$-embedding}%
 \index{embedding!$C^*$-algebraic}%
\df{embedding} of $A$ into~$B$. In this situation it is common practice to treat $A$ and
the range of~$\iota$ as identical $C^*$-algebras.  The pair $(B,\iota)$ is a
 \index{unitization}%
\df{unitization} of~$A$ if $B$ is a unital $C^*$-algebra and $\iota\colon A \sto B$ is an
embedding.   The unitization $(B,\iota)$ is
  \index{unitization!essential}%
  \index{essential!unitization}%
\df{essential} if the range of $\iota$ is an essential ideal in~$B$.
\end{defn}

\begin{defn} Let $X$ and $Y$ be Hausdorff topological spaces.  We say that $X$ is
\df{embedded} in~$Y$ if there exists a homeomorphism $j$ from $X$ to a subspace of~$Y$.
The homeomorphism $j$ is a
 \index{topological!embedding}%
 \index{embedding!topological}%
\df{embedding} of $X$ into~$Y$.  As in $C^*$-algebras it is common practice to identify
the range of $j$ with the space~$X$.  The pair $(Y,j)$ is a
 \index{compactification}%
\df{compactification} of~$X$ if $Y$ is a compact Hausdorff space and $j\colon X \sto Y$
is an embedding. The compactification $(Y,j)$ is
  \index{compactification!essential}%
  \index{essential!compactification}%
\df{essential} if the range of $j$ is dense in~$Y$.
\end{defn}

We have discussed the smallest unitization of a $C^*$-algebra and the smallest
compactification of a locally compact Hausdorff space. Now what about a largest, or even
maximal, unital algebra containing~$A$?  Clearly there is no such thing, for if $B$ is a
unital algebra containing~$A$, then so is $B \oplus C$ where $C$ is any unital
$C^*$-algebra. Similarly, there is no largest compact space containing~$X$: if $Y$ is a
compact space containing $X$, then so is the topological disjoint union $Y \uplus K$
where $K$ is any nonempty compact space.  However, it does make sense to ask whether
there is a maximal essential unitization of a $C^*$-algebra or a maximal essential
compactification of a locally compact Hausdorff space. The answer is \emph{yes} in both
cases. The well-known \emph{Stone-\v{C}ech compactification} $\beta(X)$ is maximal among
essential compactifications of a noncompact locally compact Hausdorff space~$X$. Details
can be found in any good topology text.  One readable standard treatment
is~\cite{Willard:1968}, items 19.3--19.12. More sophisticated approaches make use of some
functional analysis---see, for example, \cite{Conway:1990}, chapter V, section~6.  There
turns out also to be a maximal essential unitization of a nonunital $C^*$-algebra
$A$---it is called the \emph{multiplier algebra} of~$A$.

We say that an essential unitization $M$ of a $C^*$-algebra $A$ is \emph{maximal} if any
$C^*$-algebra that contains $A$ as an essential ideal embeds in~$M$.  Here is a more
formal statement.

\begin{defn} An essential unitization $(M,j)$ of a $C^*$-algebra $A$ is said to be
 \index{maximal!essential unitization}%
 \index{essential!unitization!maximal}%
 \index{unitization!maximal essential}%
\df{maximal} if for every embedding $\iota\colon A \sto B$ whose range is an essential
ideal in~$B$ there exists a $*\,$-homomorphism $\phi\colon B \sto M$ such that $\phi
\circ \iota = j$.
\end{defn}

\begin{prop} In the preceding definition the $*\,$-homomorphism $\phi$, if it exists must
be injective.
\end{prop}

\begin{prop} In the preceding definition the $*\,$-homomorphism $\phi$, if it exists must
be unique.
\end{prop}

Compare the following definition with~\ref{0028}.

\begin{defn} Let $A$ and $B$ be $C^*$-algebras and $V$ be a Hilbert $A$-module.  A
$*\,$-homomorphism $\phi\colon B \sto \ofml L(V)$ is
 \index{nondegenerate!$*\,$-homomorphism}%
\df{nondegenerate} if $\phi^\sto(B)\,V$ is dense in~$V$.
\end{defn}

\begin{prop} Let $A$, $B$, and $J$ be $C^*$-algebras, $V$ be a Hilbert $B$-module, and
$\iota\colon J \sto A$ be an injective $*\,$-homomorphism whose range is an ideal in~$A$.
If $\phi\colon J \sto \ofml L(V)$ is a nondegenerate $*\,$-homomorphism, then there
exists a unique extension of $\phi$ to a $*\,$-homomorphism $\overline\phi\colon A \sto
\ofml L(V)$ which satisfies $\overline\phi \circ \iota = \phi$.
\end{prop}

\begin{prop}\label{0038621} If $A$ is a nonzero $C^*$-algebra, then $(\ofml L(A),L)$ is a
maximal essential unitization of~$A$.  It is unique in the sense that if $(M,j)$ is
another maximal essential unitization of $A$, then there exists a $*\,$-isomorphism $\phi
\colon M \sto \ofml L(A)$ such that $\phi \circ j = L$.
\end{prop}

\begin{defn}\label{0038641} Let $A$ be a $C^*$-algebra.  We define the
 \index{multiplier algebra}%
 \index{algebra!multiplier}%
\df{multiplier algebra} of $A$, to be the family $\ofml L(A)$ of adjointable operators
on~$A$.  From now on we denote this family
 \index{MA@$M(A)$ (multiplier algebra of $A$)}%
by $M(A)$.
\end{defn}









\endinput
\chapter{SURVIVAL WITHOUT IDENTITY}

\section{Unitization of Banach Algebras}
\begin{defn}\label{00150005} Let $A$ be an algebra. Define
 \index{<binopunit@$A \bowtie \C$ (unitization of an algebra)}%
$A \bowtie \C$ to be the set $A \times \C$ on which addition and scalar multiplication
are defined pointwise and multiplication is defined by
  \[ (a,\lambda)\cdot(b,\mu) = (ab + \mu a + \lambda b, \lambda\mu). \]
If the algebra $A$ is equipped with an involution, define an involution on $A \bowtie \C$
pointwise; that is, $(a,\lambda)^* := (a^*, \conj\lambda)$. (The notation $A \bowtie \C$
is not standard.)
\end{defn}

\begin{prop}\label{0015001} If $A$ is an algebra, then $A \bowtie \C$ is a unital algebra
in which $A$ is embedded as (that is, isomorphic to) an ideal such that $(A \bowtie \C)/A
\cong \C$.  The identity of $A \bowtie \C$ is $(0,1)$. If $A$ is a $*\,$-algebra, then $A
\bowtie \C$ is a unital $*\,$-algebra in which $A$ is a $*\,$-ideal such that $(A \bowtie
\C)/A \cong \C$.
\end{prop}

\begin{defn} The algebra $A \bowtie \C$ is the
 \index{unitization!of a $*\,$-algebra}%
\df{unitization} of the algebra (or $*\,$-algebra)~$A$.
\end{defn}

\begin{notn}\label{00150012} In the preceding construction elements of the algebra
$A \bowtie \C$ are technically ordered pairs $(a,\lambda)$. They are usually written
differently.  Let $\iota\colon A \sto A \bowtie \C \colon a \mapsto (a,0)$ and $\pi\colon
A \bowtie \C \sto \C\colon (a,\lambda) \mapsto \lambda$. It follows, since $(0,1)$ is the
identity in $A \bowtie \C$, that
   \begin{align*} (a,\lambda) &= (a,0) + (\vc 0,\lambda) \\
                              &= \iota(a) + \lambda \vc 1_{A \bowtie \C}
   \end{align*}
It is conventional to treat $\iota$ as an inclusion mapping.  Thus it is reasonable to
write $(a,\lambda)$ as $a + \lambda \vc 1_{A \bowtie \C}$ or simply as $a + \lambda \vc
1$.  No ambiguity seems to follow from omitting reference to the multiplicative identity,
so a standard notation for the pair $(a,\lambda)$ is $a + \lambda$.
\end{notn}

\begin{defn}\label{00150014} Let $A$ be a \emph{nonunital} algebra.  Define the
 \index{spectrum!in nonunital algebras}%
\df{spectrum} of an element $a \in A$ to be the spectrum of $a$ regarded as an element
of~$A \bowtie \C$;  that is, $\sigma_A(a) := \sigma_{A \bowtie \C}(a)$.
\end{defn}

\begin{defn}\label{00150015} Let $A$ be a normed algebra (with or without involution). On the
unitization $A \bowtie \C$ of $A$ define $\norm{(a,\lambda)} := \norm a + \abs\lambda$.
\end{defn}

\begin{prop}\label{001500151} Let $A$ be a normed algebra.  The mapping
 \index{unitization!of a normed algebra}%
 \index{unitization!of a Banach $*\,$-algebra}%
$(a,\lambda) \mapsto \norm a + \abs\lambda$ defined above is a norm under which $A
\bowtie \C$ is a normed algebra.  If $A$ is a Banach algebra (respectively, a Banach
$*\,$-algebra), then $A \bowtie \C$ is a Banach algebra (respectively, a Banach
$*\,$-algebra).  The resulting Banach algebra (or Banach $*\,$-algebra) is the
\df{unitization} of $A$ and will be
 \index{<unitization@$A_e$ (unitization of a Banach algebra)}%
denoted by~$A_e$.
\end{prop}

With this expanded definition of \emph{spectrum} many of the earlier facts for unital
Banach algebras remain true in the more general setting.  In particular, for future
reference we restate items \ref{000661}, \ref{C073134}, \ref{0006703}, and~\ref{C073157}.

\begin{prop}\label{001500157} Let $a$ be an element of a Banach algebra~$A$.  Then the spectrum
of $a$ is compact and $\abs\lambda \le \norm a$ for every $\lambda \in \sigma(a)$.
\end{prop}

\begin{prop}\label{00150016} The spectrum of every element of a Banach algebra is nonempty.
\end{prop}

\begin{prop}\label{00150017} For every element $a$ of a Banach algebra $\rho(a) \le
\norm a$ and $\rho(a^n) = \bigr(\rho(a)\bigr)^n$.
\end{prop}

\begin{thm}[Spectral radius formula]\label{00150018} If $a$ is an element of a Banach
 \index{spectral!radius!formula for}%
algebra, then
   \[ \rho(a) = \inf\bigl\{\norm{a^n}^{1/n}\colon n \in \N\bigr\}
              = \lim_{n \sto \infty} \norm{a^n}^{1/n}. \]
\end{thm}

\begin{defn} Let $A$ be an algebra. A left ideal $J$ in $A$ is a
 \index{modular!left ideal}%
 \index{ideal!left modular}%
\df{modular left ideal} if there exists an element $u$ in $A$ such that $au - a \in J$
for every $a \in A$. Such an element $u$ is called a
 \index{identity!left!with respect to an ideal}%
\df{right identity with respect to $J$}.  Similarly, a right ideal $J$ in $A$ is a
 \index{modular!right ideal}%
 \index{ideal!right modular}%
\df{modular right ideal} if there exists an element $v$ in $A$ such that $va - a \in J$
for every $a \in A$. Such an element $v$ is called a
 \index{identity!left!with respect to an ideal}%
\df{left identity with respect to $J$}.  A two-sided ideal $J$ is a
 \index{modular!ideal}%
 \index{ideal!modular}%
\df{modular ideal} if there exists an element $e$ which is both a left and a right
identity with respect to~$J$.
\end{defn}

\begin{prop} An ideal $J$ in an algebra is modular if and only if it is both left modular
and right modular.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Show that if $u$ is a right identity with respect
to $J$ and $v$ is a left identity with respect to $J$, then $vu$ is both a right and left
identity with respect to~$J$. \ns
\end{proof}

\begin{prop} An ideal $J$ in an algebra $A$ is modular if and only if the quotient
algebra $A/J$ is unital.
\end{prop}

\begin{exam} Let $X$ be a locally compact Hausdorff space.  For every $x \in X$ the ideal
$J_x$ is a maximal modular ideal in the $C^*$-algebra $\fml C_0(X)$ of continuous complex
valued functions on~$X$.
\end{exam}

\begin{proof} By the locally compact Hausdorff space version of \emph{Urysohn's lemma}
(see, for example, \cite{Erdman:2007}, theorem 17.2.10) there exists a function $g \in
\fml C_0(X)$ such that $g(x) = 1$. Thus $J_x$ is modular because $g$ is an identity with
respect to~$J_x$.  Since $\fml C_0(X) = J_x \oplus\,\, \spn\{g\}$ the ideal $J_x$ has
codimension $1$ and is therefore maximal.
\end{proof}

\begin{prop} If $J$ is a proper modular ideal in a Banach algebra, then so is its closure.
\end{prop}

\begin{cor} Every maximal modular ideal in a Banach algebra is closed.
\end{cor}

\begin{prop} Every proper modular ideal in a Banach algebra is contained in a maximal
modular ideal.
\end{prop}

\begin{prop}\label{00150144} Let $A$ be a commutative Banach algebra and $\phi \in
\Delta A$. Then $\ker\phi$ is a maximal modular ideal in~$A$ and $A/\ker\phi$ is a field.
Furthermore, every maximal modular ideal is the kernel of exactly one character
in~$\Delta A$.
\end{prop}

\begin{prop}\label{00150147} Every multiplicative linear functional on a commutative
Banach algebra $A$ is continuous.  In fact, every character is contractive.
\end{prop}

\begin{exam} Let $A_e$ be the unitization of a commutative Banach algebra~$A$ (see
proposition~\ref{001500151}).
 \index{phi@$\phi_\infty$ (a character on the unitization of an algebra)}%
Define
   \[ \phi_\infty\colon A_e \sto \C\colon (a,\lambda) \mapsto \lambda\,. \]
Then $\phi_\infty$ is a character on~$A_e$.
\end{exam}

\begin{prop}\label{00150154} Every character $\phi$ on a commutative Banach algebra
$A$ has a unique extension to a
 \index{phi@$\phi_e$ (extension of a character $\phi$ to the unitization of an algebra)}%
character $\phi_e$ on the unitization $A_e$ of~$A$.  And the restriction to $A$ of any
character on $A_e$, with the obvious exception of $\phi_\infty$, is a character on~$A$.
\end{prop}

\begin{prop} If $A$ is a commutative Banach algebra, then
 \begin{enumerate}
    \item[(a)] $\Delta A$ is a locally compact Hausdorff space,
    \item[(b)] $\Delta A_e = \Delta A \cup \{\phi_\infty\}$,
    \item[(c)] $\Delta A_e$ is the one-point compactification of~$\Delta A$, and
    \item[(d)] the map $\phi \mapsto \phi_e$ is a homeomorphism from $\Delta A$ onto
$\Delta A_e \setminus \{\phi_\infty\}$.
 \end{enumerate}
If $A$ is unital (so that $\Delta A$ is compact), then $\phi_\infty$ is an isolated point
of~$\Delta A_e$.
\end{prop}

\begin{thm} If $A$ is a commutative Banach algebra with a nonempty character space, then
the Gelfand transform
  \[ \Gamma = \Gamma_A\colon \fml C_0(\Delta A)\colon a \mapsto \wh a = \Gamma_a \]
is a contractive algebra homomorphism and $\rho(a) = \norm{\wh a}_u$.  Furthermore, if
$A$ is \emph{not} unital, then $\sigma(a) = \ran \wh a \cup \{0\}$ for every $a \in A$.
\end{thm}

















\section{Exact Sequences and Extensions}
The rather simple procedure for the unitization of Banach $*\,$-algebras
(see~\ref{00150005} and~\ref{00150015}) does not carry over to $C^*$-algebras.  The norm
defined in~\ref{00150015} does not satisfy the $C^*$-condition (definition~\ref{0013}).
It turns out that the unitization of $C^*$-algebras is a bit more complicated.  Before
examining the details we look at some preparatory material on \emph{exact sequences} and
\emph{extensions} of $C^*$-algebras.

In the world of $C^*$-algebras exact sequences are defined in essentially the same way as
exact sequences of Banach algebras (see~\ref{defn_Bsp_exact_seq}).

\begin{defn}  A sequence of $C^*$-algebras and $*\,$-homomorphisms
   \[\xymatrix{{\cdots}\ar[r] & A_{n-1}\ar[r]^{\phi_n}
          & A_n\ar[r]^{\phi_{n+1}} & A_{n+1}\ar[r] & {\cdots}
   }\]
is said to be
 \index{sequence!exact}%
 \index{exact!sequence}%
\df{exact at} $A_n$ if $\ran \phi_n = \ker \phi_{n+1}$. A sequence is \df{exact} if it is
exact at each of its constituent $C^*$-algebras.  A sequence of $C^*$-algebras and
$*\,$-homomorphisms of the form
   \begin{equation}\label{001500202i2}\xymatrix{
      0 \ar[r] & A \ar[r]^\phi & E\ar[r]^\psi & B\ar[r] & 0
   }\end{equation}
 \index{sequence!short exact}%
 \index{short exact sequence}%
is a \df{short exact sequence}.  (Here $\vc 0$ denotes the trivial $0$-dimensional
$C^*$-algebra, and the unlabeled arrows are the obvious $*$-homomorphisms.)  The short
exact sequence of $C^*$-algebras~\eqref{001500202i} is
 \index{exact!sequence!split}%
 \index{sequence!split exact}%
 \index{split exact!sequence}%
\df{split exact} if there exists a $*\,$-homomorphism $\xi\colon B \sto E$ such that
$\psi\circ\xi = \id B$.
\end{defn}

The preceding definitions were for the category $\cat{CSA}$ of $C^*$-algebras and
$*\,$-homomorphisms.  Of course there is nothing special about this particular category.
Exact sequences make sense in many situations, in, for example, various categories of
Banach spaces, Banach algebras, Hilbert spaces, vector spaces, Abelian groups, modules,
and so on.

Often in the context of $C^*$-algebras the exact sequence \eqref{001500202i} is referred
to as an
 \index{extension}%
\df{extension}. Some authors refer to it as an \emph{extension of $A$ by $B$} (for
example, \cite{Wegge-Olsen:1993} and \cite{Davidson:1996}) while others say it is an
\emph{extension of $B$ by $A$} (\cite{Murphy:1990}, \cite{Fillmore:1996}, and
\cite{Blackadar:2006}). In \cite{Fillmore:1996} and \cite{Blackadar:2006} the
\emph{extension} is defined to be the sequence~\ref{001500202i}; in
\cite{Wegge-Olsen:1993} it is defined to be the ordered triple $(\phi,E,\psi)$; and in
\cite{Murphy:1990} and \cite{Davidson:1996} it is defined to be the $C^*$-algebra $E$
itself.  Regardless of the formal definitions it is common to say that \emph{$E$ is an
extension of $A$ by $B$ (or of $B$ by $A$)}.

\begin{conv}\label{00151212} In a $C^*$-algebra the word \emph{ideal} will always
 \index{conventions!ideals are self-adjoint}%
 \index{conventions!ideals are closed}%
 \index{ideal!in a $C^*$-algebra}%
mean a closed two-sided \hbox{$*\,$-ideal} (unless, of course, the contrary is explicitly
stated). We will show shortly (in proposition~\ref{0019017}) that requiring an ideal in a
$C^*$-algebra to be self-adjoint is redundant.  A two-sided algebra ideal of a
$C^*$-algebra which is not necessarily closed will be called an
 \index{ideal!algebraic}%
 \index{algebraic!ideal}%
\df{algebraic ideal}. A self-adjoint algebraic ideal of a $C^*$-algebra will be called an
 \index{ideal!algebraic $\ast$-}%
 \index{algebraic!$\ast$-ideal}%
\df{algebraic $\ast$-ideal}.
\end{conv}

Compare the next two propositions to the nearly identical~\ref{00151231} and~\ref{00151232}.

\begin{prop}\label{00151231c} Consider the following diagram in the category of $C^*$-algebras
and ${}^*$-homomorphisms
 \[\xymatrix{
      0\ar[r] & A\ar[d]^f\ar[r]^j & B\ar[d]^g\ar[r]^k
              & C\ar@{-->}[d]^h \ar[r] & 0 \\
      0\ar[r] & A'\ar[r]_{j'} & B'\ar[r]_{k'}
      & C'\ar[r] & 0
 }\]
If the rows are exact and the left square commutes, then there exists a unique
${}^*$-homomorphism $h\colon C \sto C'$ which makes the right square commute.
\end{prop}

\begin{prop}[The Five Lemma]\label{00151232c}
 \index{five lemma}%
Suppose that in the following diagram of $C^*$-algebras and ${}^*$-homomorphisms
 \[\xymatrix{
      0\ar[r] & A\ar[d]^f\ar[r]^j & B\ar[d]^g\ar[r]^k
              & C\ar[d]^h \ar[r] & 0 \\
      0\ar[r] & A'\ar[r]_{j'} & B'\ar[r]_{k'}
      & C'\ar[r] & 0
 }\]
the rows are exact and the squares commute.  Prove the following.
 \begin{enumerate}
  \item If $g$ is surjective, so is $h$.
  \item If $f$ is surjective and $g$ is injective, then $h$ is injective.
  \item If $f$ and $h$ are surjective, so is $g$.
  \item If $f$ and $h$ are injective, so is $g$.
 \end{enumerate}
\end{prop}

\begin{defn} Let $A$ and $B$ be $C^*$-algebras.  We define the
 \index{direct sum!of $C^*$-algebras}%
 \index{external!direct sum}%
 \index{C*@$C^*$-algebra!direct sum of}%
 \index{<binopsum@$A \oplus B$ (direct sum of $C^*$-algebras)}%
\df{(external) direct sum} of $A$ and $B$, denoted by $A \oplus B$, to be the Cartesian
product $A \times B$ with pointwise defined algebraic operations and norm given by
   \[ \norm{(a,b)} = \max\{\norm a,\norm b\} \]
for all $a \in A$ and $b \in B$.  An alternative notation for the element $(a,b)$ in $A
\oplus B$
 \index{<binopsum@$a \oplus b$ (an element of $A \oplus B$)}%
is $a \oplus b$.
\end{defn}

\begin{exam} Let $A$ and $B$ be $C^*$-algebras.  Then the direct sum of $A$ and $B$ is a
$C^*$-algebra and the following sequence is split short exact:
 \[\xymatrix{
    \vc 0 \ar[r] & A \ar[r]^-{\iota_1} & A \oplus B \ar[r]^-{\pi_2}
             & B \ar[r] & \vc 0
 }\]
\end{exam}
\noindent The indicated maps in the preceding are the obvious ones:
   \[ \iota_1\colon A \sto A \oplus B\colon a \mapsto (a,\vc 0) \qquad
        \text{and} \qquad  \pi_2 \colon A \oplus B \sto B: (a,b) \mapsto b\,. \]
 \index{extension!direct sum}%
 \index{direct sum!extension}
This is the \df{direct sum extension}.

\begin{prop} If $A$ and $B$ are nonzero $C^*$-algebras, then their
 \index{direct sum!of $C^*$-algebras}%
 \index{product!in $\cat{CSA}$}%
 \index{csa@$\cat{CSA}$!products in}%
direct sum $A \oplus B$ is a product in the category $\cat{CSA}$ of $C^*$-algebras and
$*\,$-homomorphisms (see example~\ref{0007812}). The direct sum is unital if and only if
both $A$ and $B$ are.
\end{prop}

\begin{defn}\label{0015151}  Let $A$ and $B$ be $C^*$-algebras and $E$ and $E'$ be
extensions of $A$ by $B$.  These extensions are
 \index{equivalence!strong!of extensions}%
 \index{extension!strong equivalence of}%
\df{strongly equivalent} if there exists a ${}^*$-isomorphism $\theta\colon E \sto E'$
that makes the diagram
 \[\xymatrix{
     0\ar[r] & A\ar[r]\ar@{=}[d] & E\ar[r]\ar[d]^\theta
             & B\ar[r]\ar@{=}[d] & 0 \\
     0\ar[r] & A\ar[r] & E'\ar[r] & B\ar[r] & 0
 }\]
commute.
\end{defn}

\begin{prop}\label{0015152} In the preceding definition it is enough to require
$\theta$ to be a ${}^*$-homomorphism.
\end{prop}

\begin{prop}\label{0015155} Let $A$ and $B$ be $C^*$-algebras. An extension
 \[\xymatrix{
     \vc 0\ar[r] & A\ar[r]^\phi & E\ar[r]^\psi & B\ar[r] & \vc 0
 }\]
is strongly equivalent to the direct sum extension $A \oplus B$ if and only if there
exists a ${}^*$-homomorphism $\nu\colon E \sto A$ such that $\nu\circ\phi = \id A$.
\end{prop}

\begin{prop}\label{0015157} If the sequences of $C^*$-algebras
 \begin{equation}\label{0015157i}\xymatrix{
     0\ar[r] & A\ar[r]^\phi & E\ar[r]^\psi & B\ar[r] & 0
 }\end{equation}
and
 \begin{equation}\label{0015157ii}\xymatrix{
     0\ar[r] & A\ar[r]^{\phi'} & E'\ar[r]^{\psi'} & B\ar[r] & 0
 }\end{equation}
are strongly equivalent and \eqref{0015157i} splits, then so does~\eqref{0015157ii}.
\end{prop}


















\section{Unitization of $C^*$-algebras}
The rather simple procedure for the unitization of Banach $*\,$-algebras
(see~\ref{00150005} and~\ref{00150015}) does not carry over to $C^*$-algebras.  The norm
defined in~\ref{00150015} does not satisfy the $C^*$-condition (definition~\ref{0013}).
It turns out that the unitization of $C^*$-algebras is a bit more complicated.

\begin{prop} The kernel of a $*\,$-homomorphism $\phi\colon A \sto B$ between
$C^*$-algebras is a closed $*\,$-ideal in~$A$ and its range is a $*\,$-subalgebra of~$B$.
\end{prop}

\begin{prop}\label{001315} If $a$ is an element in a $C^*$-algebra, then
  \begin{align*}
    \norm{a} &= \sup\{\norm{xa}\colon \norm{x} \le 1\} \\
             &= \sup\{\norm{ax}\colon \norm{x} \le 1\}.
  \end{align*}
\end{prop}

\begin{cor}\label{0013151} If $a$ is an element of a $C^*$-algebra $A$, the operator $L_a$,
called
 \index{left!multiplication operator}%
 \index{operator!left multiplication}%
\df{left multiplication by~$a$} and defined by
   \[ L_a\colon A \sto A\colon x \mapsto ax \]
is a (bounded linear) operator on~$A$.  Furthermore, the map
   \[ L\colon A \sto \ofml B(A)\colon a \mapsto L_a \]
is both an isometry and an algebra homomorphism.
\end{cor}

\begin{prop}\label{0015213}  Let $A$ be a $C^*$-algebra. Then there exists a unital
$C^*$-algebra $\wt A$ in which $A$ is embedded as an ideal such that the sequence
 \begin{equation}\label{0015213i}
   \xymatrix{
     \vc 0\ar[r] & A\ar[r] & \wt A \ar[r] & \C \ar[r] & \vc 0
 }\end{equation}
is split exact.  If $A$ is unital then the sequence \eqref{0015213i} is strongly
equivalent to the direct sum extension, so that $\wt A \cong A \oplus \C$. If $A$ is
\emph{not} unital, then $\wt A$ is not isomorphic to $A \oplus \C$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] The proof this result is a little complicated.
Everyone should go through all the details at least once in his/her life.  What follows
is an outline of a proof.

Notice that we speak of the unitization of $C^*$-algebra $A$ \emph{whether or not $A$
already has a unit (multiplicative identity)}.  We divide the argument into two cases.

\vskip 8 pt

\noindent\textbf{Case 1: the algebra $A$ is unital.}
\begin{enumerate}
    \item On the algebra $A \bowtie \C$ define
       \[ \norm{(a,\lambda)} := \max\{\norm{a + \lambda \vc 1_A}, \abs\lambda\} \]
and let $\wt A := A \bowtie \C$ together with this norm.
    \item Prove that the map $(a,\lambda) \mapsto \norm{(a,\lambda)}$ is a norm on~$A
\bowtie \C$.
    \item Prove that this norm is an algebra norm.
    \item Show that it is, in fact, a $C*$-norm on $A \bowtie \C$.
    \item Observe that it is an extension of the norm on~$A$.
    \item Prove that $A \bowtie \C$ is a $C^*$-algebra by verifying completeness of the
metric space induced by the preceding norm.
    \item Prove that the sequence
       \[ \vc 0 \to A \to^\iota \wt A \two/->`<-/^Q_\psi \C \to \vc 0  \]
is split exact (where $\iota\colon a \mapsto (a,0)$, $Q\colon (a,\lambda) \mapsto
\lambda$, and $\psi\colon \lambda \mapsto (0,\lambda)$).
    \item Prove that $\cstariso{\wt A}{A \oplus \C}$.
\end{enumerate}

\vskip 8 pt

\noindent\textbf{Case 2: the algebra $A$ is \emph{not} unital.}
\begin{enumerate}
\setcounter{enumi}{8}
    \item Prove the following simple fact.
  \begin{lem} Let $A$ be an algebra and $B$ be a normed algebra. If $\phi\colon A \sto B$
  is an algebra homomorphism, the function $a \mapsto \norm a := \norm{\phi(a)}$ is a
  seminorm on~$A$.  The function is a norm if $\phi$ is injective.
  \end{lem}
    \item Recall that we defined the operator $L_a$, \emph{left multiplication by $a$},
in~\ref{0013151}.  Now let
      \[ A^{\sharp} :=
        \{ L_a + \lambda I_A \in \ofml B(A)\colon a \in A \text{ and } \lambda \in \C\} \]
and show that $A^\sharp$ is a normed algebra.
    \item Make $A^\sharp$ into a $*\,$-algebra by defining
      \[ (L_a + \lambda I_A)^* := L_{a^*} + \conj\lambda I_A \]
for all $a \in A$ and $\lambda \in \C$.
    \item Define
      \[ \phi\colon A \bowtie \C \sto A^\sharp\colon
                   (a,\lambda) \mapsto L_a + \lambda I_A \]
and verify that $\phi$ is a $*\,$-homomorphism.
    \item Prove that $\phi$ is injective.
    \item Use (9) to endow $A \bowtie \C$ with a norm which makes it into a unital normed
algebra. Let $\wt A := A \bowtie \C$ with the norm pulled back by $\phi$ from $A^\sharp$


    \item Verify the following facts.
      \begin{enumerate}
        \item The map $\phi\colon \wt A \sto A^\sharp$ is an isometric isomorphism.
        \item $\ran L$ is a closed subalgebra of $\ran \phi = A^\sharp \subseteq \ofml
B(A)$.
        \item $I_A \notin \ran L$.
      \end{enumerate}
    \item Prove that the norm on $\wt A$ satisfies the $C^*$-condition.
    \item Prove that $\wt A$ is a unital $C^*$-algebra.  (To show that $\wt A$ is complete
we need only show that $A^\sharp$ is complete.  To this end let $\bigl(\phi(a_n,
\lambda_n)\bigr)_{n=1}^\infty$ be a Cauchy sequence in~$A^\sharp$.  To show that this
sequence converges it suffices to show that it has a convergent subsequence. Showing that
the sequence $(\lambda_n)$ is bounded allows us to extract from it a convergent
subsequence~$\bigl(\lambda_{n_k}\bigr)$.  Prove that $(L_{a_{n_k}})$ converges.)
    \item Prove that the sequence
        \begin{equation}\xymatrix{
     0\ar[r] & A\ar[r] & \wt A\ar[r] & \C\ar[r] & 0
        }\end{equation}
is split exact.
    \item The $C^*$-algebra $\wt A$ is \emph{not} equivalent to $A \oplus \C$.
\end{enumerate}  \ns
\end{proof}

\begin{defn}\label{00152131} The $C^*$-algebra $\wt A$ constructed in the
preceding proposition is the
 \index{unitization!of a $C^*$-algebra}%
\df{unitization} of~$A$.
\end{defn}

Note that the expanded definition of \emph{spectrum} given in~\ref{00150014} applies to
$C^*$-algebras since the added identity is purely an algebraic matter and is the same for
$C^*$-algebras as it is for general Banach algebras.  Thus many of the earlier facts
stated for unital $C^*$-algebras remain true. In particular, for future reference we
restate items \ref{00131101}, \ref{00131102}, \ref{00131103}, \ref{00131104},
\ref{001312}, \ref{001406}, and~\ref{0014062}.

\begin{prop}\label{00152141} Let $a$ be a normal element of a $C^*$-algebra. Then
$\norm{a^2} = {\norm a}^2$ and therefore $\rho(a)~=~\norm a$.
\end{prop}

\begin{cor}\label{00152142} If $A$ is a commutative $C^*$-algebra, then
$\norm{a^2} = {\norm a}^2$ and $\rho(a)~=~\norm a$ for every $a \in A$.
\end{cor}

\begin{cor}\label{00152143} On a commutative $C^*$-algebra $A$ the Gelfand transform
$\Gamma$ is an isometry; that is, $\norm{\Gamma_a}_u = \norm{\wh a}_u = \norm a$ for
every $a \in A$.
\end{cor}

\begin{cor}\label{00152144} The norm of a $C^*$-algebra is unique in the sense that
given a algebra $A$ with involution there is at most one norm which makes $A$ into a
$C^*$-algebra.
\end{cor}

\begin{prop}\label{00152145} If $h$ is a self-adjoint element of a $C^*$-algebra,
 \index{spectrum!of a self-adjoint element}%
 \index{self-adjoint!element!spectrum of}%
then $\sigma(h) \subseteq \R$.
\end{prop}

\begin{prop}\label{00152152} If $a$ is a self-adjoint element in a $C^*$-algebra,
then its Gelfand transform $\wh a$ is real valued.
\end{prop}

\begin{prop}\label{00152153}  Every character on a $C^*$-algebra $A$ preserves
involution, thus the Gelfand transform $\sbsb{\Gamma}{\!\!A}$ is a $*$-homomorphism.
\end{prop}

An immediate result of the preceding results is the second version of the
\emph{Gelfand-Naimark theorem}, which says that any commutative $C^*$-algebra is
(isometrically unitally $*\,$-isomorphic to) the algebra of all those continuous
functions on some locally compact Hausdorff space which vanish at infinity. As was the
case with the first version of this theorem (see~\ref{00142}) the locally compact
Hausdorff space referred to is the character space of the algebra.

\begin{thm}[Gelfand-Naimark Theorem II]\label{00152156} Let $A$ be a commutative
 \index{Gelfand-Naimark!Theorem II}%
$C^*$-algebra.  Then the Gelfand transform $\Gamma_A\colon a \mapsto \wh a$ is an
isometric unital $*\,$-isomorphism of $A$ onto $\fml C_0(\Delta A)$.
\end{thm}

It follows from the next proposition that the unitization process is functorial.

\begin{prop} \label{00152161} Every $*$-homomorphism $\phi\colon A \sto B$ between
$C^*$-algebras has a unique extension to a unital $*$-homomorphism $\wt \phi\colon \wt A
\sto \wt B$ between their unitizations.
\end{prop}

In contrast to the situation in general Banach algebras there is no distinction between
topological and geometric categories of $C^*$-algebras.  One of the most remarkable
aspects of $C^*$-algebra theory is that $*$-homomorphisms between such algebras are
automatically continuous---in fact, contractive.  It follows that if two $C^*$-algebras
are algebraically $*$-isomorphic, then they are isometrically isomorphic.

\begin{prop}\label{00152171} Every $*$-homomorphism between $C^*$-algebras is contractive.
\end{prop}

\begin{prop}\label{00152181} Every injective $*$-homomorphism between $C^*$-algebras is
an isometry.
\end{prop}

\begin{prop}\label{00152191} Let $X$ be a locally compact Hausdorff space and
$\wt X = X \cup \{\infty\}$ be its one-point compactification. Define
  \[ \iota\colon \fml C_0(X) \sto \,\fml C(\wt X)\colon f \mapsto \wt f \]
where
 \[ \wt f(x) = \left\{%
  \begin{array}{ll}
     f(x), & \hbox{if $x \in X$;} \\
     0, & \hbox{if $x = \infty$.} \\
  \end{array}%
            \right. \]
Also let $E_\infty$ be defined on $\fml C(\wt X)$ by $E_\infty(g) = g(\infty)$.  Then the
sequence
  \[ \vc 0 \to \fml C_0(X) \to^\iota \,\,\fml C(\wt X)
                                        \to^{E_\infty} \C \to \vc 0 \]
is exact.
\end{prop}

In the preceding proposition we refer to $\wt X$ as the one-point compactification of $X$
even in the case that $X$ is compact to begin with.  Most definitions of
\emph{compactification} require a space to be dense in any compactification. (See my
remarks in the beginning of section 17.3 of~\cite{Erdman:2007}.)  We have previously
adopted the convention that the unitization of a unital algebra gets a new multiplicative
identity.  In the spirit of consistency with this choice we will in the sequel subscribe
to the convention that the one-point compactification of a compact space gets an
additional (isolated) point. (See also the terminology we introduce in~\ref{0038614}.)

 \vskip 2 pt

From the point of view of the Gelfand-Naimark theorem~(\ref{00152156}) the fundamental
insight prompted by the next proposition is that the unitization of a commutative
$C^*$-algebra is, in some sense, the ``same thing'' as the one-point compactification of
a locally compact Hausdorff space.

\begin{prop}\label{00152194} If $X$ is a locally compact Hausdorff space, then the
unital $C^*$-algebras $\bigl(\fml C_0(X)\bigr)^\sim$ and $\fml C(\wt X)$ are
isometrically $*\,$-isomorphic.
\end{prop}

\begin{proof} Define
  \[ \theta\colon \bigl(\fml C_0(X)\bigr)^\sim \!\!\sto \fml C(\wt X)\colon
                         (f,\lambda) \mapsto \wt f + \lambda \vc 1_{\wt X} \]
(where $\vc 1_{\wt X}$ is the constant function $1$ on~$\wt X$).  Then consider the
diagram
  \[\xymatrix{
     0\ar[r] & \fml C_0(X)\ar[r]\ar@{=}[d] & \bigl(\fml C_0(X)\bigr)^\sim\ar[r]\ar[d]^\theta
             & \C\ar[r]\ar@{=}[d] & 0 \\
     0\ar[r] & \fml C_0(X)\ar[r]^\iota & \fml C(\wt X)\ar[r]^{E_\infty} & \C\ar[r] & 0
  }\]
The top row is exact by proposition~\ref{0015213}, the bottom row is exact by
proposition~\ref{00152191}, and the diagram obviously commutes. It is routine to check
that $\theta$ is a $*\,$-homomorphism.  Therefore $\theta$ is an isometric
$*\,$-isomorphism by proposition~\ref{0015152} and corollary~\ref{00152181}.
\end{proof}


















\section{Quasi-inverses}
\begin{defn} An element $b$ of an algebra $A$ is a \df{left quasi-inverse} for $a \in A$ if
$ba = a + b$. It is a \df{right quasi-inverse} for $a$ if $ab = a + b$.  If $b$ is both a
left and a right quasi-inverse for $a$ it is a
 \index{quasi-inverse}%
\df{quasi-inverse} for~$a$. When $a$ has a quasi-inverse denote it by~$a'$.
\end{defn}

\begin{prop} If $b$ is a left quasi-inverse for $a$ in an algebra $A$ and $c$ is a right
quasi-inverse for $a$ in $A$, then $b = c$.
\end{prop}

\begin{prop}Let $A$ be a unital algebra and let $a$, $b \in A$. Then
 \begin{enumerate}
  \item[(a)] $a'$ exists if and only if $(\vc 1 - a)^{-1}$ exists; and
  \item[(b)] $b^{-1}$ exists if and only if $(\vc 1 - b)'$ exists.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] For the reverse direction in (a) consider
$a + c - ac$ where $c = \vc 1 - (\vc 1 - a)^{-1}$. \ns
\end{proof}

\begin{prop} An element $a$ of a Banach algebra $A$ is quasi-invertible if and only if it
is not an identity with respect to any maximal modular ideal in~$A$.
\end{prop}

\begin{prop} Let $A$ be a Banach algebra and $a \in A$.  If $\rho(a) < 1$, then $a'$ exists
and $a' = -\sum_{k=1}^\infty a^k$.
\end{prop}

\begin{prop}Let $A$ be a Banach algebra and $a \in A$.  If $\norm{a} < 1$, then $a'$ exists
and
   \[ \frac{\norm{a}}{1 + \norm{a}} \le \norm{a'} \le \frac{\norm{a}}{1 - \norm{a}}. \]
\end{prop}

\begin{prop} Let $A$ be a unital Banach algebra and $a \in A$. If $\rho(\vc 1 - a) < 1$, then
$a$ is invertible in~$A$.
\end{prop}

\begin{prop} Let $A$ be a Banach algebra and $a$, $b \in A$.  If $b'$ exists and
$\norm{a} < (1 + \norm{b'})^{-1}$, then $(a + b)'$ exists and
  \[ \norm{(a + b)' - b'}
           \le \frac{\norm{a}\,(1 + \norm{b'})^2}{1 - \norm{a}\,(1 + \norm{b'})}\,.\]
\end{prop}
 \begin{proof}[\emph{Hint for proof}] Show first that $u = (a - b'a)'$ exists and that
 $u + b' - ub'$ is a left quasi-inverse for~$a~+~b$.  \ns
\end{proof}

Compare the next result with propositions~\ref{000644fa} and~\ref{000646fa}
\begin{prop} The set $Q_A$ of
 \index{q@$Q_A$ (quasi-invertible elements}%
quasi-invertible elements of a Banach algebra $A$ is open in $A$, and the map $a \mapsto
a'$ is a homeomorphism of $Q_A$ onto itself.
\end{prop}

\begin{notn} If $a$ and $b$ are elements in an algebra we define
 \index{<binop@$a \circ b$ (operation in an algebra)}%
$a \circ b := a + b - ab$.
\end{notn}

\begin{prop} If $A$ is an algebra, then $Q_A$ is a group under~$\circ$.
\end{prop}

\begin{prop} If $A$ is a unital Banach algebra and $\inv A$ is the set of its invertible
elements, then
   \[ \Psi\colon Q_A \sto \inv A\colon  a \mapsto \vc 1 - a \]
is an isomorphism.
\end{prop}

\begin{defn} If $A$ is a algebra and $a \in A$, we define the
 \index{q-spectrum}%
 \index{spectrum!q-}%
\df{q-spectrum} of $a$ in $A$ by
  \[ \breve{\sigma}_A(a) := \{\lambda \in \C\colon  \lambda \ne 0
                      \text{ and } \dfrac a\lambda \notin Q_A\}\cup\{0\}. \]
\end{defn}

In a unital algebra, the preceding definition is ``almost'' the usual one.
\begin{prop} Let $A$ be a unital algebra and $a \in A$. Then for all $\lambda \ne 0$, we have
$\lambda \in \sigma(a)$ if and only if $\lambda \in \breve{\sigma}(a)$.
\end{prop}

\begin{prop} If an algebra $A$ is not unital and $a \in A$, then $\breve{\sigma}_A(a) =
\breve{\sigma}_{\wt A}(a)$, where $\wt A$ is the unitization of~$A$.
\end{prop}























\section{Positive Elements in $C^*$-algebras}
\begin{defn} A self-adjoint element $a$ of a $C^*$-algebra $A$ is
 \index{positive!element of a $C^*$-algebra}%
\df{positive} if $\sigma(a) \subseteq [0,\infty)$.  In this case we write $a \ge \vc 0$.
We denote the set of all positive elements of $A$
 \index{<plus@$A^+$ (positive cone)}%
by~$A^+$. This is the
 \index{positive!cone}%
 \index{cone!positive}%
\df{positive cone} of~$A$.  For any subset $B$ of $A$ let $B^+ = B \cap A^+$.  We will
use the positive cone to induce a partial ordering on~$A$: we write
 \index{<relation@$a \le b$ (order relation in a $C^*$-algebra)}%
$a \le b$ when $b - a \in A^+$.
\end{defn}

\begin{defn} Let $\le$ be a relation on a nonempty set $S$. If the relation
 \index{<relation@$\le$ (notation for a partial ordering)}%
 $\le$ is reflexive and transitive, it is a
 \index{preordering}%
 \index{ordering!pre-}%
\df{preordering}.  If $\le$ is a preordering and is also antisymmetric, it is a
 \index{partial!ordering}%
 \index{ordering!partial}%
\df{partial ordering}.

A partial ordering $\le$ on a real vector space $V$ is
 \index{compatibility!of orderings with operations}%
\df{compatible} with (or
 \index{respects operations}%
\df{respects}) the operations (addition and scalar multiplication) on~$V$ if for all $x$,
$y$, $z \in V$
 \begin{enumerate}
  \item[(a)] $x \le y$ implies $x+z \le y+z$, and
  \item[(b)] $x \le y$, $\alpha \ge 0$ imply $\alpha x \le \alpha y$.
 \end{enumerate}
A real vector space equipped with a partial ordering which is compatible with the vector
space operations is an
 \index{ordered!vector space}%
 \index{vector!space!ordered}%
\df{ordered vector space}.
\end{defn}

\begin{defn} Let $V$ be a vector space.  A subset $C$ of $V$ is a
 \index{cone}%
\df{cone} in $V$ if $\alpha C \subseteq C$ for every $\alpha \ge 0$.  A cone $C$ in $V$
is
 \index{proper!cone}%
 \index{cone!proper}%
\df{proper} if $C \cap (-C) = \{\vc 0\}$.
\end{defn}

\begin{prop}\label{0018006} A cone $C$ in a vector space is convex if and only if $C + C
\subseteq C$.
\end{prop}

\begin{exam}\label{0018007} If $V$ is an ordered vector space, then the set
 \index{<dualp@$V^+$ (positive cone in an ordered vector space)}%
   \[ V^+ := \{x \in V \colon x \ge \vc 0\} \]
is a proper convex cone in~$V$.  This is the
 \index{positive!cone!in an ordered vector space}%
 \index{cone!positive}%
\df{positive cone} of~$V$ and its members are the
 \index{positive!element (of an ordered vector space)}%
\df{positive elements} of~$V$.
\end{exam}

\begin{prop}\label{0018008} Let $V$ be a real vector space and $C$ be a proper convex cone
in~$V$. Define $x \le y$ if $y - x \in C$.  Then the relation $\le$ is a partial ordering
on $V$ and is compatible with the vector space operations on~$V$.  This relation is the
\df{partial ordering
 \index{induced!partial ordering}%
 \index{partial!ordering!induced by a proper convex cone}%
induced by} the cone~$C$. The positive cone $V^+$ of the resulting ordered vector space
is just $C$ itself.
\end{prop}

\begin{prop} If $a$ is a self-adjoint element of a unital $C^*$-algebra and $t \in \R$, then
 \begin{enumerate}
   \item[(i)] $a \ge \vc 0$ whenever $\norm{a - t\vc 1} \le t$; and
   \item[(ii)] $\norm{a - t\vc 1} \le t$ whenever $\norm a \le t$ and $a \ge \vc 0$.
  \end{enumerate}
\end{prop}

\begin{exam} The positive cone of a $C^*$-algebra $A$ is a closed proper convex
cone in the real vector space~$\ofml H(A)$.
\end{exam}

\begin{prop}\label{0018015} If $a$ and $b$ are positive elements of a $C^*$-algebra
and $ab = ba$, then $ab$ is positive.
\end{prop}

\begin{prop}\label{001804} Every positive element of a $C^*$-algebra $A$ has a unique positive
$n^{\text{th}}$ root ($n \in \N$).  That is, if $a \in A^+$, then there exists a unique
$b \in A^+$ such that $b^n = a$.
\end{prop}

\begin{proof} \emph{Hint.} The existence part is a simple application of the $C^*$-functional
calculus (that is, the \emph{abstract spectral theorem}~\ref{00144}).  The element $b$
given by the functional calculus is positive in the algebra $C^*(\vc 1,a)$.  Explain why
it is also positive in~$A$. The uniqueness argument deserves considerable care.  \ns
\end{proof}

\begin{thm}[Jordan Decomposition]\label{001807} If $c$ is a self-adjoint element of a
 \index{Jordan decomposition}%
 \index{decomposition!Jordan}%
$C^*$-algebra $A$, then there exist positive
 \index{<positive@$c^+$ (positive part of $c$)}%
 \index{positive!part!of a self-adjoint element}%
 \index{<positive@$c^-$ (negative part of $c$)}%
 \index{negative!part!of a self-adjoint element}%
elements $c^+$ and $c^-$ of $A$ such that $c = c^+ - c^-$ and $c^+c^- = \vc 0$.
\end{thm}

\begin{lem} If $c$ is an element of a $C^*$-algebra such that $-c^*c \ge \vc 0$, then $c
= \vc 0$.
\end{lem}

\begin{prop} If $a$ is an element of a $C^*$-algebra, then $a^*a \ge \vc 0$.
\end{prop}

\begin{prop}\label{00180731} If $c$ is an element of a $C^*$-algebra, then the following
are equivalent:
 \begin{enumerate}
    \item[(i)] $c \ge \vc 0$;
    \item[(ii)] there exists $b \ge \vc 0$ such that $c = b^2$; and
    \item[(iii)] there exists $a \in A$ such that $c = a^*a$,
 \end{enumerate}
\end{prop}

\begin{exam}\label{00180741} If $T$ is an operator on a Hilbert space $H$, then $T$ is
a positive member of the $C^*$-algebra $\ofml B(H)$ if and only if $\langle Tx,x \rangle
\ge 0$ for all $x \in H$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Showing that if $T \ge \vc 0$ in $\ofml B(H)$, then
$\langle Tx,x \rangle \ge 0$ for all $x \in H$ is easy: use proposition~\ref{00180731} to
write $T$ as $S^*S$ for some operator~$S$.

For the converse suppose that $\langle Tx,x \rangle \ge 0$ for all $x \in H$. It is easy
to see that this implies that $T$ is self-adjoint.  Use the \emph{Jordan decomposition
theorem}\ref{001807} to write $T$ as $T^+ - T^-$. For arbitrary $u \in H$ let $x = T^-u$
and verify that $0 \le \langle Tx,x \rangle = -\langle (T^-)^3u,u \rangle$. Now $(T^-)^3$
is a positive element of $\ofml B(H)$. (Why?)  Conclude that $(T^-)^3 = \vc 0$ and
therefore $T^- = \vc 0$. (For additional detail see~\cite{DoranB:1986}, page~37.) \ns
\end{proof}

\begin{defn} For an arbitrary element $a$ of a $C^*$-algebra we define
 \index{absolute!value!in a $C^*$-algebra}%
 \index{<unop@$\abs a$ (absolute value in a $C^*$-algebra)}%
$\abs a$ to be $\sqrt{a^*a}$.
\end{defn}

\begin{prop} If $a$ is a self-adjoint element of a $C^*$-algebra, then
  \[ \abs a = a^+ + a^-\,. \]
\end{prop}

\begin{exam} The absolute value in a $C^*$-algebra need not be
subadditive; that is, $\abs{a + b}$ need not be less than $\abs{a}
+ \abs{b}$.  For example, in $\M 2\C$ take $a = \begin{bmatrix} 1 & 1 \\
1 & 1 \end{bmatrix}$ and $b = \begin{bmatrix} 0 & 0 \\ 0 & -2
\end{bmatrix}$.  Then $\abs{a} = a$, $\abs{b} = \begin{bmatrix} 0 &
0 \\ 0 & 2 \end{bmatrix}$, and $\abs{a + b} = \begin{bmatrix} \sqrt{2} & 0 \\ 0 &
\sqrt{2}
\end{bmatrix}$.  If $\abs{a + b} - \abs{a} - \abs{b}$ were positive, then, according to
example~\ref{00180741}, $\langle\,(\abs{a + b} - \abs{a} - \abs{b})\,x\,,\,x\,\rangle$
would be positive for every vector $x \in \C^2$. But this is not true for $x = (1,0)$.
\end{exam}

\begin{prop}\label{0018201} If $\phi\colon A \sto B$ is a $*\,$-homomorphism between
$C^*$-algebras, then $\phi(a) \in B^+$ whenever $a \in A^+$.  If $\phi$ is a
$*\,$-isomorphism, then $\phi(a) \in B^+$ if and only if $a \in A^+$.
\end{prop}

\begin{prop} Let $a$ be a self-adjoint element of a $C^*$-algebra $A$ and $f$ a continuous
complex valued function on the spectrum of~$a$.  Then $f \ge \vc 0$ in $\fml
C(\sigma(a))$ if and only if $f(a) \ge \vc 0$ in~$A$.
\end{prop}

\begin{prop}\label{0018203} If $a$ is a self-adjoint element of a $C^*$-algebra $A$, then
$\norm a \vc 1_A \pm a \ge \vc 0$.
\end{prop}

\begin{prop}\label{0018204} If $a$ and $b$ are self-adjoint elements of a
$C^*$-algebra~$A$ and $a \le b$, then $x^*ax \le x^*bx$ for every $x \in A$.
\end{prop}

\begin{prop} If $a$ and $b$ are elements of a $C^*$-algebra with $0 \le a \le b$, then
$\norm a \le \norm b$.
\end{prop}

\begin{prop}\label{0018301} Let $A$ be a unital $C^*$-algebra and $c \in A^+$. Then
$c$ is invertible if and only if $c \ge \epsilon\vc 1$ for some $\epsilon > 0$.
\end{prop}

\begin{prop}Let $A$ be a unital $C^*$-algebra and $c \in A$. If $c \ge \vc 1$, then $c$ is
invertible and $\vc 0 \le c^{-1} \le \vc 1$.
\end{prop}

\begin{prop} If $a$ is a positive invertible element in a unital $C^*$-algebra, then $a^{-1}$
is positive.
\end{prop}

Next we show that the notation $a^{\ssst{-\frac12}}$ is unambiguous.

\begin{prop} Let $a \in A^+$ where $A$ is a unital $C^*$-algebra. If $a$ is invertible, so is
$a^{\ssst{\frac12}}$ and $\bigl(a^{\ssst{\frac12}}\bigr)^{-1} =
\bigl(a^{-1}\bigr)^{\ssst{\frac12}}$.
\end{prop}

\begin{cor}\label{00183313} If $a$ is an invertible element in a unital $C^*$-algebra,
 \index{inverse!of an absolute value}%
then so is~$\abs a$.
\end{cor}

\begin{prop}\label{0018401} Let $a$ and $b$ be elements of a $C^*$-algebra.  If
$\vc 0 \le a \le b$ and $a$ is invertible, then $b$ is invertible and $b^{-1} \le
a^{-1}$.
\end{prop}

\begin{prop} If $a$ and $b$ are elements of a $C^*$-algebra and $\vc 0 \le a \le b$,
then $\sqrt a \le \sqrt b$.
\end{prop}

\begin{exam} Let $a$ and $b$ be elements of a $C^*$-algebra with $\vc 0 \le a \le b$.
It is \emph{not} necessarily the case that $a^2 \le b^2$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] In the $C^*$-algebra $\mathbf M_2$ let $a =
\begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$ and
$b = a + \tfrac12 \begin{bmatrix} 1 & 1 \\ 1 & 1 \end{bmatrix}$.  \ns
\end{proof}
























\section{Approximate Identities}
\begin{defn} An
 \index{identity!approximate}%
 \index{approximate!identity}%
\df{approximate identity} (or
 \index{unit!approximate}%
 \index{approximate!unit}%
\df{approximate unit}) in a $C^*$-algebra $A$ is an increasing net
${(e_\lambda)}_{\lambda \in \Lambda}$ of positive elements of $A$ such that
$\norm{e_\lambda} \le 1$ and $ae_\lambda \sto a$ (equivalently, $e_\lambda\, a \sto a$)
for every $a \in A$. If such a net is in fact a sequence, we have a
 \index{sequential!approxiamte identity}%
 \index{approximate!identity!sequential}%
\df{sequential approximate identity}. In the literature be careful of varying
definitions: many authors omit the requirements that the net be increasing and/or that it
be bounded.
\end{defn}

\begin{exam} Let $A = \fml C_0(\R)$.  For each $n \in \N$ let $U_n = (-n,n)$ and let
$e_n\colon \R \sto [0,1]$ be a function in $A$ whose support is contained in $U_{n+1}$
and such that $e_n(x) = 1$ for every $x \in U_n$.  Then $(e_n)$ is a (sequential)
approximate identity for~$A$.
\end{exam}

\begin{prop} If $A$ is a $C^*$-algebra, then the set
 \index{lambda@$\Lambda$ (members of $A^+$ in the open unit ball)}%
   \[ \Lambda := \{a \in A^+\colon \norm a < 1\} \]
is a directed set (under the ordering it inherits from $\ofml H(A)$).
\end{prop}

\begin{proof}[\emph{Hint for proof}] Verify that the function
  \[ f\colon [0,1) \sto \R^+\colon t \mapsto (1 - t)^{-1} - 1 = \frac t{1 - t} \]
induces an order isomorphism $f\colon \Lambda \sto A^+$.  (An
 \index{order!isomorphism}%
 \index{isomorphism!order}%
\df{order isomorphism} between partially ordered sets is an order preserving bijection
whose inverse also preserves order.) A careful proof of this result involves checking a
rather large number of details.  \ns
\end{proof}

\begin{prop} If $A$ is a $C^*$-algebra, then the set
  \[ \Lambda := \{a \in A^+\colon \norm a < 1\} \]
is an approximate identity for~$A$.
\end{prop}

\begin{cor}\label{00190161} Every $C^*$-algebra $A$ has an approximate identity.
 \index{approximate!identity!existence of an}%
If $A$ is separable then it has a sequential approximate identity.
\end{cor}

\begin{prop}\label{0019017} Every closed (two-sided) algebraic ideal in a $C^*$-algebra
is self-adjoint.
\end{prop}

\begin{prop}\label{001902} If $J$ is an ideal in a $C^*$-algebra $A$, then
 \index{quotient!$C^*$-algebra}%
$A/J$ is a $C^*$-algebra.
\end{prop}

\begin{proof} See \cite{HigsonR:2000}, theorem 1.7.4, or \cite{Davidson:1996}, pages
13--14, or~\cite{Fillmore:1996}, theorem 2.5.4. \ns
\end{proof}

\begin{exam} If $J$ is an ideal in a $C^*$-algebra $A$, then the sequence
  \[ \vc 0 \to J \to A \to^\pi A/J \to \vc 0 \]
is short exact.
\end{exam}

\begin{prop}\label{0019023} The range of a $*$-homomorphism between $C^*$-algebras is closed
(and therefore itself a $C^*$-algebra).
\end{prop}

\begin{defn} A $C^*$-subalgebra $B$ of a $C^*$-algebra $A$ is
 \index{hereditary}%
\df{hereditary} if $a \in B$ whenever $a \in A$, $b \in B$, and $\vc 0 \le a \le b$.
\end{defn}

\begin{prop}\label{001915} Suppose $x^*x \le a$ in a $C^*$-algebra $A$. Then there
exists $b \in A$ such that $x = ba^{\frac14}$ and $\norm b \le {\norm a}^\frac14$.
\end{prop}

\begin{proof} See \cite{Davidson:1996}, page 13. \ns
\end{proof}

\begin{prop} Suppose $J$ is an ideal in a $C^*$-algebra $A$, $j \in J^+$, and
$a^*a \le j$. Then $a \in J$.  Thus ideals in $C^*$-algebras are hereditary.
\end{prop}

\begin{thm}\label{001922} Let $A$ and $B$ be $C^*$-algebras and $J$ be an ideal in~$A$.
If $\phi$ is a $*\,$-homomorphism from $A$ to $B$ and $\ker\phi \supseteq J$, then there
exists a unique $*\,$-homomorphism $\widetilde\phi\colon A/J \sto B$ which makes the
following diagram commute.
 \[ \xymatrix@+30pt{A \ar[d]_*+{\pi} \ar[dr]^*+{\phi} & \\
            A/J \ar[r]_*+{\widetilde\phi} & B  } \]
Furthermore, $\widetilde\phi$ is injective if and only if $\ker\phi = J$; and
$\widetilde\phi$ is surjective if and only if $\phi$ is.
\end{thm}

\begin{cor}\label{001923} If $\vc 0 \to A \to^\phi E \to B \to \vc 0$ is a short exact
sequence of $C^*$-algebras, then $E/\ran\phi$ and $B$ are isometrically $*\,$-isomorphic.
\end{cor}

\begin{cor}\label{001924} Every $C^*$-algebra $A$ has codimension one in its
unitization~$\wt A$; that is, $\dim \wt A/A = 1$.
\end{cor}

\begin{prop} Let $A$ be a $C^*$-algebra, $B$ be a $C^*$-subalgebra of $A$, and $J$ be an
ideal in~$A$. Then
   \[  B/(B \cap J) \cong (B + J)/J\,. \]
\end{prop}

Let $B$ be a  unital subalgebra of an arbitrary algebra $A$.  It is clear that if an
element $b \in B$ is invertible in $B$, then it is also invertible in~$A$. The converse
turns out to be true in $C^*$-algebras: if $b$ is invertible in $A$, then its inverse
lies in~$B$.  This is usually expressed by saying that \emph{every unital
$C^*$-subalgebra of a $C^*$-algebra is}
 \index{inverse!closed}%
 \index{closed!inverse}%
\df{inverse closed}.

\begin{prop} Let $B$ be a unital $C^*$-subalgebra of a $C^*$-algebra $A$.  If $b \in \inv(A)$,
then $b^{-1} \in B$.
\end{prop}

\begin{cor}\label{0019283} Let $B$ be a unital $C^*$-subalgebra of a $C^*$-algebra $A$ and
$b \in B$.   Then
   \[ \sigma_B(b) = \sigma_A(b)\,. \]
\end{cor}

\begin{cor}\label{0019286} Let $\phi\colon A \sto B$ be a unital $C^*$-monomorphism of a
$C^*$-algebra $A$ and $a \in A$.   Then
   \[ \sigma(a) = \sigma(\phi(a))\,. \]
\end{cor}



\endinput
\chapter{NORMED LINEAR SPACES}

\section{Norms}
In the world of analysis the predominant denizens are function spaces, vector spaces of
real or complex valued functions. To be of interest to an analyst such a space should
come equipped with a topology.  Often the topology is a metric topology, which in turn
frequently comes from a norm (see proposition~\ref{0001503}).

\begin{exam}\label{C023125}  For $x = (x_1,\dots, x_n) \in \K^n$ let $\norm{x} =
\left(\sum_{k=1}^n {\abs{x_k}}^2\right)^{1/2}$.  This is the
 \index{usual!norm!on $\K^n$}%
 \index{norm!usual!on $\K^n$}%
\df{usual norm} (or
 \index{Euclidean!norm!on $\K^n$}%
 \index{norm!Euclidean!on $\K^n$}%
 \index{K@$\K^n$!as a normed linear space}%
 \index{normed!linear space!$\K^n$ as a}%
\df{Euclidean norm}) on $\K^n$; unless the contrary is explicitly stated, $\K^n$ when
regarded as a normed linear space will always be assumed to possess this norm.  It is
clear that this norm induces the usual (Euclidean) metric on~$\K^n$.
\end{exam}

\begin{exam}\label{C023127}  For $x = (x_1,\dots,x_n) \in \K^n$ let
 \index{<unopn@$\norm{\hphantom{x}}_1$ ($1$-norm)}%
$\norm{x}_1 = \sum_{k=1}^n \abs{x_k}$.  The function $x \mapsto \norm{x}_1$ is easily
seen to be a norm on~$\K^n$. It is sometimes called
 \index{norm!$1$- (on $\K^n$)}%
 \index{one@$1$-norm!on $\K^n$}%
 \index{K@$\K^n$!as a normed linear space}%
 \index{normed!linear space!$\K^n$ as a}%
the \df{$1$-norm} on~$\K^n$.  It induces the so-called \emph{taxicab metric} on~$\R^2$.
\end{exam}

Of the next six examples the last is the most general.  Notice that all the others are special cases of it
or subspaces of special cases.

\begin{exam}\label{C023129}  For $x = (x_1,\dots,x_n) \in \K^n$ let
 \index{<unopn@$\norm{\hphantom{x}}_u$ (uniform norm)}%
 \index{<unopn@$\norm{\hphantom{x}}_\infty$ (uniform norm)}%
$\norm{x}_\infty = \max\{\,\abs{x_k} \colon 1 \le k \le n\}$.  This defines a norm on $\K^n$;
it is the
 \index{uniform!norm!on~$\K^n$}%
 \index{norm!uniform!on $\K^n$}%
 \index{K@$\K^n$!as a normed linear space}%
 \index{normed!linear space!$\K^n$ as a}%
\df{uniform norm} on~$\K^n$.  It induces the uniform metric on~$\K^n$.  An alternative notation for
$\norm{x}_\infty$ is $\norm{x}_U$.
\end{exam}

\begin{exam}\label{C0231301} The set $l_\infty$ of all bounded sequences of complex numbers is
clearly a vector space under pointwise operations of addition and scalar multiplication.
For $x = (x_1,x_2,\dots,) \in l_\infty$ let
 \index{<unopn@$\norm{\hphantom{x}}_\infty$ (uniform norm)}%
$\norm{x}_\infty = \sup\{\,\abs{x_k} \colon 1 \le k\}$.  This defines a norm on
$l_\infty$; it is the
 \index{uniform!norm!on~$l_\infty$}%
 \index{norm!uniform!on $l_\infty$}%
 \index{l@$l_\infty$!as a normed linear space}%
 \index{l@$l_\infty$!bounded sequences}%
 \index{normed!linear space!$l_\infty$ as a}%
\df{uniform norm} on~$l_\infty$.
\end{exam}

\begin{exam}\label{C0231302} A subspace of~\ref{C0231301} is the space $c$ of all convergent 
 \index{c@$c$!as a normed linear space}%
 \index{c@$c$!convergent sequences}%
 \index{normed!linear space!$c$ as a}%
sequences of complex numbers.  Under the uniform norm it is a normed linear space.
\end{exam}

\begin{exam}\label{C0231303} Contained in $c$ is another interesting normed linear space $\sbsb c0$
 \index{c@$c_0$!sequences which converge to zero}%
 \index{c@$c_0$!as a normed linear space}%
 \index{normed!linear space!$\sbsb c)$ as a}%
the space of all sequences of complex numbers which converge to zero (again with the uniform norm).
\end{exam}

\begin{exam}\label{C023131}  Let $S$ be a nonempty set.  If $f$ is a bounded $K$-valued function on $S$, let
  \[ \norm f_u := \sup\{\abs{f(x)} \colon x \in S\}\,. \]
This is a norm on the vector space $\fml B(S)$ of all bounded complex valued functions on $S$ and is called the
 \index{<unopn@$\norm{\hphantom{x}}_u$ (uniform norm)}%
 \index{uniform!norm!on~$\fml B(S)$}%
 \index{norm!uniform!on~$\fml B(S)$}%
 \index{B@$\fml B(S)$!bounded functions on~$S$}%
 \index{B@$\fml B(S)$!as a normed linear space}%
 \index{normed!linear space!$\fml B(S)$ as a}%
\df{uniform norm}.  Clearly this norm gives rise to the uniform metric on~$\fml B(S)$.
Notice that examples~\ref{C023129} and~\ref{C0231301} are
special cases of this one. (Let $S = \N_n := \{1,2,\dots,n\}$ or $S = \N$.)
\end{exam}











\begin{exam}\label{C023133} It is easy to make a substantial generalization of the preceding
example by replacing the field $\K$ by an arbitrary normed linear space. Suppose then
that $S$ is a nonempty set and $V$ is a normed linear space. For $f$ in the vector space $\fml B(S,V)$ of all bounded $V$-valued functions on~$S$, let
  \[ \norm f_u := \sup\{\norm{f(x)} \colon x \in S\}\,. \]
Then this is a norm and it is called the
 \index{<unopn@$\norm{\hphantom{x}}_u$ (uniform norm)}%
 \index{uniform!norm!on~$\fml B(S,V)$}%
 \index{norm!uniform!on~$\fml B(S,V)$}%
 \index{B@$\fml B(S,V)$!Bounded $V$-valued functions on~$S$}%
 \index{B@$\fml B(S,V)$!as a normed linear space}%
 \index{normed!linear space!$\fml B(S,V)$ as a}%
\df{uniform norm} on $\fml B(S,V)$.   (Why is the word ``easy'' in the first sentence of this example appropriate?)
\end{exam}

\begin{defn} Let $S$ be a set, $V$ be a normed linear space, and
 \index{F@$\fml F(S,V)$ ($V$-valued functions on~$S$)}%
$\fml F(S,V)$ be the vector space of all $V$-valued functions on~$S$.
Consider a sequence $(f_n)$ of functions in $\fml F(S,V)$.  If there is a function $g$ in $\fml F(S,V)$ such that
   \[ \sup\{\norm{f_n(x) - g(x)}\colon x \in S\} \sto 0 \quad \text{as $n \sto \infty$,} \]
then we say that the sequence $(f_n)$
 \index{<arrowto@$f_n \sto g$ (unif) (uniform convergence)}%
 \index{uniform!convergence}%
 \index{convergence!uniform}%
\df{converges uniformly} to~$g$ and write $f_n \sto g \text{\,(unif)}$.  The function $g$
is the
 \index{uniform!limit}%
 \index{limit!uniform}%
\df{uniform limit} of the sequence $(f_n)$.  Notice that if $g$ and all the $f_n$'s
belong to $\fml B(S,V)$, then uniform convergence of $(f_n)$ to $g$ is just convergence
of $(f_n)$ to $g$ with respect to the uniform metric.
\end{defn}

There are many ways in which sequences of functions converge.  Arguably the two most
common modes of convergence are uniform convergence, which we have just discussed, and
pointwise convergence.

\begin{defn}  Let $S$ be a set, $V$ be a normed linear space, and $(f_n)$ be a sequence in
$\fml F(S,V)$.  If there is a function $g$ such that
  \[ f_n(x) \sto g(x) \qquad  \text{for all $x \in S$}, \]
then $(fn)$
 \index{pointwise!convergence}%
 \index{convergence!pointwise}%
\df{converges pointwise} to~$g$.  In this case we write
  \[ f_n \sto g \text{\,(ptws)}. \]
The function $g$ is the
 \index{pointwise!limit}%
 \index{limit!pointwise}%
 \index{<arrowto@$f_n \sto g$ (ptws) (pointwise convergence)}%
\df{pointwise limit} of the~$f_n$'s.

If $(f_n)$ is an increasing sequence of real (or extended real) valued functions and $f_n
\sto g \text{\,(ptws)}$, we write
 \index{<arrowmonoup@$f_n \uparrow g$ (ptws) (pointwise monotone convergence)}%
$f_n \uparrow g \text{\,(ptws)}$. And if $(f_n)$ is decreasing and has $g$ as a pointwise
limit, we write
 \index{<arrowmonodown@$f_n \downarrow g$ (ptws) (pointwise monotone convergence)}%
$f_n \downarrow g \text{\,(ptws)}$.
\end{defn}

The most obvious connection, familiar from any real analysis course, between these two types of convergence is that uniform
convergence implies pointwise convergence.

\begin{prop}\label{C023141} Let $S$ be a set and $V$ be a normed linear space.  If a sequence $(f_n)$ in $\fml F(S,V)$
converges uniformly to a function $g$ in $\fml F(S,V)$, then $(f_n)$ converges pointwise to~$g$.
\end{prop}

The converse is not true.

\begin{exam}  For each $n \in \N$ let $f_n\colon [0,1] \sto \R\colon x \mapsto x^n$. Then the
sequence $(f_n)$ converges pointwise on $[0,1]$, but not uniformly.
\end{exam}

\begin{exam} For each $n \in \N$ and each $x \in \R^+$ let $f_n(x) = \frac1n\, x$.  Then on
each interval of the form $[0,a]$ where $a > 0$ the sequence $(f_n)$ converges uniformly
to the constant function~$0$.  On the interval $[0,\infty)$ it converges pointwise to
$0$, but not uniformly.
\end{exam}

Recall also from real analysis that a uniform limit of bounded functions must itself be bounded.

\begin{prop}\label{C023151} Let $S$ be a set, $V$ be a normed linear space, and $(f_n)$ be a
sequence in $\fml B(S,V)$ and $g$ be a member of $\fml F(S,V)$.  If $f_n \sto g\text{\,(unif)}$,
then $g$ is bounded.
\end{prop}

And a uniform limit of continuous functions is continuous.

\begin{prop}\label{C023154} Let $X$ be a topological space, $V$ be a normed linear space, and
$(f_n)$ be a sequence of continuous $V$-valued functions on~$X$ and $g$ be a member of $\fml F(X,V)$.  If $f_n
\sto g\text{\,(unif)}$, then $g$ is continuous.
\end{prop}

\begin{prop}\label{C023163} If $x$ and $y$ are elements of a vector space $V$ equipped with a
norm $\norm{\hphantom{x}}$, then
  \[ \bigabs{\strut\,\norm{x} - \norm{y}\,} \le \norm{x - y}\,. \]
\end{prop}

\begin{cor} The norm on a normed linear space is a uniformly continuous function.
 \index{continuity!of norms}%
 \index{norm!continuity of}%
\end{cor}

\begin{notn} Let $M$ be a metric space, $a \in M$, and $r > 0$.  We denote $\{x \in M\colon d(x,a) < r\}$, the
 \index{B@$B_r(a)$, $B_r$ (open ball of radius $r$)}%
 \index{open!ball}%
 \index{ball!open}%
open ball of radius $r$ about $a$, by $B_r(a)$.  Similarly, we denote $\{x \in M\colon d(x,a) \le r\}$, the
 \index{C@$C_r(a)$, $C_r$ (closed ball of radius $r$)}%
 \index{closed!ball}%
 \index{ball!closed}%
\df{closed ball} of radius $r$ about $a$, by $C_r(a)$ and $\{x \in M\colon d(x,a) = r\}$, the
 \index{S@$S_r(a)$, $S_r$ (sphere of radius $r$)}%
 \index{sphere}%
\df{sphere} of radius $r$ about $a$, by $S_r(a)$.  We will use $B_r$, $C_r$, and $S_r$ as abbreviations for $B_r(0)$, $C_r(0)$,
and $S_r(0)$, respectively.
\end{notn}

\begin{prop} If $V$ is a normed linear space, if $x \in V$, and if $r$, $s > 0$, then
 \begin{enumerate}[label=\rm{(\alph*)}]
   \item $B_r(\vc 0) = -B_r(\vc 0)$;
   \item $B_{rs}(\vc 0) = rB_s(\vc 0)$;
   \item $x + B_r(\vc 0) = B_r(x)$; and
   \item $B_r(\vc 0) + B_r(\vc 0) = 2\,B_r(\vc 0)$.  (Is it true in general that
$A + A = 2A$ when $A$ is a subset of a vector space?)
 \end{enumerate}
\end{prop}

\begin{defn}\label{C017314} If $a$ and $b$ are vectors in the vector space $V$, then the
 \index{closed!segment}%
 \index{segment, closed}%
\df{closed segment} between $a$ and $b$, denoted
 \index{<bracsqr@$[a,b]$ (closed segment in a vector space)}%
by~$[a,b]$, is $\{(1 - t)a + tb \colon 0 \le t \le 1\}$.
\end{defn}

\begin{cau}\label{C017317} Notice that there is a slight conflict between this notation for closed \emph{segments}, when
applied to the vector space $\R$ of real numbers, and the usual notation for closed \emph{intervals} in~$\R$.
In $\R$ the closed segment $[a,b]$ is the same as the closed
interval $[a,b]$ provided that $a \le b$. If $a > b$, however, the closed segment $[a,b]$
is the same as the segment $[b,a]$, it contains all numbers $c$ such that $b \le c \le
a$, whereas the closed interval $[a,b]$ is empty.
\end{cau}

\begin{defn} A subset $C$ of a vector space $V$ is
 \index{convex!set}%
\df{convex} if the closed segment $[a,b]$ is contained in $C$ whenever $a$, $b \in C$.
\end{defn}

\begin{exam}\label{C023169}  In a normed linear space every open ball is a convex set.  And so is every closed ball.
\end{exam}

\begin{prop}\label{prop_intrsctn_convex} The intersection of a family of convex subsets of a vector space is convex.
\end{prop}

\begin{defn} Let $V$ be a vector space. Recall that a \emph{linear combination} of a finite set $\{x_1, \dots, x_n\}$
of vectors in $V$ is a vector of the form $\sum_{k=1}^n \alpha_k x_k$ where $\alpha_1, \dots, \alpha_n \in \R$.
If $\alpha_1 = \alpha_2 = \dots = \alpha_n = 0$, then the linear combination is \emph{trivial}; if at least one
$\alpha_k$ is different from zero, the linear combination is \emph{nontrivial}.  A linear combination
$\sum_{k=1}^n\alpha_k x_k$ of the vectors $x_1, \dots, x_n$ is a
 \index{convex!combination}%
 \index{combination!convex}%
\df{convex combination} if $\alpha_k \ge 0$ for each $k$ ($1 \le k \le n$) and if $\sum_{k=1}^n \alpha_k = 1$.
\end{defn}

\begin{defn}\label{smplx005def} Let $A$ be a nonempty subset of a vector space~$V$.  We define the
 \index{convex!hull}%
 \index{hull!convex}%
 \index{co@$\co A$ (convex hull of~$A$)}%
\df{convex hull} of $A$, denoted by $\co A$, to be the smallest convex subset of $V$ which contain~$A$.
\end{defn}

The preceding definition should provoke instant suspicion.  How do we know there \emph{is} a `smallest' convex subset of~$V$
which contains~$A$?  One fairly obvious candidate (let's call it the `abstract' one) for such a set is the intersection
of \emph{all} the convex subsets of $V$ which contain~$A$.  (Of course, this might also be nonsense---\emph{unless} we know that there
is at least one convex set in $V$ which contains~$A$ and that the intersection of the family of all convex sets containing~$A$ is
itself a convex set containing~$A$.)  Another plausible candidate (we'll call it the `constructive' one) is the set of all convex
combinations of elements of~$A$.  (But \emph{is} this a convex set?  And is it the smallest one containing~$A$?)  Finally, even if these two
approaches both make sense, do they constitute equivalent definitions of \emph{convex hull}?  That is, do they both define the same set, and
does that set satisfy definition~\ref{smplx005def}?

\begin{prop}\label{smplx006} The definitions suggested in the preceding paragraph, labeled `abstract' and `constructive', make sense and
are equivalent.  Moreover, the set they describe satisfies the definition given in~\ref{smplx005def} for \emph{convex hull}.
\end{prop}

\begin{prop} If $T\colon V \sto W$ is a linear map between vector spaces and $C$ is a convex
subset of $V$, then $T^{\sto}(C)$ is a convex subset of~$W$.
\end{prop}

\begin{prop} In a normed linear space the closure of every convex set is convex.
\end{prop}

\begin{prop} Let $V$ be a normed linear space. For each $a \in V$ the map $T_a\colon V \sto V\colon x \mapsto x + a$ (called
 \index{translation}%
\df{translation} by~$a$) is a homeomorphism.
\end{prop}

\begin{cor} If $U$ is a nonempty open subset of a normed linear space $V$, then $U - U$
contains a neighborhood of~$0$.
\end{cor}

\begin{prop} If $(x_n)$ is a sequence in a normed linear space and $x_n \sto a$, then
$\D\frac1n\sum_{k=1}^nx_k~\sto~a$.
\end{prop}

In proposition~\ref{00015025} we showed how an inner product on a vector space induces a norm on that space.
It is reasonable to ask if all norms can be generated in this fashion from an inner product.  The answer
is \emph{no}.  The next proposition gives a very simple necessary and sufficient condition for a norm to
arise from an inner product.

\begin{prop} Let $V$ be a normed linear space.  There exists an inner product on $V$ which induces the
norm on $V$ if and only if the norm satisfies the \emph{parallelogram law}~\ref{parallelogram_law}.
\end{prop}

\begin{proof}[\emph{Hint for proof}]  To prove that if a norm satisfies the \emph{parallelogram law}
then it is induced by an inner product, use the equation given in the \emph{polarization identity}
(proposition~\ref{0001507}) as a definition.  Prove first that $\langle y,x \rangle = \conj{\langle x,y \rangle}$
for all $x$, $y\in V$ and that $\langle x,x \rangle > 0$ whenever $x \ne 0$.  Next, for arbitrary
$z \in V$, define a function $f\colon V \sto \C\colon x \mapsto \langle x,z \rangle$.  Prove that
   \[ f(x + y) + f(x - y) = 2f(x) \tag{$\ast$} \]
for all $x$, $y \in V$.  Use this to prove that $f(\alpha x) = \alpha f(x)$ for all $x \in V$ and $\alpha \in \R$.
(Start with $\alpha$ being a natural number.)  Then show that $f$ is additive. (If $u$ and $v$ are arbitrary
elements of $V$ let $x = u + v$ and $y = u - v$.  Use~$(\ast)$.)  Finally prove that $f(\alpha x) = \alpha f(x)$
for complex $\alpha$ by showing that $f(ix) = i\,f(x)$ for all $x \in V$.  \ns
\end{proof}

\begin{defn} Let $(X,\sfml T)$ be a topological space. A family $\sfml B \subseteq \sfml T$
is a
 \index{base!for a topology}%
\df{base} for $\sfml T$ if each member of $\sfml T$ is a union of members of~$\sfml B$.
In other words, a family $\sfml B$ of open sets is a base for $\sfml T$ if for each open
set $U$ there exists a subfamily $\sfml B'$ of $\sfml B$ such that $U = \bigcup \sfml B'$.
\end{defn}

\begin{exam}\label{xmpl_top_metric} In the plane $\R^2$ with its usual (Euclidean) topology, the family of
all open balls in the space is a base for its topology, since any open set in $\R^2$ can be expressed as a
union of open balls.  Indeed in any metric space whatever the family of open balls is a base for a topology
on the space.  This is the
 \index{topology!induced by a metric}%
 \index{induced!topology}%
\df{topology induced by the metric}.
\end{exam}

In practice (in metric spaces for example) it is often more convenient to specify a base for a topology than to specify
the topology itself.  It is important to realize, however, that there may be many
different bases for the same topology.  Once a particular base has been chosen we refer
to its members as
 \index{basic!open sets}%
 \index{open!set!basic}%
\emph{basic open sets}.

The word ``induces'' is regarded as a transitive relation.  For example, given an inner product space $V$, no one would
refer to the \emph{topology on $V$ induced by the metric induced by the norm induced by the inner product}.  One just speaks of
the \emph{topology on $V$ induced by the inner product}.  Sometimes one says ``generated by'' instead of ``induced by''.

\begin{prop} Let $V$ be a vector space with two norms $\norm{\,\cdot\,}_1$ and
$\norm{\,\cdot\,}_2$.  Let $\sfml T_k$ be the topology induced on $V$ by
$\norm{\,\cdot\,}_k$ (for $k = 1,2$). If there exists a constant $\alpha > 0$ such that
$\norm{x}_1 \le \alpha \norm{x}_2$ for every $x \in V$, then $\sfml T_1 \subseteq \sfml T_2$.
\end{prop}

\begin{defn} Two norms on a vector space $V$ are
 \index{equivalent!norms}%
 \index{norm!equivalent}%
\df{equivalent} if there exist constants $\alpha$, $\beta > 0$ such that for all $x \in
V$
   \[ \alpha\norm{x}_1 \le \norm{x}_2 \le \beta \norm{x}_1\,. \]
\end{defn}

\begin{prop}\label{C023184} If $\norm{\hphantom{x}}_1$ and $\norm{\hphantom{x}}_2$
 \index{equivalent!norms!induce identical topologies}%
are equivalent norms on a vector space~$V$, then they induce the same topology on~$V$.
\end{prop}

Proposition~\ref{C023184} gives us an easily verifiable sufficient condition for two norms
to induce identical topologies on a vector space.  Thus, if we are trying to show, for
example, that some subset of a normed linear space is open it may very well be the case
that the proof can be simplified by replacing the given norm with an equivalent one.

Similarly, suppose that we are attempting to verify that a function $f$ between two
normed linear spaces is continuous.  Since continuity is defined in terms of open sets
and equivalent norms produce exactly the same open sets (see proposition~\ref{C023184}),
we are free to replace the norms on the domain of $f$ and the codomain of $f$ with any
equivalent norms we please.  This process can sometimes simplify arguments significantly.
(This possibility of simplification, incidentally, is one major advantage of giving a
topological definition of continuity in the first place.)

\begin{defn}\label{C02319} Let $x = (x_n)$ a sequence of vectors in a normed linear space $V$.
The infinite series $\sum_{k=1}^\infty x_k$ is
 \index{convergent!series}%
 \index{series!convergent}%
\df{convergent} if there exists a vector $b \in V$ such that $\norm{b - s_n} \sto 0$ as $n
\sto \infty$, where $s_n = \sum_{k=1}^n x_k$ is the $n^{\text{th}}$ partial sum of the
sequence~$x$.  The series is
 \index{absolutely!convergent!series}%
 \index{convergent!absolutely}%
\df{absolutely convergent} if $\sum_{k=1}^\infty \norm{x_k} < \infty$.
\end{defn}

\begin{exer}\label{C023191} Let
 \index{l@$l_1$!as a normed linear space}%
 \index{l@$l_1$!absolutely summable sequences}%
 \index{normed!linear space!$l_1$ as a}%
$l_1$ be the set of all sequences $x = (x_n)$ of complex numbers such that the series $\sum x_k$ is absolutely
convergent. Make $l_1$ into a vector space with the usual pointwise definition of addition and scalar multiplication.
For every $x \in l_1$ define
 \index{<unopn@$\norm{\hphantom{x}}_1$ ($1$-norm)}%
 \index{<unopn@$\norm{\hphantom{x}}_\infty$ (uniform norm)}%
   \begin{align*}
      \norm{x}_1 &:= \sum_{k=1}^\infty \abs{x_k} \\
   \intertext{and}
      \norm{x}_\infty &:= \sup\{\abs{x_k}\colon k \in \N\}.
   \end{align*}
(These are, respectively, the
 \index{one@$1$-norm!of a sequence}%
 \index{norm!$1$- (of a sequence)}%
$1$-norm and the
 \index{uniform!norm!of a sequence}%
\emph{uniform} norm.) Show that both $\norm{\hphantom{x}}_1$ and $\norm{\hphantom{x}}_\infty$
are norms on~$l_1$. Then prove or disprove:
 \begin{enumerate}
  \item[(a)] If a sequence $(x^i)$ of vectors in the normed linear space
$\bigl(l_1, \norm{\hphantom{x}}_1\bigr)$ converges, then the sequence also converges in
$\bigl(l_1, \norm{\hphantom{x}}_\infty\bigr)$.
  \item[(b)] If a sequence $(x^i)$ of vectors in the normed linear space
$\bigl(l_1, \norm{\hphantom{x}}_\infty\bigr)$ converges, then the sequence also converges in
$\bigl(l_1, \norm{\hphantom{x}}_1\bigr)$.
 \end{enumerate}
\end{exer}









\section{Bounded Linear Maps}\label{sec_bdd_lin_maps}
Analysts work with objects having both algebraic and topological structure.  In the
preceding section we examined vector spaces endowed with a topology derived from a norm.
If these are the objects under consideration, what morphisms are likely to be of greatest
interest?  The plausible, and correct, answer is \emph{maps which preserve both
topological and algebraic structures}; that is, continuous linear maps.  The resulting
category is denoted by
 \index{category!$\cat{NLS_\infty}$ as a}%
 \index{NLSinf@$\cat{NLS_{\boldsymbol\infty}}$!the category}
$\cat{NLS_{\boldsymbol\infty}}$.  The isomorphisms in this category are, topologically,
homeomorphisms.

Here is a very useful condition which, for linear maps, turns out to be equivalent to continuity.

\begin{defn} A linear transformation $T\colon V \sto W$ between normed linear spaces is
 \index{bounded!linear map}%
 \index{linear!map!bounded}%
\df{bounded} if $T(B)$ is a bounded subset of $W$ whenever $B$ is a bounded subset
of~$V$.  In other words, a bounded linear map takes bounded sets to bounded sets.  We
denote
 \index{B@$\ofml B(V,W)$!bounded linear maps}%
by~$\ofml B(V,W)$ the family of all bounded linear transformations from $V$ into~$W$.  A
bounded linear map from a space $V$ into itself is often called a (bounded linear)
 \index{operator}%
 \index{conventions!all operators are bounded and linear}%
\df{operator}.  The family of all operators on a normed linear space $V$ is denoted
 \index{B@$\ofml B(V)$!operators}%
by~$\ofml B(V)$.  The class of normed linear spaces together with the bounded linear maps
between them constitute a category.  We verify below that this is just the
category~$\cat{NLS_{\boldsymbol\infty}}$.
\end{defn}

\begin{cau} It is extremely important to realize that a bounded linear map will \emph{not},
in general, be a bounded function in the usual sense of a \emph{bounded function} on some set
(see examples~\ref{C023131} and~\ref{C023133}).  The use of ``bounded'' in these two conflicting
senses may be unfortunate, but it is well established.
\end{cau}

\begin{exam} The linear map $T\colon \R \sto \R\colon x \mapsto 3x$ is a bounded linear map
(since it maps bounded subsets of $\R$ to bounded subsets of~$\R$), but, regarded just as
a function, $T$ is not bounded (since its range is not a bounded subset of~$\R$).
\end{exam}

The following observation my help reduce confusion.

\begin{prop} A linear transformation, unless it is the constant map that takes every vector to
zero, \emph{cannot} be a bounded function.
\end{prop}

\begin{defn}  Let $(M,d)$ and $(N,\rho)$ be metric spaces.  A function $f \colon M \sto N$ is
 \index{uniform!continuity}%
 \index{continuity!uniform}%
\df{uniformly continuous} if for every $\epsilon > 0$ there exists $\delta > 0$ such that
$\rho(f(x),f(y)) < \epsilon$ whenever  $x$, $y \in M$ and $d(x,y) < \delta$.
\end{defn}

One of the most striking aspects of linearity is that for linear maps the concepts of
continuity, continuity at a single point, and uniform continuity coalesce.  And in fact
they are exactly the same thing as boundedness.

\begin{thm}\label{C023221} Let $T \colon V \sto W$ be a linear transformation between normed
linear spaces. Then the following are equivalent:
  \begin{enumerate}[label=\rm{(\alph*)}]
    \item $T$ is bounded.
    \item The image of the closed unit ball under $T$ is bounded.
    \item $T$ is uniformly continuous on $V$.
    \item $T$ is continuous on $V$.
    \item $T$ is continuous at~$\vc 0$.
    \item There exists a number $M > 0$ such that $\norm{Tx} \le M\norm{x}$ for all $x \in V$.
  \end{enumerate}
\end{thm}

\begin{prop} Let $T\colon V \sto W$ be a bounded linear transformation between normed linear
spaces.  Then the following four numbers (exist and) are equal.
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $\sup\{\norm{Tx}\colon \norm{x} \le 1\}$
  \item $\sup\{\norm{Tx}\colon \norm{x} = 1\}$
  \item $\sup\{\norm{Tx}\,\norm{x}^{-1}\colon x \ne \vc 0\}$
  \item $\inf\{M > 0\colon \norm{Tx} \le M\norm{x}\ \text{ for all $x \in V$}\}$
 \end{enumerate}
\end{prop}

\begin{defn}\label{defn_op_norm} If $T$ is a bounded linear map, then $\norm{T}$, called the
 \index{norm!of a bounded linear map}%
 \index{<unopn@$\norm T$ (norm of a bounded linear map~$T$)}%
\df{norm} (or the
 \index{norm!operator}%
 \index{operator!norm}%
\df{operator norm}  of $T$, is defined to be any one of the four expressions in the previous exercise.
\end{defn}

\begin{exam}\label{exam_op_norm_norm} If $V$ and $W$ are normed linear spaces, then the function
   \[ \norm{\hphantom{T}}\colon \ofml B(V,W) \sto \R   \colon T \mapsto \norm{T} \]
is, in fact, a norm.
\end{exam}

\begin{exam}\label{C023228} The family $\ofml B(V,W)$ of all bounded linear maps
 \index{B@$\ofml B(V,W)$!as a normed linear space}%
 \index{normed!linear space!$\ofml B(V,W)$ as a}%
between normed linear spaces is itself a normed linear space under the operator norm defined above and the usual
pointwise operations of addition and scalar multiplication.  Of special importance is the family $V^* = \ofml B(V,\K)$
of the \emph{continuous} members of~$V^{\#}$, the
 \index{continuous!linear functionals}%
 \index{linear!functional!continuous}%
 \index{functional!continuous linear}%
\emph{continuous linear functionals}, also known as
 \index{bounded!linear functionals}%
 \index{linear!functional!bounded}%
 \index{functional!bounded linear}%
\emph{bounded linear functionals}.
Recall that in~\ref{def_alg_dual} we defined the algebraic dual $V^{\#}$ of a vector space~$V$. Of
greater importance in our work here is $V^*$, which is called the
 \index{<unopurstar@$V^*$ (dual space of a normed linear space)}%
 \index{V@$V^*$ (dual space of a normed linear space)}%
 \index{dual!space}%
 \index{space!dual of a normed linear}%
\df{dual space} of~$V$.  To distinguish it from the algebraic dual and the order dual, it is sometimes called the
 \index{dual!norm}%
 \index{norm!dual}%
\df{norm dual} or the
 \index{dual!topological}%
 \index{topological!dual}%
\df{topological dual} of~$V$.  (Note that this distinction is unnecessary in finite dimensional spaces
since there $V^* = V^{\#}$ (see~\ref{00015033}).
\end{exam}

\begin{prop}\label{C023228a} The family $\ofml B(V,W)$ of all bounded linear maps
 \index{B@$\ofml B(V,W)$!as a Banach space}%
 \index{Banach!space!$\ofml B(V,W)$ as a}%
between normed linear spaces is complete (with respect to the metric induced by its norm)
whenever $W$ is complete.  In particular, the dual space of every normed linear space is complete.
\end{prop}

















\begin{prop}\label{0003073} Let $U$, $V$, and $W$ be normed linear spaces.  If $S \in
\ofml B(U,V)$ and $T \in \ofml B(V,W)$, then $TS \in \ofml B(U,W)$ and $\norm{TS} \le
\norm T \norm S$.
\end{prop}

\begin{exam} On any normed linear space $V$ the
 \index{operator!identity}%
 \index{identity!operator}%
\emph{identity operator}
   \[ \id V = I_V =I\colon V \sto V \colon v \mapsto v \] is bounded and $\norm{I_V} = 1$.  The
 \index{operator!zero}%
 \index{zero!operator}%
\emph{zero operator}
   \[ \vc 0_V = \vc 0\colon V \sto V\colon v \mapsto \vc 0 \]
is also bounded and $\norm{0_V} = 0$.
\end{exam}

\begin{exam} Let $T \colon \R^2 \sto \R^3\colon (x,y) \mapsto (3x, x + 2y, x - 2y)$.
Then~$\norm{T} = \sqrt{11}$.
\end{exam}

Many students just finishing a beginning calculus course feel that differentiation is a ``nicer'' operation
than integration---probably because the rules for integration seem more complicated than those for differentiation.
However, when we regard them as linear maps exactly the opposite is true.

\begin{exam} As a linear map on the space of differentiable functions on $[0,1]$ (with the uniform norm)
integration is bounded; differentiation is not (and so is continuous nowhere!).
\end{exam}


\begin{defn} Let $f\colon V_1 \sto V_2$ be a function between normed linear spaces.  For $k = 1,2$ let
$\norm{\hphantom{M}}_k$ be the norm on~$V_k$ and $d_k$ be the metric on $V_k$ induced
by~$\norm{\hphantom{M}}_k$ (see example~\ref{0001503}).  Then $f$ is an
 \index{isometry}%
\df{isometry} (or
an \df{isometric} map) if $d_2(f(x),f(y)) = d_1(x,y)$ for all $x$, $y \in V_1$.  It is
 \index{norm!preserving}%
\df{norm preserving} if $\norm{f(x)}_2 = \norm{x}_1$ for all $x \in V_1$.)
\end{defn}

Previously in this section we have been discussing the category $\cat{NLS_{\boldsymbol\infty}}$ of normed
linear spaces and continuous linear maps.  In this category, isomorphisms preserve topological structure
(in addition to the vector space structure).  What would happen if instead we wished the isomorphisms to
preserve the metric space structure of the normed linear spaces; that is, to be isometries?  It is easy to
see that this is equivalent (in the presence of linearity) to asking the isomorphisms to be norm preserving.
In this new category, denoted by
 \index{category!$\cat{NLS_{\vc 1}}$ as a}%
 \index{NLS1@$\cat{NLS_{\vc 1}}$!the category}
$\cat{NLS_{\vc 1}}$, the morphisms would be
 \index{contractive}%
\df{contractive linear maps}, that is, linear maps $T\colon V \sto W$ between normed
linear spaces such that $\norm{Tx} \le \norm x$ for all $x \in V$.  It seems appropriate
to refer to $\cat{NLS_{\boldsymbol\infty}}$ as the
 \index{topological!category}%
 \index{category!topological}%
\emph{topological category} of normed linear spaces and to $\cat{NLS_{\vc 1}}$ as the
 \index{geometric category}%
 \index{category!geometric}%
\emph{geometric category} of normed linear spaces.  Many authors use the unmodified term
``isomorphism'' for an isomorphism in the topological category
$\cat{NLS_{\boldsymbol\infty}}$ and
 \index{isometric!isomorphism}%
 \index{isomorphism!isometric}%
``isometric isomorphism'' for an isomorphism in the geometric category $\cat{NLS_{\vc 1}}$. In these notes
we will focus on the topological category.  The isometric theory of normed linear spaces although important
is somewhat more specialized.

\begin{exer} Verify the unproved assertions in the preceding paragraph.  In particular, prove that
 \begin{enumerate}
    \item[(a)] $\cat{NLS_{\boldsymbol\infty}}$ is a category.
    \item[(b)] An isomorphism in $\cat{NLS_{\boldsymbol\infty}}$ is both a vector space isomorphism and a homeomorphism.
    \item[(c)] A linear map between normed linear spaces is an isometry if and only if it is norm preserving.
    \item[(d)] $\cat{NLS_{\vc 1}}$ is a category.
    \item[(e)] An isomorphism in $\cat{NLS_{\vc 1}}$ is both an isometry and a vector space isomorphism.
    \item[(f)] The category $\cat{NLS_{\vc 1}}$ is a subcategory of $\cat{NLS_{\boldsymbol\infty}}$
in the sense that every morphism of the former belongs to the latter.
 \end{enumerate}
\end{exer}
















\section{Finite Dimensional Spaces}
In this section are listed a few standard and useful facts about finite dimensional spaces. Proofs of these results
are quite easy to find.  Consult, for example, \cite{BrownP:1970}, section 4.4; \cite{Conway:1990}, section III.3;
\cite{Kreyszig:1978}, sections 2.4--5; or \cite{Rudin:1991}, pages 16--18.

\begin{defn} A sequence $(x_n)$ in a metric space is a
 \index{Cauchy!sequence}%
 \index{sequence!Cauchy}%
\df{Cauchy sequence} if for every $\epsilon > 0$ there exists $n_0 \in \N$ such that
$d(x_m,x_n) < \epsilon$ whenever $m$, $n \ge n_0$. This condition is often abbreviated as
follows: $d(x_m, x_n) \sto 0$ as $m$, $n \sto \infty$ (or $\lim_{m,n \sto
\infty}d(x_m,x_n) = 0$).
\end{defn}

\begin{defn} A metric space $M$ is
 \index{complete!metric space}%
 \index{metric space!complete}%
 \index{space!complete metric}%
\df{complete} if every Cauchy sequence in $M$ converges. (For a review of the most elementary facts about complete
metric spaces see the first section of chapter 20 of my online notes~\cite{Erdman:2007}.)
\end{defn}

\begin{prop}\label{prop_fdnls_Kn} If $V$ is an $n$-dimensional normed linear space over $\K$, then there exists a map
 \index{finite!dimensional normed linear spaces}%
from $V$ onto $\K^n$ which is both a homeomorphism and a vector space isomorphism.
\end{prop}

\begin{cor} Every finite dimensional normed linear space is complete.
\end{cor}

\begin{cor}\label{cor_fdvs_closed} Every finite dimensional vector subspace of a normed linear space is closed.
\end{cor}

\begin{cor}\label{cor_fdvs_norms_equiv} Any two norms on a finite dimensional vector space are equivalent.
\end{cor}

\begin{defn} A subset $K$ of a topological space $X$ is
 \index{compact}%
\df{compact} if every family $\sfml U$ of open subsets of $X$ whose union contains $K$ can be reduced to a finite
subfamily of $\sfml U$ whose union contains~$K$.
\end{defn}

\begin{prop}\label{00015032} The closed unit ball in a normed linear space is compact if and only if
the space is finite dimensional.
\end{prop}

\begin{prop}\label{00015033} If $V$ and $W$ are normed linear spaces and $V$ is finite dimensional, then
every linear map $T \colon V \sto W$ is continuous.
\end{prop}

Let $T$ be a bounded linear map between (perhaps infinite dimensional) normed linear spaces. Just as in the finite
dimensional case, the kernel and range of $T$ are objects of great importance.  In both the finite and infinite dimensional
case they are vector subspaces of the spaces which contain them.  Additionally, in both cases the kernel of $T$ is closed.
(Under a continuous function the inverse image of a closed set is closed.)  There is, however, a major difference between
the two cases.  In the finite dimensional case the range of $T$ must be closed (by~\ref{cor_fdvs_closed}).  As we will see
in example~\ref{exam_ran_nonclosed} linear maps between infinite dimensional spaces need not have closed range.






















\section{Quotients of Normed Linear Spaces}

\begin{defn}\label{quo001def} Let $A$ be an object in a concrete category $\cat C$.
A surjective morphism $A \to^\pi B$ in $\cat C$ is a
 \index{quotient!map!for concrete categories}%
 \index{pi@$\pi$ (quotient map)}%
\df{quotient map} for $A$ if a function $g\colon B \sto C$ (in $\cat{SET}$) is a morphism
(in $\cat C$) whenever $g \circ \pi$ is a morphism. An object $B$ in $\cat C$ is a
 \index{quotient!object}%
\df{quotient object} for $A$ if it is the range of some quotient map for~$A$.
\end{defn}

\begin{exam} In the category $\cat{VEC}$ of vector spaces and linear maps every surjective linear
 \index{quotient!map!in $\cat{VEC}$}%
map is a quotient map.
\end{exam}

\begin{exam}\label{C021734} In the category $\cat{TOP}$ not every epimorphism is a
 \index{quotient!map!in $\cat{TOP}$}%
quotient map.
\end{exam}

\begin{proof}[\emph{Hint for proof}] Consider the identity map on the reals with different
topologies on the domain and codomain. \ns
\end{proof}

The next item, which should be familiar from linear algebra, shows how a particular quotient object can be generated by
``factoring out a subspace''.

\begin{defn}\label{quo002def} Let $M$ be a subspace of a vector space $V$.  Define an
equivalence relation $\sim$ on $V$ by
   \[ x \sim y \qquad \text{if and only if} \qquad y - x \in M. \]
For each $x \in V$ let $[x]$ be the equivalence class containing~$x$.
 \index{<bracsqr@$[x]$ (equivalence class containing~$x$)}%
 \index{<binop@$V/M$ (quotient of $V$ by $M$)}%
Let $V/M$ be the set of all equivalence classes of elements of~$V$.  For $[x]$ and $[y]$
in $V/M$ define
  \[ [x] + [y] := [x+y]\]
and for $\alpha \in \R$ and $[x] \in V/M$ define
  \[\alpha[x] := [\alpha x].\]
Under these operations $V/M$ becomes a vector space.  It is the
 \index{quotient!vector space}%
 \index{space!quotient!vector}%
\df{quotient space} of $V$ by~$M$.  The notation $V/M$ is usually read ``$V$ mod~$M$''.
The linear map
  \[ \pi\colon V \sto V/M\colon x \mapsto [x] \]
is called the
 \index{quotient!map!for vector spaces}%
 \index{pi@$\pi$ (quotient map)}%
\df{quotient map}.
\end{defn}

\begin{exer} Verify the assertions made in definition~\ref{quo002def}. In particular,
show that $\sim$ is an equivalence relation, that addition and scalar multiplication of
the set of equivalence classes are well defined, that under these operations $V/M$ is a
vector space, that the function called here the ``quotient map'' is in fact a quotient map
in the sense of~\ref{quo001def}, and that this quotient map is linear.
\end{exer}

The following result is called the \emph{fundamental quotient theorem} or the
 \index{quotient!theorem!for $\cat{VEC}$}%
 \index{first isomorphism theorem}%
{first isomorphism theorem} for vector spaces.

\begin{thm}\label{quo005} Let $V$ and $W$ be vector spaces and $M \preceq V$. If
$T \in \ofml L(V,W)$ and $\ker T \supseteq M$, then there exists a unique $\wt T \in
\ofml L(V/M\,,W)$ which makes the following diagram commute.
 \[\xymatrix@+30pt{V \ar[d]_*+{\pi} \ar[dr]^*+{T} & \\
            V/M \ar[r]_*+{\widetilde T} & W  }\]
Furthermore, $\wt T$ is injective if and only if $\ker T = M$; and $\wt T$ is surjective
if and only if $T$ is.
\end{thm}

\begin{cor} If $T\colon V \sto W$ is a linear map between vector spaces, then
$\ran T \cong V/\ker T$.
\end{cor}

\begin{prop}\label{C023711} Let $V$ be a normed linear space and $M$ be a closed subspace
of~$V$.
 \index{quotient!normed linear space}%
 \index{normed!linear space!quotient}%
 \index{space!quotient!normed linear}%
Then the map
  \[ \norm{\hphantom{x}} \colon V/M \sto \R \colon [x]
                        \mapsto \inf\{\norm u \colon u \sim x\} \]
is a norm on~$V/M$.  Furthermore,
 \index{pi@$\pi$ (quotient map)}%
 \index{quotient!map!for normed linear spaces}%
the quotient map
  \[ \pi \colon V \sto V/M \colon x \mapsto [x] \]
is a bounded linear surjection with $\norm\pi \le 1$.
\end{prop}

\begin{defn} The norm defined on $V/M$ is called the
 \index{quotient!norm}%
 \index{norm!quotient}%
\df{quotient norm} on~$V/M$.  Under this norm $V/M$ is a normed linear space and is called the
 \index{quotient space}%
 \index{space!quotient}%
\df{quotient space} of $V$ by~$M$.  It is often referred to as ``$V$ mod $M$''.
\end{defn}

\begin{thm}[Fundamental quotient theorem for $\cat{NLS_{\boldsymbol\infty}}$]\label{C023717}
 \index{quotient!theorem!for $\cat{NLS_{\boldsymbol\infty}}$}%
Let $V$ and $W$ be normed linear spaces and $M$ be a closed subspace of~$V$.  If $T$ is a
bounded linear map from $V$ to $W$ and $\ker T \supseteq M$, then there exists a unique
bounded linear map $\wt T\colon V/M \sto W$ which makes the following diagram commute.
  \[ \xymatrix@+30pt{V \ar[d]_*+{\pi} \ar[dr]^*+{T} & \\
             V/M \ar[r]_*+{\wt T} & W  } \]
Furthermore: $\norm{\wt T} = \norm{T}$; $\wt T$ is injective if and only if $\ker T = M$;
and $\wt T$ is surjective if and only if $T$ is.
\end{thm}










































\section{Products of Normed Linear Spaces}\label{C0155}
Products and coproducts, like quotients, are best described in terms of what they \emph{do}.

\begin{defn}\label{C015511} Let $A_1$ and $A_2$ be objects in a category~$\cat C$. We say that
the triple $(P,\pi_1,\pi_2)$, where $P$ is an object and $\pi_k\colon P \sto A_k$ ($k =
1,2$) are morphisms, is a
 \index{product!in a category}%
\df{product} of $A_1$ and $A_2$ if for every object $B$ and every pair of morphisms
$f_k\colon B \sto A_k$ ($k = 1,2$) there exists a unique morphism $f\colon B \sto P$ such
that $f_k = \pi_k \circ f$ for $k = 1,2$.

It is conventional to say, ``Let $P$ be a product of \dots'' for, ``Let $(P,\pi_1,\pi_2)$
be a product of \dots''. The product of $A_1$ and $A_2$ is often written as $A_1 \times
A_2$ or as $\prod_{k = 1,2} A_k$.
\end{defn}

In a particular category products may or may not exist.  It is an interesting and
elementary fact that whenever they exist they are unique (up to isomorphism), so that we
may unambiguously speak of \emph{the} product of two objects.  When we say that a
categorical object satisfying some condition(s) is \emph{unique up to isomorphism} we
mean, of course, that any two objects satisfying the condition(s) must be isomorphic.  We
will often use the phrase
 \index{essentially!unique}%
 \index{conventions!essentially unique means unique up to isomorphism}%
 \index{uniqueness!essential}%
``essentially unique'' for ``unique up to isomorphism.''

\begin{prop} In any category products (if they exist) are
 \index{product!uniqueness of}%
 \index{uniqueness!of products}%
essentially unique.
\end{prop}

\begin{exam}\label{C015517} In the category $\cat{SET}$ the
 \index{product!in $\cat{SET}$}%
 \index{SET@$\cat{SET}$!products in}%
product of two sets $A_1$ and $A_2$ exists and is in fact the usual Cartesian product
$A_1 \times A_2$ together with the usual coordinate projections $\pi_k\colon A_1 \times
A_2 \sto A_k \colon (a_1,a_2) \mapsto a_k$.
\end{exam}

\begin{exam}\label{C015521} If $V_1$ and $V_2$ are vector spaces we make the Cartesian product
$V_1 \times V_2$ into a vector space as follows. Define addition by
  \[ (u,v) + (w,x) := (u + w, v + x)  \]
and scalar multiplication by
  \[ \alpha (u,v) := (\alpha u, \alpha v) \,. \]
This makes $V_1 \times V_2$ into a vector space and that this space together with the
usual coordinate projections $\pi_k\colon V_1 \times V_2 \sto V_k \colon (v_1,v_2)
\mapsto v_k$ ($k = 1,2$) is a
 \index{product!in $\cat{VEC}$}%
 \index{VEC@$\cat{VEC}$!products in}%
 \index{vector!space!product}%
product in the category $\cat{VEC}$. It is usually called the
 \index{direct!sum!of vector spaces}%
 \index{sum!direct}%
 \index{vector!space!direct sum}%
 \index{<binopdirectsum@$\oplus$ (direct sum)}%
\df{direct sum} of $V$ and $W$ and is denoted by~$V \oplus W$.
\end{exam}

It often possible and desirable to take the product of an arbitrary family of objects in
a category.  Following is a generalization of definition~\ref{C015511}.

\begin{defn}\label{C015524} Let $\bigl(A_\lambda\bigr)_{\lambda \in \Lambda}$ be an
indexed family of objects in a category~$\cat C$. We say that the object $P$ together
with an indexed family $\bigl(\pi_\lambda\bigr)_{\lambda \in \Lambda}$ of morphisms
$\pi_\lambda\colon P \sto A_\lambda$ is a
 \index{product!in a category}%
\df{product} of the objects $A_\lambda$ if for every object $B$ and every indexed family
$\bigl(f_\lambda\bigr)_{\lambda \in \Lambda}$ of morphisms $f_\lambda\colon B \sto
A_\lambda$ there exists a unique map $g \colon B \sto P$ such that $f_\lambda =
\pi_\lambda \circ g$ for every $\lambda \in \Lambda$.
\end{defn}

A category in which arbitrary products exist is said to be
 \index{product!complete}%
 \index{complete!product}%
\df{product complete}.  Many of the categories we encounter in these notes are product
complete.

\begin{defn}\label{C015531} Let $\bigl(S_\lambda\bigr)_{\lambda \in \Lambda}$ be an indexed
family of sets. The
 \index{Cartesian product}%
 \index{product!Cartesian}%
\df{Cartesian product} of the indexed family, denoted by
 \index{<<@$\prod S_\lambda$ (Cartesian product)}%
$\prod_{\lambda \in \Lambda} S_\lambda$ or just $\prod S_\lambda$, is the set of all
functions $f \colon \Lambda \sto \bigcup S_\lambda$ such that $f(\lambda) \in S_\lambda$
for each $\lambda \in \Lambda$.  The maps $\pi_\lambda \colon \prod S_\lambda \sto
S_\lambda \colon f \mapsto f(\lambda)$ are the canonical
 \index{coordinate!projections}%
 \index{pi@$\pi_\lambda$ (coordinate projections)}%
 \index{projection!onto coordinates}%
\df{coordinate projections}. In many cases the notation $f_\lambda$ is more convenient
than~$f(\lambda)$.  (See, for example, example~\ref{C015537} below.)
\end{defn}

\begin{exam}\label{C015534} A very important special case of the preceding definition occurs
when all of the sets $S_\lambda$ are identical: say $S_\lambda = A$ for every $\lambda
\in \Lambda$.  In this case the Cartesian product comprises all the functions which map
$\Lambda$ into~$A$. That is, $\prod_{\lambda \in \Lambda} S_\lambda = A^\Lambda$. Notice
also that in this case the coordinate projections are
 \index{evaluation!map}%
 \index{function!evaluation at a point}%
\emph{evaluation maps}. For each $\lambda$ the coordinate projection $\pi_\lambda$ takes
each point $f$ in the product (that is, each function $f$ from $\Lambda$ into~$A$) to
$f(\lambda)$ its value at~$\lambda$.  Briefly, each coordinate projection is an
evaluation map at some point.
\end{exam}

\begin{exam}\label{C015537} What is $\R^n$? It is just the set of all $n$-tuples of real numbers.
That is, it is the set of functions from $\N_n = \{1,2,\dots,n\}$ into~$\R$.  In other
words $\R^n$ is just shorthand for~$\R^{\N_n}$.  In $\R^n$ one usually writes $x_j$ for
the $j^{\text{th}}$ coordinate of a vector $x$ rather than~$x(j)$.
\end{exam}

\begin{exam} In the category $\cat{SET}$ the
 \index{product!in $\cat{SET}$}%
 \index{SET@$\cat{SET}$!products in}%
product of an indexed family of sets exists and is in fact the Cartesian product of these
sets together with the canonical coordinate projections.
\end{exam}

\begin{exam}\label{C015544} If $\bigl(V_\lambda\bigr)_{\lambda \in \Lambda}$ is an indexed
family of vector spaces we make the Cartesian product $\prod V_\lambda$ into a vector
space as follows. Define addition and scalar multiplication pointwise: for $f$, $g \in
\prod A_\lambda$ and $\alpha \in \R$
  \[ (f + g)(\lambda) := f(\lambda) + g(\lambda)  \]
and
  \[ (\alpha f)(\lambda) := \alpha f(\lambda) \,. \]
This makes $\prod V_\lambda$ into a vector space, which is sometimes called the
 \index{direct!product}%
 \index{product!direct}%
\df{direct product} of the spaces $V_\lambda$, and this space together with the canonical
coordinate projections (which are certainly  linear maps) is a
 \index{product!in $\cat{VEC}$}%
 \index{VEC@$\cat{VEC}$!products in}%
 \index{vector!space!product}%
product in the category $\cat{VEC}$.
\end{exam}

\begin{exam}\label{C023411} Let $V$ and $W$ be normed linear spaces. On the Cartesian product
$V \times W$ define
  \index{<unopn@$\norm{\hphantom{x}}_2$ (Euclidean norm or $2$-norm)}%
  \index{<unopn@$\norm{\hphantom{x}}_1$ ($1$-norm)}%
  \index{<unopn@$\norm{\hphantom{x}}_u$ (uniform norm)}%
 \begin{align*}
    \norm{(x,y)}_2 &= \sqrt{\norm x^2 + \norm y^2},  \\
    \norm{(x,y)}_1 &= \norm x + \norm y,  \\
   \intertext{and}
    \norm{(x,y)}_u &= \max\{\norm x, \norm y\}\,.
 \end{align*}
The first of these is the
 \index{Euclidean!norm!on the product of two normed spaces}%
 \index{norm!Euclidean!on the product of normed spaces}%
\df{Euclidean norm} (or
 \index{two@$2$-norm}%
 \index{norm!$2$- (on the product of normed spaces)}%
\df{$2$-norm}) on $V \times W$, the second is the
 \index{one@$1$-norm!on the product of normed spaces}%
 \index{norm!$1$- (on the product of normed spaces)}%
\df{$1$-norm}, and the last is the
 \index{uniform!norm!on the product of normed spaces}%
 \index{norm!uniform!on the product of normed spaces}%
\df{uniform norm}. Verifying that these are all norms on $V \times W$ is quite similar to
the arguments required in examples~\ref{C023125}, \ref{C023127}, and~\ref{C023129}. That
they are equivalent norms is a consequence of the following inequalities and
proposition~\ref{C023184}.
   \[ \norm x + \norm y \le \sqrt 2 \sqrt{\norm x^2 + \norm y^2} \le 2 \max\{\norm x,\norm y\} \le 2(\norm x + \norm y) \]
\end{exam}

\begin{conv}\label{C023414} In the preceding example we defined three (equivalent) norms on
the product space $V \times W$.  We will take the first of these $\norm{\;\;}_1$ as the
 \index{conventions!on choice of product norm}%
 \index{product!norm}%
 \index{norm!product}%
\df{product norm} on $V \times W$.  Thus whenever $V$ and $W$ are normed linear spaces,
unless the contrary is specified we will regard the product $V \times W$ as a normed
linear space under this norm.  Usually we write just $\norm{(x,y)}$ instead of
$\norm{(x,y)}_1$.  The product of the normed linear spaces $V$ and $W$ is usually denoted
by $V \oplus W$ and is called the
 \index{product!of normed linear spaces}%
 \index{normed!linear space!product}%
 \index{<binopdirectsum@$\oplus$ (direct sum)}%
 \index{direct!sum!of normed linear spaces}%
 \index{normed!linear space!direct sum}%
\df{direct sum} of $V$ and~$W$. (In the special case where $V = W = \R$, what metric does
the product norm induce on~$\R^2$?)
\end{conv}

\begin{exam}\label{exam_prod_nls} Show that product $V \oplus W$ of normed linear spaces is in fact a product
 \index{product!in $\cat{NLS_{\boldsymbol\infty}}$}%
 \index{NLSinf@$\cat{NLS_{\boldsymbol\infty}}$!products in}%
in the category $\cat{NLS_{\boldsymbol\infty}}$  of normed linear spaces and bounded
linear maps.
\end{exam}

\begin{prop} Let $\bigl((x_n,y_n)\bigr)$ be a sequence in the direct sum $V \oplus W$ of two normed linear
spaces. Prove that $\bigl((x_n,y_n)\bigr)$ converges to a point $(a,b)$ in $V \oplus W$ if and only
if $x_n \sto a$ in $V$ and $y_n \sto b$ in~$W$.
\end{prop}

\begin{prop}\label{C023427} Addition is a continuous operation on a normed linear space~$V$.
 \index{continuity!of addition}%
 \index{addition!continuity of}%
That is, the map
   \[ A \colon V \oplus V \sto V \colon (x,y) \mapsto x + y \]
is continuous.
\end{prop}

\begin{exer} Give a \emph{very} short proof (no $\epsilon$'s or $\delta$'s or open sets) that if
$(x_n)$ and $(y_n)$ are sequences in a normed linear space which converge to $a$ and $b$,
respectively, then $x_n + y_n \sto a + b$.
\end{exer}

\begin{prop}\label{C023434} Scalar multiplication is a continuous operation on a normed
 \index{continuity!of scalar multiplication}%
 \index{scalar!multiplication!continuity of}%
linear space~$V$ in the sense that the map
   \[ S \colon \K \times V \sto V \colon  (\alpha,x) \mapsto \alpha x \]
is continuous.
\end{prop}

\begin{prop} If $B$ and $C$ are subsets of a normed linear space and $\alpha$ is a scalar, then
 \begin{enumerate}
   \item $\clo{\alpha B} = \alpha \clo B$; and
   \item $\clo B + \clo C  \subseteq  \clo{B + C}$.
 \end{enumerate}
\end{prop}

\begin{exam} If $B$ and $C$ are closed subsets of a normed linear space, then it does not
necessarily follow that $B + C$ is closed (and therefore $\clo B + \clo C$ and $\clo{B +
C}$ need not be equal).
\end{exam}

\begin{prop} Let $C_1$ and $C_2$ be compact subsets of a normed linear space~$V$. Then
   \begin{enumerate}
     \item $C_1 \times C_2$ is a compact subset of $V \times V$, and
     \item $C_1 + C_2$ is a compact subset of~$V$.
   \end{enumerate}
\end{prop}

\begin{exam}\label{C023441}  Let $X$ be a topological space.  Then the family
$\fml C(X)$ of all continuous complex valued functions on~$X$ is a
 \index{C@$\fml C(X)$!as a vector space}%
vector space under the usual pointwise operations of addition and scalar multiplication.
\end{exam}

\begin{exam}\label{C023443}  Let $X$ be a topological space.  We denote
 \index{C@$\fml C_b(X)$!continuous bounded functions on $X$}%
by $\fml C_b(X)$ the family of all bounded continuous complex valued functions on~$X$.  It is a
 \index{C@$\fml C_b(X)$!as a normed linear space}%
 \index{normed!linear space!$\fml C_b(X)$ as a}%
normed linear space under the uniform norm.  In fact, it is a subspace of~$\fml B(X)$
(see example~\ref{C023131}).
\end{exam}

\begin{exam}\label{C023464} If $S$ is a set and $a \in S$, then the evaluation functional
     \[ E_a\colon \fml B(S) \sto \R\colon f \mapsto f(a) \]
is a bounded linear functional on the space $\fml B(S)$ of all bounded real valued
functions on~$S$. If $S$ happens to be a topological space then we may also regard $E_a$
as a member of the dual space of $\fml C_b(S)$.  (In either case, what is~$\norm{E_a}$?)
\end{exam}

\begin{defn}\label{def_norm_alg} Let $A$ be an algebra on which a norm has been defined.  Suppose that additionally the
 \index{submultiplicative}%
\df{submultiplicative} property
  \[ \norm{xy} \le \norm x\,\norm y \]
is satisfied for all $x$, $y \in A$.  Then $A$ is a
 \index{normed!algebra}%
 \index{algebra!normed}%
\df{normed algebra}.  We make one further requirement: if the algebra $A$ is unital, then
   \[ \norm{\vc 1} = 1\,. \]
In this case $A$ is a
 \index{unital!normed algebra}%
 \index{normed!algebra!unital}%
 \index{algebra!unital normed}%
\df{unital normed algebra}.
\end{defn}

\begin{exam}\label{C023448} In example~\ref{C023131} we showed that the family
 \index{B@$\fml B(S)$!as a normed algebra}%
 \index{normed!algebra!$\fml B(S)$ as a}%
$\fml B(S)$ of bounded real valued functions on a set $S$ is a normed linear space under
the uniform norm.  It is also a commutative unital normed algebra.
\end{exam}

\begin{prop} Multiplication is a continuous operation on a normed algebra~$V$ in the sense
that the map
   \[ M \colon V \times V \sto V \colon (x,y) \mapsto  xy \]
is continuous.
\end{prop}

\begin{proof}[\emph{Hint for proof}] If you can prove that multiplication on the real numbers
is continuous you can almost certainly prove that it is continuous on arbitrary normed
algebras. \ns
\end{proof}

\begin{exam}\label{C023454} The family $\fml C_b(X)$ of bounded continuous real valued
 \index{C@$\fml C_b(X)$!as a normed algebra}%
 \index{normed!algebra!$\fml C_b(X)$ as a}%
functions on a topological space~$X$ is, under the usual pointwise operations and the
uniform norm, a commutative unital normed algebra. (See example~\ref{C023443}.)
\end{exam}

\begin{exam}\label{C023456} The family $\ofml B(V)$ of all (bounded linear) operators
 \index{B@$\ofml B(V)$!as a unital normed algebra}%
 \index{normed!algebra!$\ofml B(V)$ as a}%
on a normed linear space $V$ is a unital normed algebra. (See~\ref{C023228} and~\ref{C023228a}.)
\end{exam}





























\section{Coproducts of Normed Linear Spaces}\label{C0156}
Coproducts are like products---except all the arrows are reversed.

\begin{defn}\label{C015611} Let $A_1$ and $A_2$ be objects in a category~$\cat C$.  The triple
$(Q,\iota_1,\iota_2)$, ($Q$ is an object and $\iota_k\colon A_k \sto Q$, $k = 1,2$ are morphisms) is a
 \index{coproduct}%
\df{coproduct} of $A_1$ and $A_2$ if for every object $B$ and every pair of morphisms
$f_k\colon A_k \sto B$ (k = 1,2) there exists a unique morphism $f\colon P \sto B$ such
that $f_k = f \circ \iota_k$ for $k = 1,2$.
\end{defn}

\begin{prop} In any category coproducts (if they exist) are
 \index{coproduct!uniqueness of}%
essentially unique.
\end{prop}

\begin{defn}\label{C0006127} Let $A$ and $B$ be sets.  When $A$ and $B$ are disjoint we will
often use the notation
 \index{<binopuniond@$\uplus$, $\biguplus$ (disjoint union)}%
$A \uplus B$ instead of $A \cup B$ (to emphasize the disjointness of $A$ and~$B$). When
$C = A \uplus B$ we say that $C$ is the
 \index{disjoint!union}%
 \index{union!disjoint}%
\df{disjoint union} of $A$ and~$B$.  Similarly we frequently choose to write the union of
a \emph{pairwise disjoint} family $\sfml A$ of sets as $\biguplus\sfml A$. And the
notation $\biguplus_{\lambda \in \Lambda} A_\lambda$ may be used to denote the union of a
\emph{pairwise disjoint} indexed family $\{A_\lambda\colon  \lambda \in \Lambda\}$ of
sets. When $C = \biguplus\sfml A$ or $C = \biguplus_{\lambda \in \Lambda} A_\lambda$ we
say that $C$ is the \df{disjoint union} of the appropriate family.
\end{defn}

\begin{defn} In the preceding definition we introduced the notation $A \uplus B$ for the
union of two disjoint sets $A$ and~$B$.  We now extend this notation somewhat and allow
ourselves to take the \emph{disjoint union} of sets $A$ and $B$ even if they are not
disjoint. We ``make them disjoint'' by identifying (in the obvious way) the set $A$ with the
set $A' = \{(a,1)\colon a \in A\}$ and $B$ with the set $B' = \{(b,2)\colon b \in B\}$.  Then we
denote the union of $A'$ and $B'$, which \emph{are} disjoint, by $A \uplus B$ and call it the
 \index{disjoint!union}%
 \index{union!disjoint}%
 \index{<binopuniond@$\uplus$, $\biguplus$ (disjoint union)}%
\df{disjoint union} of $A$ and~$B$.

In general, if $\bigl(A_\lambda\bigr)_{\lambda \in \Lambda}$ is an indexed family of
sets, its
 \index{disjoint!union}%
 \index{union!disjoint}%
\df{disjoint union}, \smash[b]{$\biguplus\limits_{\lambda \in \Lambda}A_\lambda$} is
defined to be $\bigcup \{(A_\lambda,\lambda)\colon \lambda \in \Lambda\}$.
\end{defn}

\begin{exam}\label{C015617} In the category $\cat{SET}$ the
 \index{coproduct!in $\cat{SET}$}%
 \index{SET@$\cat{SET}$!coproducts in}%
coproduct of two sets $A_1$ and $A_2$ exists and is their disjoint union $A_1 \uplus A_2$
together with the obvious inclusion maps $\iota_k\colon A_k  \sto A_1 \uplus A_2$ ($k = 1,2$).
\end{exam}

It is interesting to observe that while the product and coproduct of a finite collection
of objects in the category $\cat{SET}$ are quite different, in the more complex category
$\cat{VEC}$ they turn out to be exactly the same thing.

\begin{exam} If $V_1$ and $V_2$ are vector spaces make the Cartesian product $V_1 \times V_2$
into a vector space as in example~\ref{C015521}. This space together with the obvious
injections is a
 \index{coproduct!in $\cat{VEC}$}%
 \index{VEC@$\cat{VEC}$!coproducts in}%
 \index{vector!space!coproduct}%
coproduct in the category $\cat{VEC}$.
\end{exam}

It often possible and desirable to take the coproduct of an arbitrary family of objects
in a category.  Following is a generalization of definition~\ref{C015611}.

\begin{defn}\label{C015624} Let $\bigl(A_\lambda\bigr)_{\lambda \in \Lambda}$ be an indexed
family of objects in a category~$\cat C$. We say that the object $C$ together with an
indexed family $\bigl(\iota_\lambda\bigr)_{\lambda \in \Lambda}$ of morphisms
$\iota_\lambda\colon A_\lambda \sto C$ is a
 \index{coproduct!in a category}%
\df{coproduct} of the objects $A_\lambda$ if for every object $B$ and every indexed
family $\bigl(f_\lambda\bigr)_{\lambda \in \Lambda}$ of morphisms $f_\lambda\colon
A_\lambda \sto B$ there exists a unique map $g \colon C \sto B$ such that $f_\lambda = g
\circ \iota_\lambda$ for every $\lambda \in \Lambda$.  The usual notation for the
coproduct of the objects $A_\lambda$ is
 \index{<<@$\coprod A_\lambda$ (coproduct)}%
$\coprod_{\lambda \in \Lambda} A_\lambda$.
\end{defn}

\begin{exam} In the category $\cat{SET}$ the
 \index{coproduct!in $\cat{SET}$}%
 \index{SET@$\cat{SET}$!coproducts in}%
coproduct of an indexed family of sets exists and is the disjoint union of these sets.
\end{exam}

\begin{defn}  Let $S$ be a set and $V$ be a vector space. The
 \index{support!of a function}%
\df{support} of a function $f \colon S \sto V$ is $\{s \in S \colon f(s) \ne \vc 0\}$.
\end{defn}

\begin{exam}\label{X_dir_sum_VEC} If $\bigl(V_\lambda\bigr)_{\lambda \in \Lambda}$ is an indexed family of
vector spaces we make the Cartesian product into a vector space as in example~\ref{C015544}. The set of functions $f$
belonging to $\prod V_\lambda$ which have finite support (that is, which are nonzero only finitely often) is clearly
a subspace of $\prod V_\lambda$.  This subspace is the
 \index{direct!sum}%
 \index{sum!direct}%
\df{direct sum} of the spaces $V_\lambda$.  It is denoted by $\bigoplus_{\lambda \in
\Lambda} V_\lambda$.  This space together with the obvious injective maps (which are
linear) is a
 \index{coproduct!in $\cat{VEC}$}%
 \index{VEC@$\cat{VEC}$!coproducts in}%
 \index{vector!space!coproduct}%
coproduct in the category $\cat{VEC}$.
\end{exam}

\begin{exer}  What are the coproducts in the category $\cat{NLS_{\boldsymbol\infty}}$ ?
\end{exer}

\begin{exer} Identify the product and coproduct of two spaces $V$ and $W$ in the category
$\cat{NLS_{\vc 1}}$ of normed linear spaces and linear contractions.
\end{exer}













\section{Nets}\label{C0314}
Nets (sometimes called \emph{generalized sequences}) are useful for characterizing many
properties of general topological spaces.  In such spaces they are used in basically
the same way as sequences are used in metric spaces.

\begin{defn} A preordered set in which every pair of elements has an upper bound is a
 \index{directed!set}%
 \index{set!directed}%
\df{directed set}.
\end{defn}

\begin{exam}\label{C031414}  If $S$ is a set, then $\fin S$ is a directed set under
inclusion $\subseteq$.
\end{exam}

\begin{defn} Let $S$ be a set and $\Lambda$ be a directed set.  A mapping
$x\colon \Lambda \sto S$ is a
 \index{net}%
\df{net} in $S$ (or a \emph{net of members of $S$}). The value of $x$ at $\lambda \in
\Lambda$ is usually written $\sbsb x\lambda$ (rather than $x(\lambda)$) and the net $x$
itself is often denoted by $\bigl(\sbsb x\lambda\bigr)_{\lambda \in \Lambda}$ or just
$\bigl(\sbsb x\lambda\bigr)$.
\end{defn}

\begin{exam} The most obvious examples of nets are sequences.  These are functions whose domain is the (preordered) set $\N$ of natural numbers.
\end{exam}

\begin{defn} A net $x = \bigl(\sbsb x\lambda\bigr)$ is said to
 \index{eventually}%
\df{eventually} have some specified property if there exists $\lambda_0 \in \Lambda$ such that $\sbsb x\lambda$ has that property
whenever $\lambda \ge \lambda_0$; and it has the property
 \index{frequently}%
\df{frequently} if for every $\lambda_0 \in \Lambda$ there exists $\lambda \in \Lambda$ such that $\lambda \ge \lambda_0$
and $\sbsb x\lambda$ has the property.
\end{defn}

\begin{defn} A
 \index{neighborhood}%
\df{neighborhood} of a point in a topological space is any set which contains an open set containing the point.
\end{defn}

\begin{defn}  A net $x$ in a topological space $X$
 \index{convergence!of nets}%
 \index{net!convergence of a}%
\df{converges} to a point $a \in X$ if it is eventually in every neighborhood of~$a$. In
this case $a$ is the
 \index{limit!of a net}%
 \index{net!limit of a}%
\df{limit} of $x$ and we write
   \[ x_{{}_{\sst \lambda}} \to_{\lambda \in \Lambda}a \]
or just
 \index{<arrowto@$x_\lambda \sto a$ (convergence of nets)}%
$x_{{}_{\sst \lambda}} \sto a$.  When limits are unique we may also use the notation
$\lim_{\lambda \in \Lambda} \sbsb x\lambda = a$ or, more simply, $\lim \sbsb x\lambda =
a$.

The point $a$ in $X$ is a
 \index{cluster point!of a net}%
 \index{net!cluster point of a}%
\df{cluster point} of the net $x$ if $x$ is frequently in every neighborhood of~$a$.
\end{defn}

\begin{defn} Let $(X,\sfml T)$ be a topological space.  A subfamily $\sfml S \subseteq
\sfml T$ is a
 \index{subbase}%
\df{subbase} for the topology $\sfml T$ if the family of all finite intersections of
members of $\sfml S$ is a base for~$\sfml T$.
\end{defn}

\begin{exam}\label{C031429} Let $X$ be a topological space and $a \in X$. A
 \index{neighborhood!base for}%
 \index{base!for neighborhoods of a point}%
\df{base} for the family of neighborhoods of the point $a$ is a family $\sfml B_a$ of
neighborhoods of $a$ with the property that every neighborhood of $a$ contains at least
one member of~$\sfml B_a$.  In general, there are many choices for a neighborhood base at
a point. Once such a base is chosen we refer to its members as
 \index{basic!neighborhoods}%
 \index{neighborhood!basic}%
\emph{basic neighborhoods} of~$a$.

Let $\sfml B_a$ be a base for the family of neighborhoods at $a$.  Order $\sfml B_a$ by
containment (the reverse of inclusion); that is, for $U$, $V \in \sfml B_a$ set
  \[ U \preceq V \text{ if and only if } U \supseteq V\,. \]
This makes $\sfml B_a$ into a directed set. Now (using the axiom of choice) choose one
element $x_{{}_U}$ from each set $U \in \sfml B_a$.  Then $(\sbsb xU)$ is a net in $X$
and $\sbsb xU \sto a$.
\end{exam}

The next two propositions assure us that in order for a net to converge to a point $a$ in
a topological space it is sufficient that it be eventually in every basic (or even
subbasic) neighborhood of~$a$.

\begin{prop}\label{C031431} Let $\sfml B$ be a base for the topology on a topological
space~$X$ and $a$ be a point of~$X$. A net $(x_\lambda)$  converges to  $a$ if and only
if it is eventually in every neighborhood of~$a$ which belongs to~$\sfml B$.
\end{prop}

\begin{prop}\label{C031434} Let $\sfml S$ be a subbase for the topology on a topological
space~$X$ and $a$ be a point of~$X$.  A net $(x_\lambda)$ converges to $a$ if and only if
it is eventually in every neighborhood of~$a$ which belongs to~$\sfml S$.
\end{prop}

\begin{exam}\label{C031437} Let $J = [a,b]$ be a fixed interval in the real line,
$\vc x = (x_0,x_1,\dots,x_n)$ be an $(n+1)$-tuple of points of $J$, and $\vc t =
(t_1,\dots,t_n)$ be an $n$-tuple. The pair $(\vc x;\vc t)$ is a
 \index{partition!with selection}%
 \index{selection}%
\df{partition with selection} of the interval $J$ if:
 \begin{enumerate}
  \item $x_{k-1} < x_k$ for $1 \le k \le n$;
  \item $t_k \in [x_{k-1},x_k]$ for $1 \le k \le n$;
  \item $x_0 = a$; and
  \item $x_n = b$.
 \end{enumerate}
The idea is that $\vc x$ \emph{partitions} the interval into subintervals and $t_k$ is
the point \emph{selected} from the $k^{\text{th}}$ subinterval.  If $\vc x =
(x_0,x_1,\dots,x_n)$, then $\{\vc x\}$ denotes the \emph{set} $\{x_0,x_1,\dots,x_n\}$.
Let $P = (\vc x;\vc s)$ and $Q = (\vc y;\vc t)$ be partitions (with selections) of the
interval~$J$. We write $P \preccurlyeq Q$ and say that $Q$ is a
 \index{refinement}%
\df{refinement} of $P$ if $\{\vc x\} \subseteq \{\vc y\}$. Under the relation
$\preccurlyeq$ the family $\sfml P$ of partitions with selection on $J$ is a directed
(but \emph{not} partially ordered) set.

Now suppose $f$ is a bounded function on the interval $J$ and $P = (\vc x;\vc t)$ is a
partition of $J$ into $n$ subintervals. Let $\Delta x_k := x_k - x_{k-1}$ for $1 \le k
\le n$. Then define
   \[ S_f(P) := \sum_{k=1}^n f(t_k)\,\Delta x_k\,. \]
Each such sum $S_f(P)$ is a
 \index{Riemann!sum}%
 \index{sum!Riemann}%
\df{Riemann sum} of $f$ on~$J$. Notice that since $\sfml P$ is a directed set, $S_f$ is a
net of real numbers.  If the net $S_f$ of Riemann sums converges we say that the function
$f$ is
 \index{Riemann!integrable}%
 \index{integrable!Riemann}%
\df{Riemann integrable}.  The limit of the net $S_f$ (when it exists) is the
 \index{Riemann!integral}%
 \index{integral!Riemann}%
\df{Riemann integral} of $f$ over the interval~$J$ and is denoted by $\int_a^b f(x)\,dx$.
\end{exam}

\begin{exam}\label{exam_net_indiscrete} In a topological space $X$ with the
indiscrete topology (where the only open sets are $X$ and the empty set) every net in the space
converges to every point of the space.
\end{exam}

\begin{exam} In a topological space $X$ with the discrete topology (where every subset of $X$ is open) a net
in the space  converges if and only if it is eventually constant.
 \end{exam}

\begin{defn} A
 \index{Hausdorff}%
 \index{separation!axiom!Hausdorff}%
 \index{topological!space!Hausdorff}%
 \index{topology!Hausdorff}%
\df{Hausdorff} topological space is one in which every pair of distinct points can be
separated by open sets; that is, for every pair $x$ and $y$ of distinct points in the space there exist
open neighborhoods of $x$ and $y$ that are disjoint.
\end{defn}

As example~\ref{exam_net_indiscrete} shows nets in general topological spaces need not have unique
limits. A space does not require very restrictive assumptions however to make this
pathology go away. For limits to be unique it is sufficient to require that a space be
Hausdorff. Interestingly, it turns out that this condition is also necessary.

\begin{prop}\label{C031441} If $X$ is a topological space, then the following are equivalent:
 \begin{enumerate}
  \item $X$ is Hausdorff.
  \item No net in $X$ can converge to more than one point.
  \item The diagonal in $X \times X$ is closed.
 \end{enumerate}
\end{prop}
(The
 \index{diagonal}%
\df{diagonal} of a set $S$ is $\{(s,s) \colon s \in S\} \subseteq S \times S$.)

\begin{exam} Recall from beginning calculus that the $n^{\text{th}}$
 \index{partial!sum}%
 \index{sum!partial}%
 \index{infinite series!partial sum of an}%
 \index{series!infinite}%
\df{partial sum} of an infinite series $\sum_{k=1}^\infty x_k$ is defined to be $s_n =
\sum_{k=1}^n x_k$.  The series is said to
 \index{convergence!of infinite series}%
 \index{infinite series!convergence of an}%
\df{converge} if the sequence $(s_n)$ of partial sums converges and, if the series
converges, its
 \index{sum!of an infinite series}%
 \index{infinite series!sum of an}%
 \index{series!sum of a}%
\emph{sum} is the limit of the sequence $(s_n)$ of partial sums.  Thus, for example, the
infinite series $\sum_{k=1}^\infty 2^{-k}$ converges and its sum is~$1$.
\end{exam}

The idea of \emph{summing} an infinite series depends heavily on the fact that the
partial sums of the series are linearly ordered by inclusion. The ``partial sums'' of a
net of numbers need not be partially ordered, so we need a new notion of
``summation'', sometimes referred to as \emph{unordered summation}.

\begin{notn} If $A$ is a set we denote
 \index{fin@$\fin A$ (family of finite subsets of a set $A$)}%
by $\fin A$ the family of all finite subsets of~$A$. (Note that this is a directed set under the
relation~$\subseteq$.)
\end{notn}

\begin{defn}\label{def_summable_R} Let $A \subseteq \R$.  For every $F \in \fin A$ define $S_F$ to be the sum of
the numbers in~$F$, that is, $S_F = \sum F$.  Then $S = \bigl(S_F\bigr)_{F \in \fin A}$
is a net in~$\R$.  If this net converges, the set $A$ of numbers is said to be
 \index{summable!set of real numbers}%
\df{summable}; the limit of the net is the
 \index{sum!of a summable set}%
\df{sum} of $A$ and is denoted by $\sum A$.
\end{defn}

\begin{defn}  A net $(x_\lambda)$ of real numbers is
 \index{increasing!net in $\R$}%
 \index{net!increasing}%
\df{increasing} if $\lambda \le \mu$ implies $x_\lambda \le x_\mu$.  A net of real (or complex)
numbers is
 \index{bounded!net in $\K$}%
 \index{net!bounded}%
\df{bounded} if its range is.
\end{defn}

\begin{exam}\label{C031454} Let $f(n) = 2^{-n}$ for every $n \in \N$, and let $x \colon
\fin(\N) \sto \R$ be defined by $x(A) = \sum_{n \in A}f(n)$.
 \begin{enumerate}
  \item The net $x$ is increasing.
  \item The net $x$ is bounded.
  \item The net $x$ converges to 1.
  \item The set of numbers $\{\frac12,\frac14,\frac18, \frac1{16},\dots\}$ is summable and
its sum is~$1$.
  \end{enumerate}
\end{exam}

\begin{exam} In a metric space every convergent sequence is bounded.  Give an example to show
that a convergent net in a metric space (even in~$\R$) need not be bounded.
\end{exam}

It is familiar from beginning calculus that bounded increasing sequences in $\R$
converge. The preceding example is a special case of a more general observation: bounded
increasing nets in $\R$ converge.

\begin{prop}\label{C031461} Every bounded increasing net
$\bigl(x_{{}_{\sst \lambda}}\bigr)_{\lambda \in \Lambda}$ of real numbers converges and
  \[ \lim x_{{}_{\sst \lambda}} = \sup\{x_{{}_{\sst \lambda}}\colon \lambda \in \Lambda\}\,. \]
\end{prop}

\begin{exam} Let $(x_k)$ be a sequence of distinct positive real numbers and $A = \{x_k \colon k \in \N\}$.
Then $\sum A$ (as defined above) is equal to the sum of the series $\sum_{k=1}^\infty
x_k$.
\end{exam}

\begin{exam} The set $\bigl\{\frac1k \colon k \in \N \bigr\}$ is not summable and the infinite
series $\sum_{k=1}^\infty \frac1k$ does not converge.
\end{exam}

\begin{exam} The set $\bigl\{(-1)^k \frac1k\colon k \in \N \bigr\}$ is not summable but the
infinite series $\sum_{k=1}^\infty (-1)^k \frac1k$ does converge.
\end{exam}

\begin{prop} Every summable subset of real numbers is countable.
\end{prop}

\begin{proof}[\emph{Hint for proof.}] If $A$ is a summable set of real numbers and $n \in \N$,
how many members of $A$ can be larger than~$1/n$?  \ns
\end{proof}

In proposition~\ref{C031441} we have already seen one example of a way in which nets
characterize a topological property: a space is Hausdorff if and only if limits of nets
in the space are unique (when they exist).  Here is another topological
property---continuity at a point---that can be conveniently characterized in terms of
nets.

\begin{prop}\label{C031471}  Let $X$ and $Y$ be topological spaces.  A function
 \index{continuity!characterization by nets}%
$f\colon X \sto Y$ is continuous at a point $a$ in $X$ if and only if $f\bigl(\sbsb x\lambda\bigr) \sto f(a)$ in $Y$
whenever $\bigl(\sbsb x\lambda\bigr)$ is a net converging to $a$ in~$X$.
\end{prop}

In metric spaces we characterize closures and interiors of sets in terms of sequences. For
general topological spaces we must replace sequences by nets.

In defining the terms ``interior'' and ``closure'' we have choices similar to the ones we had in defining ``convex hull'' in
\ref{smplx005def}.  There is an 'abstract' definition and there is a `constructive' one.  You may find it helpful to recall
proposition~\ref{smplx006} and the paragraph preceding it.

\begin{defn}\label{def_interior} The
 \index{interior}%
\df{interior} of a set $A$ in a topological space $X$ is the largest open subset of $X$ contained in~$A$; that is, the
union of all open subsets of $X$ contained in~$A$. The interior of $A$
 \index{<unopur@$\intr A$ (interior of~$A$)}%
is denoted by $\intr A$.
\end{defn}

\begin{prop} The preceding definition makes sense.  Furthermore, a point $a$ in a topological space $X$ is in the interior of a subset
 \index{interior!characterization of by nets}%
 \index{characterization!by nets!of interior}%
 \index{nets!characterization!of interior by}%
$A$ of $X$ if and only if every net in $X$ that converges to $a$ is eventually in~$A$.
\end{prop}

\begin{proof}[\emph{Hint for proof}]  One direction is easy. In the other use proposition~\ref{C031431} and the sort
of net constructed in example~\ref{C031429}.   \ns
\end{proof}

\begin{defn}\label{def_closure} The
 \index{closure}%
\df{closure} of a set $A$ in a topological space $X$ is the smallest closed subset of $X$ containing~$A$; that is, the
intersection of all closed subsets of $X$ which contain~$A$.  The closure of $A$
 \index{<unopu@$\clo A$ (closure of~$A$)}%
is denoted by~$\clo A$.
\end{defn}

\begin{prop}\label{C031481} The preceding definition makes sense.  Furthermore, a point $b$ in a topological space $X$ belongs to the
 \index{closure!characterization of by nets}%
 \index{charactrization!by nets!of closure}%
 \index{nets!characterization!of closure by}%
closure of a subset $A$ of~$X$ if and only if some net in $A$ converges to~$b$.
\end{prop}

\begin{cor} A subset $A$ of a topological space is closed if and only if the limit of every convergent net in $A$ belongs to~$A$.
\end{cor}


















\section{The Hahn-Banach Theorems}

It should be said at the beginning that referring to ``the'' \emph{Hahn-Banach theorem} is something of a misnomer.
A writer who claims to be using the \emph{Hahn-Banach theorem} may be using any of a dozen or so versions of the
theorem or its corollaries.  What do the various versions have in common?  Basically they assure us that normed
linear spaces have, in general, a rich enough supply of bounded linear functionals to make their dual spaces useful
tools in studying the spaces themselves. That is, they guarantee an interesting and productive duality theory.
Below we consider four versions of the `theorem' and three of its corollaries.  We will succumb to usual practice:
when in the sequel we make use of any of these results, we will say we are using `the' \emph{Hahn-Banach theorem}.

As is often the case in mathematics the attribution of these results to Hahn and Banach is a bit peculiar.  It seems
that the basic insights were due originally to Eduard Helly.  For an interesting history of the `Hahn-Banach' results
see~\cite{Hochstadt:1980}.

Incidentally, in the name ``Banach'', the ``a'''s are pronounced as in \emph{car} and the ``ch'' as in \emph{chaos};
and the accent is on the first syllable (BAH-nakh).

\begin{defn} A real valued function $f$ on a real vector space $V$ is a
 \index{sublinear}%
\df{sublinear functional} if $f(x + y) \le f(x) + f(y)$ and $f(\alpha x) = \alpha f(x)$
for all $x$, $y \in V$ and $\alpha \ge 0$.
\end{defn}

\begin{exam} A norm on a real vector space is a sublinear functional.
\end{exam}

Our first version of the \emph{Hahn-Banach theorem} is pure linear algebra and applies only to \emph{real} vector spaces.
It allows us to extend certain linear functionals on such spaces from subspaces to the whole space.

\begin{thm}[Hahn-Banach theorem I]\label{HBTI} Let $M$ be a subspace of a real vector space $V$ and $p$
 \index{Hahn-Banach theorem}%
be a sublinear functional on~$V$. If $f$ is a linear functional on $M$ such that $f \le p$ on $M$, then
$f$ has an extension $g$ to all of $V$ such that $g \le p$ on~$V$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] In this theorem the notation $h \le p$ means that $\dom h \subseteq \dom p = V$
and that $h(x) \le p(x)$ for all $x \in \dom h$.  In this case we say that $h$ is
 \index{dominate}%
\df{dominated} by~$p$.  We write
 \index{<binrelle@$f \preccurlyeq g$ ($g$ is an extension of~$f$)}%
$f \preccurlyeq g$ if $g$ is an extension of~$f$ (that is,
if $f(x) = g(x)$ for all $x \in \dom f$).

Let $\fml S$ be the family of all real valued linear functionals on subspaces of $V$ which are extensions of $f$ and are
dominated by~$p$. The family $\fml S$ is partially ordered by $\preccurlyeq$.  Use \emph{Zorn's lemma} to produce a
maximal element $g$ of~$\fml S$.  The proof is finished if you can show that $\dom{g} = V$.

To this end, suppose to the contrary that there is a vector $c \in V$ which does not belong to~$D = \dom{g}$.  Prove first that
   \begin{equation}\label{eq_HBTI}
      g(y) - p(y - c) \le - g(x) + p(x + c)
   \end{equation}
for all $x$, $y \in D$.  Conclude that there is a number $\gamma$ which lies between the supremum of all the numbers
on the left side of~\eqref{eq_HBTI} and the infimum of those on the right.

Let $E$ be the span of $D \cup \{c\}$.  Define $h$ on $E$ by $h(x + \alpha c) = g(x) + \alpha\gamma$ for $x \in D$ and
$\alpha \in \R$.  Now show $h \in \fml S$.  (Take $\alpha > 0$ and work separately with $h(x + \alpha c)$ and $h(x - \alpha c)$.) \ns
\end{proof}

The next theorem is the version of theorem~\ref{HBTI} for spaces with complex scalars.  It is sometimes referred to as the
 \index{Bohnenblust-Sobczyk-Suhomlinov theorem}%
\emph{Bohnenblust-Sobczyk-Suhomlinov theorem}.

\begin{thm}[Hahn-Banach theorem II]\label{HBTII} Let $V$ be a complex vector space, $p$ be a seminorm on~$V$, and $M$ be a subspace of~$V$.
If $f\colon M \sto \C$ is a linear functional such that $\abs f \le p$, then $f$ has an extension $g$ which is a linear functional
on all of~$V$ satisfying $\abs{g} \le p$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] This result is essentially a straightforward corollary to the real version of the
\emph{Hahn-Banach theorem} given above in~\ref{HBTI}.  Let $V_{\R}$ be the real vector space associated with~$V$ (same vectors
but using only $\R$ as the scalar field).  Notice that if $u$ is the real part of $f$ (that is,
 \index{R@$\Re(z)$ (the real part of $z \in \C$)}%
$u(x) = \Re(f(x))$ for each $x \in M$), then $u$ is an $\R$-linear functional on $M$ (that is, $f(x + y) = f(x) + f(y)$ and $f(\alpha x) = \alpha f(x)$
for all $x$, $y \in M$ and all $\alpha \in \R$) and $f(x) = u(x) - i\,u(ix)$.  And conversely, if $u$ is a $\R$-linear functional on some vector space,
then $f(x) := u(x) - i\,u(ix)$ defines a (complex valued) linear functional on that space.  Notice also that if $u$ is the real part of~$f$,
then $\abs{u(x)} \le p$ for all $x$ if and only if $\abs{f(x)} \le p$ for all~$x$.  With these observations in mind just apply~\ref{HBTI}
to the real part of $f$ on~$V_\R$.  \ns
\end{proof}

Next is perhaps the most often used version of the \emph{Hahn-Banach theorem}.

\begin{thm}[Hahn-Banach theorem III]\label{HBTIII} Let $M$ be a vector subspace of a (real or
 \index{Hahn-Banach theorem}%
complex) normed linear space $V$. Then every continuous linear functional $f$ on $M$ has
an extension $g$ to all of $V$ such that $\norm{g} = \norm f$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Consider the function $p\colon V \sto \R \colon x \mapsto \norm f \norm x$.  \ns
\end{proof}

It is sometimes useful, when proving a theorem, to consider the possibility of more ``natural'' or even  more ``obvious'' proofs.
On occasion one is rewarded by an enlightening simplification and clarification of complex material.  And sometimes one is fooled.

\begin{exer} Your buddy Fred R. Dimm thinks he has discovered a really simple direct proof of
the version of the Hahn-Banach theorem given in proposition~\ref{HBTIII}: A continuous
linear functional $f$ defined on a (linear) subspace $M$ of a normed linear space~$V$ can
be extended to a continuous linear functional $g$ on all of $V$ without increasing
its norm. He says that all one needs to do is let $N$ be any vector space complement
of~$M$ and let $B$ be a (Hamel) basis for $N$.  Define $g(e) = 0$ for every vector $e
\in B$ and $g = f$ on~$M$. Then extend $g$ by linearity to all of~$V$. Surely,
Fred says, $g$ is linear by construction and its norm has not increased since we
have made it zero where it was not previously defined.  Show Fred the error of his ways.
\end{exer}

\begin{thm}[Hahn-Banach theorem IV]\label{HBTIV} Let $V$ be a (real or complex) normed
 \index{Hahn-Banach theorem}%
linear space, $M$ be a vector subspace of $V$, and suppose that $y \in V$ is such that
 \index{dist@$\dist(y,M)$ (the distance between $y$ and $M$)}%
$d := \dist(y,M) > 0$.  Then there exists a functional $g \in V^*$ such that $g^{\sto}(M) = \{0\}$,
$g(y) = d$, and $\norm g = 1$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Let $N := \spn{(M \cup \{y\})}$ and define
$f\colon N \sto \K \colon x + \alpha y \mapsto \alpha d$ ($x \in M$, $\alpha \in \K$). \ns
\end{proof}

The most useful special case of the preceding theorem is when $M = \{\vc 0\}$ and $y \ne \vc 0$.
This tells us that the only way for $f(x)$ to vanish for every $f \in V^*$ is for $x$ to
be the zero vector.

\begin{cor}\label{HBCorI} Let $V$ be a normed linear space and $x \in V$.  If $f(x) = 0$ for
every $f \in V^*$, then $x = \vc 0$.
\end{cor}

\begin{defn} A family $\fml G(S)$ of scalar valued functions on a set~$S$
 \index{separation!of pairs of points by a family of functions}%
\df{separates} points of $S$ if for every pair $x$ and $y$ of distinct points in $S$ there exists
a function $f \in \fml G(S)$ such that $f(x) \ne f(y)$.
\end{defn}

\begin{cor}\label{HBCorII} If $V$ is a normed linear space, then $V^*$ separates points of~$V$.
\end{cor}

\begin{cor}\label{HBCorIII} Let $V$ be a normed linear space and $x \in V$.  Then there exists
$f \in V^*$ such that $\norm f = 1$ and $f(x) = \norm x$.
\end{cor}

\begin{prop} Let $V$ be a normed linear space, $\{x_1,\dots,x_n\}$ be a linearly independent
subset of $V$, and $\alpha_1, \dots, \alpha_n$ be scalars. Then there exists an $f$ in
$V^*$ such that $f(x_k) = \alpha_k$ for $1\le k \le n$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For $x = \sum_{k=1}^n \beta_k x_k$ let $g(x) =
\sum_{k=1}^n \alpha_k \beta_k$.  \ns
\end{proof}

\begin{prop}\label{C067131} Let $V$ be a normed linear space and $\vc 0 \ne x \in V$.  Then
   \[ \norm x = \max\{\abs{f(x)}\colon f \in V^* \text{ and } \norm f \le 1\}\,. \]
\end{prop}

The use of $\max$ instead of $\sup$ in the preceding proposition is intentional.  The proposition claims that there
exists an $f$ in the closed unit ball of $V^*$ such that $f(x) = \norm x$.

\begin{defn} A subset $A$ of a topological space $X$ is
 \index{dense}%
\df{dense} in $X$ if $\clo A = X$.  The space $X$ is
 \index{separable}%
\df{separable} if it contains a countable dense subset.
\end{defn}

\begin{exam} The space $\K^n$ is separable under its usual topology; that is
 \index{usual!topology!for $\K^n$}%
 \index{topology!on $\K^n$}%
 \index{K@$\K^n$!usual topology on}%
the topology it inherits from its usual norm. (See \ref{C023125}, \ref{0001503}, and \ref{xmpl_top_metric}.)
\end{exam}

\begin{exam} Given the discrete topology the real line $\R$ is not separable.
\end{exam}

\begin{prop} Let $V$ be a normed linear space.  If $V^*$ is separable, then so is~$V$.
\end{prop}






















\endinput
\chapter*{PREFACE}

Paul Halmos famously remarked in his beautiful \emph{Hilbert Space Problem
Book}~\cite{Halmos:1982} that ``The only way to learn mathematics is to do mathematics.''
Halmos is certainly not alone in this belief.  The current set of notes is an
activity-oriented companion to the study of linear functional analysis and operator algebras. It is intended as a
pedagogical companion for the beginner, an introduction to some of the main ideas in this area of analysis, a
compendium of problems I think are useful in learning the subject, and an annotated reading/reference list.

The great majority of the results in beginning functional analysis are straightforward and can
be verified by the thoughtful student. Indeed, that is the main point of these notes---to
convince the beginner that the subject is accessible.  In the material that follows there
are numerous indicators that suggest activity on the part of the reader: words such as
``proposition'', ``example'', ``exercise'', and ``corollary'', if not followed by a proof
or a reference to a proof, are invitations to verify the assertions made.  Of course, there
are a few theorems where, in my opinion, the time and effort which go into proving them is
greater than the benefits derived.  In such cases, when I have no improvements
to offer to the standard proofs, instead of repeating them I give references.

The material in these notes reflects the substance of portions of two year-long graduate courses covering some elementary topics in
\emph{(linear) functional analysis} and \emph{operator algebras}, which I taught numerous times at Portland State University.
During these courses I asked students to choose, in addition to these notes, other sources of information, either printed texts
or online documents, which suited their individual learning styles (and budgets).  The prerequisites for working through these
notes are quite modest; even the most casual introduction to \emph{linear algebra}, \emph{real analysis}, and \emph{topology}
should suffice.  That is to say, terms such as \emph{vector space}, \emph{linear map}, \emph{limit}, \emph{Lebesgue measure} and
\emph{integral}, \emph{open set}, \emph{compact set}, and \emph{continuous function} should sound familiar.

Since \emph{linear functional analysis} can be regarded, in some sense at least, as `infinite dimensional linear
algebra', chapter one of these notes is devoted to a (rather condensed) review of some of the essential ideas that, in my fantasy
world, students take away from their first course in linear algebra.  Three pivotal insights that are developed in the first chapter are:
\begin{enumerate}
  \item a diagonalizable operator on a finite dimensional vector space is a linear combination of projections;
  \item an operator on a finite dimensional inner product space is unitarily diagonalizable if and only if it is normal; and
  \item two normal operators on a finite dimensional inner product space are unitarily equivalent if and only if they have the same
eigenvalues, each with the same multiplicity.
\end{enumerate}
I regard these results as of fundamental importance because of the remarkable insights that have flowed from the attempts made over the last century
by operator theorists to bring them into the realm of \emph{functional analysis}, that is, to generalize them to the infinite dimensional
case.  One consequence of this fertile and incredibly complicated exploration has been the discovery of many astonishing, intimate,
and complex relationships that exist between \emph{operator theory} and such areas as \emph{complex variables}, \emph{algebraic topology},
and \emph{homological algebra}.  I attempt to retrace a few of these steps in later portions of these notes.  The lectures on which these
notes are based culminated in the amazing results of Brown, Douglas, and Fillmore first published in 1973 (see~\cite{BrownDF:1973}). Much of
the background required for the development of this gorgeous material is, even now, difficult for most graduate students to find in a
reasonably digestible form.  Unfortunately, the business of transfinite arm-waving that works passably well in the lecture room translates
very poorly indeed to paper or electronic media.  As a result, these notes are in a sadly unfinished state.  I'm working on them; and
conceivably someone will finish what I do not.

If the material on linear algebra in the first chapter seems too condensed, a somewhat more leisurely and thorough account can be found in
my online notes~\cite{Erdman:2010}.  Similarly, supplementary background material in advanced calculous, metric and topological spaces,
Lebesgue integrals, and the like, can be found in very many places, including~\cite{Erdman:2005} and~\cite{Erdman:2007}.

There are of course a number of advantages and disadvantages in consigning a document to
electronic life. One advantage is the rapidity with which links implement
cross-references.  Hunting about in a book for \emph{lemma 3.14.8} can be time-consuming
(especially when an author engages in the entirely logical but utterly infuriating
practice of numbering lemmas, propositions, theorems, corollaries, and so on,
separately).  A perhaps more substantial advantage is the ability to correct errors, add
missing bits, clarify opaque arguments, and remedy infelicities of style in a timely
fashion.  The correlative disadvantage is that a reader returning to the web page after a
short time may find everything (pages, definitions, theorems, sections) numbered
differently.  ({\LaTeX} is an amazing tool.) I will change the date on the title page to
inform the reader of the date of the last nontrivial update (that is, one that affects
numbers or cross-references).

The most serious disadvantage of electronic life is impermanence.  In most cases when a
web page vanishes so, for all practical purposes, does the information it contains.  For
this reason (and the fact that I want this material to be freely available to anyone who
wants it) I am making use of a ``Share Alike'' license from \emph{Creative Commons}. It
is my hope that anyone who finds this material useful will correct what is wrong, add
what is missing, and improve what is clumsy.  I will include the {\LaTeX} source files
on my website so that using the document, or parts of it, does not involve endless retyping.
Concerning the text itself, please send corrections, suggestions, complaints, and other
comments to the author at
   \[ \textrm{erdman@pdx.edu} \]


\vfill\eject


\section*{Greek Letters}

 \index{Greek letters}%
\[ \table{}
   Upper case  !  Lower case                   !  English name (approximate pronunciation)  \rr
   A           !  $\alpha$                     !  Alpha (AL-fuh)                            \r
   B           !  $\beta$                      !  Beta (BAY-tuh)                            \r
   $\Gamma$    !  $\gamma$                     !  Gamma (GAM-uh)                            \r
   $\Delta$    !  $\delta$                     !  Delta (DEL-tuh)                           \r
   $E$         !  $\epsilon$ or $\varepsilon$  !  Epsilon (EPP-suh-lon)                     \r
   Z           !  $\zeta$                      !  Zeta (ZAY-tuh)                            \r
   H           !  $\eta$                       !  Eta  (AY-tuh)                             \r
   $\Theta$    !  $\theta$                     !  Theta (THAY-tuh)                          \r
   I           !  $\iota$                       !  Iota  (eye-OH-tuh)                        \r
   K           !  $\kappa$                     !  Kappa  (KAP-uh)                           \r
   $\Lambda$   !  $\lambda$                    !  Lambda  (LAM-duh)                         \r
   M           !  $\mu$                        !  Mu  (MYOO)                                \r
   N           !  $\nu$                        !  Nu  (NOO)                                 \r
   $\Xi$       !  $\xi$                        !  Xi  (KSEE)                                \r
   O           !  o                            !  Omicron  (OHM-ih-kron)                    \r
   $\Pi$       !  $\pi$                        !  Pi  (PIE)                                 \r
   P           !  $\rho$                       !  Rho  (ROH)                                \r
   $\Sigma$    !  $\sigma$                     !  Sigma  (SIG-muh)                          \r
   T           !  $\tau$                       !  Tau  (TAU)                                \r
   Y           !  $\upsilon$                   !  Upsilon  (OOP-suh-lon)                    \r
   $\Phi$      !  $\phi$                       !  Phi  (FEE or FAHY)                        \r
   X           !  $\chi$                       !  Chi  (KHAY)                               \r
   $\Psi$      !  $\psi$                       !  Psi  (PSEE or PSAHY)                      \r
   $\Omega$    !  $\omega$                     !  Omega  (oh-MAY-guh)                    \hfil
  \caption{} \]








\vfill\eject







\section*{Fraktur Fonts}

In these notes Fraktur fonts are used (most often for families of sets and families of operators).  Below are the Roman equivalents for each letter.
When writing longhand or presenting material on a blackboard it is usually best to substitute script English letters.
 \index{Fraktur fonts}

\[ \table{}
                Fraktur  !  Fraktur           !   Roman           \r
              Upper case !  Lower case        !  Lower Case       \rr
         $\mathfrak A$   !  $\mathfrak a$     !     a             \r
         $\mathfrak B$   !  $\mathfrak b$     !     b             \r
         $\mathfrak C$   !  $\mathfrak c$     !     c             \r

         $\mathfrak D$   !  $\mathfrak d$     !     d             \r
         $\mathfrak E$   !  $\mathfrak e$     !     e             \r
         $\mathfrak F$   !  $\mathfrak f$     !     f             \r
         $\mathfrak G$   !  $\mathfrak g$     !     g             \r
         $\mathfrak H$   !  $\mathfrak h$     !     h             \r
         $\mathfrak I$   !  $\mathfrak i$     !     i             \r
         $\mathfrak J$   !  $\mathfrak j$     !     j             \r
         $\mathfrak K$   !  $\mathfrak k$     !     k             \r
         $\mathfrak L$   !  $\mathfrak l$     !     l             \r
         $\mathfrak M$   !  $\mathfrak m$     !     m             \r
         $\mathfrak N$   !  $\mathfrak n$     !     n             \r
         $\mathfrak O$   !  $\mathfrak o$     !     o             \r
         $\mathfrak P$   !  $\mathfrak p$     !     p             \r
         $\mathfrak Q$   !  $\mathfrak q$     !     q             \r
         $\mathfrak R$   !  $\mathfrak r$     !     r             \r
         $\mathfrak S$   !  $\mathfrak s$     !     s             \r
         $\mathfrak T$   !  $\mathfrak t$     !     t             \r
         $\mathfrak U$   !  $\mathfrak u$     !     u             \r
         $\mathfrak V$   !  $\mathfrak v$     !     v             \r
         $\mathfrak W$   !  $\mathfrak w$     !     w             \r
         $\mathfrak X$   !  $\mathfrak x$     !     x             \r
         $\mathfrak Y$   !  $\mathfrak y$     !     y             \r
         $\mathfrak Z$   !  $\mathfrak z$     !     z           \hfill
 \caption{}  \]


  \vfill\eject


\section*{Notation for Sets of Numbers}\label{C0009}
Here is a list of fairly standard notations for some sets of numbers which occur
frequently in these notes:
 \index{real!line!special subsets of}%
 \index{numbers!special sets of}%
 \index{C@$\C$ (set of complex numbers)}%
 \index{R@$\R$ (set of real numbers)}%
 \index{R@$\R^n$!is the set of $n$-tuples of real numbers}%
 \index{Q@$\Q$ (set of rational numbers)}%
 \index{Z@$\Z$ (set of integers)}%
 \index{N@$\N$ (set of natural numbers)}%
 \index{N@$\N_n$ (first $n$ natural numbers)}%
 \index{D@$\Di$ (open unit disk)}%
 \index{T@$\T$ (unit circle)}%
 \index{S@$\Sp^1$ (unit circle)}%
\begin{align*}
 &\C \text{ is the set of complex numbers} \\
 &\R \text{ is the set of real numbers} \\
 &\R^n \text{ is the set of all $n$-tuples $(r_1,r_2,\dots,r_n)$ of real numbers} \\
 &\R^+ = \{x \in \R\colon x \ge 0\}, \text{ the positive real numbers} \\
 &\Q \text{ is the set of rational numbers} \\
 &\Q^+ = \{x \in \Q\colon x \ge 0\}, \text{ the positive rational numbers} \\
 &\Z \text{ is the set of integers} \\
 &\Z^+ = \{0,1,2,\dots\}, \text{ the positive integers} \\
 &\N = \{1,2,3,\dots\}\text{, the set of natural numbers} \\
 &\N_n = \{1,2,3,\dots,n\}\text{ the first $n$ natural numbers} \\
 &[a,b] = \{x \in \R\colon a \le x \le b\} \\
 &[a,b) = \{x \in \R\colon a \le x < b\} \\
 &(a,b] = \{x \in \R\colon a < x \le b\} \\
 &(a,b) = \{x \in \R\colon a < x < b\} \\
 &[a,\infty) = \{x \in \R\colon a \le x\} \\
 &(a,\infty) = \{x \in \R\colon a < x\} \\
 &(-\infty,b] = \{x \in \R\colon x \le b\} \\
 &(-\infty,b) = \{x \in \R\colon  x < b\} \\
\index{open!unit disc}%
 &\Di = \{(x,y) \in \R^2\colon x^2 + y^2 < 1\},\text{ the open unit disc} \\
 &\T = \Sp^1 = \{(x,y) \in \R^2\colon x^2 + y^2 = 1\},\text{ the unit circle} \\
\end{align*}



\vfill\eject










\section*{Notation for Functions}
Functions are familiar from beginning calculus. Informally, a \emph{function} consists of a pair
of sets and a ``rule'' which associates with each member of the first set (the
\emph{domain}) one and only one member of the second (the \emph{codomain}). While this
informal ``definition'' is certainly adequate for most purposes and seldom leads to any
misunderstanding, it is nevertheless sometimes useful to have a more precise formulation.
This is accomplished by defining a function in terms of a special type of relation between two
sets.

A
 \index{function}%
\df{function} $f$ is an ordered triple $(S,T,G)$ where $S$ and $T$ are sets and $G$ is a
subset of $S \times T$ satisfying:
 \begin{enumerate}
   \item for each $s \in S$ there is a $t \in T$ such that $(s,t) \in
G$, and
   \item if $(s,t_1)$ and $(s,t_2)$ belong to G, then $t_1 = t_2$.
 \end{enumerate}
In this situation we say that $f$ is a \emph{function from} $S$ \emph{into} $T$ (or that
$f$ \emph{maps} $S$ \emph{into} $T$) and write $f\colon S \sto T$. The set $S$ is the
 \index{domain}%
\df{domain} (or the
 \index{input space}%
 \index{space!input}%
\df{input space}) of~$f$.  The set $T$ is the
 \index{codomain}%
\df{codomain} (or
 \index{target space}%
 \index{space!target}%
\df{target space}, or the
 \index{output space}%
 \index{space!output}%
\df{output space}) of~$f$.  And the relation $G$ is the
 \index{graph}%
\df{graph} of~$f$.  In order to avoid explicit reference to the graph $G$ it is usual to
replace the expression ``$(x,y) \in G\,$'' by ``$y = f(x)$''; the element $f(x)$ is the
 \index{image!of a point}%
\df{image} of $x$ under~$f$. In these notes (but not everywhere!) the words
 \index{transformation}%
 \index{conventions!function = map = transformation}%
``transformation'',
 \index{map}%
``map'', and
 \index{mapping}%
``mapping'' are synonymous with ``function''.  The domain of $f$ is denoted
 \index{domain@$\dom f$ (domain of a function $f$)}%
by~$\dom f$.

If $S$ and $T$ are sets, then the family of all functions with
domain $S$ and codomain $T$ is denoted
 \index{F@$\fml F(S,T)$ (set of functions from $S$ to~$T$)}%
by~$\fml F(S,T)$.

If $f \colon S \sto T$ and $A\subseteq S$, then the
 %\index{<@$\bigl.f\bigr"|_A$ (restriction of $f$ to $A$)}%
 \index{<@$f\mid_{{}_A}$ (restriction of $f$ to $A$)}%
 \index{restriction}%
\df{restriction} of $f$ to $A$, denoted by $\bigl.f\bigr|_A$, is the
mapping from $A$ into $T$ whose value at each $x$ in $A$ is $f(x)$.

If $f\colon S \sto T$ and $A \subseteq S$, then $f^{\sto} (A)$, the
 \index{<arrowto@$f^{\sto}(A)$ (image of set $A$ under function $f$)}%
 \index{image!of a set}%
\df{image} of $A$ under $f$, is $\{f(x)\colon x \in A\}$. It is common practice to write
$f(A)$ for $f^{\sto}(A)$.  The set $f^{\sto}(S)$ is the
 \index{range}%
 \index{range@$\ran f$ (range of a function $f$)}%
\df{range} (or
 \index{image!of a function}%
\df{image}) of $f$; usually we write $\ran f$ for $f^{\sto}(S)$.

If $f\colon S \sto T$ and $B \subseteq T$, then
$f^\gets (B)$, the
 \index{<arrowgets@$f^{\gets}(B)$ (inverse image of set $B$ under function $f$)}%
 \index{inverse!image!of a set}%
 \index{image!inverse!of a set}%
\df{inverse image} of $B$ under~$f$, is $\{x \in S\colon f(x) \in B\}$. In many texts
$f^{\gets}(B)$ is denoted by $f^{-1}(B)$. This notation may cause confusion by suggesting
that the function $f$ has an inverse when, in fact, it may not.

A function $f\colon S \sto T$ is
 \index{injective}%
\df{injective} (or
 \index{one-to-one}%
\df{one-to-one}) if $f(x) = f(y)$ implies $x = y$ for all $x$, $y \in S$.  It is
 \index{surjective}%
\df{surjective} (or
 \index{onto}%
\df{onto}) in for every $t \in T$ there exists $x \in S$ such that $f(x) = t$. And it is
 \index{bijective}%
\df{bijective} (or
 \index{one-to-one correspondence}%
 \index{correspondence!one-to-one}%
\df{a one-to-one correspondence} if it is both injective and surjective.

It is frequently useful to think of functions as arrows in diagrams. For example, the
situation $j\colon R \sto U$, $f\colon R \sto S$, $k\colon S \sto T$, $h\colon U \sto T$
may be represented by the following
 \index{diagram}%
diagram.
  \[ \xy
       \square[R`U`S`T;j`f`h`k]
  \endxy \]
The diagram is said to
 \index{commute}%
\df{commute} (or to be a
 \index{commutative!diagram}%
 \index{diagram!commutative}%
\df{commutative diagram}) if $h \circ j = k \circ f$.

Diagrams need not be rectangular. For instance,
 \[ \xy
     \btriangle[R`S`T;f`g`k]
 \endxy \]
is a commutative diagram if $g = k \circ f$.


\vfill\eject












  \endinput

\chapter{SOME SPECTRAL THEORY}\label{spectrum}

In this chapter we will, for the first time, assume, unless otherwise stated, that vector
spaces and algebras have complex, rather than real, scalars. A good part of the reason
 \index{conventions!in chapter~\ref{spectrum} scalars are complex}%
for this is the necessity of using \emph{Liouville's theorem}~\ref{C073131} to prove
proposition~\ref{C073134}.

\section{The Spectrum}

\begin{defn} An element $a$ of a unital algebra $A$ is
 \index{invertible!left!in an algebra}%
\df{left invertible} if there exists an element $a_l$ in $A$ (called a \df{left inverse}
of~$a$) such that $a_l a = \vc 1$ and is
 \index{invertible!right!in an algebra}%
\df{right invertible} if there exists an element $a_r$ in $A$ (called a \df{right
inverse} of~$a$)such that $aa_r = \vc 1$. The element is
 \index{invertible!in an algebra}%
\df{invertible} if it is both left invertible and right invertible. The set of all
invertible elements of $A$ is denoted
 \index{invertible@$\inv A$ (invertible elements in an algebra)}%
by~$\inv A$.
\end{defn}

An element of a unital algebra can have at most one multiplicative inverse.  In fact,
more is true.

\begin{prop}\label{000521} If an element of a unital algebra has both a left inverse and a
right inverse, then these two inverses are equal (and so the element is invertible).
\end{prop}

When an element $a$ of a unital algebra is invertible its (unique) inverse is denoted
by~$a^{-1}$.

\begin{prop} If $a$ is an invertible element of a unital algebra, then $a^{-1}$ is also
 \index{inverse!of an inverse}%
invertible and
   \[ \bigl(a^{-1}\bigr)^{-1} = a\,. \]
\end{prop}

\begin{prop} If $a$ and $b$ are invertible elements of a unital algebra, then their product
 \index{inverse!of a product}%
$ab$ is also invertible and
  \[ (ab)^{-1} = b^{-1}a^{-1}\,. \]
\end{prop}

\begin{prop}\label{000524} If $a$ and $b$ are invertible elements of a unital algebra,
 \index{inverses!difference of}%
then
  \[ a^{-1} - b^{-1} = a^{-1}(b - a)b^{-1}\,. \]
\end{prop}

\begin{prop} Let $a$ and $b$ be elements of a unital algebra.  If both $ab$ and
$ba$ are invertible, then so are $a$ and $b$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use proposition~\ref{000521}. \ns \end{proof}

\begin{cor} Let $a$ be an element of a unital $*\,$-algebra.  Then $a$ is invertible if and only if $a^*a$ and $aa^*$
are invertible, in which case
   \[ a^{-1} = \bigl(a^*a\bigr)^{-1}a^* = a^*\big(aa^*\bigr)^{-1}\,. \]
\end{cor}

\begin{prop}\label{000526} Let $a$ and $b$ be elements of a unital algebra.  Then
$\vc 1 - ab$ is invertible if and only if $\vc 1 - ba$ is.
\end{prop}

\begin{proof} If $\vc 1 - ab$ is invertible, then $\vc 1 + b(\vc 1 - ab)^{-1}a$ is the
inverse of $\vc 1 - ba$.
\end{proof}

\begin{exam} The simplest example of a complex algebra is the family $\C$ of complex
numbers.  It is both unital and commutative.
\end{exam}

\begin{exam}\label{000532} If $X$ is a nonempty topological space, then the family $\fml C(X)$
of all continuous complex valued functions on $X$ is an
 \index{C@$\fml C(X)$!as a unital algebra}%
algebra under the usual pointwise operations of addition, scalar multiplication, and
multiplication. It is both unital and commutative.
\end{exam}

\begin{exam} If $X$ is a locally compact Hausdorff space which is not compact, then (under
pointwise operations) the family $\fml C_0(X)$ of all continuous complex valued functions
on $X$ which vanish at infinity is a commutative algebra.
However, it is \emph{not} a
 \index{C@$\fml C_0(X)$!as a nonunital algebra}%
unital algebra.
\end{exam}

\begin{exam}\label{000533} The family
 \index{Ma@$\mathbf M_n = \M n\C$!$n \times n$ matrices of complex numbers}%
$\mathbf M_n$ of $n \times n$ matrices of complex numbers is a
 \index{Ma@$\mathbf M_n = \M n\C$!as a unital algebra}%
unital algebra under the usual matrix operations of addition, scalar multiplication, and
multiplication. It is not commutative when $n > 1$.
\end{exam}

\begin{exam}\label{000533a} Let $A$ be an algebra. Make the family
 \index{Ma@$\M nA$!$n \times n$ matrices of elements of an algebra}%
$\M n{A}$ of $n \times n$ matrices of elements of $A$ into an
 \index{Ma@$\M nA$!as an algebra}%
algebra by using the same rules for matrix operations that are used for~$\mathbf M_n$.
Thus $\mathbf M_n$ is just $\M n\C$. The algebra $\M nA$ is unital if and only if $A$~is.
\end{exam}

\begin{exam}\label{000534} If $V$ is a normed linear space, then
 \index{B@$\ofml B(V)$!as a unital algebra}%
$\ofml B(V)$ is a unital algebra.  If $\dim V~>~1$, the algebra is not commutative.
\end{exam}

\begin{defn}\label{000535} Let $a$ be an element of a unital algebra~$A$.  The
 \index{spectrum}%
\df{spectrum} of $a$, denoted by
 \index{sigma@$\sigma_A(a)$ or $\sigma(a)$ (spectrum of~$a$)}%
$\sigma_A(a)$ or just $\sigma(a)$, is the set of all complex numbers $\lambda$ such that
$a - \lambda\vc 1$ is not invertible.
\end{defn}

\begin{exam} If $z$ is an element of the algebra $\C$ of complex numbers, then
 \index{spectrum!of a complex number}%
$\sigma(z) = \{z\}$.
\end{exam}

\begin{exam}\label{0005712} Let $X$ be a compact Hausdorff space.  If $f$ is an element
 \index{spectrum!of a continuous function}%
of the algebra $\fml C(X)$ of continuous complex valued functions on $X$, then the
spectrum of $f$ is its range.
\end{exam}

\begin{exam} Let $X$ be an arbitrary topological space.  If $f$ is an element of the
 \index{spectrum!of a bounded continuous function}%
algebra $\fml C_b(X)$ of bounded continuous complex valued functions on $X$, then the
spectrum of $f$ is the closure of its range.
\end{exam}

\begin{exam} Let $S$ be a positive measure space and $f \in \lfs{\infty}{S}$ be an
(equivalence class of) essentially bounded function(s) on~$S$. Then the
 \index{spectrum!of an essentially bounded function}%
spectrum of $f$ is its essential range.
\end{exam}

\begin{exam} The family $\mathbf M_3$ of $3 \times 3$ matrices of complex numbers is a unital
algebra under the usual matrix operations. The spectrum of the matrix $\begin{bmatrix} 5
& -6 & -6 \\ -1 & 4 & 2 \\ 3 & -6 & -4 \end{bmatrix}$ is $\{1,2\}$.
\end{exam}

\begin{prop} Let $a$ be an element of a unital algebra such that $a^2 = \vc 1$.
 \index{spectrum!of an element whose square is $1$}%
Then either
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $a = \vc 1$, in which case $\sigma(a) = \{1\}$, or
  \item $a = -\vc 1$, in which case $\sigma(a) = \{-1\}$, or
  \item $\sigma(a) = \{-1,1\}$.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}]  In (c) to prove $\sigma(a) \subseteq \{-1,1\}$,
consider $\D\frac1{1 - \lambda^2}(a + \lambda \mathbf 1).$  \ns
\end{proof}

\begin{prop} Recall that an element $a$ of an algebra is
 \index{idempotent}%
 \index{spectrum!of an idempotent element}%
\df{idempotent} if $a^2 = a$. Let $a$ be an idempotent element of a unital algebra. Then
either
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $a = \vc 1$, in which case $\sigma(a) = \{1\}$, or
  \item $a = \vc 0$, in which case $\sigma(a) = \{0\}$, or
  \item $\sigma(a) = \{0,1\}$.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] In (c) to prove $\sigma(a) \subseteq \{0,1\}$,
consider $\D \frac1{\lambda - \lambda^2}\bigl(a + (\lambda - 1)\vc 1\bigr).$ \ns
\end{proof}

\begin{prop} Let $a$ be an invertible element of a unital algebra. Then a complex number
$\lambda$ belongs to the spectrum of $a$ if and only if $1/\lambda$ belongs to the
spectrum of~$a^{-1}$.
\end{prop}

The next proposition is a simple corollary of proposition~\ref{000526}.

\begin{prop}\label{000575} If $a$ and $b$ are elements of a unital algebra, then,
except possibly for $0$, the spectra of $ab$ and $ba$ are the same.
\end{prop}

\begin{defn}\label{C035854} A map $f \colon A \sto B$ between Banach algebras is a
 \index{Banach!algebra!homomorphism}%
 \index{homomorphism!of Banach algebras}%
\df{(Banach algebra) homomorphism} if it is both an algebraic homomorphism and a
continuous map between $A$ and $B$.  We denote
 \index{BALG@$\cat{BALG}$!the category}%
 \index{category!$\cat{BALG}$ as a}%
by $\cat{BALG}$ the category of Banach algebras and continuous algebra homomorphisms.
\end{defn}

\begin{prop} Let $A$ and $B$ be Banach algebras.  The
 \index{direct!sum!of Banach algebras}%
 \index{Banach!algebra!direct sum}%
\df{direct sum} of $A$ and $B$, denoted by $A \oplus B$, is defined to be the set $A
\times B$ equipped with pointwise operations:
   \begin{enumerate}[label=\rm{(\alph*)}]
     \item $(a,b) + (a',b') = (a + a',b + b')$;
     \item $(a,b)\,(a',b') = (aa',bb')$;
     \item $\alpha(a,b) = (\alpha a,\alpha b)$
   \end{enumerate}
for $a$,$a' \in A$, $b$, $b' \in B$, and $\alpha \in \C$. As in~\ref{C023414} define
$\norm{(a,b)} = \norm a + \norm b$.  This makes $A \oplus B$ into a Banach algebra.
\end{prop}

\begin{exer} Identify products and coproducts (if they exist) in the category $\cat{BALG}$
of Banach algebras
 \index{product!in $\cat{BALG}$}%
 \index{Banach!algebra!product}%
 \index{Banach!algebra!coproduct}%
 \index{coproduct!in $\cat{BALG}$}%
 \index{BALG@$\cat{BALG}$!products and coproducts in}%
and continuous algebra homomorphisms and in the category of Banach algebras and
contractive algebra homomorphisms. (Here
 \index{contractive}%
\emph{contractive} means $\norm{Tx} \le \norm x$ for all~$x$.)
\end{exer}

\begin{prop} In a Banach algebra $A$ the operations of addition and multiplication (regarded as
maps from $A \oplus A$ to $A$) are continuous and scalar multiplication (regarded as a map from
$\K \oplus A$ to $A$) is also continuous.
\end{prop}

\begin{prop}[The Neumann series]\label{prop_Neumann_series} Let $a$ be an element of a unital Banach algebra.
 \index{Neumann series}%
 \index{series!Neumann}%
If $\norm a < 1$, then
$\vc 1 - a \in \inv A$ and $(\vc 1 - a)^{-1} = \sum_{k=0}^\infty a^k$.
\end{prop}

\begin{proof}[\emph{Hint for proof}]  Show that the sequence $(\vc 1, a, a^2, \dots)$ is
summable.  (In a unital algebra we take $a^0$ to mean~$\vc 1$.)  \ns
\end{proof}

\begin{cor}\label{cor_Neumann_series} If $a$ is an element of a unital Banach algebra with $\norm{\vc 1 - a} < 1$, then $a$ is invertible and
    \[ \norm{a^{-1}} \le \frac{1}{1 - \norm{\vc 1 - a}}\,.  \]
\end{cor}

\begin{cor}\label{cor2_Neumann_series} Suppose that $a$ is an invertible element of a unital Banach algebra $A$, that $b \in A$,
and that $\norm{a - b} < \norm{a^{-1}}^{-1}$.  Then $b$ is invertible in~$A$.
\end{cor}

\begin{proof}[\emph{Hint for proof}] Use the preceding corollary to show that $a^{-1}b$ is invertible.
\ns \end{proof}

\begin{cor} If $a$ belongs to a unital Banach algebra and $\norm a < 1$, then
   \[ \norm{(\vc 1 - a)^{-1} - \vc 1} \le \frac{\norm a}{1 - \norm a}\,. \]
\end{cor}

\begin{prop}\label{000644fa} If $A$ is a unital Banach algebra, then $\open{\inv A}A$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Let $a \in \inv A$.  Show, for sufficiently small $h$,
that $\vc 1 - a^{-1}h$ is invertible.  \ns
\end{proof}

\begin{prop}\label{000646fa} Let $A$ be a unital Banach algebra.  The map $a \mapsto a^{-1}$ from $\inv A$ into
itself is a homeomorphism.
\end{prop}

\begin{prop} Let $A$ be a unital Banach algebra.  The map $r\colon a \mapsto a^{-1}$ from
$\inv A$ into itself is differentiable and at each invertible element $a$, we have
$dr_a(h) = -a^{-1}ha^{-1}$ for all $h \in A$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] For a quick introduction to differentiation on infinite dimensional
spaces, see Chapter 13 of my notes~\cite{Erdman:2007} on Real Analysis.  \ns
\end{proof}

\begin{prop}\label{000661} Let $a$ be an element of a unital Banach algebra~$A$.  Then the spectrum of $a$
is compact and $\abs\lambda \le \norm a$ for every $\lambda \in \sigma(a)$.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Use the \emph{Heine-Borel theorem}.  To prove that the
spectrum is closed notice that $(\sigma(a))^c = f^\gets(\inv A)$ where $f(\lambda) = a -
\lambda\mathbf 1$ for every complex number~$\lambda$. Also show that if $\abs\lambda >
\norm a$, then $\mathbf 1 - \lambda^{-1}a$ is invertible.  \ns
\end{proof}

\begin{defn}  Let $a$ be an element of a unital Banach algebra.  The
 \index{resolvent!mapping}%
\df{resolvent mapping} for $a$ is defined by
   \[ R_a \colon \C \setminus \sigma(a) \sto A
                 \colon \lambda \mapsto (a  - \lambda\vc 1)^{-1}\,. \]
\end{defn}

\begin{defn} Let $\open U\C$ and $A$ be a unital Banach algebra. A function $f \colon U \sto A$
 \index{analytic}%
is \df{analytic} on $U$ if
   \[ f'(a) := \lim_{z \sto a} \frac{f(z) - f(a)}{z - a} \]
exists for every $a \in U$.  A complex valued function which is analytic on all of $\C$
is an
 \index{entire}
\df{entire} function.
\end{defn}

\begin{prop} For $a$ an element of a unital Banach algebra $A$ and $\phi$ a bounded linear
functional on~$A$ let $f := \phi \circ R_a \colon \C \setminus \sigma(a) \sto \C$.  Then
 \begin{enumerate}[label=\rm{(\alph*)}]
  \item $f$ is analytic on its domain, and
  \item $f(\lambda) \sto 0$ as $\abs\lambda \sto \infty$.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] For (i) notice that in a unital algebra $a^{-1} - b^{-1}
= a^{-1}(b - a)b^{-1}$.   \ns
\end{proof}

\begin{thm}[Liouville]\label{C073131} Every bounded entire function on $\C$ is constant.
 \index{Liouville's theorem}%
\end{thm}

\begin{proof}[\emph{Comment on proof}] The proof of this theorem requires a little background
in the theory of analytic functions, material that is not covered in these notes.  The
theorem would surely be covered in any first course in complex variables.  For example, you can find a
nice proof in \cite{Rudin:1987}, theorem 10.23.   \ns
\end{proof}

\begin{prop}\label{C073134} The spectrum of every element of a unital Banach algebra is nonempty.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Argue by contradiction.  Use \emph{Liouville's theorem}~\ref{C073131}
to show that $\phi \circ R_a$ is constant for every bounded linear functional $\phi$ on~$A$. Then use
(a version of) the \emph{Hahn-Banach theorem} to prove that $R_a$ is constant.  Why must this constant be~$0$?   \ns
\end{proof}

\noindent\emph{Note.} It is important to keep in mind that we are working only with
\emph{complex} algebras.  This result is \emph{false} for real Banach algebras.  An easy
counterexample is the matrix $\begin{bmatrix}0 & 1 \\ -1 & 0 \end{bmatrix}$ regarded as
an element of the (real) Banach algebra $M_2$ of all $2 \times 2$ matrices of real
numbers.

 \vskip 1 pt

\begin{thm}[Gelfand-Mazur]\label{C073141} If $A$ is a unital Banach algebra in which every
 \index{Gelfand-Mazur theorem}%
nonzero element is invertible, then $A$ is isometrically isomorphic to~$\C$.
\end{thm}

\begin{proof}[\emph{Hint for proof}] Let $B = \{ \lambda\vc 1\colon \lambda \in \C\}$. Use
the preceding proposition~\ref{C073134}) to show that $B = A$.    \ns
\end{proof}

The following is an immediate consequence of the \emph{Gelfand-Mazur theorem}
\ref{C073141} and proposition~\ref{C037821}.

\begin{cor}\label{C073147} If $I$ is a maximal ideal in a commutative unital Banach algebra $A$,
then $A/I$ is isometrically isomorphic to~$\C$.
\end{cor}

\begin{prop}\label{0006703fa} Let $a$ be an element of a unital algebra.  Then
   \[ \sigma(a^n) = [\sigma(a)]^n \]
for every $n \in \N$.  (The notation $[\sigma(a)]^n$ means $\{\lambda^n\colon \lambda \in
\sigma(a)\}$.)
\end{prop}

\begin{defn}\label{C073154}  Let $a$ be an element of a unital algebra.  The
 \index{spectral!radius}%
 \index{radius!spectral}%
\df{spectral radius} of~$a$, denoted by $\rho(a)$, is defined to be
$\sup\{\abs\lambda\colon \lambda \in \sigma(a)\}$.
\end{defn}

\begin{prop}\label{0006703} If $a$ is an element of a unital Banach algebra, then $\rho(a) \le
\norm a$ and $\rho(a^n) = \bigr(\rho(a)\bigr)^n$.
\end{prop}

\begin{prop} Let $\phi\colon A \sto B$ be a unital homomorphism between unital algebras. If $a$ is an invertible element of $A$,
then $\phi(a)$ is invertible in $B$ and its inverse is~$\phi(a^{-1})$.
\end{prop}

\begin{cor} If $\phi\colon A \sto B$ is a unital homomorphism between unital algebras, then
   \[ \sigma(\phi(a)) \subseteq \sigma(a) \]
for every $a \in A$.
\end{cor}

\begin{cor} If  $\phi\colon A \sto B$ is a unital homomorphism between unital algebras, then
   \[ \rho(\phi(a)) \le \rho(a) \]
for every $a \in A$.
\end{cor}

\begin{thm}[Spectral radius formula]\label{C073157} If $a$ is an element of a unital Banach
 \index{spectral!radius!formula for}%
algebra, then
   \[ \rho(a) = \inf\bigl\{\norm{a^n}^{1/n}\colon n \in \N\bigr\}
                                = \lim_{n \sto \infty} \norm{a^n}^{1/n}\,. \]
\end{thm}

\begin{exer} The Volterra operator $V$ (defined in example~\ref{000319}) is an
 \index{Volterra operator!and integral equations}%
 \index{operator!Volterra!and integral equations}%
integral operator on the Banach space~$\fml C([0,1])$.
 \begin{enumerate}
  \item[(a)] Compute the spectrum of the operator $V$.  (\emph{Hint.} Show that $\norm{V^n}
\le (n!)^{-1}$ for every $n \in \N$ and use the \emph{spectral radius formula}.)
  \item[(b)] What does (a) say about the possibility of finding a continuous function $f$ which
satisfies the integral equation
   \[ f(x) - \mu\int_0^xf(t)\,dt = h(x), \]
where $\mu$ is a scalar and $h$ is a given continuous function on~$[0,1]$?
  \item[(c)] Use the idea in (b) and the Neumann series expansion for $\vc 1 - \mu V$ (see
proposition~\ref{prop_Neumann_series}) to calculate explicitly (and in terms of elementary functions)
a solution to the integral equation
 \[f(x) - {\textstyle \frac12}\int_0^xf(t)\,dt = e^x.\]
 \end{enumerate}
\end{exer}

\begin{prop} Let $A$ be a unital Banach algebra and $B$ a closed subalgebra containing~$\vc 1_A$.
Then
 \begin{enumerate}[label=\rm{(\alph*)}]
   \item $\inv B$ is both open and closed in $B \cap \inv A$;
   \item $\sigma_A(b) \subseteq \sigma_B(b)$ for every $b \in B$; and
   \item if $b \in B$ and $\sigma_A(b)$ has no holes (that is, if its complement in $\C$ is
connected), then $\sigma_A(b) =    \sigma_B(b)$.
 \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] Consider the function
$f_b\colon (\sigma_A(b))^c \sto B \cap \inv A\colon \lambda \mapsto b - \lambda \vc 1$.   \ns
\end{proof}















\section{Spectra of Hilbert Space Operators}
\begin{prop} For a self-adjoint operator $T$ on a Hilbert space the following are
equivalent:
 \begin{enumerate}
    \item[(i)] $\sigma(T) \subseteq [0,\infty)$;
    \item[(ii)] there exists an operator $S$ on $H$ such that $T = S^*S$; and
    \item[(iii)] $T$ is positive.
 \end{enumerate}
\end{prop}

\begin{notn} If $A$ is a set of the complex numbers we
 \index{<unopurstar@$A^*$ (set of complex conjugates of $A \subseteq \C$)}%
define $A^* := \{\conj\lambda \colon \lambda \in A\}$.  We do this to avoid confusion with the closure of the set~$A$.
\end{notn}

\begin{prop} If $T$ is a Banach space operator then $\sigma(T^*) = \sigma(T)$, while if it is a Hilbert space
operator then $\sigma(T^*) = (\sigma(T))^*$.
\end{prop}

In a finite dimensional space there is only one way that a complex number $\lambda$ can end up in the spectrum of an operator~$T$.
The reason for this is that there is only one way an operator, in this case $T - \lambda I$, on a finite dimensional space can fail
to be invertible (see the paragraph before definition~\ref{defn_similar_vs_ops}).  On the other hand there are several ways for $\lambda$
to get into the spectrum of an operator on an infinite dimensional space---because an operator has several ways in which it may fail
to be invertible. (See, for example, proposition~\ref{prop_nasc_op_invert}.) Historically this has led to a number of proposed
taxonomies of the spectrum of an operator.

\begin{defn} Let $T$ be an operator on a Hilbert (or Banach) space~$H$. A complex number $\lambda$ belongs to the
 \index{resolvent!set}%
\df{resolvent set} of $T$ if $T - \lambda I$ is invertible. The resolvent set is the complement of the \emph{spectrum}
of $T$. The set of complex numbers $\lambda$ for which $T - \lambda I$ fails to be injective is the
 \index{point spectrum}%
 \index{spectrum!point}%
\df{point spectrum} of $T$ and is denoted
 \index{sigma@$\sigma_p(T)$ (point spectrum of~$T$)}%
by $\sigma_p(T)$. Members of the point spectrum of $T$ are
 \index{eigenvalue}%
\df{eigenvalues} of $T$. The set of complex numbers $\lambda$ for which $T - \lambda I$ is not bounded away from zero is the
 \index{approximate point spectrum}%
 \index{spectrum!approximate point}%
\df{approximate point spectrum} of $T$ and is denoted
 \index{sigma@$\sigma_{ap}(T)$ (approximate point spectrum of~$T$)}%
by $\sigma_{ap}(T)$. The set of all complex numbers $\lambda$ such that the closure of the range of $T - \lambda I$ is a proper
subspace of $H$ is the
 \index{compression spectrum}%
 \index{spectrum!compression}%
\df{compression spectrum} of $T$ and is denoted
 \index{sigma@$\sigma_c(T)$ (compression spectrum of~$T$)}%
by $\sigma_c(T)$.  The
 \index{residual spectrum}%
 \index{spectrum!residual}%
\df{residual spectrum} of $T$, which we denote
 \index{sigma@$\sigma_r(T)$ (residual spectrum of~$T$)}%
by $\sigma_r(T)$, is $\sigma_c(T) \setminus \sigma_p(T)$.
\end{defn}

\begin{prop} If $T$ is a Hilbert space operator, then $\sigma_p(T) \subseteq \sigma_{ap}(T) \subseteq \sigma(T)$.
\end{prop}

\begin{prop} If $T$ is a Hilbert space operator, then $\sigma(T) = \sigma_{ap}(T) \cup \sigma_c(T)$.
\end{prop}

\begin{prop} If $T$ is a Hilbert space operator, then $\sigma_p(T^*) = (\sigma_c(T))^*$.
\end{prop}

\begin{cor} If $T$ is a Hilbert space operator, then $\sigma(T) = \sigma_{ap}(T) \cup (\sigma_{ap}(T^*))^*$.
\end{cor}

\begin{thm}{Spectral Mapping Theorem}\label{SMThm} If $T$ is a Hilbert space operator and $p$ is a polynomial, then
   \[ \sigma(p(T)) = p(\sigma(T)). \]
\end{thm}

The preceding theorem is true more generally for any analytic function defined in a neighborhood of the spectrum of~$T$.
(For a proof see~\cite{Conway:1990}, chapter VII, theorem 4.10.)  The result also holds for parts of the spectrum.

\begin{prop} If $T$ is a Hilbert space operator and $p$ is a polynomial, then
   \begin{enumerate}[label=\rm{(\alph*)}]
     \item $\sigma_p(p(T)) = p(\sigma_p(T))$,
     \item $\sigma_{ap}(p(T)) = p(\sigma_{ap}(T))$, and
     \item $\sigma_c(p(T)) = p(\sigma_c(T))$,
   \end{enumerate}
\end{prop}

\begin{prop}
    If $T$ is an invertible operator on a Hilbert space, then  $\sigma(T^{-1}) = (\sigma(T))^{-1}$.
\end{prop}

\begin{defn}\label{defn_similar_Hsp_ops} As with vector spaces, two operators $R$ and $T$ on a Hilbert (or Banach) space $H$ are
 \index{similar!operators}%
\df{similar} if there exists an invertible operator $S$ on $H$ such that $R = STS^{-1}$.
\end{defn}

\begin{prop} If $S$ and $T$ are similar operators on a Hilbert space, then $\sigma(S) = \sigma(T)$, $\sigma_p(S) = \sigma_p(T)$,
$\sigma_{ap}(S) = \sigma_{ap}(T)$, and $\sigma_c(S) = \sigma_c(T)$.
\end{prop}

\begin{prop} If $T$ is a normal operator on a Hilbert space, then $\sigma_c(T) = \sigma_p(T)$, so that $\sigma_r(T)~=~\emptyset$.
\end{prop}

\begin{prop} If $T$ is an operator on a Hilbert space, then $\sigma_{ap}(T)$ is closed.
\end{prop}

\begin{exam} Let $D = \diag(a_1,a_2,a_3,\dots)$ be a diagonal operator on a Hilbert space and let
 \index{spectral!parts!of a diagonal operator}%
 \index{operator!diagonal!spectral parts of}%
 \index{diagonal!operator!spectral parts of}%
$A = \cup_{k=1}^\infty a_k$. Then
   \begin{enumerate}[label=(\alph*)]
     \item $\sigma_p(D) = \sigma_c(D) = A$,
     \item $\sigma_{ap}(D) = \clo A$, and
     \item $\sigma_r(D) = \emptyset$.
   \end{enumerate}
\end{exam}

\begin{exam} Let $S$ be the unilateral shift operator on $l_2$
 \index{spectral!parts!of the unilateral shift}%
 \index{operator!unilateral shift!spectral parts of}%
 \index{unilateral shift!spectral parts of}%
(see example~\ref{C063526}). Then
    \begin{enumerate}[label=(\alph*)]
      \item $\sigma(S) = D$,
      \item $\sigma_p(S) = \emptyset$,
      \item $\sigma_{ap}(S) = C$,
      \item $\sigma_c(S) = B$,
      \item $\sigma(S^*) = D$,
      \item $\sigma_p(S^*) = B$,
      \item $\sigma_{ap}(S^*) = D$, and
      \item $\sigma_c(S^*) = \emptyset$.
    \end{enumerate}
where $B$ is the open unit disk, $D$ is the closed unit disk, and $C$ is the unit circle in the complex plane.
\end{exam}

\begin{exam} Let $(S,\sfml A, \mu)$ be a $\sigma$-finite positive measure space, $\phi \in L_\infty(S,\mu)$, and $M_\phi$ be the
 \index{spectral!parts!of a multiplication operator}%
 \index{operator!multiplication!spectral parts of}%
 \index{multiplication operator!spectral parts of}%
corresponding multiplication operator on the Hilbert space $\lfs2{S,\mu}$ (see example~\ref{C063527}). Then
the spectrum and the approximate point spectrum of $M_\phi$ are the essential range of~$\phi$, while the point spectrum
and compression spectrum of $M_\phi$ are $\{\lambda \in \C\colon \mu(\phi^{\gets}(\{\lambda\})) > 0\}$.
\end{exam}






















\endinput








\chapter{TOPOLOGICAL VECTOR SPACES}

\section{Balanced Sets and Absorbing Sets}

\begin{defn} A set $S$ in a vector space is
 \index{balanced}%
\df{balanced} (or \df{circled}) if $\Dsk S \subseteq S$.
\end{defn}

\begin{notn} Let $\Dsk$ denote the closed unit disk
 \index{D@$\Dsk$ (closed unit disk in~$\C$)}%
in the complex plane $\{z \in \C\colon \abs z \le 1\}$.
\end{notn}

\begin{prop} Let $B$ be a balanced subset of a vector space. If $\alpha$ and $\beta$ are scalars such that $\abs\alpha \le \abs\beta$,
then $\alpha B \subseteq \beta B$.
\end{prop}

\begin{cor} If $B$ is a balanced subset of a vector space and $\abs\lambda =1$, then $\lambda B = B$.
\end{cor}

\begin{defn} If $S$ is a subset of a vector space, then the set $\Dsk S$ is the
 \index{balanced!hull}%
 \index{hull!balanced}%
\df{balanced hull} of~$S$.
\end{defn}

\begin{prop} Let $S$ be a subset of a vector space~$V$.  Then the balanced hull of $S$ is the intersection of all
the balanced subsets of $V$ which contain~$S$; thus it is the smallest balanced subset of $V$ containing~$S$.
\end{prop}

\begin{defn} A subset $A$ of a vector space $V$
 \index{absorb}%
\df{absorbs} a subset $B \subseteq V$ if there exists $M > 0$ such that $B \subseteq tA$ whenever $\abs t \ge M$.
The set $A$ is
 \index{absorbing}%
\df{absorbing} (or \df{radial}) if it absorbs every one-element set (equivalently, every finite set).
\end{defn}

Thus, speaking loosely, a set is absorbing if any vector in the space can be covered by an appropriate dilation of the set.

\begin{exam} Let $B$ be the union of the $x$- and $y$-axes in the real vector space $\R^2$.  Then $B$ is balanced but not convex.
\end{exam}

\begin{exam} In the vector space $\C$ an ellipse with one focus at the origin is an example of an absorbing set which is not balanced.
\end{exam}

\begin{exam} In the vector space $\C^2$ the set $\Dsk \times \{0\}$ is balanced but not absorbing.
\end{exam}

\begin{prop} A balanced set $B$ in a vector space $V$ is absorbing if and only if for every $x \in V$ there exists a scalar
$\alpha \ne 0$ such that $\alpha x \in B$.
\end{prop}








\section{Filters}

In the study of topological vector spaces it is convenient to use the language of \emph{filters} to characterize topological properties
of the spaces.  It is a tool, alternative, but largely equivalent, to nets, useful especially in (nonmetric) situations where sequences
are no longer helpful.

Roughly, a \emph{filter} is a nonempty family of nonempty sets which is closed under finite intersections and supersets.

\begin{defn} A nonempty family $\sfml F$ of nonempty subsets of a set $S$ is a
 \index{filter!on a set}%
\df{filter} on $S$ if
   \begin{enumerate}[label=(\alph*)]
     \item $A$, $B \in \sfml F$ implies $A \cap B \in \sfml F$, and
     \item if $A \in \sfml F$ and $A \subseteq B$, then $B \in \sfml F$.
   \end{enumerate}
\end{defn}

\begin{exam} A simple (and, for our work, the most important) example of a filter is the family of neighborhoods of a point $a$ in a
topological space.  This is the
 \index{neighborhood!filter at a point}%
 \index{filter!neighborhood}%
\df{neighborhood filter} at~$a$. It will be
 \index{N@$\sfml N_a$ (neighborhood filter at~$a$)}%
denoted by $\sfml N_a$.
\end{exam}

\begin{defn} A nonempty family $\sfml B$ of nonempty sets of a set $S$ is a
 \index{filterbase}%
\df{filterbase} on $S$ if for every $A$, $B \in \sfml B$ there exists $C \in \sfml B$ such that $C \subseteq A \cap B$.

We say that a subfamily $\sfml B$ of a filter $\sfml F$ is a
 \index{filterbase!for a filter}%
 \index{base!for a filter}%
\df{filterbase for} $\sfml F$ if every member of $\sfml F$ contains a member of~$\sfml B$.  (It is important to check that
any such subfamily really is a filterbase.)
\end{defn}

\begin{exam} Every filter is a filterbase.
\end{exam}

\begin{exam} Let $a$ be a point in a topological space.  Any neighborhood base at $a$ is a filterbase for the filter $\sfml N_a$
of all neighborhoods of~$a$. In particular, the family of all open neighborhoods of $a$ is a filterbase for~$\sfml N_a$.
\end{exam}

\begin{exam} Let $\sfml B$ be a filterbase in a set $S$. Then the family $\sfml F$ of all supersets of members of $\sfml B$
is a filter in $S$ and $\sfml B$ is a filterbase for~$\sfml F$.  We will call this filter the
 \index{filter!generated by a filterbase}%
\df{filter generated by}~$\sfml B$.
\end{exam}

\begin{defn} A filterbase (in particular, a filter) $\sfml B$ on a topological space $X$
 \index{convergence!of a filter}%
 \index{filter!convergence of a}%
 \index{convergence!of a filterbase}%
 \index{filterbase!convergence of a}%
 \index{limit!of a filterbase}%
\df{converges} to a point $a \in X$ if every member $N$ of $\sfml N_a$, the neighborhood filterbase at $a$, contains a member of~$\sfml B$.
In this case we
 \index{<arrowto@$\sfml B \sto a$ (convergence of a filterbase)}%
write $\sfml B \sto a$.
\end{defn}

\begin{prop} A filterbase on a Hausdorff topological space converges to at most one point.
\end{prop}

\begin{exam} Let $(x_\lambda)$ be a net in a set~$S$ and $\sfml F$ be the family of all $A \subseteq S$ for which the net $(x_\lambda)$
is eventually in~$A$.  Then $\sfml F$ is a filter in~$S$.  We will call this the filter
 \index{filter!generated by a net}%
\df{generated by} the net~$(x_\lambda)$.
\end{exam}

\begin{prop} Suppose that $(x_\lambda)$ is a net in a topological space~$X$, that $\sfml F$ is the filter generated by~$(x_\lambda)$,
and $a \in X$.  Then $\sfml F \sto a$ if and only if $x_\lambda \sto a$.
\end{prop}

\begin{exam} Let $\sfml B$ be a filter base on a set~$S$.  Let $D$ be the set of all ordered pairs $(b,B)$ such that $b \in B \in \sfml B$.
For $(b_1,B_1)$, $(b_2,B_2) \in D$ define $(b_1,B_1) \le (b_2,B_2)$ if $B_2 \subseteq B_1$.  This makes $D$ into a directed set.  Now create
a net in $S$ by defining
   \[ x\colon D \sto S\colon (b,B) \mapsto b \]
that is, let $x_\lambda = b$ for all $\lambda = (b,B) \in D$.  Then $\bigl(x_\lambda\bigr)_{\lambda \in D}$ is a net in~$S$.  It is the net
 \index{net!generated by a filter base}%
 \index{net!based on a filter base}%
\df{generated by} (or \df{based on}) the filter base~$\sfml B$.
\end{exam}

\begin{prop} Let $\sfml B$ be a filter base in a topological space $X$, let $(x_\lambda)$ the the net generated by $\sfml B$, and let $a \in X$.
Then $x_\lambda \sto a$ if and only if $\sfml B \sto a$.
\end{prop}

















\section{Compatible Topologies}

\begin{defn} Let $\sfml T$ be a topology on a vector space~$X$.  We say that $\sfml T$ is
 \index{compatible}%
\df{compatible} with the linear structure on $X$ if the operations of addition $A\colon X \times X \sto X\colon (x,y) \mapsto x + y$
and scalar multiplication $M\colon \K \times X\colon (\alpha,x) \mapsto \alpha x$ are continuous.  (Here we understand $X \times X$ and
$\K \times X$ to be equipped with the product topology.)

If $X$ is a vector space on which has been defined a topology $\sfml T$ compatible with its linear structure, we say that the pair
$(X,\sfml T)$ is a
 \index{topological!vector space}%
 \index{vector!space!topological}%
 \index{space!topological vector}%
\df{topological vector space}.  (As you doubtless expect, we will almost always continue using standard illogical abridgements such as
``Let $X$ be a topological vector space \dots''.)  We denote by $\cat{TVS}$ the category of topological vector space
 \index{category!$\cat{TVS}$ as a}%
 \index{TVS@$\cat{TVS}$!the category}%
and continuous linear maps.
\end{defn}

\begin{exam} Let $X$ be a nonzero vector space.  The discrete topology on $X$ is not compatible with the linear structure on~$X$.
\end{exam}

\begin{prop} Let $X$ be a topological vector space, $a \in X$, and $\alpha \in \K$.
   \begin{enumerate}[label=\rm{(\alph*)}]
    \item The mapping $T_a\colon x \mapsto x + a$
 \index{translation}%
(\df{translation} by~$a$) of $X$ into itself is a homeomorphism.
    \item If $\alpha \ne 0$, the mapping $x \mapsto \alpha x$ of $X$ into itself is a homeomorphism.
   \end{enumerate}
\end{prop}

\begin{cor} A set $U$ is a neighborhood of a point $x$ in a topological vector space if and only if $-x + U$ is a neighborhood of~$\vc 0$.
\end{cor}

\begin{cor} Let $\sfml U$ be a filterbase for the neighborhood filter at the origin in a topological vector space~$X$.  A subset $V$ of $X$
is open if and only if for every $x \in V$ there exists $U \in \sfml U$ such that $x + U \subseteq V$.
\end{cor}

What the preceding corollary tells us is that the topology of a topological vector space is completely determined by a filterbase for the
neighborhood filter at the origin of the space.  We call such a filterbase a
 \index{local!base}%
 \index{base!local}%
\df{local base} (for the topology).

\begin{prop} If $U$ is a neighborhood of the origin in a topological vector space and $0 \ne \lambda \in \K$, then $\lambda U$ is a
neighborhood of the origin.
\end{prop}

\begin{prop} In a topological vector space every neighborhood of $\vc 0$ contains a balanced neighborhood of~$\vc 0$.
\end{prop}

\begin{prop} In a topological vector space every neighborhood of $\vc 0$ every neighborhood of the origin is absorbing.
\end{prop}

\begin{prop} If $V$ is a neighborhood of $\vc 0$ in a topological vector space, then there exists a neighborhood $U$ of $\vc 0$
such that $U + U \subseteq V$.
\end{prop}

The next proposition is (almost) a converse of the preceding three.

\begin{prop}\label{prop_fbase_induces_top} Let $X$ be a vector space and $\sfml U$ be a filterbase on $X$ which satisfies the following conditions:
   \begin{enumerate}[label=\rm{(\alph*)}]
     \item every member of $\fml U$ is balanced;
     \item every member of $\fml U$ is absorbing; and
     \item for every $V \in \fml U$ there exists $U \in \fml U$ such that $U + U \subseteq V$.
   \end{enumerate}
Then there exists a topology $\sfml T$ on $X$ under which $X$ is a topological vector space and $\sfml U$ is a local base for~$\sfml T$.
\end{prop}

\begin{exam} In a normed linear space the family $\{C_r(\vc 0)\colon r > 0\}$ of closed balls about the origin is a local base for
the topology on the space.
\end{exam}

\begin{exam} Let $\Omega$ be a nonempty open subset of $\R^n$.  For every compact subset $K \subseteq \Omega$ and every $\epsilon > 0$ let
$U_{K,\epsilon} = \{ f \in \fml C(\Omega)\colon \abs{f(x)} \le \epsilon \text{ whenever } x \in K\}$.  Then the family of all such
sets $U_{K,\epsilon}$ is a local base for a topology on~$\fml C(\Omega)$.  This is the
 \index{topology!of uniform convergence on compacta}%
 \index{uniform!convergence!on compacta}%
 \index{convergence!uniform!on compacta}%
 \index{compacta!topology of uniform convergence on}%
\df{topology of uniform convergence on compact sets}.
\end{exam}

\begin{prop} Let $A$ be a subset of a topological vector space and $0 \ne \alpha \in \K$.  Then $\alpha \clo A = \clo{\alpha A}$.
\end{prop}

\begin{prop} Let $S$ be a set in a topological vector space.
   \begin{enumerate}[label=\rm{(\alph*)}]
    \item If $S$ is balanced, so is its closure.
    \item The balanced hull of $S$ may fail to be closed even if $S$ itself is closed.
   \end{enumerate}
\end{prop}

\begin{proof}[\emph{Hint for proof}] For (b) let $S$ be the hyperbola in $\R^2$ whose equation is $xy = 1$.  \ns
\end{proof}

\begin{prop} If $M$ is a vector subspace of a topological vector space, then so is~$\clo M$.
\end{prop}

It is clear that every absorbing set in a topological vector space must contain the origin.  The next example shows, however, that
it need not contain a neighborhood of the origin.

\begin{exam}[Schatz's apple] In the real vector space $\R^2$ the set
    \[ C_1\bigl((-1,0)\bigr) \cup C_1\bigl((1,0)\bigr) \cup \bigl(\{0\} \times [-1,1]\bigr) \]
is absorbing, but is not a neighborhood of the origin.
\end{exam}

The next result tells us that in a topological vector space compact sets and closed sets can be separated by open sets if they are disjoint.

\begin{prop} If $K$ and $C$ are nonempty disjoint subsets of a topological space $X$ with $K$ compact and $C$ closed, then there is a
neighborhood $V$ of the origin such that $(K + V) \cap (C + V)~=~\emptyset$.
\end{prop}

\begin{cor} If $K$ is a compact subset of a Hausdorff topological vector space $X$ and $C$ is a closed subset of~$X$, then $K + C$ is
closed in~$X$.
\end{cor}

\begin{proof}[\emph{Hint for proof}] Let $x \in (K + C)^c$.  Consider the sets $x - K$ and~$C$.  \ns
\end{proof}

\begin{cor} Every member of a local base $\sfml B$ for a topological vector space contains the closure of a member of~$\sfml B$.
\end{cor}

\begin{prop} Let $X$ be a topological vector space.  Then the following are equivalent.
  \begin{enumerate}[label=\rm{(\alph*)}]
    \item $\{\vc 0\}$ is a closed set in~$X$.
    \item $\{x\}$ is a closed set for every $x \in X$.
    \item For every $x \ne 0$ in $X$ there exists a neighborhood of the origin to which $x$ does not belong.
    \item $\bigcap \sfml U = \{0\}$ where $\sfml U$ is a local base for~$X$.
    \item $X$ is Hausdorff
  \end{enumerate}
\end{prop}

\begin{defn} A topological space is
 \index{regular!topological space}%
 \index{topological!space!regular}%
\df{regular} if points and closed sets can be separated by open sets; that is, if $C$ is a closed subset of the space and $x$ is a point
of the space not in $C$, then there exist disjoint open sets $U$ and $V$ such that $C \subseteq U$ and $x \in V$.
\end{defn}

\begin{prop} Every Hausdorff topological vector space is regular.
\end{prop}

\begin{cor} If $X$ is a topological space in which points are closed sets, then $X$ is regular.
\end{cor}

\begin{prop} Let $X$ be a Hausdorff topological vector space. If $M$ is a subspace of $X$ and $F$ is a finite dimensional
vector subspace of $X$, then $M + F$ is closed in~$X$. (In particular, $F$ is closed.)
\end{prop}

\begin{defn} A subset $B$ of a topological vector space is
 \index{bounded!set in a TVS}%
\df{bounded} if for every neighborhood $U$ of $\vc 0$ in the space there exists $\alpha > 0$ such that $B \subseteq \alpha U$.
\end{defn}

\begin{prop} A subset of a topological vector space is bounded if and only if it is absorbed by every neighborhood of~$\vc 0$.
\end{prop}

\begin{prop} A subset $B$ of a topological vector space is bounded if and only if $\alpha_n x_n \sto \vc 0$ whenever $(x_n)$ is a
sequence of vectors in $B$ and $(\alpha_n)$ is a sequence of scalars in $c_0$.
\end{prop}

\begin{prop} If a subset of a topological vector space is bounded, then so is its closure.
\end{prop}

\begin{prop} If subsets $A$ and $B$ of a topological vector space are bounded, then so is $A \cup B$.
\end{prop}

\begin{prop} If subsets $A$ and $B$ of a topological vector space are bounded, then so is $A + B$.
\end{prop}

\begin{prop} Every compact subset of a topological vector space is bounded.
\end{prop}

As is the case with normed linear spaces, we say that linear maps are \emph{bounded} if they take bounded sets to bounded sets.

\begin{defn} A linear map $T\colon X \sto Y$ between topological vector spaces is
 \index{bounded!linear map}%
\df{bounded} if $T^\sto(B)$ is a bounded subset of $Y$ whenever $B$ is a bounded subset of~$X$.
\end{defn}

\begin{prop} Every continuous linear map between topological vector spaces is bounded.
\end{prop}

\begin{defn} A filter $\sfml F$ on a subset $A$ of a topological vector space $X$ is a
 \index{Cauchy!filter}%
 \index{filter!Cauchy}%
\df{Cauchy filter} if for every neighborhood $U$ of the origin in $X$ there exists $B \subseteq A$ such that
   \[ B - B \subseteq U\,. \]
\end{defn}

\begin{prop} On a topological vector space every convergent filter is Cauchy.
\end{prop}

\begin{defn} A subset $A$ of a topological vector space $X$ is
 \index{complete!topological vector space}%
 \index{topological!vector space!complete}%
 \index{space!topological vector!complete}%
\df{complete} if every Cauchy filter on $A$ converges to a point of~$A$.
\end{defn}

\begin{prop} Every complete subset of a Hausdorff topological vector space is closed.
\end{prop}

\begin{prop} If $C$ is a closed subset of a complete set in a topological vector space, then $C$ is complete.
\end{prop}

\begin{prop} A linear map $T\colon X \sto Y$ between topological vector spaces, is continuous if and only if it is continuous at zero.
\end{prop}






















\section{Quotients}

\begin{conv} As is the case with Hilbert and Banach spaces the word ``subspace'' in the context of topological vector spaces
 \index{subspace!of a topological vector space}%
 \index{conventions!subspaces of topological vector spaces are closed}%
means \emph{closed vector subspace}.
\end{conv}

\begin{defn} Let $M$ be a vector subspace of a topological vector space~$X$ and $\pi\colon X \sto X/M \colon x \mapsto [x]$
be the usual quotient map.  On the quotient vector space $X/M$ we define a topology, which we call the
 \index{quotient!topology}%
 \index{quotient!topological vector space}%
 \index{topological!vector space!quotient}%
\df{quotient topology}, by specifying the neighborhood filter at the origin in~$X/M$ to be the image under $\pi$ of the
neighborhood filter at the origin in~$X$.  That is, we declare a subset $V$ of $X/M$ to be a neighborhood of $\vc 0$ when,
and only when, there exists a neighborhood $U$ of the origin in $X$ such that $V = \pi^{\sto}(U)$.
\end{defn}

\begin{prop} Let $M$ be a vector subspace of a topological vector space~$X$. When $X/M$ is given the quotient topology the
quotient map $\pi\colon X \sto X/M$ is both an open map and continuous.
\end{prop}

\begin{exam} Let $M$ be the $y$-axis in the topological vector space $\R^2$ (with its usual topology). Identify $\R^2/M$ with
the $x$-axis.  Although the quotient map takes open sets to open sets, the hyperbola $\{(x,y) \in \R^2\colon xy = 1\}$ shows
that $\pi$ need not take closed sets to closed sets.
\end{exam}

\begin{prop}\label{prop_quotient_top_strong} Let $M$ be a vector subspace of a topological vector space~$X$.  The quotient
topology on $X/M$ is the largest topology under which the quotient map is continuous.
\end{prop}

The next proposition assures us that the quotient topology, as defined above, makes the quotient vector space into a topological vector space.

\begin{prop} Let $M$ be a vector subspace of a topological vector space~$X$.  The quotient topology on $X/M$ is compatible with
the vector space operations on that space.
\end{prop}

\begin{prop} If $M$ is a vector subspace of a topological vector space~$X$, then the quotient space $X/M$ is a Hausdorff if and only
if $M$ is closed in~$X$.
\end{prop}

\begin{exer} Explain also how the preceding proposition enables us to associate with a non-Hausdorff topological vector space another
space that is Hausdorff.
\end{exer}

\begin{prop} Let $T\colon X \sto Y$ be a linear map between topological vector spaces and $M$ be a subspace of $X$.  If
 \index{quotient!theorem!for $\cat{TVS}$}%
$M \subseteq \ker T$, then there exists a unique function $\wt T\colon X/M \sto Y$ such that $T = \wt T \circ \pi$ (where $\pi$
is the quotient map).  The function $\wt T$ is linear; it is continuous if and only if $T$ is; it is open if and only if $T$~is.
\end{prop}
















\section{Locally Convex Spaces and Seminorms}

\begin{prop} In a topological vector space the closure of a convex set is convex.
\end{prop}

\begin{prop} In a topological vector space every convex neighborhood of $\vc 0$ contains a balanced convex neighborhood of~$\vc 0$.
\end{prop}

\begin{prop} A closed subset $C$ of a topological vector space is convex if and only if $\frac12(x + y)$ belongs to $C$ whenever $x$ and $y$ do.
\end{prop}

\begin{prop} If a subset of a topological vector space is convex so are its closure and its interior.
\end{prop}

\begin{defn} A topological vector space is
 \index{local!convexity}%
 \index{convex!locally}%
\df{locally convex} if it has a local base consisting of convex sets. For brevity we will refer to a locally convex
topological vector space simply as a
 \index{locally!convex space}%
\df{locally convex space}.
\end{defn}

\begin{prop} If $p$ is a seminorm on a vector space, then $p(\vc 0) = 0$.
 \index{seminorm!elementary properties of}%
\end{prop}

\begin{prop} If $p$ is a seminorm on a vector space $V$, then $\abs{p(x) - p(y)} \le p(x - y)$ for all $x$, $y \in V$.
\end{prop}

\begin{cor} If $p$ is a seminorm on a vector space $V$, then $p(x) \ge 0$ for all $x \in V$.
\end{cor}

\begin{prop} If $p$ is a seminorm on a vector space $V$, then $p^{\gets}(\{0\})$ is a vector subspace of~$V$.
\end{prop}

\begin{defn} Let $p$ be a seminorm on a vector space $V$ and $\epsilon > 0$. We will call the set
 \index{B@$B_{p,\epsilon}$, $B_p$ (open semiball determined by~$p$)}%
$B_{p,\epsilon} := p^{\gets}\bigl([0,\epsilon)\bigr)$ the
 \index{open!semiball}%
 \index{semiball}%
\df{open semiball} of radius $\epsilon$ at the origin determined by~$p$. Similarly, the set
\index{C@$C_{p,\epsilon}$, $C_p$ (closed semiball determined by~$p$)}%
$C_{p,\epsilon} := p^{\gets}\bigl([0,\epsilon]\bigr)$ is the
 \index{closed!semiball}%
\df{closed semiball} of radius $\epsilon$ at the origin determined by~$p$.  For the open and closed semiballs of radius one determined
by $p$ use the notations $B_p$ and $C_p$, respectively.
\end{defn}

\begin{prop} Let $\fml P$ be a family of seminorms on a vector space~$V$. The family $\sfml U$ of all finite intersections of open semiballs
at the origin of $V$ induces a (not necessarily Hausdorff) topology on $V$ which is compatible with the linear structure of $V$ and for which
$\sfml U$ is a local base.
\end{prop}

\begin{proof}[\emph{Hint for proof}] Proposition~\ref{prop_fbase_induces_top}.  \ns
\end{proof}

\begin{prop} Let $p$ be a seminorm on a topological vector space~$X$. Then the following are equivalent:
   \begin{enumerate}[label=\rm{(\alph*)}]
     \item $B_p$ is open in~$X$;
     \item $p$ is continuous at the origin; and
     \item $p$ is continuous.
   \end{enumerate}
\end{prop}

\begin{prop} If $p$ and $q$ are continuous seminorms on a topological vector space, then so are $p + q$ and~$\max\{p,q\}$.
\end{prop}

\begin{exam} Let $N$ be an absorbing, balanced, convex subset of a vector space~$V$. Define
   \[ \sbsb \mu N\colon V \sto \R \colon x \mapsto \inf\{\lambda > 0\colon x \in \lambda N\}. \]
The function $\sbsb \mu N$ is a seminorm on~$V$.  It is called the
 \index{Minkowski!functional}%
 \index{functional!Minkowski}%
 \index{mu@$\sbsb \mu N$ (Minkowski functional for~$N$)}%
\df{Minkowski functional} of~$N$.
\end{exam}

\begin{prop} If $N$ is an absorbing balanced convex set in a vector space~$V$ and $\sbsb \mu N$ is the Minkowski functional of $N$, then
     \[ B_{\sbsb \mu{\!N}} \subseteq N \subseteq C_{\sbsb \mu{\!N}} \]
and the Minkowski functionals generated by the three sets $B_{\sbsb \mu{\!N}}$, $N$, and $C_{\sbsb \mu{\!N}}$ are all identical.
\end{prop}

\begin{prop} If $N$ is an absorbing balanced convex set in a vector space~$V$ and $\sbsb \mu N$ is the Minkowski functional of $N$, then
$\sbsb \mu N$ is continuous if and only if $N$ is a neighborhood of zero, in which case
  \[ B_{\sbsb \mu{\!N}} = \intr N \quad \text{and} \quad  C_{\sbsb \mu{\!N}} = \clo N. \]
\end{prop}

\begin{cor} Let $N$ be an absorbing, balanced, convex, closed subset of a topological vector space~$X$.   Then the Minkowski functional
$\sbsb \mu N$ is the unique seminorm on $X$ such that $N = C_{\sbsb \mu{\!N}}$.
\end{cor}

\begin{prop} Let $p$ be a seminorm on a vector space $V$.  Then the set $B_p$ is absorbing, balanced, and convex.  Furthermore, $p = \sbsb \mu{B_p}$.
\end{prop}

\begin{defn} A family $\fml P$ of seminorms on a vector space $V$ is
 \index{separating!family of seminorms}%
\df{separating} if for every $x \in X$ there exists $p \in \fml P$ such that $p(x) \ne 0$.
\end{defn}

\begin{prop} In a topological vector space let $\sfml B$ be a local base consisting of balanced convex open sets. Then
$\{\sbsb \mu{\!N}\colon N \in \sfml B\}$ is a separating family of continuous seminorms on the space such that
$N = B_{\sbsb \mu{\!N}}$ for every $N \in \sfml B$.
\end{prop}

The next proposition demonstrates the way in which many of the most important locally convex spaces arise---from a separating family
of seminorms.

\begin{prop}\label{prop_clbase_from_snorms} Let $\fml P$ be a separating family of seminorms on a vector space~$V$. For each
$p \in \fml P$ and each $n \in \N$ let $U_{p,n} = p^{\gets}\bigl([0,\frac1n)\bigr)$. Then the family of all finite
intersections of the sets $U_{p,n}$ comprises a balanced convex local base for a topology on $V$ under which $V$ becomes
a locally convex space and every seminorm in $\fml P$ is continuous.
\end{prop}

\begin{exam}\label{X_LCS1} Let $X$ be a locally compact Hausdorff space. For every nonempty compact subset $K$ of $X$ define
   \[ \sbsb pK\colon \fml C(X) \sto \K\colon f \mapsto \sup\{\abs{f(x)}\colon x \in K\}. \]
Then the family $\fml P$ of all $\sbsb pK$ where $K$ is a nonempty compact subset of $X$ is a family of seminorms
under which $\fml C(X)$ becomes a locally convex space.
\end{exam}

\begin{prop} A linear map $T\colon V \sto W$ from a topological vector space to a locally convex spaces is continuous if and only if
$p \circ T$ is a continuous seminorm on $V$ for every continuous seminorm $p$ on~$W$.
\end{prop}

\begin{prop} Let $p$ be a continuous seminorm on a subspace $M$ of a locally convex space~$X$.  Then $p$ can be extended to a
continuous linear functional on all of~$X$.
\end{prop}

\begin{proof} See~\cite{Conway:1990}, proposition IV.5.13.   \ns
\end{proof}





















\section{Fr\'echet Spaces}

\begin{defn} A topological space $(X,\sfml T)$ is
 \index{metrizable}%
\df{metrizable} if there exists a metric $d$ on $X \times X$ such that the topology generated by $d$ is~$\sfml T$.
\end{defn}

\begin{defn} A metric $d$ on a vector space $V$ is
 \index{translation!invariant}%
 \index{invariant!under translation}%
\df{translation invariant} if
   \[ d(x,y) = d(x + a, y + a) \]
for all $x$, $y$, and $a$ in~$V$.
\end{defn}

\begin{prop} Let $(X,\sfml T)$ be a Hausdorff topological vector space with a countable local base.  Then there exists a translation invariant
metric $d$ on $V$ which generates the topology $\sfml T$ and whose open balls at the origin are balanced.  If $X$ is a locally convex space
then $d$ may be chosen so that, additionally, its open balls are convex.
\end{prop}

\begin{proof} See \cite{Rudin:1991}, theorem 1.24.  \ns
\end{proof}

\begin{exam}\label{X_metric_from_seminorms} Let $\{p_n\colon n \in \N\}$  be a a countable separating family of seminorms on a vector
space~$X$ and $\sfml T$ be the topology on $X$ guaranteed by proposition~\ref{prop_clbase_from_snorms}.  In this case we can explicitly
define a translation invariant metric on $X$ which induces the topology~$\sfml T$. For an arbitrary sequence $\bigl(\alpha_k\bigr) \in c_0$
a metric with the desired properties is given by
   \[ d(x,y) = \max_{k \in \N}\biggl\{\frac{\alpha_k p_k(x-y)}{1 + p_k(x-y)}\biggr\}. \]
\end{exam}

\begin{proof} See \cite{Rudin:1991}, remark 1.38(c).  \ns
\end{proof}

\begin{defn} A
 \index{Fr\'echet space}%
\df{Fr\'echet space} is a complete metrizable locally convex space.
\end{defn}

\begin{notn} Let $A$ and $B$ be subsets of a topological space.  We write
 \index{<binrell@$A \prec B$ ($\clo A \subseteq \intr B$)}%
$A\prec B$ if $\clo A \subseteq \intr B$.
\end{notn}

\begin{prop} The following hold for subsets $A$, $B$, and $C$ of a topological space:
 \begin{enumerate}
  \item[(a)] if $A \prec B$ and $B \prec C$, then $A \prec C$;
  \item[(b)] if $A \prec C$ and $B \prec C$, then $A \cup B \prec C$; and
  \item[(c)] if $A \prec B$ and $A \prec C$, then $A \prec B \cap C$.
 \end{enumerate}
\end{prop}

Here is a standard result from elementary topology.

\begin{prop}\label{prop_open_lim_cpt} Let $\Omega$ be a nonempty open subset of $\R^n$.  The there exists a sequence
$(K_n)$ of nonempty compact subsets of $\Omega$ such that $K_i \prec K_j$ whenever $i < j$ and
\smash[b]{$\bigcup\limits_{i=1}^\infty K_i = \Omega$}.
\end{prop}

\begin{proof} See \cite{Treves:1967}, lemma 10.1.  \ns
\end{proof}

\begin{exam} Let $\Omega$ be a nonempty open subset of some Euclidean space $\R^n$ and $\fml C(\Omega)$ be the family of all
continuous scalar valued functions on~$\Omega$.  By the preceding proposition we can write $\Omega$ as the union of a countable
collection of nonempty compact sets $K_1$, $K_2$, \dots such that $K_i \prec K_j$ whenever $i < j$.  For each $n \in \N$ and
$f \in \fml C(\Omega)$ let $p_n(f) = \sup\{\abs{f(x)}\colon  x \in K_n\}$ as in example~\ref{X_LCS1}. According to
proposition~\ref{prop_clbase_from_snorms} this family of seminorms makes $\fml C(\Omega)$ into a locally convex space.  Let
$\sfml T$ be the resulting topology on this space.  By example~\ref{X_metric_from_seminorms} the function
   \[ d(f,g) = \max_{k \in \N}\frac{2^{-k}p_k(f-g)}{1 + p_k(f-g)} \]
defines a metric on $\fml C(\Omega)$ which induces the topology~$\sfml T$.  Under this metric $\fml C(\Omega)$ is a Fr\'echet
space.
\end{exam}

\begin{notn}[Multi-index notation]\label{mi_notn} Let $\Omega$ be an open subset of a Euclidean space~$\R^n$.  We will consider
infinitely differentiable
 \index{multi-index notation}%
scalar valued functions on $\Omega$; that is, functions on $\Omega$ which have derivatives of all orders at each point of~$\Omega$.   We
will refer to such functions as
 \index{smooth functions}%
\df{smooth} functions on~$\Omega$.  The class of all smooth functions on $\Omega$ is denoted
 \index{C@$\fml C^\infty(\Omega)$!infinitely differentiable functions}%
by $\fml C^\infty(\Omega)$.

Another important class we will consider
 \index{C@$\fml C^\infty_c(\Omega)$!infinitely differentiable functions with compact support}%
is $\fml C_c^\infty(\Omega)$, the family of all infinitely differentiable scalar valued functions on $\Omega$ which have compact support;
that is, which vanish outside of some compact subset of~$\Omega$.  The functions in this family are frequently called
 \index{test functions}%
\df{test functions} on~$\Omega$.  Another, shorter,
 \index{D@$\fml D(\Omega)$!space of test functions; infinitely differentiable functions with compact support}%
notation in common use for this family is~$\fml D(\Omega)$.

We will be interested in differential operators on the spaces~$\fml D(\Omega)$. Let $\bs\alpha = (\alpha_1, \dots, \alpha_n)$ be a
 \index{multi-index}%
\df{multi-index}, that is, an $n$-tuple of integers where each $\alpha_k \ge 0$, and let
  \[ D^{\bs\alpha} = \biggl(\frac{\partial}{\partial x_1}\biggr)^{\alpha_1} \dots
                                                     \biggl(\frac{\partial}{\partial x_n}\biggr)^{\alpha_n}. \]
The
 \index{D@$D^{\bs\alpha}$ (multi-index notation for differentiation)}%
 \index{order!of a multi-index}%
 \index{differential operator!acting on smooth functions}%
\df{order} of the differential operator $D^{\bs\alpha}$ is $\abs{\bs\alpha} := \sum_{k=1}^n \alpha_k$.  (When
$\abs{\bs\alpha} = 0$ we understand $D^{\bs\alpha}(f) = f$ for each function~$f$.)  An alternative notation for
 \index{<unopura@$f^{\bs\alpha}$ (multi-index notation for differentiation)}%
$D^{\bs\alpha}f$ is~$f^{(\bs\alpha)}$.
\end{notn}

\begin{exam}\label{X_smooth_fcns_Frechet} On the family $\fml C^\infty(\Omega)$ of smooth functions on a open subset $\Omega$ of a
Euclidean space~$\R^n$ define a family $\{\sbsb{p}{K,j}\}$ of seminorms.  For each compact subset $K$ of $\Omega$ and each
$j \in \N$ let
   \[ \sbsb{p}{K,j}(f) = \sup\{\abs{D^{\bs\alpha}f(x)}\colon x \in K \text{ and } \abs{\bs\alpha} \le j\}. \]
This family of seminorms makes $\fml C^\infty(\Omega)$ into a Fr\'echet space.  The resulting topology on $\fml C^\infty(\Omega)$
is sometimes called the
 \index{topology!of uniform convergence on compacta}%
 \index{uniform!convergence!on compacta}%
 \index{compacta!topology of uniform convergence on}%
 \index{convergence!uniform!on compacta}%
\emph{topology of uniform convergence on compacta of the functions and all their derivatives}.
\end{exam}

\begin{proof}[\emph{Hint for proof}] To see that the space is metrizable use example~\ref{X_metric_from_seminorms} and
proposition~\ref{prop_open_lim_cpt}.  \ns
\end{proof}

\begin{exam}\label{Schwartz_space} Let $n \in N$.  Denote by $\fml S = \fml S(\R^n)$
 \index{S@$\fml S = \fml S(\R^n)$ (Schwartz space)!as a Fr\'echet space}%
the space of all smooth functions $f$ defined on all of~$\R^n$ which satisfy
   \[ \lim_{\norm x \sto \infty}{\norm x}^k \abs{D^{\bs\alpha}f(x)} = 0 \]
for all multi-indices $\bs\alpha = (\alpha_1.\dots,\alpha_n) \in \N^n$ and all integers $k \ge 0$.  Such functions are frequently referred
to as
 \index{rapidly decreasing functions}%
 \index{function!rapidly decreasing}%
 \index{derivatives!functions with rapidly decreasing}%
 \index{space!of rapidly decreasing functions}%
\emph{functions with rapidly decreasing derivatives} or even more simply as \emph{rapidly decreasing functions}. On $\fml S$ we place the
topology defined by the seminorms
   \[ \sbsb p{m,k} := \sup_{\abs{\bs\alpha} \le m}\bigl\{\sup_{x \in \R^n} \{(1 + {\norm x}^2)^m \abs{(D^{\bs\alpha}f)(x)}\}\bigr\} \]
for $k$, $m = 0,1,2,\dots$.  The resulting space is a Fr\'echet space called the
 \index{Schwartz space}%
 \index{space!Schwartz}%
\df{Schwartz space} of~$\R^n$.
\end{exam}

\begin{proof}[\emph{Hint for proof}] For a proof of completeness see~\cite{Treves:1967}, Example IV, pages 92--94. \ns
\end{proof}

\begin{prop}  A test function $\psi$ on $\R$ is the derivative of another test function if and only if $\int_{-\infty}^\infty\psi = 0$.
\end{prop}

\begin{prop} Let $F$ be a Fr\'echet space whose topology is induced by a translation invariant metric. Then every bounded subset
of $F$ a has finite diameter.
\end{prop}

\begin{exam} Consider the Fr\'echet space $\R$ with the topology induced by the translation invariant metric
\smash[b]{$d(x,y) = \dfrac{\abs{x - y}}{1 + \abs{x - y}}$}.  Then the set $\N$ of natural numbers has finite diameter but is not bounded.
\end{exam}




































\endinput
\hphantom{MMMM}


\vskip 1.5 in



\begin{quote}\noindent  \emph{It is not essential for the value of an education that every idea be understood
at the time of its accession.  Any person with a genuine intellectual interest and a wealth of
intellectual content acquires much that he only gradually comes to understand fully in the
light of its correlation with other related ideas. \dots Scholarship is a progressive process,
and it is the art of so connecting and recombining individual items of learning by the force
of one's whole character and experience that nothing is left in isolation, and each idea
becomes a commentary on many others.}
\end{quote}

\vskip .2 true in

\hskip 3 true in     - NORBERT WIENER

% May: Elements of Modern Mathematics, p. xiv

\endinput
